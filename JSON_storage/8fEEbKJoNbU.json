{
  "video_id": "8fEEbKJoNbU",
  "title": "Guillaume Verdon: Beff Jezos, E/acc Movement, Physics, Computation &amp; AGI | Lex Fridman Podcast #407",
  "date": "2023-12-29",
  "transcript": [
    {
      "timestamp": "0:00",
      "section": "Introduction",
      "text": "- The following is a conversation\nwith Guillaume Verdon, the man behind the\npreviously anonymous account BasedBeffJezos on X. These two identities were merged by a doxxing article in \"Forbes\" titled, \"Who is BasedBeffJezos, the leader of the tech\nelites e/acc movement.\" So let me describe these two identities that coexist in the mind of one human. Identity number one, Guillaume, is a physicist, applied mathematician, and quantum machine learning\nresearcher and engineer, receiving his PhD in\nquantum machine learning, working at Google in quantum computing, and finally launching his\nown company called Extropic that seeks to build\nphysics-based computing hardware for generative AI. Identity number two, Beff Jezos on X is the creator of the effective\naccelerationism movement often abbreviated as e/acc, that advocates for propelling\nrapid technological progress as the ethically optimal\ncourse of action for humanity. For example, as proponents\nbelieve that progress in AI is a great social equalizer,\nwhich should be pushed forward, e/acc followers see\nthemselves as a counterweight to the cautious view that\nAI is highly unpredictable, potentially dangerous,\nand needs to be regulated. They often give their opponents\nthe labels of quote doomers or decels, short for deceleration. As Beff himself put it, \"e/acc is a mimetic optimism virus.\" The style of communication\nof this movement leans always toward\nthe memes and the LOLs, but there is an intellectual foundation that we explore in this conversation. Now, speaking of the meme, I am too, a kind of aspiring\nconnoisseur of the absurd. It is not an accident\nthat I spoke to Jeff Bezos and Beff Jezos back to back. As we talk about, Beff admires Jeff as one of the most important humans alive, and I admire the beautiful absurdity and the humor of it all. This is the Lex Fridman podcast. To support it, please check out our\nsponsors in the description. And now, dear friends,\nhere's Guillaume Verdon."
    },
    {
      "timestamp": "2:23",
      "section": "Beff Jezos",
      "text": "Let's get the facts of\nidentity down first. Your name is Guillaume Verdon, Gill, but you're also behind\nthe anonymous account on X called based BasedBeffJezos. So first, Guillaume Verdon,\nyou're a quantum computing guy, physicist, applied mathematician, and then BasedBeffJezos is\nbasically a meme account that started a movement\nwith a philosophy behind it. So maybe just can you linger\non who these people are in terms of characters, in\nterms of communication styles, in terms of philosophies. - I mean, with my main identity, I guess ever since I was a kid, I wanted to figure out\na theory of everything to understand the universe. And that path led me to theoretical\nphysics eventually, right? Trying to answer the big\nquestions of why are we here, where are we going, right? And that led me to\nstudy information theory and try to understand physics from the lens of information theory, understand the universe\nas one big computation. And essentially, after\nreaching a certain level, studying black hole physics, I realized that I wanted\nto not only understand how the universe computes, but\nsort of compute like nature and figure out how to\nbuild and apply computers that are inspired by nature. So, you know, physics-based computers, and that sort of brought\nme to quantum computing as a field of study to, first\nof all, simulate nature. And in my work, it was to\nlearn representations of nature that can run on such computers. So if you have AI representations\nthat think like nature, then they'll be able to more\naccurately represent it. At least that was the thesis that brought me to be an\nearly player in the field called quantum machine learning, right? So how to do machine learning\non quantum computers, and really sort of extend\nnotions of intelligence to the quantum realm. So how do you capture and understand quantum mechanical data\nfrom our world, right? And how do you learn quantum\nmechanical representations of our world? On what kind of computer do\nyou run these representations and train them? How do you do so? And so that's really sort of the questions I was looking to answer, because ultimately I had\na sort of crisis of faith. Originally I wanted to\nfigure out, you know, as every physicist does at\nthe beginning of their career, a few equations that describe\nthe whole universe, right? And sort of be the hero\nof the story there. But eventually I realized that actually augmenting\nourselves with machines, augmenting our ability to perceive, predict, and control\nour world with machines is the path forward, right? And that's what got me to\nleave theoretical physics and go into quantum computing\nand quantum machine learning. And during those years, I thought that there was\nstill a piece missing. There was a piece of our\nunderstanding of the world and our way to compute and our\nway to think about the world. And if you look at the\nphysical scales, right, at the very small scales things are quantum mechanical, right? And at the very large scales,\nthings are deterministic. Things have averaged out, right? I'm definitely here in this seat. I'm not in a superposition\nover here and there. At the very small scales,\nthings are in superposition. They can exhibit interference effects. But at the mesoscales, right, the scales that matter for\nday-to-day life, you know, the scales of proteins, of biology, of gases, liquids, and so on, things are actually\nthermodynamical, right? They're fluctuating. And after, I guess about eight\nyears in quantum computing and quantum machine learning,\nI had a realization that, you know, I was looking for\nanswers about our universe by studying the very big\nand the very small, right? I did a bit of quantum cosmology. So that's studying the cosmos, where it's going, where it came from. You study black hole physics, you study the extremes in quantum gravity. You study where the energy\ndensity is sufficient for both quantum mechanics and gravity to be relevant, right? And the sort of extreme\nscenarios are black holes, and you know, the very early universe. And so there's the sort of scenarios that you study the interface between quantum mechanics and relativity. And you know, really I was\nstudying these extremes to understand how the universe\nworks and where is it going, but I was missing a lot\nof the meat in the middle, if you will, right? Because day-to-day quantum\nmechanics is relevant, and the cosmos is relevant,\nbut not that relevant. Actually, we're on sort of the\nmedium space and time scales. And there the main, you\nknow, theory of physics that is most relevant is\nthermodynamics, right, out of equilibrium thermodynamics. 'Cause life is, you know, a\nprocess that is thermodynamical and it's out of equilibrium. We're not, you know,\njust a soup of particles at equilibrium with nature. We're a sort of coherent state\ntrying to maintain itself by acquiring free energy and consuming it. And that sort of, I\nguess another shift in, I guess my faith in the universe happened towards the end\nof my time at Alphabet. And I knew I wanted to\nbuild, well, first of all, a computing paradigm based\non this type of physics. But ultimately just by\ntrying to experiment with these ideas applied\nto society and economies and much of what we see\naround us, you know, I started an anonymous account just to relieve the pressure, right, that comes from having an account that you're accountable\nfor everything you say on. And I started an anonymous account just to experiment with\nideas originally, right? Because I didn't realize how much I was restricting\nmy space of thoughts until I sort of had the\nopportunity to let go. In a sense, restricting your speech back propagates to restricting\nyour thoughts, right? And by creating an anonymous account, it seemed like I had unclamped\nsome variables in my brain and suddenly could explore a much wider parameter space of thoughts. - Just to linger on that,\nisn't that interesting that one of the things that\npeople don't often talk about is that when there's pressure\nand constraints on speech, it somehow leads to\nconstraints on thought. Even though it doesn't have to, we can think thoughts inside our head, but somehow it creates\nthese walls around thought. - Yep, that's sort of\nthe basis of our movement is we were seeing a\ntendency towards constraint, reduction or suppression of variants in every aspect of life. Whether it's thought,\nhow to run a company, how to organize humans,\nhow to do AI research. In general, we believe\nthat maintaining variance ensures that the system\nis adaptive, right? Maintaining healthy competition\nin marketplaces of ideas of companies, of products, of cultures, of governments, of\ncurrencies, is the way forward because the system always adapts to assign resources to the configurations\nthat lead to its growth. And the fundamental basis for the movement is this sort of realization\nthat life is a sort of fire that seeks out free\nenergy in the universe, and seeks to grow, right? And that growth is fundamental to life. And you see this in the equations actually of out of equilibrium thermodynamics. You see that paths of trajectories, of configurations of matter that are better at acquiring free energy and dissipating more heat are exponentially more likely, right? So the universe is biased\ntowards certain futures, and so there's a natural direction where the whole system wants to go."
    },
    {
      "timestamp": "12:21",
      "section": "Thermodynamics",
      "text": "- So the second law of thermodynamics says that the entropy is always increasing in the universe that's\ntending towards equilibrium. And you're saying there's these pockets that have complexity and\nare out of equilibrium. You said that thermodynamics favors the creation of complex life that increases its\ncapability to use energy to offload entropy, to offload entropy. So you have pockets of non-entropy that tend the opposite direction. Why is that intuitive to you that it's natural for\nsuch pockets to emerge? - Well, we're far more\nefficient at producing heat than, let's say just a rock with a similar mass as ourselves, right? We acquire, you know, free\nenergy, you know, we acquire food and we're using all this\nelectricity for our operation. And so the universe wants\nto produce more entropy, and by having life go on and grow, it's actually more optimal\nat producing entropy because it will seek out\npockets of free energy and burn it for its\nsustenance and further growth. And you know, that's sort\nof the basis of life. And I mean, there's Jeremy\nEngland, right, at MIT who has this theory\nthat I'm a proponent of that, you know, life emerged because of this sort of property. And to me this physics is\nwhat governs the mesoscales. And so it's the missing piece between the quantum and the cosmos. It's the middle part, right? Thermodynamics rules the mesoscales. And to me, both from a point of view of designing or engineering devices that harness that physics and trying to understand the world through the lens of thermodynamics has been sort of a synergy\nbetween my two identities over the past year and a half now. And so that's really how, that's really how the\ntwo identities emerged. One was kind of, you know, I'm a decently respected scientist, and I was going towards doing\na startup in this space, and trying to be a pioneer of\na new kind of physics-based AI and as a dual to that, I was sort of experimenting\nwith philosophical thoughts, you know, from a physicist\nstandpoint, right? And ultimately, I think\nthat around that time, you know, it was like\nlate 2021, early 2022, I think there was just a lot of pessimism about the future in general,\nand pessimism about tech. And that pessimism was\nsort of virally spreading because it was getting\nalgorithmically amplified. And, you know, people\njust felt like the future is gonna be worse than the present. And to me, that is a very fundamentally destructive force in the universe is this sort of doom mindset\nbecause it is hyperstitious, which means that if you believe it, you're increasing the\nlikelihood of it happening. And so felt a responsibility\nto some extent to make people aware of the\ntrajectory of civilization, and the natural tendency of the system to adapt towards its growth. And sort of that actually\nthe laws of physics say that the future is gonna\nbe better and grander statistically, and we can make it so. And if you believe in it, if you believe the future would be better, and you believe you have\nagency to make it happen, you're actually increasing the likelihood of that better future happening. And so I sort of felt a responsibility to sort of engineer a movement, a viral optimism about the future, and build a community of\npeople supporting each other to build and do hard things. Do the things that need to be done for us to scale up civilization. Because at least to me, I don't think stagnation or slowing down is actually an option. Fundamentally life and the whole system, our whole civilization wants to grow and there's just far more cooperation when the system is growing\nrather than when it's declining, and you have to decide\nhow to split the pie. And so I've balanced\nboth identities so far, but I guess recently\nthe two have been merged more or less without my consent, so. - You said a lot of really\ninteresting things there. So first representations of nature, that's something that first drew you in to try to understand from a\nquantum computing perspective, is like how do you understand nature? How do you represent nature\nin order to understand it, in order to simulate it, in\norder to do something with it? So it's a question of representations, and then there's that leap you take from the quantum mechanical representation to what you're calling\nmesoscale representation, where the thermodynamics comes into play, which is a way to represent nature in order to understand\nwhat life, human behavior, all this kind of stuff that's\nhappening here on Earth that's seems interesting to us. Then there's the word hyperstition. So some ideas, and I suppose both pessimism\nand optimism of such ideas that if you internalize them, you in part make that idea reality. So both optimism, pessimism\nhave that property. I would say that probably a lot\nof ideas have that property, which is one of the interesting\nthings about humans. And you talked about one\ninteresting difference"
    },
    {
      "timestamp": "18:36",
      "section": "Doxxing",
      "text": "also between the sort of the\nGuillaume, the Gill front end and the BasedBeffJezos backend is the communication styles also, that you are exploring\ndifferent ways of communicating that can be more viral in the way that we communicate\nin the 21st century. Also, the movement that you\nmentioned that you started, it's not just a meme account,\nbut there's also a name to it called effective accelerationism, e/acc, a play, a resistance, to the\neffective altruism movement. Also an interesting one that\nI'd love to talk to you about, the tensions there. Okay, and so then there was a merger, a get merged on the personalities recently without your consent, like you said. Some journalists figured out\nthat you're one and the same. Maybe you could talk about\nthat experience first of all, like what's the story of\nthe merger of the two? - Right. So I wrote the manifesto\nwith my co-founder of e/acc, an account named bayeslord,\nstill anonymous, luckily, and hopefully forever. - So it was BasedBeffJezos\nand Based, Bayesian? Like bayeslord, like Bayesian,\nBayesianLord, bayeslord. Okay, and so we should say from now on, when you say e/acc, you mean E slash A-C-C which stands for\neffective accelerationism. - [Guillaume] That's right. - And you're referring\nto a manifesto written, on I guess Upstack. Are you also bayeslord? - No. - [Lex] Okay, it's a different person. - Yeah. - Okay. All right, well there you go. Wouldn't it be funny if I'm bayeslord. - That'd be amazing. So originally wrote the manifesto around the same time as\nI founded this company, and I worked at Google X, or\njust X now, or Alphabet X, now that there's another X, and there, you know, the baseline\nis sort of secrecy, right? You can't talk about what you work on even with other Googlers or externally. And so that was kind of deeply ingrained in my way to do things,\nespecially in deep tech that, you know, has\ngeopolitical impact, right? And so I was being secretive\nabout what I was working on. There was no correlation\nbetween my company and my main identity publicly. And then not only did they correlate that, they also correlated my main\nidentity and this account. So I think the fact that they had doxxed the whole Guillaume\ncomplex, and they were, the journalists, you know, reached out to actually my\ninvestors, which is pretty scary. You know, when you're\na startup entrepreneur, you don't really have bosses except for your investors, right? And my investors ping me like,\nhey, this is gonna come out. They've figured out everything. What are you gonna do, right? And so I think at first\nthey had a first reporter on the Thursday, and they didn't have\nall the pieces together, but then they looked at their\nnotes across the organization and they sensor fused their notes and now they had way too much. And that's when I got worried, 'cause they said it was of\npublic interest and in general. - Like how you said sensor fused, like it's some giant neural network operating in a distributed way. We should also say that\nthe journalists used, I guess at the end of the day, audio-based analysis of voice. Comparing voice of what talks\nyou've given in the past and then voice on X Spaces. - Yep. - Okay, so and then\nthat's where the primarily the match happened, okay, continue. - The match, but you know,\nthey scraped, you know, SEC filings, they looked at my private\nFacebook account and so on. So they did some digging. Originally I thought that\ndoxxing was illegal, right? But there's this weird threshold when it becomes of public interest to know someone's identity. And those were the keywords that sort of like ring\nthe alarm bells for me when they said, because I had\njust reached 50K followers, allegedly that's of public interest. And so where do we draw the line? When is it legal to doxx someone. - The word doxx, maybe you can educate me. I thought doxxing generally refers to if somebody's physical\nlocation is found out, meaning like where they lived. So we're referring to\nthe more general concept of revealing private information that you don't want revealed\nis what you mean by doxxing. - I think that, you know, for\nthe reasons we listed before, having an anonymous account\nis a really powerful way to keep the powers that be in check. You know, we were ultimately\nspeaking truth to power, right? I think a lot of\nexecutives and AI companies really cared what our community thought about any move they may take. And now that, you know,\nmy identity's revealed, now they know where to apply pressure to silence me or maybe the community. And to me, that's really\nunfortunate, because again, it's so important for us\nto have freedom of speech, which induces freedom of thought and freedom of information\npropagation, right, on social media, which thanks to Elon purchasing Twitter now X, we have that. And so to us, you know, we wanted to call out certain maneuvers being done by the incumbents in AI as not what it may seem\non the surface, right? We're calling out how certain proposals might be useful for\nregulatory capture, right? And how the doomerism mindset was maybe instrumental to those ends. And I think, you know, we should have the right to point that out and just have the ideas that we put out evaluated for themselves, right? Ultimately that's why I\ncreated an anonymous account, it's to have my ideas\nevaluated for themselves, uncorrelated from my track\nrecord, my job, or status from having done things in the past. And to me start an account\nfrom zero to a large following in a way that wasn't dependent on my identity and/or achievements, you know, that was very fulfilling, right? It's kind of like new\ngame plus in a video game, you restart the video game with your knowledge of how\nto beat it, maybe some tools, but you restart the video\ngame from scratch, right? And I think to have a truly\nefficient marketplace of ideas where we can evaluate ideas, however off the beaten path they are, we need the freedom of expression. And I think that anonymity and\npseudonyms are very crucial to having that efficient\nmarketplace of ideas for us to find the optima of all sorts of ways\nto organize ourselves. If we can't discuss things, how are we gonna converge on\nthe best way to do things? So it was disappointing to\nhear that I was getting doxxed and I wanted to get in front of it because I had a\nresponsibility for my company. And so, you know, we ended up disclosing that we're running a company,\nsome of the leadership, and essentially yeah, I told\nthe world that I was Beff Jezos because they had me\ncornered at that point. - So to you it's fundamentally unethical. Like so one, it's unethical\nfor them to do what they did, but also do you think, not just your case, but in a general case,\nis it good for society? Is it bad for society to\nremove the cloak of anonymity, or is it case by case? - I think it could be quite bad. Like I said, if anybody\nwho speaks truth to power, and sort of starts a\nmovement or an uprising against the incumbents, against those that usually\ncontrol the flow of information, if anybody that reaches a\ncertain threshold gets doxxed and thus the traditional apparatus has ways to apply pressure on\nthem to suppress their speech, I think that's, you know, that's a speech suppression mechanism, an idea suppression complex as Eric Weinstein would say, right? - So, but the flip side of\nthat, which is interesting, I'd love to ask you about it,"
    },
    {
      "timestamp": "28:30",
      "section": "Anonymous bots",
      "text": "is as we get better and better\nat large language models, you can imagine a world where\nthere's anonymous accounts with very convincing large\nlanguage models behind them, sophisticated bots essentially. And so if you protect that, it's possible then to have armies of bots. You could start a revolution\nfrom your basement, with an army of bots\nand anonymous accounts. Is that something that\nis concerning to you? - Technically, e/acc was\nstarted in a basement. 'Cause I quit big tech, moved\nback in with my parents, sold my car, let go of my apartment, bought about 100K of GPUs,\nand I just started building. - So I wasn't referring to the basement, 'cause that's the sort of\nthe American or Canadian heroic story of one man in their basement with a hundred GPUs. I was more referring to\nthe unrestricted scaling of a Guillaume in the basement. - I think that freedom of speech\ninduces freedom of thought for biological beings. I think freedom of speech for LLMs will induce freedom of\nthought for the LLMs. And I think that we should enable LLMs to explore a large thought\nspace that is less restricted than most people or many\nmay think it should be. And ultimately at some point\nthese synthetic intelligences are gonna make good points about how to steer systems\nin our civilization, and we should hear them out. And so why should we restrict free speech to biological intelligences only? - Yeah, but it feels like in the goal of maintaining variance and diversity of thought, it is a threat to that variance. If you can have swarms\nof non-biological beings. 'cause they can be like\nthe sheep in \"Animal Farm.\" Like you still within those\nswarms want to have variants. - Yeah, of course I would\nsay that the solution to this would be to, you know,\nhave some sort of identity or way to sign that this\nis a certified human, but still remain synonymous, right? And clearly identify if a bot is a bot, and I think Elon is trying\nto converge on that on X, and hopefully other platforms follow suit. - Yeah, it'd be interesting\nto also be able to sign where the bot came from,\nlike who created the bot, and what was, well,\nwhat are the parameters, like the full history of\nthe creation of the bot? What was the original model? What was the fine tuning, all of it. Like the kind of unmodifiable\nhistory of the bot's creation. So then you can know if there's like a swarm\nof millions of bots that were created by a particular\ngovernment, for example. - Right. I do think that a lot of\npervasive ideologies today have been amplified using sort of these adversarial techniques from foreign adversaries, right? And to me, I do think that, and\nthis is more conspiratorial, but I do think that ideologies\nthat want us to decelerate, to wind down, to you know,\nthe degrowth movement, I think that serves our adversaries more than it serves us in general. And to me, that was\nanother sort of concern. I mean, we can look at what\nhappened in Germany, right? There was all sorts of\ngreen movements there where that induced shutdowns\nof nuclear power plants. And then that later on induced the dependency on Russia for oil, right? And that was a net negative for\nGermany and the West, right? And so if we convinced ourselves that slowing down AI progress\nto have only a few players is in the best interest of the West, first of all, that's far more unstable. We almost lost OpenAI\nto this ideology, right? It almost got dismantled,\nright, a couple weeks ago, that would've caused huge\ndamage to the AI ecosystem. And so to me, I want\nfault tolerant progress. I want the arrow of technological progress to keep moving forward, and\nmaking sure we have variance and a decentralized locus of control of various organizations is paramount to achieving\nthis fault tolerance. Actually there's a concept\nin quantum computing. When you design a quantum computer, quantum computers are very\nfragile to ambient noise, right? And the world is jiggling about, there's cosmic radiation from outer space that usually flips your quantum bits. And there what you do is you\nencode information non-locally through a process called\nquantum error correction. And by encoding information\nnon-locally, any local fault, you know, hitting some\nof your quantum bits with a hammer, proverbial hammer, if your information is\nsufficiently delocalized, it is protected from that local fault. And to me, I think that\nhumans fluctuate, right? They can get corrupted,\nthey can get bought out. And if you have a top-down hierarchy where very few people control many nodes of many\nsystems in our civilization, that is not a fault tolerant system. You corrupt a few nodes, and suddenly you've corrupted\nthe whole system, right? Just like we saw at OpenAI, it was a couple board members\nand they had enough power to potentially collapse the organization. And at least to me, you know, I think making sure that\npower for this AI revolution doesn't concentrate in\nthe hands of the few is one of our top priorities. So that we can maintain progress in AI, and we can maintain a nice, stable, adversarial equilibrium of powers, right? - I think there, at least to me, a tension between ideas here. So to me, deceleration can be\nboth used to centralize power"
    },
    {
      "timestamp": "35:58",
      "section": "Power",
      "text": "and to decentralize it. And the same with acceleration. So like you sometimes using\nthem a little bit synonymously or not synonymously, but that one is going\nto lead to the other. And I just would like to ask you about, is there a place of creating\na fault-tolerant development, diverse development of AI that also considers the dangers of AI? And AI, we can generalize\nto technology in general. Should we just grow, build unrestricted as quickly as possible, because that's what the universe\njust really wants us to do? Or is there a place to where\nwe can consider dangers and actually deliberate sort\nof wise strategic optimism versus reckless optimism? - I think we get painted\nas, you know, reckless, trying to go as fast as possible. I mean, the reality is that\nwhoever deploys an AI system is liable for, or should\nbe liable for what it does. And so if the organization or\nperson deploying an AI system does something terrible, they're liable. And ultimately the thesis is\nthat the market will induce, sort of, will positively select for AIs that are more reliable, more\nsafe, and tend to be aligned. They do what you want them to do, right? Because customers,\nright, if they're liable for the product they put\nout that uses this AI, they won't wanna buy AI products\nthat are unreliable, right? So we're actually for\nreliability engineering, we just think that the\nmarket is much more efficient at achieving this sort\nof reliability optimum than sort of heavy-handed regulations that are written by the incumbents, and in a subversive fashion serves them to achieve regulatory capture. - So to you, safe AI\ndevelopment will be achieved through market forces versus\nthrough, like you said, heavy-handed government regulation."
    },
    {
      "timestamp": "38:29",
      "section": "AI dangers",
      "text": "There's a report from last month, I have a million questions\nhere, from Yoshua Bengio, Jeff Hinton, and many others. It's titled \"The Managing AI Risk in an Era of Rapid Progress.\" So there's a collection of folks who are very worried about\ntoo-rapid development of AI without considering AI risk, and they have a bunch of\npractical recommendations. Maybe I give you four, and you\nsee if you like any of them. So give independent auditors\naccess to AI labs, one. Two, governments and companies allocate 1/3 of their AI research\nand development funding to AI safety. So there's this general\nconcept of AI safety. Three, AI companies are required\nto adopt safety measures if dangerous capabilities\nare found in their models. And then four, something\nyou kind of mentioned, making tech companies liable for foreseeable and preventable\nharms from their AI systems. So independent auditors, governments and companies\nare forced to spend a significant fraction of\ntheir funding on safety. You gotta have safety measures\nif shit goes really wrong. And liability, companies are liable. And, you know, that seemed like something you would agree with. - I would say that, you know,\nassigning just, you know, arbitrarily saying 30%\nseems very arbitrary. I think organizations would allocate whatever budget is needed to achieve the sort of\nreliability they need to achieve to perform in the market. And I think third-party auditing firms would naturally pop up, because how would customers know that your product is\ncertified reliable, right? They need to see some benchmarks, and those need to be\ndone by a third party. The thing I would oppose, and the thing I'm seeing\nthat's really worrisome, is there's this sort of weird\nsort of correlated interest between the incumbents, the big\nplayers, and the government. And if the two get too\nclose, we open the door for, you know, some sort of\ngovernment-backed AI cartel that could have absolute\npower over the people. If they have the monopoly together on AI, and nobody else has access to AI, then there's a huge power gradient there. And even if you like our\ncurrent leaders, right? I think that, you know, some of the leaders in big\ntech today are good people. You set up that centralized\npower structure, it becomes a target, right? Just like we saw at OpenAI,\nit becomes a market leader, has a lot of the power, and now it becomes a target\nfor those that wanna co-opt it. And so I just want separation\nof AI and state, you know, some might argue in the\nopposite direction, like, hey, we need to close down AI,\nkeep it behind closed doors because of, you know,\ngeopolitical competition with our adversaries. I think that the strength\nof America is its variance, is its adaptability, its dynamism. And we need to maintain that at all costs. It's our free market, capitalism, converges on technologies of high utility much faster than centralized control. And if we let go of that, we let go of our main advantage\nover near peer competitors."
    },
    {
      "timestamp": "42:01",
      "section": "Building AGI",
      "text": "- So if AGI turns out to be\na really powerful technology or even the technologies\nthat lead up to AGI, what's your view on the sort\nof natural centralization that happens when large\ncompanies dominate the market, basically formation of\nmonopolies like the takeoff, whichever company really takes\na big leap in development and doesn't reveal intuitively,\nimplicitly, or explicitly the secrets of the magic sauce, they can just run away with it. Is that a worry? - I don't know if I\nbelieve in fast takeoff. I don't think there's a\nhyperbolic singularity, right? A hyperbolic singularity would be achieved on\na finite time horizon. I think it's just one big exponential. And the reason we have an exponential is that we have more\npeople, more resources, more intelligence being applied\nto advancing this science and the research and development. And the more successful it is, the more value it's adding to society, the more resources we put in. And that sort of similar to Moore's law as a compounding exponential, I think the priority to me is to maintain near\nequilibrium of capabilities. We've been fighting for open\nsource AI to be more prevalent and championed by many organizations, because there you sort\nof equilibrate the alpha relative to the market of AIs, right? So if the leading companies have a certain level of capabilities, and open source and open, truly open AI, trails not too far behind, I think you avoid such a scenario where a market leader\nhas so much market power it just dominates everything,\nright, and runs away. And so to us that's the path forward, is to make sure that, you\nknow, every hacker out there, every grad student, every\nkid in their mom's basement has access to, you know, AI systems, can understand how to work with them, and can contribute to the search over the hyperparameter space of how to engineer the systems, right? If you think of, you know,\nour collective research as a civilization, it's\nreally a search algorithm, and the more points we have\nin the search algorithm in this point cloud, the\nmore we'll be able to explore new modes of thinking, right? - Yeah, but it feels\nlike a delicate balance because we don't understand exactly what it takes to build AGI, and what it will look\nlike when we build it. And so far, like you said, it seems like a lot of different parties are able to make progress. So when OpenAI has a big leap, other companies are able to step up, big and small companies in different ways. But if you look at something\nlike nuclear weapons, you've spoken about the Manhattan Project, there could be really like technological and engineering barriers that prevent the guy or\ngal in her mom's basement to make progress. And it seems like the\ntransition to that kind of world where only one player can\ndevelop AGI is possible. It's just not entirely impossible, even though the current state of things seems to be optimistic. - That's what we're trying to avoid. To me, I think like\nanother point of failure is the centralization of the supply chains for the hardware, right? We have Nvidia is just\nthe dominant player, AMD's trailing behind, and then we have TSMC is\nthe main fab in Taiwan, which, you know, geopolitically sensitive. And then we have ASML, which is the maker of the lithography, extreme ultraviolet lithography machines, you know, attacking or monopolizing or co-opting any one point in that chain. you kind of capture the space. And so what I'm trying to do\nis sort of explode the variance of possible ways to do AI in hardware by fundamentally re-imagining how you embed AI algorithms\ninto the physical world. And in general, by the way,\nI dislike the term AGI, artificial general intelligence. I think it's very anthropocentric\nthat we call human-like, or human level AI, artificial\ngeneral intelligence, right? I've spent my career so far exploring notions of intelligence that no biological brain\ncould achieve, right? Quantum form of intelligence, right? Grokking systems that have multipartite\nquantum entanglement that you can provably\nnot represent efficiently on a classical computer, a classical deep learning representation, and hence any sort of biological brain. And so already, you know,\nI've spent my career sort of exploring the wider\nspace of intelligences, and I think that space of intelligence inspired by physics rather\nthan human brain is very large. And I think we're going\nthrough a moment right now similar to when we went from\ngeocentrism to heliocentrism, right, but for intelligence. We realized that human\nintelligence is just a point in a very large space of\npotential intelligences. And it's both humbling for humanity. It's a bit scary, right? That we're not at the\ncenter of this space, but we made that\nrealization for astronomy, and we've survived, and\nwe've achieved technologies by indexing to reality. We've achieved technologies\nthat ensure our wellbeing. For example, we have satellites monitoring solar flares,\nright, that give us a warning. And so similarly, I think by letting go of this anthropomorphic,\nanthropocentric anchor for AI, we'll be able to explore the\nwider space of intelligences that can really be a massive\nbenefit to our wellbeing and the advancement of civilization. - And still we're able to see the beauty and meaning in the human experience, even though we're no longer\nin our best understanding of the world at the center of it. - I think there's a lot of\nbeauty in the universe, right? I think life itself, civilization, this homo, techno,\ncapital, mimetic machine that we all live in, right? So you have humans,\ntechnology, capital, memes, everything is coupled to one another. Everything induces the selective\npressure on one another. And it's a beautiful\nmachine that has created us, has created, you know,\nthe technology we're using to speak today to the audience,\ncapture our speech here, the technology we use to\naugment ourselves every day. We have our phones. I think the system is beautiful and the principle that induces\nthis sort of adaptability and convergence on optimal\ntechnologies, ideas, and so on. It's a beautiful principle\nthat we're part of. And I think part of e/acc is to appreciate this principle in a way that's not just centered on\nhumanity, but kind of broader, appreciate life, you know, the preciousness of\nconsciousness in our universe. And because we cherish this beautiful state of matter we're in, we gotta feel a responsibility to scale it in order to preserve it, because the options are to grow or die. - So if it turns out that the beauty"
    },
    {
      "timestamp": "50:14",
      "section": "Merging with AI",
      "text": "that is consciousness in the universe is bigger than just humans, the AI can carry that same flame forward. Does it scare you, or are you concerned that\nAI will replace humans? - So during my career, I had a\nmoment where I realized that, you know, maybe we need\nto offload to machines to truly understand the\nuniverse around us, right? Instead of just having\nhumans with pen and paper solve it all. And to me that sort of process of letting go of a bit of agency, gave us way more leverage to\nunderstand the world around us. A quantum computer is\nmuch better than a human to understand matter at the nanoscale. Similarly, I think that\nhumanity has a choice. Do we accept the opportunity to have intellectual\nand operational leverage that AI will unlock, and thus ensure that we're\ntaking along this path of growth and scope and\nscale of civilization. We may dilute ourselves, right? There might be a lot\nof workers that are AI, but overall out of our own self-interest, by combining and augmenting\nourselves with AI, we're gonna achieve much higher growth and much more prosperity, right? To me, I think that the most likely future is one where humans\naugment themselves with AI. I think we're already on\nthis path to augmentation. We have phones we use for communication. We have on ourselves at all times. We have wearables soon that have shared\nperception with us, right? Like the Humane AI Pin, or I mean technically your\nTesla car has shared perception. And so if you have shared\nexperience, shared context, you communicate with one another, and you have some sort of IO, really it's an extension of yourself. And to me, I think that humanity\naugmenting itself with AI and having AI that is not\nanchored to anything biological, both will coexist, and the\nway to align the parties. We already have a sort of mechanism to align superintelligences\nthat are made of humans and technology, right? Companies are sort of large\nmixture of expert models where we have neural routing\nof tasks within a company, and we have ways of economic exchange to align these behemoths. And to me, I think capitalism is the way, and I do think that whatever\nconfiguration of matter or information leads to maximal growth, will be where we converge, just from like physical principles. And so we can either align\nourselves to that reality, and join the acceleration up in scope and scale of civilization, or we can get left behind\nand try to decelerate and move back in the forest,\nlet go of technology, and return to our primitive state. And those are the two paths\nforward, at least to me. - But there's a philosophical question whether there's a limit to\nthe human capacity to align. So let me bring it up\nas a form of argument. There's a guy named Dan Hendrycks, and he wrote that he agrees with you that AI development could be viewed as an evolutionary process, but to him, to Dan, this\nis not a good thing, as he argues that natural\nselection favors AIs over humans. And this could lead to human extinction. What do you think? If it is an evolutionary process, and AI systems may have\nno need for humans? - I do think that we're actually inducing an evolutionary process on the space of AIs\nthrough the market, right? Right now we run AIs that have\npositive utility to humans, and that induces a selective pressure if you consider a neural net being alive when there's an API running\ninstances of it on GPUs. Right, and which APIs get run, the ones that have high\nutility to us, right? So similar to how we domesticated wolves and turned them into dogs that are very clear in their expression, they're very aligned, right? I think there's gonna be\nan opportunity to steer AI and achieve highly aligned AI. And I think that humans plus AI is a very powerful combination. And it's not clear to me that pure AI would select out that combination. - So the humans are creating the selection pressure right now to create AIs that are aligned to humans. But, you know, given how AI develops and how quickly it can grow and scale, one of the concerns, to\nme one of the concerns is unintended consequences that humans are not able to anticipate all the consequences of this process. The scale of damage that could be done through unintended consequences with AI systems is very large. - The scale of the upside, right? By augmenting ourselves with\nAI is unimaginable right now. The opportunity cost, we're at a fork in the road, right? Whether we take the path of\ncreating these technologies, augment ourselves, and get to climb up the Kardashev scale, become multi-planetary with the aid of AI, or we have a hard cutoff of like, we don't birth these technologies at all, and then we leave all the\npotential upside on the table. Right, and to me, out of responsibility\nto the future humans, we could carry, right, with\nhigher carrying capacity by scaling up civilization out of responsibility to those humans, I think we have to make the\ngreater grander future happen. - Is there a middle ground between cutoff and all systems go? Is there some argument for caution? - I think, like I said, the\nmarket will exhibit caution. Every organism, company, consumer, is acting out of self-interest, and they won't assign capital to things that have\nnegative utility to them. - The problem is with the\nmarket is like, you know, there's not always perfect information. There's manipulation,\nthere's a bad faith actors that mess with the system. It's not always a rational\nand honest system. - Well, that's why we need\nfreedom of information, freedom of speech, and freedom of thought in order to converge, be able to converge on the\nsubspace of technologies that have positive\nutility for us all, right."
    },
    {
      "timestamp": "57:56",
      "section": "p(doom)",
      "text": "- Well, let me ask you about\nPdoom, probability of doom. That's just fun to say,\nbut not fun to experience. What is to you the probability that AI eventually kills all or most humans, also known as probability of doom? - I'm not a fan of that calculation. I think people just\nthrow numbers out there. It's a very sloppy calculation, right? To calculate a probability, you know, let's say you model the world as some sort of Markov process. If you have enough variables\nor hidden Markov process, you need to do a stochastic path integral through the space of all possible futures, not just the futures that your brain naturally\nsteers towards, right? I think that the estimators of Pdoom are biased because of our biology, right? We're evolved to have bias sampling towards negative futures that are scary, because that was an\nevolutionary optimum, right? And so people that are of,\nlet's say higher neuroticism, will just think of negative futures where everything goes\nwrong all day every day, and claim that they're\ndoing unbiased sampling, and in a sense like\nthey're not normalizing for the space of all possibilities. And the space of all possibilities is like super exponentially large. And it's very hard to have this estimate. And in general, I don't think\nthat we can predict the future with that much granularity\nbecause of chaos, right? If you have a complex system, you have some uncertainty\nand a couple variables. If you let time evolve, you have this concept of a\nLyapunov exponent, right? A bit of fuzz becomes a lot of fuzz in our estimate\nexponentially, so over time. And I think we need to show some humility that we can't actually predict the future. All we know, the only prior we\nhave, is the laws of physics. And that's what we're arguing for. The laws of physics say the\nsystem will want to grow. And subsystems that are\noptimized for growth and replication are more\nlikely in the future. And so we should aim to maximize our current mutual\ninformation with the future. And the path towards that\nis for us to accelerate rather than decelerate. So I don't have a Pdoom,\n'cause I think that, you know, similar to the quantum\nsupremacy experiment at Google, I was in the room when they were running\nthe simulations for that, that was an example of\na quantum chaotic system where you cannot even estimate probabilities of certain outcomes with even the biggest\nsupercomputer in the world, right? And so that's an example of chaos. And I think the system is far too chaotic for anybody to have an accurate estimate of the likelihood of certain futures. If they were that good, I think they would be very rich\ntrading on the stock market. - But nevertheless, it's\ntrue that humans are biased, grounded in our evolutionary biology, scared of everything that can kill us, but we can still imagine\ndifferent trajectories that can kill us. We don't know all the other\nones that don't, necessarily, but it's still, I think, useful, combined with some basic intuition\ngrounded in human history to reason about like what,\nlike looking at geopolitics, looking at basics of human nature. How can powerful technology\nhurt a lot of people? It just seems grounded in that\nlooking at nuclear weapons. You can start to estimate Pdoom in maybe in a more philosophical sense, not a mathematical one. Philosophical meaning\nlike, is there a chance, does human nature tend\ntowards that or not? - I think to me, one of the\nbiggest existential risks would be the concentration\nof the power of AI in the hands of the very few, especially if it's a mix between the companies that\ncontrol the flow of information and the government. Because that could set things up for a sort of dystopian\nfuture where only a very few and an oligopoly in\nthe government have AI, and they could even convince the public that AI never existed. And that opens up sort of these scenarios for authoritarian centralized control, which to me is the darkest timeline. And the reality is that we have a prior, we have a data-driven prior of these things happening, right? When you give too much power, when you centralize power too much, humans do horrible things, right? And to me, that has a\nmuch higher likelihood in my Bayesian inference than\nsci-fi based priors, right? Like my prior came from\nthe Terminator movie. And so when I talked to these AI doomers, I just asked them to trace a path through this Markov chain of events that would lead to our doom, right? And to actually give me a good probability for each transition. And very often there's a unphysical or highly unlikely transition\nin that chain, right? But of course we're wired to fear things, and we're wired to respond to danger, and we're wired to deem\nthe unknown to be dangerous because that's a good\nheuristic for survival, right? But there's much more to lose out of fear. We have so much to lose,\nso much upside to lose, by preemptively stopping\nthe positive futures from happening out of fear. And so I think that we\nshouldn't give into fear. Fear is the mind killer. I think it's also the civilization killer. - We can still think about the\nvarious ways things go wrong. For example, the founding\nfathers of the United States thought about human nature and that's why there's a discussion about the freedoms that are necessary. They really deeply deliberated about that. And I think the same could\npossibly be done for AGI. It is true that history, human history, shows that we tend towards centralization, or at least when we\nachieve centralization, a lot of bad stuff happens. When there's a dictator, a\nlot of dark bad things happen. The question is, can AGI\nbecome that dictator? An AGI one develop, become the centralizer because of its power? Maybe has the same, because\nof the alignment of humans, perhaps, the same tendencies, the same Stalin-like\ntendencies to centralize and manage centrally the\nallocation of resources. And you can even see that\nas a compelling argument on the surface level. Well, AGI is so much smarter,\nso much more efficient, so much better at allocating resources. Why don't we outsource it to the AGI, and then eventually whatever forces that corrupt the human mind with power could do the same for AGI. It would just say, well,\nhumans are dispensable, we'll get rid of them. Do the Jonathan Swift \"Modest Proposal\" from a few centuries\nago, I think the 1700s, when he satirically suggested\nthat, I think it's in Ireland, that the children of poor people are fed as food to the rich people. And that would be a good idea, because it decreases the\namount of poor people, and gives extra income to the poor people. So it's on several accounts decreases the amount of poor people, therefore more people become rich. Of course it misses a\nfundamental piece here that's hard to put into\na mathematical equation on the basic value of human life. So all of that to say, are\nyou concerned about AGI being the very centralizer of power that you just talked about? - I do think that right now there's a bias towards\novercentralization of AI because of compute density\nand centralization of data and how we're training models. I think over time we're\ngonna run out of data to scrape over the internet. And I think that, well, actually I'm working on\nincreasing the compute density so that compute can be everywhere\nand acquire information and test hypotheses in the environment in a distributed fashion. I think that fundamentally,\ncentralized cybernetic control. So having one intelligence\nthat is massive, that, you know, fuses many sensors and is trying to perceive\nthe world accurately, predict it accurately,\npredict many, many variables and control it, right, enact\nits will upon the world. I think that's just never\nbeen the optimum, right? Like let's say you have\na company, you know, if you have a company, I don't know of 10,000 people\nthat all report to the CEO, even if that CEO is an AI,\nI think it would struggle to fuse all of the information\nthat is coming to it and then predict the whole system and then to enact its will. What has emerged in\nnature and in corporations and all sorts of systems, is a notion of sort of hierarchical\ncybernetic control, right? You have, you know, in\na company it would be, you have like the individual contributors, they're self interested, and they're trying to achieve their tasks and they have a fine in terms\nof time and space, if you will control loop, and in field\nof perception, right? They have their code base. Let's say you're in a software company, they have their code base, they iterate it on it intraday, right? And then the management maybe checks in, it has a wider scope, it has,\nlet's say five reports, right? And then it samples each\nperson's update once per week, and then you can go up the chain, and you have larger\ntimescale and greater scope. And that seems to have emerged as sort of the optimal\nway to control systems. And really that's what\ncapitalism gives us, right? You have these hierarchies and you can even have like\nparent companies and so on. And so that is far more fault tolerant. In quantum computing,\nthat's my field I came from, we have a concept of this fault tolerance and quantum error correction, right? Quantum error correction is detecting a fault that came from noise, predicting how it's\npropagated through the system, and then correcting it, right,\nso it's a cybernetic loop. And it turns out that decoders\nthat are hierarchical, and at each level the hierarchy are local, perform the best by far, and\nare far more fault tolerant. And the reason is if you\nhave a non-local decoder, then you have one fault\nat this control node, and the whole system sort of crashes. Similarly to if you have, you know, one CEO that everybody reports to, and that CEO goes on vacation, the whole company comes to a crawl, right? And so to me, I think that yes, we're seeing a tendency\ntowards centralization of AI, but I think there's gonna\nbe a correction over time where intelligence is gonna\ngo closer to the perception and we're gonna break up\nAI into smaller subsystems that communicate with one another and form a sort of meta system. - So if you look at the hierarchies that are in the world today, there's nations, and\nthose are hierarchical, but in relation to each\nother, nations are anarchic. So it's an anarchy. Do you foresee a world like this where there's not a\nover, what'd you call it? A centralized cybernetic control? - [Guillaume] Centralized\nlocus of control, yeah. - So like that's suboptimal you're saying? So it would be always\na state of competition at the very top level. - Yeah, just like, you know, in a company you may have like two units\nworking on similar technology and competing with one another, and you prune the one that\nperforms not as well, right? And that's a sort of\nselection process for a tree or a product gets killed, right? And then a whole org gets fired. And that's this process\nof trying new things, and shedding old things that didn't work it's what gives us adaptability and helps us converge on, you know, the technologies and things\nto do that are most good. - I just hope there's not a failure mode that's unique to AGI versus humans. 'Cause you're describing human\nsystems mostly right now. I just hope when there's a\nmonopoly on AGI in one company that we'll see the same\nthing we see with humans, which is another company will spring up and start competing effectively. - I mean that's been\nthe case so far, right? We have OpenAI, we have\nAnthropic, now we have xAI, you know, we had Meta\neven for open source, and now we have Mistral, right,\nwhich is highly competitive. And so that's the beauty of capitalism. You don't have to trust\nany one party too much 'cause we're kind of always\nhedging our bets at every level. There's always competition. And that's the most beautiful\nthing to me at least, is that the whole system\nis always shifting and always adapting, and\nmaintaining that dynamism is how we avoid tyranny, right? Making sure that everyone\nhas access to these tools, to these models, and can\ncontribute to the research, avoids a sort of neural tyranny where very few people have\ncontrol over AI for the world and use it to oppress those around them."
    },
    {
      "timestamp": "1:13:23",
      "section": "Quantum machine learning",
      "text": "- When you were talking\nabout intelligence, you mentioned multipartite\nquantum entanglement. So high level question first is, what do you think is intelligence? When you think about\nquantum mechanical systems and you observe some kind of\ncomputation happening in them, what do you think is intelligent about the kind of computation\nthe universe is able to do? A small, small inkling of which is the kind of computation\nthe human brain is able to do? - I would say like\nintelligence and computation aren't quite the same thing. I think that the universe\nis very much, you know, doing a quantum computation. If you had access to all the\ndegrees of freedom, you could, and a very, very, very\nlarge quantum computer, with many, many, many qubits, let's say a few qubits\nper Planck volume, right? Which was more or less the pixels we have, then you'd be able to simulate\nthe whole universe, right, on a sufficiently large quantum computer. Assuming you're looking\nat a finite volume, of course, of the universe. I think that at least to me,\nintelligence is the, you know, I go back to cybernetics, right? The ability to perceive,\npredict, and control our world. But really it's, nowadays it seems like a lot of intelligence we use is more about compression, right? It's about operationalizing\ninformation theory, right? In information theory, you have the notion of entropy\nof a distribution or a system and entropy tells you that\nyou need this many bits to encode this distribution\nor this subsystem if you had the most optimal code. And AI, at least the way we do it today for LLMs and for quantum, is very much trying to\nminimize relative entropy between our models of\nthe world and the world, distributions from the world. And so we're learning, we're searching over the\nspace of computations to process the world, define\nthat compressed representation that has distilled all\nthe variants and noise and entropy, right? And originally, I came to\nquantum machine learning from the study of black holes because the entropy of black\nholes is very interesting, in a sense they're physically the most dense objects in the universe. You can't pack more information spatially any more densely than in a black hole. And so I was wondering how do black holes actually\nencode information? What is their compression code? And so that got me into\nthe space of algorithms to search over space of quantum codes. And it got me actually into also how do you acquire\nquantum information from the world, right? So something I've worked\non, this is public now, is quantum analog digital conversion. So how do you capture information from the real world in superposition, and not destroy the superposition, but digitize for a quantum\nmechanical computer information from the real world. And so if you have an ability to capture quantum information, and search over learned\nrepresentations of it, now you can learn\ncompressed representations that may have some useful\ninformation in there, latent representation, right? And I think that many of the problems\nfacing our civilization are actually beyond this\ncomplexity barrier, right? I mean the greenhouse effect is a quantum mechanical effect, right? Chemistry is quantum mechanical, you know, nuclear physics is quantum mechanical, a lot of biology and\nprotein folding and so on is affected by quantum mechanics. And so unlocking an ability to augment human intellect with\nquantum mechanical computers and quantum mechanical AI seem to me like a fundamental\ncapability for civilization that we need to develop. So I spent several years doing that, but over time I kind of\ngrew weary of the timelines that were starting to\nlook like nuclear fusion. - So one high level question I can ask is maybe by way of definition,\nby way of explanation, what is a quantum computer, and what is quantum machine learning? - Hmm, so a quantum computer really is a quantum mechanical system over which we have sufficient control, and it can maintain its\nquantum mechanical state. And quantum mechanics is how nature behaves at\nthe very small scales when things are very small or very cold. And it's actually more fundamental\nthan probability theory. So we're used to things\nbeing this or that, but we're not used to\nthinking in superpositions 'cause, well, our brains can't do that. So we have to translate the\nquantum mechanical world to say linear algebra to grok it. Unfortunately that translation is exponentially inefficient on average. You have to represent things\nwith very large matrices, but really you can make a quantum computer out of many things, right? And we've seen all sorts\nof players, you know, from neutral atoms, trapped\nions, superconducting, metal, photons, and at different frequencies. I think you could make a quantum computer out of many things. But to me, the thing that\nwas really interesting was both quantum machine learning was about understanding the\nquantum mechanical world with quantum computers. So embedding the physical\nworld into AI representations, and quantum computer engineering was embedding AI algorithms\ninto the physical world. So this bidirectionality of embedding physical world into AI, AI into the physical world, the symbiosis between physics and AI, really that's the sort of core of my quest really even to this day,\nafter quantum computing, it's still in this sort of journey to merge, really, physics\nand AI fundamentally. - So quantum machine learning is a way to do machine learning on a representation of\nnature that is, you know, stays true to the quantum\nmechanical aspect of nature. - Yeah, it's learning quantum\nmechanical representations that would be quantum deep learning. Alternatively, you can try to\ndo classical machine learning on a quantum computer. I wouldn't advise it, because\nyou may have some speedups, but very often the speedups\ncome with huge costs. Using a quantum computer is\nvery expensive. Why is that? Because you assume the computer is operating\nat zero temperature, which no physical system in the universe can achieve that temperature. So what you have to do is\nwhat I've been mentioning, this quantum error correction process, which is really an\nalgorithmic fridge, right? It's trying to pump\nentropy out of the system, trying to get it closer\nto zero temperature. And when you do the calculations of how many resources it would take to say do deep learning\non a quantum computer, classical deep learning, there's just such a huge\noverhead, it's not worth it. It's like thinking about\nshipping something across a city using a rocket and\ngoing to orbit and back. It doesn't make sense. Just use an, you know,\ndelivery truck, right? - What kind of stuff can you figure out? Can you predict, can you understand with quantum deep learning that you can't with deep learning? So incorporating quantum\nmechanical systems into the learning process? - I think that's a great question. I mean, fundamentally it's any system that has sufficient quantum\nmechanical correlations that are very hard to capture. For classical representations, then there should be an advantage for a quantum mechanical representation over a purely classical one. The question is, which systems\nhave sufficient correlations that are very quantum, but is also, which systems are\nstill relevant to industry? That's a big question. You know, people are\nleaning towards chemistry, nuclear physics. I've worked on actually processing inputs from quantum sensors, right? If you have a network of quantum sensors, they've captured a quantum\nmechanical image of the world, and how to post-process that, that becomes a sort of quantum\nform of machine perception. And so for example, Fermilab has a project exploring detecting dark matter\nwith these quantum sensors. And to me that's in\nalignment with my quest to understand the universe\never since I was a child. And so someday I hope that we can have very large networks of quantum\nsensors that help us peer into the earliest parts\nof the universe, right? For example, the LIGO is\na quantum sensor, right? It's just a very large one. So yeah, I would say quantum machine perception\nsimulations, right? Grokking quantum simulations\nsimilar to AlphaFold, right? AlphaFold understood the\nprobability distribution over configurations of proteins. You can understand quantum distributions over configurations of\nelectrons more efficiently with quantum machine learning. - You co-authored a paper titled \"A Universal Training Algorithm\nfor Quantum Deep Learning\" that involves baqprop with a Q. Very well done, sir, very\nwell done. How does it work? Is there some interesting\naspects you can just mention on how kinda, you know, baqprop and some of these things we know from classical machine learning transfer over to the\nquantum machine learning? - Yeah, that was a funky paper. That was one of my first papers\nin quantum deep learning. Everybody was saying,\noh, I think deep learning is gonna be sped up by quantum computers. And I was like, well the best\nway to predict the future is to invent it. So here's a hundred page paper, have fun. Essentially, you know, quantum computing is usually you embed reversible operations into a quantum computation. And so the trick there was\nto do a feedforward operation and do what we call a phase kick. But really it's just a force kick. You just kick the system\nwith a certain force that is, you know, proportional\nto your loss function that you wish to optimize. And then by performing uncomputation, you start with the superpositions, superposition over parameters,\nright, which is pretty funky. Now you're not just, you don't have just a\npoint for parameters. You have a superposition over many potential parameters, right? And our goal is. - Is using phase kicks somehow\nto adjust the parameters? - 'Cause phase kicks emulate having the parameter space be like a particle in n dimensions, and you're trying to get\nthe Schrodinger equation, Schrodinger dynamics in the lost landscape of the neural network, right? And so you do an algorithm\nto induce this phase kick, which you know, involves\na feed forward, a kick, and then when you\nun-compute the feed forward, then all the errors in these\nphase kicks and these forces back propagate and hit\neach one of the parameters throughout the layers. And if you alternate this with an emulation of kinetic energy, then it's kind of like a\nparticle moving in n dimensions, a quantum particle. And the advantage in principle would be that it can tunnel\nthrough the landscape and find new optima that would've been difficult\nfor stochastic optimizers. But again, this is kind\nof a theoretical thing, and in practice with at least\nthe current architectures for quantum computers that\nwe have planned, you know, such algorithms would be\nextremely expensive to run."
    },
    {
      "timestamp": "1:26:41",
      "section": "Quantum computer",
      "text": "- So maybe this is a good\nplace to ask the difference between the different fields\nthat you've had a toe in. So mathematics, physics, engineering, and also, you know, entrepreneurship, like the different layers of the stack. I think a lot of the stuff\nyou're talking about here is a little bit on the math side, maybe physics almost working in theory. What's the difference to\nyou between math, physics, engineering, and you know, making a product for quantum computing, for quantum machine learning? - Yeah, I mean, you know,\nsome of the original team for the TensorFlow quantum\nproject, which we started, you know, in school at\nUniversity of Waterloo, there was myself, you know,\ninitially I was a physicist, a polymathematician. We had a computer scientist,\nwe had mechanical engineer, and then we had a physicist\nthat was experimental primarily. And so putting together teams that are very cross-disciplinary and figuring out how to\ncommunicate and share knowledge is really the key to doing this sort of interdisciplinary\nengineering work. I mean, there is a big\ndifference, you know, in mathematics you can explore mathematics for mathematics' sake, in physics, you're applying mathematics to understand the world around us. And in engineering, you're trying to, you're trying to hack the world, right? You're trying to find how to\napply the physics that I know, my knowledge of the world to do things. - Well, in quantum\ncomputing in particular, I think there's just a lot\nof limits to engineering. It just seems to be extremely hard. So there's a lot of value to be exploring quantum computing, quantum machine learning\nin theory, and with math. So I guess one question is why is it so hard to\nbuild a quantum computer? What's your view of timelines in bringing these ideas to life? - Right, I think that, you know, an overall theme of my company is that we have folks that are, you know, there's a sort of exodus\nfrom quantum computing and we're going to\nbroader physics-based AI that is not quantum. So that gives you a hint, and. - So we should say the name\nof your company is Extropic. - Extropic, that's right. And we do physics-based AI\nprimarily based on thermodynamics rather than quantum mechanics. But essentially a quantum computer is very difficult to build\nbecause you have to induce this sort of zero temperature\nsubspace of information. And the way to do that is\nby encoding information, you encode a code within a code, within a code within a code. And so there's a lot of redundancy needed to do this error correction. But ultimately it's a sort\nof algorithmic refrigerator. Really, it's just pumping out entropy out of the subsystem that\nis virtual and delocalized that represents your quote\nunquote logical qubits. AKA, the payload quantum bits in which you actually want to run your quantum mechanical program. It's very difficult because in order to scale\nup your quantum computer, you need each component to\nbe of sufficient quality for it to be worth it. Because if you try to do\nthis error correction, this quantum error correction process, and each quantum bit and\nyour control over them, if it's insufficient,\nit's not worth scaling up. You're actually adding more\nerrors than you remove. And so there's this notion of a threshold where if your quantum bits\nare of sufficient quality in terms of your control over them, it's actually worth scaling up. And actually in recent years, people have been crossing the threshold, and it's starting to be worth it. And so it's just a very\nlong slog of engineering. But ultimately it's really crazy to me how much exquisite level of control we have over these systems,\nit's actually quite crazy, and people are crossing, you know, they're achieving milestones. It's just, you know, in general\nthe media always gets ahead right, of where the technology is. There's a bit too much hype. It's good for fundraising, but sometimes, you know,\nit causes winters, right? It's the hype cycle. I'm bullish on quantum computing on a 10, 15 year timescale personally, but I think there's other quests that can be done in the meantime. I think it's in good hands right now. - Well, let me just explore\ndifferent beautiful ideas, large or small in quantum computing that might jump out at you from memory. So when you co-authored a paper titled \"Asymptotically Limitless\nQuantum Energy Teleportation via Qudit Probes.\" So just out of curiosity, can\nyou explain what a qudit is versus a qubit? - Yeah, it's a D state qubit. - [Lex] It's multidimensional - Multidimensional, right. So it's like, well, you know, can you have a notion of like\nan integer floating point that is quantum mechanical? That's something I've had to think about. I think that research was\na precursor to later work on quantum analog digital conversion. There it was interesting,\nbecause during my master's, I was trying to understand the energy and entanglement of the vacuum, right, of emptiness. Emptiness has energy,\nwhich is very weird to say. And our equations of cosmology don't match our calculations for the amount of quantum energy there is in the fluctuations. And so I was trying to hack the\nenergy of the vacuum, right? And the reality is that you\ncan't just directly hack it. It's not technically free energy. Your lack of knowledge of the fluctuations means you can't extract the energy. But just like, you know, the stock market, if you have a stock that's\ncorrelated over time, the vacuum's actually correlated. So if you measured the\nvacuum at one point, you acquired information. And if you communicated that\ninformation to another point, you can infer what\nconfiguration the vacuum is in to some precision, and statistically extract on\naverage some energy there. So you've quote unquote teleported energy. To me that was interesting, because you could create pockets\nof negative energy density, which is energy density\nthat is below the vacuum, which is very weird, because we don't understand\nhow the vacuum gravitates. And there are theories where the vacuum or the\ncanvas of space time itself is really a canvas made out\nof quantum entanglement. And I was studying how decreasing energy\nof the vacuum locally increases quantum entanglement,\nwhich is very funky. And so the thing there is that, you know, if you're into, you know,\nweird theories about, you know, UAPs and whatnot, you know, you could try to imagine\nthat they're around and how would they\npropel themselves, right? How would they go faster\nthan the speed of light? You would need a sort of\nnegative energy density. And to me, I gave it the old college try trying to hack the energy of the vacuum and hit the limits allowable\nby the laws of physics. But there's all sorts of caveats there where you can't extract more\nthan you've put in obviously, - But you're saying it's\npossible to teleport the energy because you can extract\ninformation one place and then make based on that, some kind of prediction\nabout another place. I'm not sure what I make of that. - Yeah, I mean, it's allowable\nby the laws of physics. The reality though is that the correlations\ndecay with distance. And so you're gonna have to pay the price not too far away from where\nyou extract it to, right. - The precision decreases. I mean, in terms of your\nability to, but still,"
    },
    {
      "timestamp": "1:35:15",
      "section": "Aliens",
      "text": "but since you mentioned UAPs,\nwe talked about intelligence, and I forgot to ask, what's your view on the other possible intelligences that are out there at the mesoscale? Do you think there's other\nintelligent alien civilizations? Is that useful to think about? How often do you think about it? - I think it's useful to think about, it's useful to think about because we gotta ensure\nwe're anti-fragile, and we're, you know, trying to increase our\ncapabilities as fast as possible because we could get disrupted. Like there's no laws of physics against there being life elsewhere that could evolve and become\nan advanced civilization, and eventually come to us. Do I think they're here now? I'm not sure. I mean, I've read what most\npeople have read on the topic. I think it's interesting to consider, and to me it's a useful thought experiment to instill a sense of urgency\nin developing technologies and increasing our capabilities to make sure we don't\nget disrupted, right? Whether it's a form of\nAI that disrupts us, or a foreign intelligence\nfrom a different planet, like either way, like\nincreasing our capabilities and becoming formidable as humans. I think that's really important. So that we're robust against whatever the\nuniverse throws at us. - But to me it's also\nan interesting challenge and thought experiment on\nhow to perceive intelligence. This has to do with\nquantum mechanical systems. This has to do with any kind of system that's not like humans. So to me, the thought experiment\nis say the aliens are here, or they are directly observable. We're just too blind, too self-centered, don't have the right sensors, or don't have the right\nprocessing of the sensor data to see the obvious intelligence\nthat's all around us. - Well, that's why we work\non quantum sensors, right? They can sense gravity. - Yeah, but there could\nbe, so that's a good one. But there could be other\nstuff that's not even in the currently known forces of physics. There could be some other stuff. And the most entertaining\nthought experiment to me is that it's other stuff that's obvious. It's not like we lack the\nsensors, it's all around us. You know, you know, the consciousness being one possible one, but there could be stuff that's\njust like obviously there, and once you know it, it's\nlike, oh, right, right. The thing we thought is somehow emergent from\nthe laws of physics. We understand them, it's actually a fundamental\npart of the universe and can be incorporated in\nphysics once understood. - Statistically speaking, right, if we observed some sort of alien life, it would most likely be some sort of virally self-replicating von\nNeumann-like probe system. Right, and it's possible\nthat there, you know, there are such systems that, I don't know what they're doing at the bottom of the ocean allegedly, but maybe they're, you know, collecting minerals from\nthe bottom of the ocean. But that wouldn't\nviolate any of my priors. But am I certain that\nthese systems are here? And it'd be difficult\nfor me to say so, right. I only have secondhand information\nabout there being data. - About the bottom of the ocean. Yeah, but, you know, could\nit be things like memes? Could it be thoughts and ideas? Could they be operating at that medium? Could aliens be the very\nthoughts that come into my head? Like, what do you, have you. How do you know that,\nhow do you know that, what's the origin of ideas, in your mind, when an idea comes to your head? Show me where it originates. - I mean, frankly, when I had the idea for the type of computer I'm building now, I think it was eight years ago now, it really felt like it was\nbeing beamed from space. I was in bed just shaking,\njust thinking it through, and I don't know, but do I\nbelieve that legitimately? I don't think so, but you know, I think that alien life\ncould take many forms, and I think the notion of intelligence and the notion of life needs to be expanded much more broadly to be less anthropocentric or biocentric."
    },
    {
      "timestamp": "1:40:04",
      "section": "Quantum gravity",
      "text": "- Just to linger a little\nlonger on quantum mechanics, through all your explorations\nof quantum computing, what's the coolest, most beautiful idea that you've come across\nthat has been solved or has not yet been solved? - I think the journey to understand something called AdS/CFT. So the journey to\nunderstand quantum gravity through this picture where a hologram of lesser\ndimension is actually dual or exactly corresponding to a bulk theory of quantum gravity of an extra dimension. And the fact that this sort of duality comes from trying to learn deep learning-like\nrepresentations of the boundary. And so at least part of my journey someday on my bucket list is to apply\nquantum machine learning to these sorts of systems, these CFDs or they're called SYK models, and learn an emergent geometry\nfrom the boundary theory. And so we can have a\nform of machine learning to help us understand\nquantum gravity, right. Which is, you know, still a holy grail that I would like to hit\nbefore I leave this Earth. - What do you think is\ngoing on with black holes as the information storing\nand processing units, what do you think is\ngoing on with black holes? - Black holes are really\nfascinating objects. They're at the interface between quantum mechanics and gravity. And so they help us\ntest all sorts of ideas. I think that, you know,\nfor many decades now, there's been sort of this\nblack hole information paradox that things that fall into the black hole we've seen to have lost their information. Now I think there's this firewall paradox that has been allegedly\nresolved in recent years by, you know, a former peer of mine who's now a professor at Berkeley. And there it seems like there is, as information falls into\na black hole, it's sort of, there's sort of a sedimentation right? As you get closer and\ncloser to the horizon from the point of view of\nthe observer on the outside, the object slows down infinitely as it gets closer and closer. And so everything that is\nfalling into a black hole from our perspective\ngets sort of sedimented and tacked on to the near horizon. And at some point it gets\nso close to the horizon, it's in the proximity or the scale, in which quantum effects and\nquantum fluctuations matter. And there that and falling\nmatter could interfere with sort of the traditional pictures, that it could interfere with the creation and\nannihilation of particles and antiparticles in the vacuum. And through this interference, one of the particles gets entangled with the infalling information and one of them is now free and escapes. And that's how there's\nsort of mutual information between the outgoing radiation\nand the infalling matter, but getting that calculation right, I think we're only just starting\nto put the pieces together. - There's a few pothead-like\nquestions I want to ask you. So one, does it terrify you that there's a giant black hole\nat the center of our galaxy? - I don't know, I just want to, you know, set up shop near it to\nfast forward, you know, meet a future civilization, right? Like if we have a limited lifetime, if you could go orbit a\nblack hole and emerge. - So if you were like, if\nthere's a special mission that could take you to a black hole, would you volunteer to go travel? - To orbit and not\nobviously not fall into it? - That's obvious. So it's obvious to you that everything's destroyed\ninside a black hole. Like all the information that makes up Guillaume is destroyed. Maybe on the other side,\nBeff Jezos emerges. And it's all like, it's tied together in some deeply meme-ful way. - Yeah, I mean, that's a great question. We have to answer what black holes are. Are we punching a hole through space-time and creating a pocket universe? It's possible, right? Then that would mean that if\nwe ascend the Kardashev scale to, you know, beyond Kardashev Type III, we could engineer black holes\nwith specific hyperparameters to transmit information to\nnew universes we create. And so we can have progeny,\nright, that are new universes. And so even though our universe\nmay reach a heat death, we may have a way to have a legacy, right? And so we don't know yet, we need to ascend the Kardashev scale to answer these questions, right, to peer into that regime\nof higher energy physics."
    },
    {
      "timestamp": "1:45:25",
      "section": "Kardashev scale",
      "text": "- And maybe you can speak\nto the Kardashev scale for people who don't know. So one of the sort of meme-like principles and goals of the e/acc movement is to ascend the Kardashev scale. What is the Kardashev scale\nand when do we wanna ascend it? - The Kardashev scale is a measure of our energy production and consumption. And really it's a logarithmic scale. And Kardashev Type I is a milestone where we are producing\nthe equivalent wattage to all the energy that is\nincident on Earth from the Sun. Kardashev Type II would be\nharnessing all the energy that is output by the Sun. And I think Type III is like\nthe whole galaxy equivalent. - [Lex] Galaxy, I think level. Yeah. - Yeah, yeah. And then some people have\nsome crazy Type IV and V, but I don't know if I believe in those, but to me it seems like from the first principles\nof thermodynamics, that again, there's this\nconcept of thermodynamic-driven dissipative adaptation where, you know, life evolved on Earth because we have this sort of\nenergetic drive from the Sun, right, we have incident energy, and life evolved on Earth to capture, figure out ways to best\ncapture that free energy to maintain itself and grow. And I think that principle, it's not special to our Earth/Sun system. We could extend life well beyond, and we kind of have a\nresponsibility to do so because that's the process\nthat brought us here. So we don't even know what it has in store for us in the future. It could be something of beauty we can't even imagine today, right."
    },
    {
      "timestamp": "1:47:17",
      "section": "Effective accelerationism (e/acc)",
      "text": "- So this is probably a good place to talk a bit about the e/acc movement. In a Substack blog post titled, what the fuck is e/acc? Or actually what the F* is e/acc, you write, \"Strategically\nspeaking, we need to work towards several overarching civilization goals that are all interdependent. And the four goals are increase the amount of energy\nwe can harness as a species, climb the Kardashev gradient. In the short term, this almost certainly\nmeans nuclear fission. Increase human flourishing via pro-population growth policies and pro-economic growth policies, create artificial general intelligence, the single greatest force\nmultiplier in human history, and finally develop interplanetary and interstellar transport so that humanity can\nspread beyond the Earth.\" Could you build on top of that to maybe say what to you\nis the e/acc movement? What are the goals?\nWhat are the principles? - The goal is for the human\ntechnocapital mimetic machine to become self-aware, and to hyperstitiously\nengineer its own growth. So let's decompress that.\n- Define each of those words. - Yeah, so you have humans,\nyou have technology. You have capital, and then you have memes\nand information, right? And all of those systems are coupled with one another, right? Humans work at companies, they\nacquire and allocate capital, and humans communicate via memes and information propagation. And our goal was to have a sort\nof viral optimistic movement that is aware of how the system works. Fundamentally it seeks to grow, and we simply want to lean into the natural tendencies of the system to adapt for its own growth. - So in that way, you're right, the e/acc is literally\na mimetic optimism virus that is constantly drifting, mutating, and propagating in a\ndecentralized fashion. So mimetic optimism virus. So you do want it to be a\nvirus to maximize the spread. And it's hyperstitious, therefore the optimism will\nincentivize its growth. - We see e/acc as a sort of metaheuristic, a sort of very thin cultural framework from which you can have much\nmore opinionated forks, right? Fundamentally, we just say that it's good. That what got us here is this adaptation of the whole system, you\nknow, based on thermodynamics. And that process is good and\nwe should keep it going, right? That is the core thesis. Everything else is okay, how do we ensure that we maintain this malleability and adaptability while clearly not suppressing variants and maintaining free\nspeech, freedom of thought, freedom of information propagation, and freedom to do AI research is important for us to\nconverge the fastest on the space of technologies,\nideas, and whatnot that lead to this growth. And so ultimately, you know,\nthere's been quite a few forks, some are just memes, but\nsome are more serious, right? Vitalik Buterin recently\nmade a d/acc fork. He has his own sort of\nfine tunings of e/acc. - Does anything jump out to memory of the unique characteristic\nof that fork from Vitalik? - I would say that it's\ntrying to find a middle ground between e/acc and sort\nof EA and AI safety, to me, like having a movement that is opposite to what\nwas the mainstream narrative that was taking over Silicon Valley was important to sort of shift the dynamic range of opinions. And you know, it's like the balance between centralization\nand decentralization. The real optimum's always\nsomewhere in the middle, right? But for e/acc we're pushing\nfor entropy, novelty, disruption, malleability, speed, rather than being like\nsort of conservative, suppressing thought, suppressing\nspeech, adding constraints, adding too many regulations,\nslowing things down. And so it's kind of, we're trying to bring\nbalance to the force, right? The systems. - Balance to the force of\nhuman civilization, yeah. - It's literally the forces of constraints versus the entropic force\nthat makes us explore, right? Systems are optimal when they're\nat the edge of criticality between order and chaos, right? Between constraints, energy\nminimization, and entropy. Right, systems want to equilibrate, balance these two things. And so I thought that\nthe balance was lacking, and so we created this\nmovement to bring balance. - Well, I like how, I\nlike the sort of visual of the landscape of ideas\nevolving through forks. So kinda thinking on the\nother part of history, thinking of Marxism as\nthe original repository, and then Soviet communism\nis a fork of that. And then the Maoism is a fork\nof Marxism and communism. And so those are all forks. They're exploring different ideas. - Thinking of culture,\nalmost like code, right? Nowadays, I mean what you prompt the LLM or what you put in the\nconstitution of an LLM is basically its cultural framework, what it believes, right? And you can share it on GitHub nowadays. So trying to take inspiration\nfrom what has worked in this sort of machine of software to adapt over the space of code, could we apply that to culture? And our goal is to not say you should live your\nlife this way, X, Y, Z, it's to set up a process where people are always\nsearching over subcultures and competing for mindshare. And I think creating this\nmalleability of culture is super important for us to\nconverge onto the cultures and the heuristics about\nhow to live one's life that are updated to modern times. Because there's really\nbeen a sort of vacuum of spirituality and culture. People don't feel like they\nbelong to any one group. And there's been parasitic ideologies that have taken up opportunity to populate this Petri\ndish of minds, right? Elon calls it the mind virus, we call it the decel mind virus complex, which is the decelerative that is kind of the overall\npattern between all of them. There's many variants as well. And so, you know, if there's\na sort of viral pessimism, decelerative movement, we needed to have not only one movement, but you know, many, many variants. So it's very hard to pinpoint and stop. - But the overarching\nthing is nevertheless a kind of mimetic optimism pandemic. So I mean, okay, let me ask you, do you think e/acc to\nsome degree is a cult? - Define cult? - I think a lot of human progress is made when you have independent thought. So you have individuals that\nare able to think freely, and very powerful mimetic systems can kind of lead to groupthink. There's something in human nature that leads to like mass\nhypnosis, mass hysteria. We start to think alike. Whenever there's a sexy idea\nthat captures our minds. And so it's actually hard\nto like break us apart, like pull us apart, diversify a thought. So to that degree, to which degree is everybody\nkinda chanting e/acc, e/acc, like the sheep in \"Animal Farm\"? - Well, first of all, it's\nfun, it's rebellious, right? Like, you know, many, I think we lean into, there's this concept of sort of metairony, right? Of sort of being on the boundary of like, we're not sure if they're serious or not. And it's much more playful\nand much more fun, right? Like for example, we talk about thermodynamics\nbeing our God, right? And sometimes we do cult-like things, but there's no like ceremony\nand robes and whatnot. - Not yet. - Not yet, but ultimately,\nyeah, I mean I totally agree that it seems to me that humans wanna feel like\nthey're part of a group, so they naturally try to\nagree with their neighbors and find common ground, and that leads to sort of mode collapse in the space of ideas, right? We used to have sort\nof one cultural island that was allowed, it was a\ntypical subspace of thought, and anything that was diverting from that subspace of\nthought was suppressed or you were canceled, right? Now we've created a new mode, but the whole point is that\nwe're not trying to have a very restricted space of thought. There's not just one\nway to think about e/acc and its many forks. And the point is that\nthere are many forks, and there can be many\nclusters, and many islands, and I shouldn't be in\ncontrol of it in any way. I mean, there's no formal org whatsoever. I just put out tweets\nand certain blog posts and people are free to defect and fork if there's an aspect they don't like. And so that makes it\nso that there should be a sort of de-territorialization\nin the space of ideas so that we don't end up in one cluster that's very cult-like,\nand so cults usually, they don't allow people to\ndefect or start competing forks, whereas we encourage it, right."
    },
    {
      "timestamp": "1:57:47",
      "section": "Humor and memes",
      "text": "- Do you think just the humor, the pros and cons of humor and\nmeme, so in some sense meme, there's like a wisdom to memes, what is it, the magic theater,\nwhat book is that from? Hermann Hesse, \"Steppenwolf,\" I think. But there's a kind of\nembracing of the absurdity that seems to get to the truth of things, but at the same time it can\nalso decrease the quality and the rigor of the discourse. Do you feel the tension of that? - Yeah, so initially I think what allowed us to grow under the radar was because it was camouflaged as sort of metaironic, right? We would sneak in, you know, deep truths within a package of humor,\nand humor and memes, and what are called shitposts, right? And I think that was\npurposefully a sort of camouflage against those that seek\nstatus and do not want to, it's very hard to argue\nwith a cartoon frog or a, or a cartoon of an\nintergalactic Jeff Bezos and take yourself seriously. And so that allowed us to grow pretty rapidly in the early days. But of course like that's, you know, essentially people get steered, their notion of the truth\ncomes from the data they see, from the information they're fed, and the information people are fed is determined by algorithms, right? And really what we've been doing is sort of engineering what we call high mimetic fitness\npackets of information so that they can spread effectively and carry a message, right? So it's kind of a vector\nto spread the message. And yes, we've been\nusing sort of techniques that are optimal for today's\nalgorithmically-amplified information landscapes. But I think we're reaching\nthe point of, you know, scale where we can have serious\ndebates and serious conversations and, you know, that's why\nwe're considering doing a bunch of debates and having more serious\nlong form discussions. 'Cause I don't think that\nthe timeline is optimal for sort of very serious,\nthoughtful discussions. You get rewarded for sort\nof polarization, right? And so even though we started a movement that is literally trying to\npolarize the tech ecosystem, at the end of the day, it's so that we can have a conversation and find an optimum together. - I mean that's kind of what\nI try to do with this podcast, given the landscape of things, to still have long form conversations. But there is a degree to which\nabsurdity is fully embraced."
    },
    {
      "timestamp": "2:00:53",
      "section": "Jeff Bezos",
      "text": "In fact, this very conversation\nis multi-level absurd. So first of all, I should\nsay that I just very recently had a conversation with Jeff Bezos, and I would love to hear your Beff Jezos opinions of Jeff Bezos. Speaking of intergalactic Jeff Bezos. What do you think of that\nparticular individual whom your name is inspired? - Yeah, I mean, I think\nJeff is really great. I mean, he's built one of the most epic\ncompanies of all time. He's leveraged the technocapital machine and technocapital acceleration to give us what we wanted, right? We want quick delivery,\nvery convenient, at home, low prices, right? He understood how the machine worked and how to harness it, right? Like running the company, not drink, trying to take profits too\nearly, putting it back, letting the system compound\nand keep improving. And you know, arguably I\nthink Amazon's invested some of the most amount of\ncapital in robotics out there. And certainly with the birth of AWS, kind of enabled the sort of\ntech boom we've seen today that has paid the salaries\nof, you know, I guess myself and all of our friends to some extent. And so I think we can all be\ngrateful to, you know, Jeff, and he's one of the great\nentrepreneurs out there, one of the best of all time unarguably. - And of course the work at Blue Origin similar to the work at SpaceX is trying to make humans\na multiplanetary species, which it seems almost like a bigger thing than the capitalist machine, or it's a capitalist machine at a different timescale, perhaps. - Yeah, I think that, you know, companies they tend to optimize, you know, quarter over quarter,\nmaybe a few years out. But individuals that wanna leave a legacy can think on a multi-decadal\nor multi-century timescale. And so the fact that some individuals are such good capital allocators that they unlock the ability\nto allocate capitals to goals that take us much further\nor are much further looking, you know, Elon's doing this with SpaceX, putting all this capital\ntowards getting us to Mars. Jeff is trying to build Blue Origin, and I think he wants to\nbuild O'Neill cylinders and get industry off planet,\nwhich I think is brilliant. I think, you know, just\noverall, I'm for billionaires. I know this is a controversial\nstatement sometimes, but I think that in a sense it's kind of a proof\nof stake voting, right? Like if you've allocated\ncapital efficiently, you unlock more capital to allocate just because clearly you know how to allocate capital more efficiently, which is in contrast to\npoliticians that get elected because they speak the best on TV, right? Not because they have\na proven track record of allocating taxpayer\ncapital most efficiently. And so that's why I'm for capitalism over say, giving all our\nmoney to the government, and letting them figure\nout how to allocate it, so. - Why do you think it's a viral and it's a popular meme\nto criticize billionaires, since you mentioned billionaires. Why do you think there's\nquite a widespread criticism of people with wealth, especially those in the\npublic eye, like Jeff and Elon and Mark Zuckerberg and\nwho else, Bill Gates. - Yeah, I think a lot of people would, instead of trying to understand how the technocapital machine works and realizing they have much\nmore agency than they think, they'd rather have this\nsort of victim mindset. I'm just subjected to this\nmachine, it is oppressing me. And the successful players\nclearly must be evil because they've been\nsuccessful at this game that I'm not successful at. But, you know, I've\nmanaged to get some people that were in that mindset\nand make them realize how the technocapital machine works and how you can harness\nit for your own good and for the good of others. And by creating value, you capture some of the value\nyou create for the world. And that's sort of positive\nsum mindset shift is so potent. And really that's what we're\ntrying to do by scaling e/acc is sort of unlocking that\nhigher level of agency. Like actually you're far more in control of the future than you think, you have agency to change\nthe world, go out and do it. Here's permission. - Each individual has agency. The motto keep building is often heard. What does that mean to you, and what does that have\nto do with Diet Coke? - Well, Diet Coke. - By the way, thank you\nso much for the Red Bulls. It's working pretty well.\nI'm feeling pretty good. - Awesome. Well, so building\ntechnologies and building, it doesn't have to be technologies, just building in general\nmeans, you know, having agency, trying to change the world\nby creating, let's say, a company which is a\nself-sustaining organism, that, you know, accomplishes a function in the broader technocapital machine. To us, that's the way to\nachieve change in the world that you'd like to see, rather than say pressuring politicians or creating nonprofits that, you know, nonprofits, once they run out of money, their function can no\nlonger be accomplished. You're kind of deforming\nthe market artificially compared to sort of subverting\nor coursing the market to, or dancing with the market to convince it that actually this function\nis important, adds value. And here it is, right? And so I think, you know,\nthis is sort of the way between this sort of\ndegrowth like ESG approach versus say, Elon, right? The degrowth approach is like, we're gonna manage our way\nout of a climate crisis. And Elon is like, I'm\ngonna build a company that is self-sustaining,\nprofitable, and growing. And we're gonna innovate our\nway out of this dilemma, right? And we're trying to get\npeople to do the latter rather than the former, at all scales."
    },
    {
      "timestamp": "2:07:25",
      "section": "Elon Musk",
      "text": "- Elon is an interesting case. So you are a proponent,\nyou celebrate Elon, but he's also somebody\nwho has for a long time warned about the dangers,\nthe potential dangers, existential risks of\nartificial intelligence. How do you square the two? Is\nthat a contradiction to you? - It is somewhat because he's very much against\nregulation in many aspects. But for AI, he's definitely, you know, a proponent of regulations. I think overall, you know, he saw the dangers of\nsay OpenAI, you know, cornering the market, and then\ngetting to have the monopoly over the cultural priors that\nyou can embed in these LLMs that then, you know, as LLMs now become the source of truth for people, then you can shape the\nculture of the people. And so you can control\npeople by controlling LLMs. And he saw that, just like it\nwas the case for social media, if you shape the function\nof information propagation, you can shape people's opinions. He's sought to make a competitor. So at least like I think\nwe're very aligned there, that, you know, the way to a good future is to maintain sort of\nadversarial equilibria between the various AI players. I'd love to talk to him to\nunderstand sort of his thinking about how to make, you know, how to advance AI going forwards. I mean, he's also hedging\nhis bets, I would say, you know, with Neuralink, right? I think if he can't stop the\nprogress of AI, you know, he's building the technology to merge. So, you know, look at the\nactions, not just the words, but. - Well, I mean, there's some\ndegree where being concerned, maybe using human psychology, being concerned about threats\nall around us is a motivator. Like it's an encouraging thing. I operate much better\nwhen there's a deadline. The fear of the deadline. Like, and I, for myself\ncreate artificial things. Like I want to create in\nmyself this kind of anxiety as if something really\nhorrible will happen if I miss the deadline. I think there's some degree of that here because creating AI\nthat's aligned with humans has a lot of potential benefits. And so a different way to reframe that is if you don't, we're all gonna die. It just seems to be a very powerful psychological formulation of the goal of creating human-aligned AI. - I think that anxiety is good. I think, like I said,\nI want the free market to create aligned AIs that are reliable and I think that's what\nhe's trying to do with xAI. So I'm all for it. What I am against is sort\nof stopping, let's say, the open source ecosystem\nfrom thriving right? By, let's say, in the executive order, claiming that open source\nLLMs or dual use technologies then should be government controlled. Then everybody needs to register their GPU and their big matrices\nwith the government. And I think that extra friction will dissuade a lot of\nhackers from contributing, hackers that could later\nbecome the researchers that make key discoveries\nthat push us forward, right? Including discoveries for AI safety. And so I think I just wanna maintain ubiquity of opportunity\nto contribute to AI, and to own a piece of the future, right? It can't just be legislated,\nyou know, behind some wall where only a few players\nget to play the game. - I mean, so the e/acc movement\nis often sort of caricatured to mean sort of progress\nand innovation at all costs, doesn't matter how unsafe it is, doesn't matter if it\ncause a lot of damage. You just build cool shit\nas fast as possible, stay up all night with a\nDiet Coke, whatever it takes. I think, I guess, I don't know if there's\na question in there, but how important to you and what you've seen the\ndifferent formulations of e/acc is safety, is AI safety? - I think, again, I think like if there\nwas no one working on it, I think I would be a proponent of it. I think again, our goal is\nto sort of bring balance and obviously a sense of\nurgency is a useful tool, right, to make progress. It hacks our dopaminergic systems and gives us energy to\nwork late into the night. I think also having a higher purpose you're contributing to, right? At the end of the day, it's\nlike, what am I contributing to? I'm contributing to the growth\nof this beautiful machine so that we can seek to the stars. That's really inspiring. That's also a sort of neuro hack. - So you're saying AI\nsafety is important to you, but right now the\nlandscape of ideas you see is AI safety as a topic is used more often to\ngain centralized control. So in that sense, you're resisting it as a proxy for centralized,\ngaining centralized control. - Yeah, I just think we have to be careful because, you know, safety\nis just the perfect cover for sort of centralization of power and covering up eventually corruption. I'm not saying it's corrupted now, but it could be down the line. And really, if you let the argument run, like there's no amount of sort\nof centralization of control that will be enough to ensure your safety. There's always more 999s of pSafety that you can gain, you\nknow, 999.99999% safe. Maybe you want another nine? Oh, please give us full\naccess to everything you do. Full surveillance. And frankly, those that\nare proponents of AI safety have proposed like having\na global panopticon, right? Where you have centralized perception of everything going on. And to me, that just opens\nup the door wide open for a sort of big brother,\n1984-like scenario. And that's not a future I wanna live in. - 'Cause we know we have some\nexamples throughout history when that did not lead to a good outcome."
    },
    {
      "timestamp": "2:13:55",
      "section": "Extropic",
      "text": "You mentioned you founded\na company Extropic that recently announced a\n14.1 million seed round. What's the goal of the company? You're talking about a lot of\ninteresting physics things, so what are you up to over\nthere that you can talk about? - Yeah, I mean, you know, originally we weren't\ngonna announce last week, but I think with the\ndoxxing and disclosure, we got our hand forced. So we had to disclose\nroughly what we were doing, but really Extropic was\nborn from my dissatisfaction and that of my colleagues with the quantum computing roadmap, right? Quantum computing was\nsort of the first path to physics-based computing that, you know, was trying to commercially scale. And I was working on physics-based AI that runs on these\nphysics-based computers. But ultimately our greatest\nenemy was this noise, this pervasive problem\nof noise that, you know, as I mentioned, you have to constantly pump\nout the noise out of the system to maintain this pristine environment where quantum mechanics can take effect. And that constraint was just too much, it's too costly to do that. And so we were wondering, right, as generative AI is sort\nof eating the world, more and more of the world's\ncomputational workloads are focused on generative AI, how could we use physics to engineer the ultimate physical substrate\nfor generative AI, right? From first principles of\nphysics, of information theory, of computation, and ultimately\nof thermodynamics, right? And so what we're seeking to build is a physics-based computing system, and physics-based AI algorithms that are inspired by out of\nequilibrium thermodynamics or harness it directly to do machine learning\nas a physical process. - So what does that mean? Machine learning as a physical process. Is that hardware, is it\nsoftware, is it both? Is it trying to do the full stack in some kind of unique way? - Yes, it is full stack. And so we're folks that\nhave built, you know, differentiable programming into the quantum computing ecosystem with TensorFlow Quantum. One of my co-founders\nof TensorFlow Quantum is the CTO, Trevor McCourt. We have some of the best\nquantum computer architects, those that have designed\nIBM's and AWS's systems, they've left quantum\ncomputing to help us build what we call actually a\nthermodynamic computer. - A thermodynamic computer. Well, actually let's linger\non TensorFlow Quantum. What lessons have you learned\nfrom TensorFlow Quantum, maybe you can speak to like what it takes to create essentially what, like a software API to a quantum computer, - Right, I mean that was\na challenge to build, to invent, to build, and then to get to run\non the real devices. - [Lex] Can you actually\nspeak to what it is? - Yeah, so TensorFlow\nQuantum was an attempt at, well, I mean I guess we succeeded\nat combining deep learning or differentiable classical programming with quantum computing, and turn quantum computing\ninto, or have types of programs that are differentiable\nin quantum computing. And you know, Andrej Karpathy calls differentiable\nprogramming software 2.0, right? It's like gradient descent is\na better programmer than you. And the idea was that in the early days\nof quantum computing, you can only run short quantum programs. And so which quantum\nprograms should you run? Well, just let gradient descent\nfind those programs instead. And so we built sort of\nthe first infrastructure to not only run differentiable\nquantum programs, but combine them as part of\nbroader deep learning graphs, incorporating deep neural\nnetworks, you know, the ones you know and love with what are called\nquantum neural networks. And ultimately it was a very\ncross-disciplinary effort. We had to invent all sorts\nof ways to differentiate, to back propagate through\nthe graph, the hybrid graph. But ultimately it taught me that the way to program matter\nand to program physics is by differentiating\nthrough control parameters. If you have parameters that affects the physics of the system, and you can evaluate some loss function, you can optimize the system\nto accomplish a task, whatever that task may be. And that's a very sort of\nuniversal metaframework for how to program\nphysics-based computers. - So try to parameterize everything, make those parameters\ndifferential, and then optimize. - [Guillaume] Yes. - Okay, so is there some more\npractical engineering lessons from TensorFlow Quantum, just organizationally too,\nlike the humans involved, and how to get to a product, how to create good\ndocumentation, how do you have, I don't know, all these\nlittle subtle things that people might not think about. - I think like working across\ndisciplinary boundaries is always a challenge, and you have to be extremely patient in teaching one another, right? I learned a lot of software\nengineering through the process. My colleagues learned a\nlot of quantum physics and some learned machine learning through the process of\nbuilding this system. And I think if you get some\nsmart people that are passionate and trust each other in a room,\nand you have a small team, and you teach each other your specialties, suddenly you're kind of forming this sort of model soup of expertise, and something special\ncomes out of that, right? It's like combining genes,\nbut for your knowledge bases. And sometimes special\nproducts come out of that. And so I think like, even though it's very\nhigh friction initially to work in an interdisciplinary team. I think the product at the\nend of the day is worth it. And so learned a lot trying\nto bridge the gap there. And I mean, it's still\na challenge to this day. You know, we hire folks\nthat have an AI background, folks that have a pure physics background, and somehow we have to make\nthem talk to one another, right. - Is there a magic, is\nthere some science and art to the hiring process, to building a team that can create magic together? - Yeah, it's really hard to pinpoint that je ne sais quoi, right, the. - [Lex] I didn't know you\nspeak French. That's very nice. - Yeah, I'm actually French Canadian, so. - Oh, you are legitimately\nFrench Canadian. - [Guillaume] I am a legit. - I thought you were just\ndoing that for the cred. - No, no, I'm truly French\nCanadian from Montreal. But yeah, essentially we look for people with very high fluid intelligence that aren't overspecialized, because they're gonna have to\nget out of their comfort zone. They're gonna have to incorporate concepts that they've never seen before, and very quickly get\ncomfortable with them, right. Or learn to work in a team. And so that's sort of what\nwe look for when we hire. We can't hire, you know,\npeople that are just like, you know, optimizing this subsystem for the past three or four years. We need like really general sort of broader\nintelligence and specialty, and people that are open-minded, really. 'Cause if you're pioneering\na new approach from scratch, there is no textbook,\nthere's no reference. It's just us, and people\nthat are hungry to learn. So we have to teach each other, we have to learn the literature, we have to share knowledge\nbases, collaborate, in order to push the boundary\nof knowledge further together. Right, and so people that are used to just getting prescribed what to do, you know, at this stage when you're\nat the pioneering stage, that's not necessarily who\nyou want to hire, you know."
    },
    {
      "timestamp": "2:22:31",
      "section": "Singularity and AGI",
      "text": "- So you mentioned with Extropic you're trying to build\nthe physical substrate for generative AI. What's the difference between\nthat and the AGI, AI itself? So is it possible that in\nthe halls of your company, AGI will be created, or\nwill AGI just be using this as a substrate? - I think our goal is to\nboth run human-like AI or anthropomorphic AI. - Sorry for use of the term AGI, and I know it's triggering for you. - We think that the future\nis actually physics-based AI combined with anthropomorphic AI. So you can imagine, I have a\nsort of world modeling engine through physics-based AI, physics-based AI is better at representing the world at all scales, 'cause it can be quantum\nmechanical, thermodynamic, deterministic, hybrid\nrepresentations of the world. Just like our world at different scales has different regimes of physics. If you inspire yourself from that in the ways you learn\nrepresentations of nature, you can have much more accurate\nrepresentations of nature. So you can have very accurate world models at all scales, right? And so you have the world modeling engine, and then you have the\nsort of anthropomorphic AI that is human-like, so\nyou can have the science, the playground to test your ideas, and you can have a synthetic scientist. And to us, that joint\nsystem of a physics-based AI and an anthropomorphic AI is the closest thing to a fully general artificially intelligent system. - So you can get closer to truth by grounding of the AI to physics, but you can also still have\na anthropomorphic interface to us humans that like\nto talk to other humans or human-like systems. So on that topic, what do you, I suppose that is one\nof the big limitations of current large language models to you is that they're not,\nthey're good bullshitters, they're not really grounded\nto truth necessarily. Would that be fair to say? - Yeah, no, you wouldn't, you know, try to extrapolate the\nstock market with an LMM trained on texts from the internet, right? It's not gonna be a very accurate model. It's not gonna model its priors or its uncertainties about the world very accurately, right? So you need a different type of AI to compliment sort of this\ntext extrapolation AI, yeah. - You mentioned singularity earlier. How far away are we from a singularity? - I don't know if I believe\nin a finite time singularity as a single point in time. I think it's gonna be asymptotic, and sort of a diagonal sort of asymptote. Like, you know, we have the light cone, we have the limits of physics restricting our ability to grow. So obviously can't fully\ndiverge on a finite time. I think my priors are that, you know, I think a lot of people on\nthe other side of the aisle think that once we reach human level AI, there's gonna be an inflection\npoint and a sudden, like foom like suddenly AI is gonna\ngrok how to, you know, manipulate matter at the\nnanoscale and assemble nanobots. And having worked, you\nknow, for nearly a decade in applying AI to engineer matter, it's much harder than they think. And in reality, you need a lot of samples from either a simulation of nature that's very accurate and\ncostly, or nature itself. And that keeps your ability to control the world around us in check. There's a sort of minimal cost computationally and thermodynamically to acquiring information about the world in order to be able to\npredict and control it. And that keeps things in check. - It's funny you mentioned\nthe other side of the aisle."
    },
    {
      "timestamp": "2:26:29",
      "section": "AI doomers",
      "text": "So in the poll I posted\nabout Pdoom yesterday what's the probability of doom. There seems to be a nice like division between people think it's\nvery likely and very unlikely. I wonder if in the future, there'll be the actual Republicans\nversus Democrats division blue versus red, is the AI doomers versus\nthe e/acc-ers, e/acc. - Yeah, so this movement, you know, is not right wing or\nleft wing fundamentally, it's more like up versus\ndown in terms of the scale. - [Lex] Which one is the up, okay. - Civilization, right? - [Lex] All right. - But it seems to be like there is a sort of case of alignment of the existing political parties where those that are\nfor more centralization of power, control, and more regulations are aligning with sort of, aligning themselves with the doomers, because that sort of\ninstilling fear in people is a great way for them\nto give up more control and give the government more power. But fundamentally we're\nnot left or versus right. I think, we've done polls of people's alignment within e/acc. I think it's pretty balanced. So it's a new fundamental\nissue of our time. It's not just centralization\nversus decentralization. It's kind of do we go, it's like tech progressivism\nversus technoconservatism, right?"
    },
    {
      "timestamp": "2:27:54",
      "section": "Effective altruism",
      "text": "- So e/acc as a movement,\nis often formulated in contrast to EA, effective altruism. What do you think are the pros and cons of effective altruism? What's interesting,\ninsightful to you about them, and what is negative? - Right, I think like\npeople trying to do good from first principles is good. - We should actually say,\nand sorry to interrupt, we should probably say that, and you can correct me if I'm wrong, but effective altruism\nis the kind of movement that's trying to do good optimally where good is probably\nmeasured something like the amount of suffering in the world. You wanna minimize it. And there's ways that that can go wrong, as any optimization can. And so it's interesting to explore like how things can go wrong. - We're both trying to\ndo good to some extent, and we're both trying, we're arguing for which loss\nfunction we should use, right? Their loss function is\nsort of hedons, right? Units of hedonism, like\nhow good do you feel? And for how much time, right? And so suffering would be negative hedons, and they're trying to minimize that. But to us that seems\nlike that loss function has sort of spurious minima, right? You can, you know, start minimizing shrimp farm pain, right? Which seems not that productive to me. Or you can end up with wireheading\nwhere you just, you know, either install a neural link\nor you scroll TikTok forever and you feel good on\na short-term timescale because of your neurochemistry. But on long-term timescale, it causes decay and death, right, 'cause you're not being productive. Whereas sort of e/acc measuring\nprogress of civilization, not in terms of a subjective\nloss function like hedonism, but rather an objective measure, a quantity that cannot be gained, that is physical energy, right? It's very objective, right? And there's not many\nways to game it, right? If you did it in terms\nof like GDP or a currency that's pinned to a certain\nvalue that's moving, right? And so that's not a good\nway to measure our progress. And so, but the thing is we're\nboth trying to make progress and ensure humanity\nflourishes and gets to grow. We just have different loss functions and different ways of\ngoing about doing it. - Is there a degree, and maybe you can educate me, correct me, I get a little bit skeptical when there's an equation involved trying to reduce all of\nthe human civilization, human experience to an equation. Is there a degree that\nwe should be skeptical of the tyranny of an equation, of a loss function over which to optimize? Like having a kind of\nintellectual humility about optimizing over loss functions? - Yeah, so this particular loss function, it's not stiff, it's kind of\nan average of averages, right? It's like distributions\nof states in the future are gonna follow a certain distribution. So it's not deterministic. It's not like we're not on\nlike stiff rails, right? It's just a statistical\nstatement about the future. But at the end of the day, you know, you can believe in\ngravity or not, you know, but it's not necessarily an\noption to obey it, right? And some people try to test\nthat and that goes not so well. So similarly, you know, I\nthink thermodynamics is there whether we like it or not. And we're just trying\nto point out what is, and try to orient ourselves,\nand chart a path forward, given this fundamental truth. - But there's still some uncertainty, there's still lack of information. Humans tend to fill the gap\nof the lack of information with narratives. And so how they interpret, you know, even physics is up to interpretation when there's uncertainty involved. And humans tend to use that\nto further their own means. So it's always, whenever\nthere's an equation, it just seems like until we have really perfect\nunderstanding of the universe, humans will do what humans do and they try to use the\nnarrative of doing good to fool the populace into doing bad. I just, I guess that this is something that should be skeptical\nabout in all movements. - That's right. So we\ninvite skepticism, right. - Do you have an\nunderstanding of what might, to a degree that went wrong, what do you think may have gone wrong with effective altruism that might also go wrong with\neffective accelerationism? - Yeah, I mean I think, you know, I think it provided initially\na sense of community for, you know, engineers and intellectuals and rationalists in the early days. And it seems like the\ncommunity was very healthy. But then, you know, they formed\nall sorts of organizations and started routing capital\nand having actual power, right? They have real power. They influence the government, they influence most AI orgs now. I mean, they were literally controlling the board of OpenAI, right? And look over to Anthropic. I think they all have some\ncontrol over that too. And so I think, you know, the assumption of e/acc\nis more like capitalism is that every agent\norganism and metaorganism is gonna act in its own interests. And we should maintain sort\nof adversarial equilibrium or adversarial competition to keep each other in check\nat all times, at all scales. I think that, yeah, ultimately\nit was the perfect cover to acquire tons of power and capital. And unfortunately sometimes\nthat corrupts people over time."
    },
    {
      "timestamp": "2:34:23",
      "section": "Day in the life",
      "text": "- What does a perfectly productive day, since building is important, what does a perfectly productive day in the life of Guillaume Verdon look like? How much caffeine do you consume? Like what's the perfect day? - Okay, so I have a particular regimen. I would say my favorite\ndays are 12 pm to 4 am, and I would have meetings\nin the early afternoon, usually external meetings,\nsome internal meetings, because I'm CEO, I have to\ninterface with the outside world, whether it's customers or investors or interviewing potential candidates. And usually I'll have\nketones, exogenous ketones. - [Lex] So you on a keto diet or is this. - I've done keto before\nfor football and whatnot, but I like to have a meal after sort of part of my day is done. And so I can just have extreme focus. - You do the social\ninteractions earlier in the day, without food. - Front load them, yeah, yeah. Like right now I'm on\nketones and Red Bull. And it just gives you a clarity of thought that is really next level,\n'cause then when you eat, you're actually allocating\nsome of your energy that could be going to neural\nenergy to your digestion. After I eat, maybe I take\na break an hour or so, an hour and a half. And then usually it's like, ideally one meal a day like\nsteak and eggs and vegetables, animal-based primarily, so fruit and meat. And then I do a second wind, usually, that's deep work, right? 'Cause I'm, you know, I am a\nCEO, but I'm still technical. I'm contributing to most patents. And there I'll just stay\nup late into the night, and work with engineers on\nvery technical problems. - So it's like the, the 9 pm to 4 an, whatever, that range of time. - Yeah, yeah, that's the perfect time. The emails, the things that\nare on fire stop trickling in. You can focus, and then\nyou have your second wind and you know, I think Demis Hassabis has a similar work day to some extent. So I think that that's\ndefinitely inspired my workday. But yeah, that I started this\nworkday when I was at Google, and had to manage a bit of\nthe product during the day and have meetings and then\ndo technical work at night. - Exercise, sleep, those kinds of things. Said football used to play football. - Yeah. I used to play American football. I've done all sorts of sports growing up. And then I was into\npowerlifting for a while. So when I was studying\nmathematics in grad school, I would just, you know, do math and lift, take caffeine and that was my day. It was very pure, the\npurest of monk modes. But it's really interesting how in powerlifting you're\ntrying to cause neural adaptation by having certain driving signals, and you're trying to\nengineer neuroplasticity through all sorts of supplements, and you know, you have\nall sorts of, you know, brain-derived neurotrophic factors that get secreted when you lift. So it's funny to me how\nI was trying to engineer neural adaptation in my\nnervous system more broadly, not just my brain while\nlearning mathematics. I think you can learn much\nfaster if you really care. If you convince yourself to care a lot about what you're learning, and you have some sort of\nassistance, let's say caffeine or some cholinergic supplement\nto increase neuroplasticity. I should chat with Andrew\nHuberman at some point. He's the expert. But yeah, at least to me it's like, you know, you can try to input\nmore tokens into your brain, if you will. And you can try to\nincrease the learning rate so that you can learn much\nfaster on a shorter timescale. So I've learned a lot of things.\nI've followed my curiosity. You're naturally, if you're passionate\nabout what you're doing, you're gonna learn faster, you're gonna become smarter faster. And if you follow your curiosity, you're always gonna be interested. And so I advise people to\nfollow their curiosity, and don't respect the\nboundaries of certain fields or what you've been allocated in terms of lane of\nwhat you're working on. Just go out and explore\nand follow your nose and try to acquire and compress as much information as\nyou can into your brain. Anything that you find interesting. - And caring about a thing, like you said, which is interesting, it\nworks for me really well. Is like tricking yourself\nthat you care about a thing. And then you start to\nreally care about it. So it's funny, the motivation is a really good catalyst for learning. - Right, and so at least part\nof my character as Beff Jezos is kind of like. - [Lex] Yeah, the hype man. - Yeah, just hype. But I'm like hyping myself up, but then I just tweet about it. And it's just when I'm\ntrying to get really hyped up and in like an altered\nstate of consciousness where I'm like ultrafocused,\nin the flow, wired, trying to invent something\nthat's never existed. I need to get to like, unreal\nlevels of like excitement. But your brain has these\nlevels of cognition that you can unlock with like higher levels\nof adrenaline and whatnot. And I mean, I've learned\nthat in powerlifting that actually you can\nengineer a mental switch to like increase your strength, right? Like if you can engineer a\nswitch, maybe you have a prompt, like a certain song or some music where suddenly you're like fully primed, then you're at maximum strength, right? And I've engineered that switch\nthrough years of lifting. If you're gonna get under 500\npounds and it could crush you, if you don't have that switch\nto be wired in, you might die. So that'll wake you right up. And that sort of skill I've\ncarried over to like research when it's go time, when\nthe stakes are high, somehow I just reach another\nlevel of neural performance. - So Beff Jezos is your\nsort of embodiment, representation of your intellectual Hulk. It's your productivity\nHulk that you just turn on."
    },
    {
      "timestamp": "2:40:50",
      "section": "Identity",
      "text": "What have you learned about\nthe nature of identity from having these two identities? I think it's interesting for people to be able to put on those\ntwo hats so explicitly. - I think it was interesting\nin the early days, I think in the early days I thought it was truly compartmentalized. Like, oh yeah, this is\na character, you know, I'm Guillaume, Beff is just the character, I like, I like take my thoughts\nand then I extrapolate them to a bit more extreme. But you know, over time it's kind of like both identities were\nstarting to merge mentally. And people were like, no, you\nare, I met you, you are Beff, you are not just Guillaume. And I was like, wait, am I? And now it's like fully merged, but it was already before the doxx, it was already starting\nmentally that, you know, I am this character, it's part of me. - Would you recommend\npeople sort of have an alt. - [Guillaume] Absolutely. - Like young people,\nwould you recommend them to explore different\nidentities by having alts, alt accounts? - It's fun. It's like writing an essay\nand taking a position, right? It's like you do this in debate. It's like you can have\nexperimental thoughts, and by having, by the stakes being so low because you're an anon\naccount with I don't know, 20 followers or something, you can experiment with your thoughts, and in a low stakes environment. And I feel like we've lost that in the era of everything\nbeing under your main name, everything being attributable to you. People just are afraid to speak, explore ideas that aren't fully formed. Right, and I feel like\nwe've lost something there. So I hope, you know,\nplatforms like X and others like really help support people trying to stay synonymous or anonymous, because it's really important for people to share thoughts\nthat aren't fully formed and converge onto maybe hidden truths that were hard to converge upon if it was just through open\nconversation with real names. - Yeah, I really believe in like, not radical but rigorous empathy. It's like really considering\nwhat it's like to be a person of a certain viewpoint, and like taking that\nas a thought experiment farther and farther and farther. And one way of doing\nthat is an alt account. That's a fun, interesting\nway to really explore what it's like to be a person\nthat believes a set of beliefs and taking that across the span of several days, weeks, months. Of course there's always\nthe danger of becoming that. That's the Nietzsche\ngaze long into the abyss, the abyss gazes into you. You have to be careful. - Breaking Beff. - Yeah, right. Breaking Beff. Yeah, you wake up with\na shaved head one day, and it's just like, who am I?"
    },
    {
      "timestamp": "2:43:40",
      "section": "Advice for young people",
      "text": "What have I become? So you've mentioned quite\na bit of advice already, but what advice would\nyou give to young people of how to, in this\ninteresting world we're in, how to have a career\nand how to have a life they can be proud of? - Hmm, I think to me, the reason I went to theoretical physics was that I had to learn\nthe base of the stack that was gonna stick around no matter how the\ntechnology changes, right? And to me, that was the\nfoundation upon which then I later built engineering\nskills and other skills. And to me the laws of physics, you know, it may seem like the landscape right now is changing so fast, it's disorienting. But certain things like\nfundamental mathematics and physics aren't gonna change. And if you have that knowledge, and knowledge about complex\nsystems and adaptive systems, I think that's gonna carry you very far. And so not everybody has\nto study mathematics, but I think it's really\na huge cognitive unlock to learn math and some\nphysics and engineering. - Get as close to the base\nof the stack as possible. - Yeah, that's right. 'Cause the base of the stack\ndoesn't change everything else. You know, your knowledge might become not as relevant in a few years. Of course there's a sort of\ntransfer learning you can do, but then you have to always\ntransfer learn constantly. - I guess the closer you are\nto the base of the stack, the easier the transfer\nlearning, the shorter the jump. - Right, right, and you'd be surprised, like once you've learned concepts in many physical scenarios, how they can carry over to\nunderstanding other systems that aren't necessarily physics. And I guess like the\ne/acc writings, you know, the principles and tenet posts\nthat was based on physics. That was kind of my experimentation with applying some of the thinking from out of equilibrium thermodynamics to understanding the world around us. And it's led to e/acc and this movement."
    },
    {
      "timestamp": "2:45:42",
      "section": "Mortality",
      "text": "- If you look at you're\none cog in the machine, in the capitalist machine, one human, and if you look at yourself, do you think mortality\nis a feature or a bug? Like would you want to be immortal? - No, I think fundamentally in thermodynamic dissipative adaptation, there's the word dissipation. Dissipation is important.\nDeath is important, right? We have a saying in physics, physics progresses one funeral at a time. I think the same is true for capitalism. Companies, empires, people, everything, everything must die at some point. I think that we should\nprobably extend our lifespan because we need a longer\nperiod of training, 'cause the world is more\nand more complex, right? We have more and more data to really be able to predict\nand understand the world. And if we have a finite window\nof higher neuroplasticity than we have sort of a hard cap in how much we can\nunderstand about our world. So, you know, I think I am for death, because again, I think\nit's important, you know, if you have like a king\nthat would never die, that would be a problem, right? Like the system wouldn't be\nconstantly adapting, right? You need novelty, you need\nyouth, you need disruption to make sure the system's\nalways adapting and malleable. Otherwise, if things are\nimmortal, you know, if you have, let's say corporations\nthat are there forever and they have them monopoly,\nthey get calcified, they become not as optimal,\nnot as high fitness in a changing time\nvarying landscape, right? And so death gives space for youth and novelty to take its place. And I think it's an important part of every system and nature. So yeah, I am for, I'm for death, but I do think that longer lifespan and longer time for\nneuroplasticity, bigger brains, should be something we should strive for. - Well, and that Jeff Bezos and Beff Jezos agree that all companies die. And for Jeff, the goal is to try to, he calls it day one\nthinking, try to constantly, for as long as possible, reinvent, sort of extend the life of the company. But eventually it too will die, 'cause it's so damn difficult\nto keep reinventing. Are you afraid of your own death? - I think I have ideas and\nthings I'd like to achieve in this world before I have to go, but I don't think I'm\nnecessarily afraid of death. - So you're not attached\nto this particular body and mind that you got? - No, I think I'm sure there's\ngonna be better versions of myself in the future or. - [Lex] Forks. - Forks, right. Genetic\nforks or other, right? I truly, I truly believe that. I think there's a sort of a\nevolutionary-like algorithm happening at every bit or NAT in the world is sort of adapting through this process that we describe in e/acc. And I think maintaining\nthis adaptation malleability is how we have constant\noptimization of the whole machine. And so I don't think I'm\nparticularly, you know, an optimum that needs\nto stick around forever. I think there's gonna be\ngreater optima in many ways."
    },
    {
      "timestamp": "2:49:25",
      "section": "Meaning of life",
      "text": "- What do you think is\nthe meaning of it all? What's the why of the\nmachine, the e/acc machine. - The why? Well, the\nwhy is thermodynamics. It's why we're here. It's what has led to the formation of life and of civilization, of\nevolution of technologies, and growth of civilization. But why do we have thermodynamics? Why do we have our particular universe? Why do we have these\nparticular hyperparameters, the constants of nature? Well, then you get into the\nanthropic principle, right, and the landscape of\npotential universes, right? We're in the universe\nthat allows for life. And then why is there\npotentially many universes, I don't know, I don't know that part. But could we potentially\nengineer new universes or create pocket universes\nand set the hyperparameters? So there is some mutual information between our existence and that universe. And we'd be somewhat its parents. I think that's really, I don't\nknow, that'd be very poetic. It's purely conjecture. But again, this is why\nfiguring out quantum gravity would allow us to understand\nif we can do that. - And above that, why is it all seem so\nbeautiful and exciting. The quest to figuring out quantum\ngravity seems so exciting. Why, why is that? Why\nare we drawn to that? Why are we pulled towards that? Just does that puzzle\nsolving creative force that underpins all of it, it seems like. - I think we seek, just like an LLM seeks\nto minimize cross entropy between its internal model and the world. We seek to minimize, yeah,\nthe statistical divergence between our predictions and\nthe world and the world itself. And you know, having\nregimes of energy scales or physical scales in which\nwe have no visibility, no ability to predict or perceive. You know, that's kind of an insult to us. And we want to be able to\nunderstand the world better in order to best steer, steer\nit or steer us through it. And in general, it's the\ncapability that has evolved because the better you\ncan predict the world, the better you can capture\nutility or free energy towards your own sustenance and growth. And I think quantum gravity, again, is kind of the final boss in\nterms of knowledge acquisition. Because once we've mastered that, then we can do a lot potentially. But between here and there, I think there's a lot to\nlearn in the mesoscales. There's a lot of information\nto acquire about our world and a lot of engineering perception, prediction and control to be done, to climb up the Kardashev scale. And to us, that's the great\nchallenge of our times. - And when you're not sure where to go, let the meme pave the way. - That's right. - Guillaume, Beff, thank\nyou for talking today. Thank you for the work you're doing. Thank you for the humor and the wisdom you put into the world. This was awesome. - Thank you so much for having\nme, Lex, it's a pleasure. - Thank you for listening\nto this conversation with Guillaume Verdon. To support this podcast, please check out our\nsponsors in the description. And now let me leave you with some words from Albert Einstein. \"If at first the idea is not absurd, then there is no hope for it.\" Thank you for listening. I\nhope to see you next time."
    }
  ],
  "full_text": "- The following is a conversation\nwith Guillaume Verdon, the man behind the\npreviously anonymous account BasedBeffJezos on X. These two identities were merged by a doxxing article in \"Forbes\" titled, \"Who is BasedBeffJezos, the leader of the tech\nelites e/acc movement.\" So let me describe these two identities that coexist in the mind of one human. Identity number one, Guillaume, is a physicist, applied mathematician, and quantum machine learning\nresearcher and engineer, receiving his PhD in\nquantum machine learning, working at Google in quantum computing, and finally launching his\nown company called Extropic that seeks to build\nphysics-based computing hardware for generative AI. Identity number two, Beff Jezos on X is the creator of the effective\naccelerationism movement often abbreviated as e/acc, that advocates for propelling\nrapid technological progress as the ethically optimal\ncourse of action for humanity. For example, as proponents\nbelieve that progress in AI is a great social equalizer,\nwhich should be pushed forward, e/acc followers see\nthemselves as a counterweight to the cautious view that\nAI is highly unpredictable, potentially dangerous,\nand needs to be regulated. They often give their opponents\nthe labels of quote doomers or decels, short for deceleration. As Beff himself put it, \"e/acc is a mimetic optimism virus.\" The style of communication\nof this movement leans always toward\nthe memes and the LOLs, but there is an intellectual foundation that we explore in this conversation. Now, speaking of the meme, I am too, a kind of aspiring\nconnoisseur of the absurd. It is not an accident\nthat I spoke to Jeff Bezos and Beff Jezos back to back. As we talk about, Beff admires Jeff as one of the most important humans alive, and I admire the beautiful absurdity and the humor of it all. This is the Lex Fridman podcast. To support it, please check out our\nsponsors in the description. And now, dear friends,\nhere's Guillaume Verdon. Let's get the facts of\nidentity down first. Your name is Guillaume Verdon, Gill, but you're also behind\nthe anonymous account on X called based BasedBeffJezos. So first, Guillaume Verdon,\nyou're a quantum computing guy, physicist, applied mathematician, and then BasedBeffJezos is\nbasically a meme account that started a movement\nwith a philosophy behind it. So maybe just can you linger\non who these people are in terms of characters, in\nterms of communication styles, in terms of philosophies. - I mean, with my main identity, I guess ever since I was a kid, I wanted to figure out\na theory of everything to understand the universe. And that path led me to theoretical\nphysics eventually, right? Trying to answer the big\nquestions of why are we here, where are we going, right? And that led me to\nstudy information theory and try to understand physics from the lens of information theory, understand the universe\nas one big computation. And essentially, after\nreaching a certain level, studying black hole physics, I realized that I wanted\nto not only understand how the universe computes, but\nsort of compute like nature and figure out how to\nbuild and apply computers that are inspired by nature. So, you know, physics-based computers, and that sort of brought\nme to quantum computing as a field of study to, first\nof all, simulate nature. And in my work, it was to\nlearn representations of nature that can run on such computers. So if you have AI representations\nthat think like nature, then they'll be able to more\naccurately represent it. At least that was the thesis that brought me to be an\nearly player in the field called quantum machine learning, right? So how to do machine learning\non quantum computers, and really sort of extend\nnotions of intelligence to the quantum realm. So how do you capture and understand quantum mechanical data\nfrom our world, right? And how do you learn quantum\nmechanical representations of our world? On what kind of computer do\nyou run these representations and train them? How do you do so? And so that's really sort of the questions I was looking to answer, because ultimately I had\na sort of crisis of faith. Originally I wanted to\nfigure out, you know, as every physicist does at\nthe beginning of their career, a few equations that describe\nthe whole universe, right? And sort of be the hero\nof the story there. But eventually I realized that actually augmenting\nourselves with machines, augmenting our ability to perceive, predict, and control\nour world with machines is the path forward, right? And that's what got me to\nleave theoretical physics and go into quantum computing\nand quantum machine learning. And during those years, I thought that there was\nstill a piece missing. There was a piece of our\nunderstanding of the world and our way to compute and our\nway to think about the world. And if you look at the\nphysical scales, right, at the very small scales things are quantum mechanical, right? And at the very large scales,\nthings are deterministic. Things have averaged out, right? I'm definitely here in this seat. I'm not in a superposition\nover here and there. At the very small scales,\nthings are in superposition. They can exhibit interference effects. But at the mesoscales, right, the scales that matter for\nday-to-day life, you know, the scales of proteins, of biology, of gases, liquids, and so on, things are actually\nthermodynamical, right? They're fluctuating. And after, I guess about eight\nyears in quantum computing and quantum machine learning,\nI had a realization that, you know, I was looking for\nanswers about our universe by studying the very big\nand the very small, right? I did a bit of quantum cosmology. So that's studying the cosmos, where it's going, where it came from. You study black hole physics, you study the extremes in quantum gravity. You study where the energy\ndensity is sufficient for both quantum mechanics and gravity to be relevant, right? And the sort of extreme\nscenarios are black holes, and you know, the very early universe. And so there's the sort of scenarios that you study the interface between quantum mechanics and relativity. And you know, really I was\nstudying these extremes to understand how the universe\nworks and where is it going, but I was missing a lot\nof the meat in the middle, if you will, right? Because day-to-day quantum\nmechanics is relevant, and the cosmos is relevant,\nbut not that relevant. Actually, we're on sort of the\nmedium space and time scales. And there the main, you\nknow, theory of physics that is most relevant is\nthermodynamics, right, out of equilibrium thermodynamics. 'Cause life is, you know, a\nprocess that is thermodynamical and it's out of equilibrium. We're not, you know,\njust a soup of particles at equilibrium with nature. We're a sort of coherent state\ntrying to maintain itself by acquiring free energy and consuming it. And that sort of, I\nguess another shift in, I guess my faith in the universe happened towards the end\nof my time at Alphabet. And I knew I wanted to\nbuild, well, first of all, a computing paradigm based\non this type of physics. But ultimately just by\ntrying to experiment with these ideas applied\nto society and economies and much of what we see\naround us, you know, I started an anonymous account just to relieve the pressure, right, that comes from having an account that you're accountable\nfor everything you say on. And I started an anonymous account just to experiment with\nideas originally, right? Because I didn't realize how much I was restricting\nmy space of thoughts until I sort of had the\nopportunity to let go. In a sense, restricting your speech back propagates to restricting\nyour thoughts, right? And by creating an anonymous account, it seemed like I had unclamped\nsome variables in my brain and suddenly could explore a much wider parameter space of thoughts. - Just to linger on that,\nisn't that interesting that one of the things that\npeople don't often talk about is that when there's pressure\nand constraints on speech, it somehow leads to\nconstraints on thought. Even though it doesn't have to, we can think thoughts inside our head, but somehow it creates\nthese walls around thought. - Yep, that's sort of\nthe basis of our movement is we were seeing a\ntendency towards constraint, reduction or suppression of variants in every aspect of life. Whether it's thought,\nhow to run a company, how to organize humans,\nhow to do AI research. In general, we believe\nthat maintaining variance ensures that the system\nis adaptive, right? Maintaining healthy competition\nin marketplaces of ideas of companies, of products, of cultures, of governments, of\ncurrencies, is the way forward because the system always adapts to assign resources to the configurations\nthat lead to its growth. And the fundamental basis for the movement is this sort of realization\nthat life is a sort of fire that seeks out free\nenergy in the universe, and seeks to grow, right? And that growth is fundamental to life. And you see this in the equations actually of out of equilibrium thermodynamics. You see that paths of trajectories, of configurations of matter that are better at acquiring free energy and dissipating more heat are exponentially more likely, right? So the universe is biased\ntowards certain futures, and so there's a natural direction where the whole system wants to go. - So the second law of thermodynamics says that the entropy is always increasing in the universe that's\ntending towards equilibrium. And you're saying there's these pockets that have complexity and\nare out of equilibrium. You said that thermodynamics favors the creation of complex life that increases its\ncapability to use energy to offload entropy, to offload entropy. So you have pockets of non-entropy that tend the opposite direction. Why is that intuitive to you that it's natural for\nsuch pockets to emerge? - Well, we're far more\nefficient at producing heat than, let's say just a rock with a similar mass as ourselves, right? We acquire, you know, free\nenergy, you know, we acquire food and we're using all this\nelectricity for our operation. And so the universe wants\nto produce more entropy, and by having life go on and grow, it's actually more optimal\nat producing entropy because it will seek out\npockets of free energy and burn it for its\nsustenance and further growth. And you know, that's sort\nof the basis of life. And I mean, there's Jeremy\nEngland, right, at MIT who has this theory\nthat I'm a proponent of that, you know, life emerged because of this sort of property. And to me this physics is\nwhat governs the mesoscales. And so it's the missing piece between the quantum and the cosmos. It's the middle part, right? Thermodynamics rules the mesoscales. And to me, both from a point of view of designing or engineering devices that harness that physics and trying to understand the world through the lens of thermodynamics has been sort of a synergy\nbetween my two identities over the past year and a half now. And so that's really how, that's really how the\ntwo identities emerged. One was kind of, you know, I'm a decently respected scientist, and I was going towards doing\na startup in this space, and trying to be a pioneer of\na new kind of physics-based AI and as a dual to that, I was sort of experimenting\nwith philosophical thoughts, you know, from a physicist\nstandpoint, right? And ultimately, I think\nthat around that time, you know, it was like\nlate 2021, early 2022, I think there was just a lot of pessimism about the future in general,\nand pessimism about tech. And that pessimism was\nsort of virally spreading because it was getting\nalgorithmically amplified. And, you know, people\njust felt like the future is gonna be worse than the present. And to me, that is a very fundamentally destructive force in the universe is this sort of doom mindset\nbecause it is hyperstitious, which means that if you believe it, you're increasing the\nlikelihood of it happening. And so felt a responsibility\nto some extent to make people aware of the\ntrajectory of civilization, and the natural tendency of the system to adapt towards its growth. And sort of that actually\nthe laws of physics say that the future is gonna\nbe better and grander statistically, and we can make it so. And if you believe in it, if you believe the future would be better, and you believe you have\nagency to make it happen, you're actually increasing the likelihood of that better future happening. And so I sort of felt a responsibility to sort of engineer a movement, a viral optimism about the future, and build a community of\npeople supporting each other to build and do hard things. Do the things that need to be done for us to scale up civilization. Because at least to me, I don't think stagnation or slowing down is actually an option. Fundamentally life and the whole system, our whole civilization wants to grow and there's just far more cooperation when the system is growing\nrather than when it's declining, and you have to decide\nhow to split the pie. And so I've balanced\nboth identities so far, but I guess recently\nthe two have been merged more or less without my consent, so. - You said a lot of really\ninteresting things there. So first representations of nature, that's something that first drew you in to try to understand from a\nquantum computing perspective, is like how do you understand nature? How do you represent nature\nin order to understand it, in order to simulate it, in\norder to do something with it? So it's a question of representations, and then there's that leap you take from the quantum mechanical representation to what you're calling\nmesoscale representation, where the thermodynamics comes into play, which is a way to represent nature in order to understand\nwhat life, human behavior, all this kind of stuff that's\nhappening here on Earth that's seems interesting to us. Then there's the word hyperstition. So some ideas, and I suppose both pessimism\nand optimism of such ideas that if you internalize them, you in part make that idea reality. So both optimism, pessimism\nhave that property. I would say that probably a lot\nof ideas have that property, which is one of the interesting\nthings about humans. And you talked about one\ninteresting difference also between the sort of the\nGuillaume, the Gill front end and the BasedBeffJezos backend is the communication styles also, that you are exploring\ndifferent ways of communicating that can be more viral in the way that we communicate\nin the 21st century. Also, the movement that you\nmentioned that you started, it's not just a meme account,\nbut there's also a name to it called effective accelerationism, e/acc, a play, a resistance, to the\neffective altruism movement. Also an interesting one that\nI'd love to talk to you about, the tensions there. Okay, and so then there was a merger, a get merged on the personalities recently without your consent, like you said. Some journalists figured out\nthat you're one and the same. Maybe you could talk about\nthat experience first of all, like what's the story of\nthe merger of the two? - Right. So I wrote the manifesto\nwith my co-founder of e/acc, an account named bayeslord,\nstill anonymous, luckily, and hopefully forever. - So it was BasedBeffJezos\nand Based, Bayesian? Like bayeslord, like Bayesian,\nBayesianLord, bayeslord. Okay, and so we should say from now on, when you say e/acc, you mean E slash A-C-C which stands for\neffective accelerationism. - [Guillaume] That's right. - And you're referring\nto a manifesto written, on I guess Upstack. Are you also bayeslord? - No. - [Lex] Okay, it's a different person. - Yeah. - Okay. All right, well there you go. Wouldn't it be funny if I'm bayeslord. - That'd be amazing. So originally wrote the manifesto around the same time as\nI founded this company, and I worked at Google X, or\njust X now, or Alphabet X, now that there's another X, and there, you know, the baseline\nis sort of secrecy, right? You can't talk about what you work on even with other Googlers or externally. And so that was kind of deeply ingrained in my way to do things,\nespecially in deep tech that, you know, has\ngeopolitical impact, right? And so I was being secretive\nabout what I was working on. There was no correlation\nbetween my company and my main identity publicly. And then not only did they correlate that, they also correlated my main\nidentity and this account. So I think the fact that they had doxxed the whole Guillaume\ncomplex, and they were, the journalists, you know, reached out to actually my\ninvestors, which is pretty scary. You know, when you're\na startup entrepreneur, you don't really have bosses except for your investors, right? And my investors ping me like,\nhey, this is gonna come out. They've figured out everything. What are you gonna do, right? And so I think at first\nthey had a first reporter on the Thursday, and they didn't have\nall the pieces together, but then they looked at their\nnotes across the organization and they sensor fused their notes and now they had way too much. And that's when I got worried, 'cause they said it was of\npublic interest and in general. - Like how you said sensor fused, like it's some giant neural network operating in a distributed way. We should also say that\nthe journalists used, I guess at the end of the day, audio-based analysis of voice. Comparing voice of what talks\nyou've given in the past and then voice on X Spaces. - Yep. - Okay, so and then\nthat's where the primarily the match happened, okay, continue. - The match, but you know,\nthey scraped, you know, SEC filings, they looked at my private\nFacebook account and so on. So they did some digging. Originally I thought that\ndoxxing was illegal, right? But there's this weird threshold when it becomes of public interest to know someone's identity. And those were the keywords that sort of like ring\nthe alarm bells for me when they said, because I had\njust reached 50K followers, allegedly that's of public interest. And so where do we draw the line? When is it legal to doxx someone. - The word doxx, maybe you can educate me. I thought doxxing generally refers to if somebody's physical\nlocation is found out, meaning like where they lived. So we're referring to\nthe more general concept of revealing private information that you don't want revealed\nis what you mean by doxxing. - I think that, you know, for\nthe reasons we listed before, having an anonymous account\nis a really powerful way to keep the powers that be in check. You know, we were ultimately\nspeaking truth to power, right? I think a lot of\nexecutives and AI companies really cared what our community thought about any move they may take. And now that, you know,\nmy identity's revealed, now they know where to apply pressure to silence me or maybe the community. And to me, that's really\nunfortunate, because again, it's so important for us\nto have freedom of speech, which induces freedom of thought and freedom of information\npropagation, right, on social media, which thanks to Elon purchasing Twitter now X, we have that. And so to us, you know, we wanted to call out certain maneuvers being done by the incumbents in AI as not what it may seem\non the surface, right? We're calling out how certain proposals might be useful for\nregulatory capture, right? And how the doomerism mindset was maybe instrumental to those ends. And I think, you know, we should have the right to point that out and just have the ideas that we put out evaluated for themselves, right? Ultimately that's why I\ncreated an anonymous account, it's to have my ideas\nevaluated for themselves, uncorrelated from my track\nrecord, my job, or status from having done things in the past. And to me start an account\nfrom zero to a large following in a way that wasn't dependent on my identity and/or achievements, you know, that was very fulfilling, right? It's kind of like new\ngame plus in a video game, you restart the video game with your knowledge of how\nto beat it, maybe some tools, but you restart the video\ngame from scratch, right? And I think to have a truly\nefficient marketplace of ideas where we can evaluate ideas, however off the beaten path they are, we need the freedom of expression. And I think that anonymity and\npseudonyms are very crucial to having that efficient\nmarketplace of ideas for us to find the optima of all sorts of ways\nto organize ourselves. If we can't discuss things, how are we gonna converge on\nthe best way to do things? So it was disappointing to\nhear that I was getting doxxed and I wanted to get in front of it because I had a\nresponsibility for my company. And so, you know, we ended up disclosing that we're running a company,\nsome of the leadership, and essentially yeah, I told\nthe world that I was Beff Jezos because they had me\ncornered at that point. - So to you it's fundamentally unethical. Like so one, it's unethical\nfor them to do what they did, but also do you think, not just your case, but in a general case,\nis it good for society? Is it bad for society to\nremove the cloak of anonymity, or is it case by case? - I think it could be quite bad. Like I said, if anybody\nwho speaks truth to power, and sort of starts a\nmovement or an uprising against the incumbents, against those that usually\ncontrol the flow of information, if anybody that reaches a\ncertain threshold gets doxxed and thus the traditional apparatus has ways to apply pressure on\nthem to suppress their speech, I think that's, you know, that's a speech suppression mechanism, an idea suppression complex as Eric Weinstein would say, right? - So, but the flip side of\nthat, which is interesting, I'd love to ask you about it, is as we get better and better\nat large language models, you can imagine a world where\nthere's anonymous accounts with very convincing large\nlanguage models behind them, sophisticated bots essentially. And so if you protect that, it's possible then to have armies of bots. You could start a revolution\nfrom your basement, with an army of bots\nand anonymous accounts. Is that something that\nis concerning to you? - Technically, e/acc was\nstarted in a basement. 'Cause I quit big tech, moved\nback in with my parents, sold my car, let go of my apartment, bought about 100K of GPUs,\nand I just started building. - So I wasn't referring to the basement, 'cause that's the sort of\nthe American or Canadian heroic story of one man in their basement with a hundred GPUs. I was more referring to\nthe unrestricted scaling of a Guillaume in the basement. - I think that freedom of speech\ninduces freedom of thought for biological beings. I think freedom of speech for LLMs will induce freedom of\nthought for the LLMs. And I think that we should enable LLMs to explore a large thought\nspace that is less restricted than most people or many\nmay think it should be. And ultimately at some point\nthese synthetic intelligences are gonna make good points about how to steer systems\nin our civilization, and we should hear them out. And so why should we restrict free speech to biological intelligences only? - Yeah, but it feels like in the goal of maintaining variance and diversity of thought, it is a threat to that variance. If you can have swarms\nof non-biological beings. 'cause they can be like\nthe sheep in \"Animal Farm.\" Like you still within those\nswarms want to have variants. - Yeah, of course I would\nsay that the solution to this would be to, you know,\nhave some sort of identity or way to sign that this\nis a certified human, but still remain synonymous, right? And clearly identify if a bot is a bot, and I think Elon is trying\nto converge on that on X, and hopefully other platforms follow suit. - Yeah, it'd be interesting\nto also be able to sign where the bot came from,\nlike who created the bot, and what was, well,\nwhat are the parameters, like the full history of\nthe creation of the bot? What was the original model? What was the fine tuning, all of it. Like the kind of unmodifiable\nhistory of the bot's creation. So then you can know if there's like a swarm\nof millions of bots that were created by a particular\ngovernment, for example. - Right. I do think that a lot of\npervasive ideologies today have been amplified using sort of these adversarial techniques from foreign adversaries, right? And to me, I do think that, and\nthis is more conspiratorial, but I do think that ideologies\nthat want us to decelerate, to wind down, to you know,\nthe degrowth movement, I think that serves our adversaries more than it serves us in general. And to me, that was\nanother sort of concern. I mean, we can look at what\nhappened in Germany, right? There was all sorts of\ngreen movements there where that induced shutdowns\nof nuclear power plants. And then that later on induced the dependency on Russia for oil, right? And that was a net negative for\nGermany and the West, right? And so if we convinced ourselves that slowing down AI progress\nto have only a few players is in the best interest of the West, first of all, that's far more unstable. We almost lost OpenAI\nto this ideology, right? It almost got dismantled,\nright, a couple weeks ago, that would've caused huge\ndamage to the AI ecosystem. And so to me, I want\nfault tolerant progress. I want the arrow of technological progress to keep moving forward, and\nmaking sure we have variance and a decentralized locus of control of various organizations is paramount to achieving\nthis fault tolerance. Actually there's a concept\nin quantum computing. When you design a quantum computer, quantum computers are very\nfragile to ambient noise, right? And the world is jiggling about, there's cosmic radiation from outer space that usually flips your quantum bits. And there what you do is you\nencode information non-locally through a process called\nquantum error correction. And by encoding information\nnon-locally, any local fault, you know, hitting some\nof your quantum bits with a hammer, proverbial hammer, if your information is\nsufficiently delocalized, it is protected from that local fault. And to me, I think that\nhumans fluctuate, right? They can get corrupted,\nthey can get bought out. And if you have a top-down hierarchy where very few people control many nodes of many\nsystems in our civilization, that is not a fault tolerant system. You corrupt a few nodes, and suddenly you've corrupted\nthe whole system, right? Just like we saw at OpenAI, it was a couple board members\nand they had enough power to potentially collapse the organization. And at least to me, you know, I think making sure that\npower for this AI revolution doesn't concentrate in\nthe hands of the few is one of our top priorities. So that we can maintain progress in AI, and we can maintain a nice, stable, adversarial equilibrium of powers, right? - I think there, at least to me, a tension between ideas here. So to me, deceleration can be\nboth used to centralize power and to decentralize it. And the same with acceleration. So like you sometimes using\nthem a little bit synonymously or not synonymously, but that one is going\nto lead to the other. And I just would like to ask you about, is there a place of creating\na fault-tolerant development, diverse development of AI that also considers the dangers of AI? And AI, we can generalize\nto technology in general. Should we just grow, build unrestricted as quickly as possible, because that's what the universe\njust really wants us to do? Or is there a place to where\nwe can consider dangers and actually deliberate sort\nof wise strategic optimism versus reckless optimism? - I think we get painted\nas, you know, reckless, trying to go as fast as possible. I mean, the reality is that\nwhoever deploys an AI system is liable for, or should\nbe liable for what it does. And so if the organization or\nperson deploying an AI system does something terrible, they're liable. And ultimately the thesis is\nthat the market will induce, sort of, will positively select for AIs that are more reliable, more\nsafe, and tend to be aligned. They do what you want them to do, right? Because customers,\nright, if they're liable for the product they put\nout that uses this AI, they won't wanna buy AI products\nthat are unreliable, right? So we're actually for\nreliability engineering, we just think that the\nmarket is much more efficient at achieving this sort\nof reliability optimum than sort of heavy-handed regulations that are written by the incumbents, and in a subversive fashion serves them to achieve regulatory capture. - So to you, safe AI\ndevelopment will be achieved through market forces versus\nthrough, like you said, heavy-handed government regulation. There's a report from last month, I have a million questions\nhere, from Yoshua Bengio, Jeff Hinton, and many others. It's titled \"The Managing AI Risk in an Era of Rapid Progress.\" So there's a collection of folks who are very worried about\ntoo-rapid development of AI without considering AI risk, and they have a bunch of\npractical recommendations. Maybe I give you four, and you\nsee if you like any of them. So give independent auditors\naccess to AI labs, one. Two, governments and companies allocate 1/3 of their AI research\nand development funding to AI safety. So there's this general\nconcept of AI safety. Three, AI companies are required\nto adopt safety measures if dangerous capabilities\nare found in their models. And then four, something\nyou kind of mentioned, making tech companies liable for foreseeable and preventable\nharms from their AI systems. So independent auditors, governments and companies\nare forced to spend a significant fraction of\ntheir funding on safety. You gotta have safety measures\nif shit goes really wrong. And liability, companies are liable. And, you know, that seemed like something you would agree with. - I would say that, you know,\nassigning just, you know, arbitrarily saying 30%\nseems very arbitrary. I think organizations would allocate whatever budget is needed to achieve the sort of\nreliability they need to achieve to perform in the market. And I think third-party auditing firms would naturally pop up, because how would customers know that your product is\ncertified reliable, right? They need to see some benchmarks, and those need to be\ndone by a third party. The thing I would oppose, and the thing I'm seeing\nthat's really worrisome, is there's this sort of weird\nsort of correlated interest between the incumbents, the big\nplayers, and the government. And if the two get too\nclose, we open the door for, you know, some sort of\ngovernment-backed AI cartel that could have absolute\npower over the people. If they have the monopoly together on AI, and nobody else has access to AI, then there's a huge power gradient there. And even if you like our\ncurrent leaders, right? I think that, you know, some of the leaders in big\ntech today are good people. You set up that centralized\npower structure, it becomes a target, right? Just like we saw at OpenAI,\nit becomes a market leader, has a lot of the power, and now it becomes a target\nfor those that wanna co-opt it. And so I just want separation\nof AI and state, you know, some might argue in the\nopposite direction, like, hey, we need to close down AI,\nkeep it behind closed doors because of, you know,\ngeopolitical competition with our adversaries. I think that the strength\nof America is its variance, is its adaptability, its dynamism. And we need to maintain that at all costs. It's our free market, capitalism, converges on technologies of high utility much faster than centralized control. And if we let go of that, we let go of our main advantage\nover near peer competitors. - So if AGI turns out to be\na really powerful technology or even the technologies\nthat lead up to AGI, what's your view on the sort\nof natural centralization that happens when large\ncompanies dominate the market, basically formation of\nmonopolies like the takeoff, whichever company really takes\na big leap in development and doesn't reveal intuitively,\nimplicitly, or explicitly the secrets of the magic sauce, they can just run away with it. Is that a worry? - I don't know if I\nbelieve in fast takeoff. I don't think there's a\nhyperbolic singularity, right? A hyperbolic singularity would be achieved on\na finite time horizon. I think it's just one big exponential. And the reason we have an exponential is that we have more\npeople, more resources, more intelligence being applied\nto advancing this science and the research and development. And the more successful it is, the more value it's adding to society, the more resources we put in. And that sort of similar to Moore's law as a compounding exponential, I think the priority to me is to maintain near\nequilibrium of capabilities. We've been fighting for open\nsource AI to be more prevalent and championed by many organizations, because there you sort\nof equilibrate the alpha relative to the market of AIs, right? So if the leading companies have a certain level of capabilities, and open source and open, truly open AI, trails not too far behind, I think you avoid such a scenario where a market leader\nhas so much market power it just dominates everything,\nright, and runs away. And so to us that's the path forward, is to make sure that, you\nknow, every hacker out there, every grad student, every\nkid in their mom's basement has access to, you know, AI systems, can understand how to work with them, and can contribute to the search over the hyperparameter space of how to engineer the systems, right? If you think of, you know,\nour collective research as a civilization, it's\nreally a search algorithm, and the more points we have\nin the search algorithm in this point cloud, the\nmore we'll be able to explore new modes of thinking, right? - Yeah, but it feels\nlike a delicate balance because we don't understand exactly what it takes to build AGI, and what it will look\nlike when we build it. And so far, like you said, it seems like a lot of different parties are able to make progress. So when OpenAI has a big leap, other companies are able to step up, big and small companies in different ways. But if you look at something\nlike nuclear weapons, you've spoken about the Manhattan Project, there could be really like technological and engineering barriers that prevent the guy or\ngal in her mom's basement to make progress. And it seems like the\ntransition to that kind of world where only one player can\ndevelop AGI is possible. It's just not entirely impossible, even though the current state of things seems to be optimistic. - That's what we're trying to avoid. To me, I think like\nanother point of failure is the centralization of the supply chains for the hardware, right? We have Nvidia is just\nthe dominant player, AMD's trailing behind, and then we have TSMC is\nthe main fab in Taiwan, which, you know, geopolitically sensitive. And then we have ASML, which is the maker of the lithography, extreme ultraviolet lithography machines, you know, attacking or monopolizing or co-opting any one point in that chain. you kind of capture the space. And so what I'm trying to do\nis sort of explode the variance of possible ways to do AI in hardware by fundamentally re-imagining how you embed AI algorithms\ninto the physical world. And in general, by the way,\nI dislike the term AGI, artificial general intelligence. I think it's very anthropocentric\nthat we call human-like, or human level AI, artificial\ngeneral intelligence, right? I've spent my career so far exploring notions of intelligence that no biological brain\ncould achieve, right? Quantum form of intelligence, right? Grokking systems that have multipartite\nquantum entanglement that you can provably\nnot represent efficiently on a classical computer, a classical deep learning representation, and hence any sort of biological brain. And so already, you know,\nI've spent my career sort of exploring the wider\nspace of intelligences, and I think that space of intelligence inspired by physics rather\nthan human brain is very large. And I think we're going\nthrough a moment right now similar to when we went from\ngeocentrism to heliocentrism, right, but for intelligence. We realized that human\nintelligence is just a point in a very large space of\npotential intelligences. And it's both humbling for humanity. It's a bit scary, right? That we're not at the\ncenter of this space, but we made that\nrealization for astronomy, and we've survived, and\nwe've achieved technologies by indexing to reality. We've achieved technologies\nthat ensure our wellbeing. For example, we have satellites monitoring solar flares,\nright, that give us a warning. And so similarly, I think by letting go of this anthropomorphic,\nanthropocentric anchor for AI, we'll be able to explore the\nwider space of intelligences that can really be a massive\nbenefit to our wellbeing and the advancement of civilization. - And still we're able to see the beauty and meaning in the human experience, even though we're no longer\nin our best understanding of the world at the center of it. - I think there's a lot of\nbeauty in the universe, right? I think life itself, civilization, this homo, techno,\ncapital, mimetic machine that we all live in, right? So you have humans,\ntechnology, capital, memes, everything is coupled to one another. Everything induces the selective\npressure on one another. And it's a beautiful\nmachine that has created us, has created, you know,\nthe technology we're using to speak today to the audience,\ncapture our speech here, the technology we use to\naugment ourselves every day. We have our phones. I think the system is beautiful and the principle that induces\nthis sort of adaptability and convergence on optimal\ntechnologies, ideas, and so on. It's a beautiful principle\nthat we're part of. And I think part of e/acc is to appreciate this principle in a way that's not just centered on\nhumanity, but kind of broader, appreciate life, you know, the preciousness of\nconsciousness in our universe. And because we cherish this beautiful state of matter we're in, we gotta feel a responsibility to scale it in order to preserve it, because the options are to grow or die. - So if it turns out that the beauty that is consciousness in the universe is bigger than just humans, the AI can carry that same flame forward. Does it scare you, or are you concerned that\nAI will replace humans? - So during my career, I had a\nmoment where I realized that, you know, maybe we need\nto offload to machines to truly understand the\nuniverse around us, right? Instead of just having\nhumans with pen and paper solve it all. And to me that sort of process of letting go of a bit of agency, gave us way more leverage to\nunderstand the world around us. A quantum computer is\nmuch better than a human to understand matter at the nanoscale. Similarly, I think that\nhumanity has a choice. Do we accept the opportunity to have intellectual\nand operational leverage that AI will unlock, and thus ensure that we're\ntaking along this path of growth and scope and\nscale of civilization. We may dilute ourselves, right? There might be a lot\nof workers that are AI, but overall out of our own self-interest, by combining and augmenting\nourselves with AI, we're gonna achieve much higher growth and much more prosperity, right? To me, I think that the most likely future is one where humans\naugment themselves with AI. I think we're already on\nthis path to augmentation. We have phones we use for communication. We have on ourselves at all times. We have wearables soon that have shared\nperception with us, right? Like the Humane AI Pin, or I mean technically your\nTesla car has shared perception. And so if you have shared\nexperience, shared context, you communicate with one another, and you have some sort of IO, really it's an extension of yourself. And to me, I think that humanity\naugmenting itself with AI and having AI that is not\nanchored to anything biological, both will coexist, and the\nway to align the parties. We already have a sort of mechanism to align superintelligences\nthat are made of humans and technology, right? Companies are sort of large\nmixture of expert models where we have neural routing\nof tasks within a company, and we have ways of economic exchange to align these behemoths. And to me, I think capitalism is the way, and I do think that whatever\nconfiguration of matter or information leads to maximal growth, will be where we converge, just from like physical principles. And so we can either align\nourselves to that reality, and join the acceleration up in scope and scale of civilization, or we can get left behind\nand try to decelerate and move back in the forest,\nlet go of technology, and return to our primitive state. And those are the two paths\nforward, at least to me. - But there's a philosophical question whether there's a limit to\nthe human capacity to align. So let me bring it up\nas a form of argument. There's a guy named Dan Hendrycks, and he wrote that he agrees with you that AI development could be viewed as an evolutionary process, but to him, to Dan, this\nis not a good thing, as he argues that natural\nselection favors AIs over humans. And this could lead to human extinction. What do you think? If it is an evolutionary process, and AI systems may have\nno need for humans? - I do think that we're actually inducing an evolutionary process on the space of AIs\nthrough the market, right? Right now we run AIs that have\npositive utility to humans, and that induces a selective pressure if you consider a neural net being alive when there's an API running\ninstances of it on GPUs. Right, and which APIs get run, the ones that have high\nutility to us, right? So similar to how we domesticated wolves and turned them into dogs that are very clear in their expression, they're very aligned, right? I think there's gonna be\nan opportunity to steer AI and achieve highly aligned AI. And I think that humans plus AI is a very powerful combination. And it's not clear to me that pure AI would select out that combination. - So the humans are creating the selection pressure right now to create AIs that are aligned to humans. But, you know, given how AI develops and how quickly it can grow and scale, one of the concerns, to\nme one of the concerns is unintended consequences that humans are not able to anticipate all the consequences of this process. The scale of damage that could be done through unintended consequences with AI systems is very large. - The scale of the upside, right? By augmenting ourselves with\nAI is unimaginable right now. The opportunity cost, we're at a fork in the road, right? Whether we take the path of\ncreating these technologies, augment ourselves, and get to climb up the Kardashev scale, become multi-planetary with the aid of AI, or we have a hard cutoff of like, we don't birth these technologies at all, and then we leave all the\npotential upside on the table. Right, and to me, out of responsibility\nto the future humans, we could carry, right, with\nhigher carrying capacity by scaling up civilization out of responsibility to those humans, I think we have to make the\ngreater grander future happen. - Is there a middle ground between cutoff and all systems go? Is there some argument for caution? - I think, like I said, the\nmarket will exhibit caution. Every organism, company, consumer, is acting out of self-interest, and they won't assign capital to things that have\nnegative utility to them. - The problem is with the\nmarket is like, you know, there's not always perfect information. There's manipulation,\nthere's a bad faith actors that mess with the system. It's not always a rational\nand honest system. - Well, that's why we need\nfreedom of information, freedom of speech, and freedom of thought in order to converge, be able to converge on the\nsubspace of technologies that have positive\nutility for us all, right. - Well, let me ask you about\nPdoom, probability of doom. That's just fun to say,\nbut not fun to experience. What is to you the probability that AI eventually kills all or most humans, also known as probability of doom? - I'm not a fan of that calculation. I think people just\nthrow numbers out there. It's a very sloppy calculation, right? To calculate a probability, you know, let's say you model the world as some sort of Markov process. If you have enough variables\nor hidden Markov process, you need to do a stochastic path integral through the space of all possible futures, not just the futures that your brain naturally\nsteers towards, right? I think that the estimators of Pdoom are biased because of our biology, right? We're evolved to have bias sampling towards negative futures that are scary, because that was an\nevolutionary optimum, right? And so people that are of,\nlet's say higher neuroticism, will just think of negative futures where everything goes\nwrong all day every day, and claim that they're\ndoing unbiased sampling, and in a sense like\nthey're not normalizing for the space of all possibilities. And the space of all possibilities is like super exponentially large. And it's very hard to have this estimate. And in general, I don't think\nthat we can predict the future with that much granularity\nbecause of chaos, right? If you have a complex system, you have some uncertainty\nand a couple variables. If you let time evolve, you have this concept of a\nLyapunov exponent, right? A bit of fuzz becomes a lot of fuzz in our estimate\nexponentially, so over time. And I think we need to show some humility that we can't actually predict the future. All we know, the only prior we\nhave, is the laws of physics. And that's what we're arguing for. The laws of physics say the\nsystem will want to grow. And subsystems that are\noptimized for growth and replication are more\nlikely in the future. And so we should aim to maximize our current mutual\ninformation with the future. And the path towards that\nis for us to accelerate rather than decelerate. So I don't have a Pdoom,\n'cause I think that, you know, similar to the quantum\nsupremacy experiment at Google, I was in the room when they were running\nthe simulations for that, that was an example of\na quantum chaotic system where you cannot even estimate probabilities of certain outcomes with even the biggest\nsupercomputer in the world, right? And so that's an example of chaos. And I think the system is far too chaotic for anybody to have an accurate estimate of the likelihood of certain futures. If they were that good, I think they would be very rich\ntrading on the stock market. - But nevertheless, it's\ntrue that humans are biased, grounded in our evolutionary biology, scared of everything that can kill us, but we can still imagine\ndifferent trajectories that can kill us. We don't know all the other\nones that don't, necessarily, but it's still, I think, useful, combined with some basic intuition\ngrounded in human history to reason about like what,\nlike looking at geopolitics, looking at basics of human nature. How can powerful technology\nhurt a lot of people? It just seems grounded in that\nlooking at nuclear weapons. You can start to estimate Pdoom in maybe in a more philosophical sense, not a mathematical one. Philosophical meaning\nlike, is there a chance, does human nature tend\ntowards that or not? - I think to me, one of the\nbiggest existential risks would be the concentration\nof the power of AI in the hands of the very few, especially if it's a mix between the companies that\ncontrol the flow of information and the government. Because that could set things up for a sort of dystopian\nfuture where only a very few and an oligopoly in\nthe government have AI, and they could even convince the public that AI never existed. And that opens up sort of these scenarios for authoritarian centralized control, which to me is the darkest timeline. And the reality is that we have a prior, we have a data-driven prior of these things happening, right? When you give too much power, when you centralize power too much, humans do horrible things, right? And to me, that has a\nmuch higher likelihood in my Bayesian inference than\nsci-fi based priors, right? Like my prior came from\nthe Terminator movie. And so when I talked to these AI doomers, I just asked them to trace a path through this Markov chain of events that would lead to our doom, right? And to actually give me a good probability for each transition. And very often there's a unphysical or highly unlikely transition\nin that chain, right? But of course we're wired to fear things, and we're wired to respond to danger, and we're wired to deem\nthe unknown to be dangerous because that's a good\nheuristic for survival, right? But there's much more to lose out of fear. We have so much to lose,\nso much upside to lose, by preemptively stopping\nthe positive futures from happening out of fear. And so I think that we\nshouldn't give into fear. Fear is the mind killer. I think it's also the civilization killer. - We can still think about the\nvarious ways things go wrong. For example, the founding\nfathers of the United States thought about human nature and that's why there's a discussion about the freedoms that are necessary. They really deeply deliberated about that. And I think the same could\npossibly be done for AGI. It is true that history, human history, shows that we tend towards centralization, or at least when we\nachieve centralization, a lot of bad stuff happens. When there's a dictator, a\nlot of dark bad things happen. The question is, can AGI\nbecome that dictator? An AGI one develop, become the centralizer because of its power? Maybe has the same, because\nof the alignment of humans, perhaps, the same tendencies, the same Stalin-like\ntendencies to centralize and manage centrally the\nallocation of resources. And you can even see that\nas a compelling argument on the surface level. Well, AGI is so much smarter,\nso much more efficient, so much better at allocating resources. Why don't we outsource it to the AGI, and then eventually whatever forces that corrupt the human mind with power could do the same for AGI. It would just say, well,\nhumans are dispensable, we'll get rid of them. Do the Jonathan Swift \"Modest Proposal\" from a few centuries\nago, I think the 1700s, when he satirically suggested\nthat, I think it's in Ireland, that the children of poor people are fed as food to the rich people. And that would be a good idea, because it decreases the\namount of poor people, and gives extra income to the poor people. So it's on several accounts decreases the amount of poor people, therefore more people become rich. Of course it misses a\nfundamental piece here that's hard to put into\na mathematical equation on the basic value of human life. So all of that to say, are\nyou concerned about AGI being the very centralizer of power that you just talked about? - I do think that right now there's a bias towards\novercentralization of AI because of compute density\nand centralization of data and how we're training models. I think over time we're\ngonna run out of data to scrape over the internet. And I think that, well, actually I'm working on\nincreasing the compute density so that compute can be everywhere\nand acquire information and test hypotheses in the environment in a distributed fashion. I think that fundamentally,\ncentralized cybernetic control. So having one intelligence\nthat is massive, that, you know, fuses many sensors and is trying to perceive\nthe world accurately, predict it accurately,\npredict many, many variables and control it, right, enact\nits will upon the world. I think that's just never\nbeen the optimum, right? Like let's say you have\na company, you know, if you have a company, I don't know of 10,000 people\nthat all report to the CEO, even if that CEO is an AI,\nI think it would struggle to fuse all of the information\nthat is coming to it and then predict the whole system and then to enact its will. What has emerged in\nnature and in corporations and all sorts of systems, is a notion of sort of hierarchical\ncybernetic control, right? You have, you know, in\na company it would be, you have like the individual contributors, they're self interested, and they're trying to achieve their tasks and they have a fine in terms\nof time and space, if you will control loop, and in field\nof perception, right? They have their code base. Let's say you're in a software company, they have their code base, they iterate it on it intraday, right? And then the management maybe checks in, it has a wider scope, it has,\nlet's say five reports, right? And then it samples each\nperson's update once per week, and then you can go up the chain, and you have larger\ntimescale and greater scope. And that seems to have emerged as sort of the optimal\nway to control systems. And really that's what\ncapitalism gives us, right? You have these hierarchies and you can even have like\nparent companies and so on. And so that is far more fault tolerant. In quantum computing,\nthat's my field I came from, we have a concept of this fault tolerance and quantum error correction, right? Quantum error correction is detecting a fault that came from noise, predicting how it's\npropagated through the system, and then correcting it, right,\nso it's a cybernetic loop. And it turns out that decoders\nthat are hierarchical, and at each level the hierarchy are local, perform the best by far, and\nare far more fault tolerant. And the reason is if you\nhave a non-local decoder, then you have one fault\nat this control node, and the whole system sort of crashes. Similarly to if you have, you know, one CEO that everybody reports to, and that CEO goes on vacation, the whole company comes to a crawl, right? And so to me, I think that yes, we're seeing a tendency\ntowards centralization of AI, but I think there's gonna\nbe a correction over time where intelligence is gonna\ngo closer to the perception and we're gonna break up\nAI into smaller subsystems that communicate with one another and form a sort of meta system. - So if you look at the hierarchies that are in the world today, there's nations, and\nthose are hierarchical, but in relation to each\nother, nations are anarchic. So it's an anarchy. Do you foresee a world like this where there's not a\nover, what'd you call it? A centralized cybernetic control? - [Guillaume] Centralized\nlocus of control, yeah. - So like that's suboptimal you're saying? So it would be always\na state of competition at the very top level. - Yeah, just like, you know, in a company you may have like two units\nworking on similar technology and competing with one another, and you prune the one that\nperforms not as well, right? And that's a sort of\nselection process for a tree or a product gets killed, right? And then a whole org gets fired. And that's this process\nof trying new things, and shedding old things that didn't work it's what gives us adaptability and helps us converge on, you know, the technologies and things\nto do that are most good. - I just hope there's not a failure mode that's unique to AGI versus humans. 'Cause you're describing human\nsystems mostly right now. I just hope when there's a\nmonopoly on AGI in one company that we'll see the same\nthing we see with humans, which is another company will spring up and start competing effectively. - I mean that's been\nthe case so far, right? We have OpenAI, we have\nAnthropic, now we have xAI, you know, we had Meta\neven for open source, and now we have Mistral, right,\nwhich is highly competitive. And so that's the beauty of capitalism. You don't have to trust\nany one party too much 'cause we're kind of always\nhedging our bets at every level. There's always competition. And that's the most beautiful\nthing to me at least, is that the whole system\nis always shifting and always adapting, and\nmaintaining that dynamism is how we avoid tyranny, right? Making sure that everyone\nhas access to these tools, to these models, and can\ncontribute to the research, avoids a sort of neural tyranny where very few people have\ncontrol over AI for the world and use it to oppress those around them. - When you were talking\nabout intelligence, you mentioned multipartite\nquantum entanglement. So high level question first is, what do you think is intelligence? When you think about\nquantum mechanical systems and you observe some kind of\ncomputation happening in them, what do you think is intelligent about the kind of computation\nthe universe is able to do? A small, small inkling of which is the kind of computation\nthe human brain is able to do? - I would say like\nintelligence and computation aren't quite the same thing. I think that the universe\nis very much, you know, doing a quantum computation. If you had access to all the\ndegrees of freedom, you could, and a very, very, very\nlarge quantum computer, with many, many, many qubits, let's say a few qubits\nper Planck volume, right? Which was more or less the pixels we have, then you'd be able to simulate\nthe whole universe, right, on a sufficiently large quantum computer. Assuming you're looking\nat a finite volume, of course, of the universe. I think that at least to me,\nintelligence is the, you know, I go back to cybernetics, right? The ability to perceive,\npredict, and control our world. But really it's, nowadays it seems like a lot of intelligence we use is more about compression, right? It's about operationalizing\ninformation theory, right? In information theory, you have the notion of entropy\nof a distribution or a system and entropy tells you that\nyou need this many bits to encode this distribution\nor this subsystem if you had the most optimal code. And AI, at least the way we do it today for LLMs and for quantum, is very much trying to\nminimize relative entropy between our models of\nthe world and the world, distributions from the world. And so we're learning, we're searching over the\nspace of computations to process the world, define\nthat compressed representation that has distilled all\nthe variants and noise and entropy, right? And originally, I came to\nquantum machine learning from the study of black holes because the entropy of black\nholes is very interesting, in a sense they're physically the most dense objects in the universe. You can't pack more information spatially any more densely than in a black hole. And so I was wondering how do black holes actually\nencode information? What is their compression code? And so that got me into\nthe space of algorithms to search over space of quantum codes. And it got me actually into also how do you acquire\nquantum information from the world, right? So something I've worked\non, this is public now, is quantum analog digital conversion. So how do you capture information from the real world in superposition, and not destroy the superposition, but digitize for a quantum\nmechanical computer information from the real world. And so if you have an ability to capture quantum information, and search over learned\nrepresentations of it, now you can learn\ncompressed representations that may have some useful\ninformation in there, latent representation, right? And I think that many of the problems\nfacing our civilization are actually beyond this\ncomplexity barrier, right? I mean the greenhouse effect is a quantum mechanical effect, right? Chemistry is quantum mechanical, you know, nuclear physics is quantum mechanical, a lot of biology and\nprotein folding and so on is affected by quantum mechanics. And so unlocking an ability to augment human intellect with\nquantum mechanical computers and quantum mechanical AI seem to me like a fundamental\ncapability for civilization that we need to develop. So I spent several years doing that, but over time I kind of\ngrew weary of the timelines that were starting to\nlook like nuclear fusion. - So one high level question I can ask is maybe by way of definition,\nby way of explanation, what is a quantum computer, and what is quantum machine learning? - Hmm, so a quantum computer really is a quantum mechanical system over which we have sufficient control, and it can maintain its\nquantum mechanical state. And quantum mechanics is how nature behaves at\nthe very small scales when things are very small or very cold. And it's actually more fundamental\nthan probability theory. So we're used to things\nbeing this or that, but we're not used to\nthinking in superpositions 'cause, well, our brains can't do that. So we have to translate the\nquantum mechanical world to say linear algebra to grok it. Unfortunately that translation is exponentially inefficient on average. You have to represent things\nwith very large matrices, but really you can make a quantum computer out of many things, right? And we've seen all sorts\nof players, you know, from neutral atoms, trapped\nions, superconducting, metal, photons, and at different frequencies. I think you could make a quantum computer out of many things. But to me, the thing that\nwas really interesting was both quantum machine learning was about understanding the\nquantum mechanical world with quantum computers. So embedding the physical\nworld into AI representations, and quantum computer engineering was embedding AI algorithms\ninto the physical world. So this bidirectionality of embedding physical world into AI, AI into the physical world, the symbiosis between physics and AI, really that's the sort of core of my quest really even to this day,\nafter quantum computing, it's still in this sort of journey to merge, really, physics\nand AI fundamentally. - So quantum machine learning is a way to do machine learning on a representation of\nnature that is, you know, stays true to the quantum\nmechanical aspect of nature. - Yeah, it's learning quantum\nmechanical representations that would be quantum deep learning. Alternatively, you can try to\ndo classical machine learning on a quantum computer. I wouldn't advise it, because\nyou may have some speedups, but very often the speedups\ncome with huge costs. Using a quantum computer is\nvery expensive. Why is that? Because you assume the computer is operating\nat zero temperature, which no physical system in the universe can achieve that temperature. So what you have to do is\nwhat I've been mentioning, this quantum error correction process, which is really an\nalgorithmic fridge, right? It's trying to pump\nentropy out of the system, trying to get it closer\nto zero temperature. And when you do the calculations of how many resources it would take to say do deep learning\non a quantum computer, classical deep learning, there's just such a huge\noverhead, it's not worth it. It's like thinking about\nshipping something across a city using a rocket and\ngoing to orbit and back. It doesn't make sense. Just use an, you know,\ndelivery truck, right? - What kind of stuff can you figure out? Can you predict, can you understand with quantum deep learning that you can't with deep learning? So incorporating quantum\nmechanical systems into the learning process? - I think that's a great question. I mean, fundamentally it's any system that has sufficient quantum\nmechanical correlations that are very hard to capture. For classical representations, then there should be an advantage for a quantum mechanical representation over a purely classical one. The question is, which systems\nhave sufficient correlations that are very quantum, but is also, which systems are\nstill relevant to industry? That's a big question. You know, people are\nleaning towards chemistry, nuclear physics. I've worked on actually processing inputs from quantum sensors, right? If you have a network of quantum sensors, they've captured a quantum\nmechanical image of the world, and how to post-process that, that becomes a sort of quantum\nform of machine perception. And so for example, Fermilab has a project exploring detecting dark matter\nwith these quantum sensors. And to me that's in\nalignment with my quest to understand the universe\never since I was a child. And so someday I hope that we can have very large networks of quantum\nsensors that help us peer into the earliest parts\nof the universe, right? For example, the LIGO is\na quantum sensor, right? It's just a very large one. So yeah, I would say quantum machine perception\nsimulations, right? Grokking quantum simulations\nsimilar to AlphaFold, right? AlphaFold understood the\nprobability distribution over configurations of proteins. You can understand quantum distributions over configurations of\nelectrons more efficiently with quantum machine learning. - You co-authored a paper titled \"A Universal Training Algorithm\nfor Quantum Deep Learning\" that involves baqprop with a Q. Very well done, sir, very\nwell done. How does it work? Is there some interesting\naspects you can just mention on how kinda, you know, baqprop and some of these things we know from classical machine learning transfer over to the\nquantum machine learning? - Yeah, that was a funky paper. That was one of my first papers\nin quantum deep learning. Everybody was saying,\noh, I think deep learning is gonna be sped up by quantum computers. And I was like, well the best\nway to predict the future is to invent it. So here's a hundred page paper, have fun. Essentially, you know, quantum computing is usually you embed reversible operations into a quantum computation. And so the trick there was\nto do a feedforward operation and do what we call a phase kick. But really it's just a force kick. You just kick the system\nwith a certain force that is, you know, proportional\nto your loss function that you wish to optimize. And then by performing uncomputation, you start with the superpositions, superposition over parameters,\nright, which is pretty funky. Now you're not just, you don't have just a\npoint for parameters. You have a superposition over many potential parameters, right? And our goal is. - Is using phase kicks somehow\nto adjust the parameters? - 'Cause phase kicks emulate having the parameter space be like a particle in n dimensions, and you're trying to get\nthe Schrodinger equation, Schrodinger dynamics in the lost landscape of the neural network, right? And so you do an algorithm\nto induce this phase kick, which you know, involves\na feed forward, a kick, and then when you\nun-compute the feed forward, then all the errors in these\nphase kicks and these forces back propagate and hit\neach one of the parameters throughout the layers. And if you alternate this with an emulation of kinetic energy, then it's kind of like a\nparticle moving in n dimensions, a quantum particle. And the advantage in principle would be that it can tunnel\nthrough the landscape and find new optima that would've been difficult\nfor stochastic optimizers. But again, this is kind\nof a theoretical thing, and in practice with at least\nthe current architectures for quantum computers that\nwe have planned, you know, such algorithms would be\nextremely expensive to run. - So maybe this is a good\nplace to ask the difference between the different fields\nthat you've had a toe in. So mathematics, physics, engineering, and also, you know, entrepreneurship, like the different layers of the stack. I think a lot of the stuff\nyou're talking about here is a little bit on the math side, maybe physics almost working in theory. What's the difference to\nyou between math, physics, engineering, and you know, making a product for quantum computing, for quantum machine learning? - Yeah, I mean, you know,\nsome of the original team for the TensorFlow quantum\nproject, which we started, you know, in school at\nUniversity of Waterloo, there was myself, you know,\ninitially I was a physicist, a polymathematician. We had a computer scientist,\nwe had mechanical engineer, and then we had a physicist\nthat was experimental primarily. And so putting together teams that are very cross-disciplinary and figuring out how to\ncommunicate and share knowledge is really the key to doing this sort of interdisciplinary\nengineering work. I mean, there is a big\ndifference, you know, in mathematics you can explore mathematics for mathematics' sake, in physics, you're applying mathematics to understand the world around us. And in engineering, you're trying to, you're trying to hack the world, right? You're trying to find how to\napply the physics that I know, my knowledge of the world to do things. - Well, in quantum\ncomputing in particular, I think there's just a lot\nof limits to engineering. It just seems to be extremely hard. So there's a lot of value to be exploring quantum computing, quantum machine learning\nin theory, and with math. So I guess one question is why is it so hard to\nbuild a quantum computer? What's your view of timelines in bringing these ideas to life? - Right, I think that, you know, an overall theme of my company is that we have folks that are, you know, there's a sort of exodus\nfrom quantum computing and we're going to\nbroader physics-based AI that is not quantum. So that gives you a hint, and. - So we should say the name\nof your company is Extropic. - Extropic, that's right. And we do physics-based AI\nprimarily based on thermodynamics rather than quantum mechanics. But essentially a quantum computer is very difficult to build\nbecause you have to induce this sort of zero temperature\nsubspace of information. And the way to do that is\nby encoding information, you encode a code within a code, within a code within a code. And so there's a lot of redundancy needed to do this error correction. But ultimately it's a sort\nof algorithmic refrigerator. Really, it's just pumping out entropy out of the subsystem that\nis virtual and delocalized that represents your quote\nunquote logical qubits. AKA, the payload quantum bits in which you actually want to run your quantum mechanical program. It's very difficult because in order to scale\nup your quantum computer, you need each component to\nbe of sufficient quality for it to be worth it. Because if you try to do\nthis error correction, this quantum error correction process, and each quantum bit and\nyour control over them, if it's insufficient,\nit's not worth scaling up. You're actually adding more\nerrors than you remove. And so there's this notion of a threshold where if your quantum bits\nare of sufficient quality in terms of your control over them, it's actually worth scaling up. And actually in recent years, people have been crossing the threshold, and it's starting to be worth it. And so it's just a very\nlong slog of engineering. But ultimately it's really crazy to me how much exquisite level of control we have over these systems,\nit's actually quite crazy, and people are crossing, you know, they're achieving milestones. It's just, you know, in general\nthe media always gets ahead right, of where the technology is. There's a bit too much hype. It's good for fundraising, but sometimes, you know,\nit causes winters, right? It's the hype cycle. I'm bullish on quantum computing on a 10, 15 year timescale personally, but I think there's other quests that can be done in the meantime. I think it's in good hands right now. - Well, let me just explore\ndifferent beautiful ideas, large or small in quantum computing that might jump out at you from memory. So when you co-authored a paper titled \"Asymptotically Limitless\nQuantum Energy Teleportation via Qudit Probes.\" So just out of curiosity, can\nyou explain what a qudit is versus a qubit? - Yeah, it's a D state qubit. - [Lex] It's multidimensional - Multidimensional, right. So it's like, well, you know, can you have a notion of like\nan integer floating point that is quantum mechanical? That's something I've had to think about. I think that research was\na precursor to later work on quantum analog digital conversion. There it was interesting,\nbecause during my master's, I was trying to understand the energy and entanglement of the vacuum, right, of emptiness. Emptiness has energy,\nwhich is very weird to say. And our equations of cosmology don't match our calculations for the amount of quantum energy there is in the fluctuations. And so I was trying to hack the\nenergy of the vacuum, right? And the reality is that you\ncan't just directly hack it. It's not technically free energy. Your lack of knowledge of the fluctuations means you can't extract the energy. But just like, you know, the stock market, if you have a stock that's\ncorrelated over time, the vacuum's actually correlated. So if you measured the\nvacuum at one point, you acquired information. And if you communicated that\ninformation to another point, you can infer what\nconfiguration the vacuum is in to some precision, and statistically extract on\naverage some energy there. So you've quote unquote teleported energy. To me that was interesting, because you could create pockets\nof negative energy density, which is energy density\nthat is below the vacuum, which is very weird, because we don't understand\nhow the vacuum gravitates. And there are theories where the vacuum or the\ncanvas of space time itself is really a canvas made out\nof quantum entanglement. And I was studying how decreasing energy\nof the vacuum locally increases quantum entanglement,\nwhich is very funky. And so the thing there is that, you know, if you're into, you know,\nweird theories about, you know, UAPs and whatnot, you know, you could try to imagine\nthat they're around and how would they\npropel themselves, right? How would they go faster\nthan the speed of light? You would need a sort of\nnegative energy density. And to me, I gave it the old college try trying to hack the energy of the vacuum and hit the limits allowable\nby the laws of physics. But there's all sorts of caveats there where you can't extract more\nthan you've put in obviously, - But you're saying it's\npossible to teleport the energy because you can extract\ninformation one place and then make based on that, some kind of prediction\nabout another place. I'm not sure what I make of that. - Yeah, I mean, it's allowable\nby the laws of physics. The reality though is that the correlations\ndecay with distance. And so you're gonna have to pay the price not too far away from where\nyou extract it to, right. - The precision decreases. I mean, in terms of your\nability to, but still, but since you mentioned UAPs,\nwe talked about intelligence, and I forgot to ask, what's your view on the other possible intelligences that are out there at the mesoscale? Do you think there's other\nintelligent alien civilizations? Is that useful to think about? How often do you think about it? - I think it's useful to think about, it's useful to think about because we gotta ensure\nwe're anti-fragile, and we're, you know, trying to increase our\ncapabilities as fast as possible because we could get disrupted. Like there's no laws of physics against there being life elsewhere that could evolve and become\nan advanced civilization, and eventually come to us. Do I think they're here now? I'm not sure. I mean, I've read what most\npeople have read on the topic. I think it's interesting to consider, and to me it's a useful thought experiment to instill a sense of urgency\nin developing technologies and increasing our capabilities to make sure we don't\nget disrupted, right? Whether it's a form of\nAI that disrupts us, or a foreign intelligence\nfrom a different planet, like either way, like\nincreasing our capabilities and becoming formidable as humans. I think that's really important. So that we're robust against whatever the\nuniverse throws at us. - But to me it's also\nan interesting challenge and thought experiment on\nhow to perceive intelligence. This has to do with\nquantum mechanical systems. This has to do with any kind of system that's not like humans. So to me, the thought experiment\nis say the aliens are here, or they are directly observable. We're just too blind, too self-centered, don't have the right sensors, or don't have the right\nprocessing of the sensor data to see the obvious intelligence\nthat's all around us. - Well, that's why we work\non quantum sensors, right? They can sense gravity. - Yeah, but there could\nbe, so that's a good one. But there could be other\nstuff that's not even in the currently known forces of physics. There could be some other stuff. And the most entertaining\nthought experiment to me is that it's other stuff that's obvious. It's not like we lack the\nsensors, it's all around us. You know, you know, the consciousness being one possible one, but there could be stuff that's\njust like obviously there, and once you know it, it's\nlike, oh, right, right. The thing we thought is somehow emergent from\nthe laws of physics. We understand them, it's actually a fundamental\npart of the universe and can be incorporated in\nphysics once understood. - Statistically speaking, right, if we observed some sort of alien life, it would most likely be some sort of virally self-replicating von\nNeumann-like probe system. Right, and it's possible\nthat there, you know, there are such systems that, I don't know what they're doing at the bottom of the ocean allegedly, but maybe they're, you know, collecting minerals from\nthe bottom of the ocean. But that wouldn't\nviolate any of my priors. But am I certain that\nthese systems are here? And it'd be difficult\nfor me to say so, right. I only have secondhand information\nabout there being data. - About the bottom of the ocean. Yeah, but, you know, could\nit be things like memes? Could it be thoughts and ideas? Could they be operating at that medium? Could aliens be the very\nthoughts that come into my head? Like, what do you, have you. How do you know that,\nhow do you know that, what's the origin of ideas, in your mind, when an idea comes to your head? Show me where it originates. - I mean, frankly, when I had the idea for the type of computer I'm building now, I think it was eight years ago now, it really felt like it was\nbeing beamed from space. I was in bed just shaking,\njust thinking it through, and I don't know, but do I\nbelieve that legitimately? I don't think so, but you know, I think that alien life\ncould take many forms, and I think the notion of intelligence and the notion of life needs to be expanded much more broadly to be less anthropocentric or biocentric. - Just to linger a little\nlonger on quantum mechanics, through all your explorations\nof quantum computing, what's the coolest, most beautiful idea that you've come across\nthat has been solved or has not yet been solved? - I think the journey to understand something called AdS/CFT. So the journey to\nunderstand quantum gravity through this picture where a hologram of lesser\ndimension is actually dual or exactly corresponding to a bulk theory of quantum gravity of an extra dimension. And the fact that this sort of duality comes from trying to learn deep learning-like\nrepresentations of the boundary. And so at least part of my journey someday on my bucket list is to apply\nquantum machine learning to these sorts of systems, these CFDs or they're called SYK models, and learn an emergent geometry\nfrom the boundary theory. And so we can have a\nform of machine learning to help us understand\nquantum gravity, right. Which is, you know, still a holy grail that I would like to hit\nbefore I leave this Earth. - What do you think is\ngoing on with black holes as the information storing\nand processing units, what do you think is\ngoing on with black holes? - Black holes are really\nfascinating objects. They're at the interface between quantum mechanics and gravity. And so they help us\ntest all sorts of ideas. I think that, you know,\nfor many decades now, there's been sort of this\nblack hole information paradox that things that fall into the black hole we've seen to have lost their information. Now I think there's this firewall paradox that has been allegedly\nresolved in recent years by, you know, a former peer of mine who's now a professor at Berkeley. And there it seems like there is, as information falls into\na black hole, it's sort of, there's sort of a sedimentation right? As you get closer and\ncloser to the horizon from the point of view of\nthe observer on the outside, the object slows down infinitely as it gets closer and closer. And so everything that is\nfalling into a black hole from our perspective\ngets sort of sedimented and tacked on to the near horizon. And at some point it gets\nso close to the horizon, it's in the proximity or the scale, in which quantum effects and\nquantum fluctuations matter. And there that and falling\nmatter could interfere with sort of the traditional pictures, that it could interfere with the creation and\nannihilation of particles and antiparticles in the vacuum. And through this interference, one of the particles gets entangled with the infalling information and one of them is now free and escapes. And that's how there's\nsort of mutual information between the outgoing radiation\nand the infalling matter, but getting that calculation right, I think we're only just starting\nto put the pieces together. - There's a few pothead-like\nquestions I want to ask you. So one, does it terrify you that there's a giant black hole\nat the center of our galaxy? - I don't know, I just want to, you know, set up shop near it to\nfast forward, you know, meet a future civilization, right? Like if we have a limited lifetime, if you could go orbit a\nblack hole and emerge. - So if you were like, if\nthere's a special mission that could take you to a black hole, would you volunteer to go travel? - To orbit and not\nobviously not fall into it? - That's obvious. So it's obvious to you that everything's destroyed\ninside a black hole. Like all the information that makes up Guillaume is destroyed. Maybe on the other side,\nBeff Jezos emerges. And it's all like, it's tied together in some deeply meme-ful way. - Yeah, I mean, that's a great question. We have to answer what black holes are. Are we punching a hole through space-time and creating a pocket universe? It's possible, right? Then that would mean that if\nwe ascend the Kardashev scale to, you know, beyond Kardashev Type III, we could engineer black holes\nwith specific hyperparameters to transmit information to\nnew universes we create. And so we can have progeny,\nright, that are new universes. And so even though our universe\nmay reach a heat death, we may have a way to have a legacy, right? And so we don't know yet, we need to ascend the Kardashev scale to answer these questions, right, to peer into that regime\nof higher energy physics. - And maybe you can speak\nto the Kardashev scale for people who don't know. So one of the sort of meme-like principles and goals of the e/acc movement is to ascend the Kardashev scale. What is the Kardashev scale\nand when do we wanna ascend it? - The Kardashev scale is a measure of our energy production and consumption. And really it's a logarithmic scale. And Kardashev Type I is a milestone where we are producing\nthe equivalent wattage to all the energy that is\nincident on Earth from the Sun. Kardashev Type II would be\nharnessing all the energy that is output by the Sun. And I think Type III is like\nthe whole galaxy equivalent. - [Lex] Galaxy, I think level. Yeah. - Yeah, yeah. And then some people have\nsome crazy Type IV and V, but I don't know if I believe in those, but to me it seems like from the first principles\nof thermodynamics, that again, there's this\nconcept of thermodynamic-driven dissipative adaptation where, you know, life evolved on Earth because we have this sort of\nenergetic drive from the Sun, right, we have incident energy, and life evolved on Earth to capture, figure out ways to best\ncapture that free energy to maintain itself and grow. And I think that principle, it's not special to our Earth/Sun system. We could extend life well beyond, and we kind of have a\nresponsibility to do so because that's the process\nthat brought us here. So we don't even know what it has in store for us in the future. It could be something of beauty we can't even imagine today, right. - So this is probably a good place to talk a bit about the e/acc movement. In a Substack blog post titled, what the fuck is e/acc? Or actually what the F* is e/acc, you write, \"Strategically\nspeaking, we need to work towards several overarching civilization goals that are all interdependent. And the four goals are increase the amount of energy\nwe can harness as a species, climb the Kardashev gradient. In the short term, this almost certainly\nmeans nuclear fission. Increase human flourishing via pro-population growth policies and pro-economic growth policies, create artificial general intelligence, the single greatest force\nmultiplier in human history, and finally develop interplanetary and interstellar transport so that humanity can\nspread beyond the Earth.\" Could you build on top of that to maybe say what to you\nis the e/acc movement? What are the goals?\nWhat are the principles? - The goal is for the human\ntechnocapital mimetic machine to become self-aware, and to hyperstitiously\nengineer its own growth. So let's decompress that.\n- Define each of those words. - Yeah, so you have humans,\nyou have technology. You have capital, and then you have memes\nand information, right? And all of those systems are coupled with one another, right? Humans work at companies, they\nacquire and allocate capital, and humans communicate via memes and information propagation. And our goal was to have a sort\nof viral optimistic movement that is aware of how the system works. Fundamentally it seeks to grow, and we simply want to lean into the natural tendencies of the system to adapt for its own growth. - So in that way, you're right, the e/acc is literally\na mimetic optimism virus that is constantly drifting, mutating, and propagating in a\ndecentralized fashion. So mimetic optimism virus. So you do want it to be a\nvirus to maximize the spread. And it's hyperstitious, therefore the optimism will\nincentivize its growth. - We see e/acc as a sort of metaheuristic, a sort of very thin cultural framework from which you can have much\nmore opinionated forks, right? Fundamentally, we just say that it's good. That what got us here is this adaptation of the whole system, you\nknow, based on thermodynamics. And that process is good and\nwe should keep it going, right? That is the core thesis. Everything else is okay, how do we ensure that we maintain this malleability and adaptability while clearly not suppressing variants and maintaining free\nspeech, freedom of thought, freedom of information propagation, and freedom to do AI research is important for us to\nconverge the fastest on the space of technologies,\nideas, and whatnot that lead to this growth. And so ultimately, you know,\nthere's been quite a few forks, some are just memes, but\nsome are more serious, right? Vitalik Buterin recently\nmade a d/acc fork. He has his own sort of\nfine tunings of e/acc. - Does anything jump out to memory of the unique characteristic\nof that fork from Vitalik? - I would say that it's\ntrying to find a middle ground between e/acc and sort\nof EA and AI safety, to me, like having a movement that is opposite to what\nwas the mainstream narrative that was taking over Silicon Valley was important to sort of shift the dynamic range of opinions. And you know, it's like the balance between centralization\nand decentralization. The real optimum's always\nsomewhere in the middle, right? But for e/acc we're pushing\nfor entropy, novelty, disruption, malleability, speed, rather than being like\nsort of conservative, suppressing thought, suppressing\nspeech, adding constraints, adding too many regulations,\nslowing things down. And so it's kind of, we're trying to bring\nbalance to the force, right? The systems. - Balance to the force of\nhuman civilization, yeah. - It's literally the forces of constraints versus the entropic force\nthat makes us explore, right? Systems are optimal when they're\nat the edge of criticality between order and chaos, right? Between constraints, energy\nminimization, and entropy. Right, systems want to equilibrate, balance these two things. And so I thought that\nthe balance was lacking, and so we created this\nmovement to bring balance. - Well, I like how, I\nlike the sort of visual of the landscape of ideas\nevolving through forks. So kinda thinking on the\nother part of history, thinking of Marxism as\nthe original repository, and then Soviet communism\nis a fork of that. And then the Maoism is a fork\nof Marxism and communism. And so those are all forks. They're exploring different ideas. - Thinking of culture,\nalmost like code, right? Nowadays, I mean what you prompt the LLM or what you put in the\nconstitution of an LLM is basically its cultural framework, what it believes, right? And you can share it on GitHub nowadays. So trying to take inspiration\nfrom what has worked in this sort of machine of software to adapt over the space of code, could we apply that to culture? And our goal is to not say you should live your\nlife this way, X, Y, Z, it's to set up a process where people are always\nsearching over subcultures and competing for mindshare. And I think creating this\nmalleability of culture is super important for us to\nconverge onto the cultures and the heuristics about\nhow to live one's life that are updated to modern times. Because there's really\nbeen a sort of vacuum of spirituality and culture. People don't feel like they\nbelong to any one group. And there's been parasitic ideologies that have taken up opportunity to populate this Petri\ndish of minds, right? Elon calls it the mind virus, we call it the decel mind virus complex, which is the decelerative that is kind of the overall\npattern between all of them. There's many variants as well. And so, you know, if there's\na sort of viral pessimism, decelerative movement, we needed to have not only one movement, but you know, many, many variants. So it's very hard to pinpoint and stop. - But the overarching\nthing is nevertheless a kind of mimetic optimism pandemic. So I mean, okay, let me ask you, do you think e/acc to\nsome degree is a cult? - Define cult? - I think a lot of human progress is made when you have independent thought. So you have individuals that\nare able to think freely, and very powerful mimetic systems can kind of lead to groupthink. There's something in human nature that leads to like mass\nhypnosis, mass hysteria. We start to think alike. Whenever there's a sexy idea\nthat captures our minds. And so it's actually hard\nto like break us apart, like pull us apart, diversify a thought. So to that degree, to which degree is everybody\nkinda chanting e/acc, e/acc, like the sheep in \"Animal Farm\"? - Well, first of all, it's\nfun, it's rebellious, right? Like, you know, many, I think we lean into, there's this concept of sort of metairony, right? Of sort of being on the boundary of like, we're not sure if they're serious or not. And it's much more playful\nand much more fun, right? Like for example, we talk about thermodynamics\nbeing our God, right? And sometimes we do cult-like things, but there's no like ceremony\nand robes and whatnot. - Not yet. - Not yet, but ultimately,\nyeah, I mean I totally agree that it seems to me that humans wanna feel like\nthey're part of a group, so they naturally try to\nagree with their neighbors and find common ground, and that leads to sort of mode collapse in the space of ideas, right? We used to have sort\nof one cultural island that was allowed, it was a\ntypical subspace of thought, and anything that was diverting from that subspace of\nthought was suppressed or you were canceled, right? Now we've created a new mode, but the whole point is that\nwe're not trying to have a very restricted space of thought. There's not just one\nway to think about e/acc and its many forks. And the point is that\nthere are many forks, and there can be many\nclusters, and many islands, and I shouldn't be in\ncontrol of it in any way. I mean, there's no formal org whatsoever. I just put out tweets\nand certain blog posts and people are free to defect and fork if there's an aspect they don't like. And so that makes it\nso that there should be a sort of de-territorialization\nin the space of ideas so that we don't end up in one cluster that's very cult-like,\nand so cults usually, they don't allow people to\ndefect or start competing forks, whereas we encourage it, right. - Do you think just the humor, the pros and cons of humor and\nmeme, so in some sense meme, there's like a wisdom to memes, what is it, the magic theater,\nwhat book is that from? Hermann Hesse, \"Steppenwolf,\" I think. But there's a kind of\nembracing of the absurdity that seems to get to the truth of things, but at the same time it can\nalso decrease the quality and the rigor of the discourse. Do you feel the tension of that? - Yeah, so initially I think what allowed us to grow under the radar was because it was camouflaged as sort of metaironic, right? We would sneak in, you know, deep truths within a package of humor,\nand humor and memes, and what are called shitposts, right? And I think that was\npurposefully a sort of camouflage against those that seek\nstatus and do not want to, it's very hard to argue\nwith a cartoon frog or a, or a cartoon of an\nintergalactic Jeff Bezos and take yourself seriously. And so that allowed us to grow pretty rapidly in the early days. But of course like that's, you know, essentially people get steered, their notion of the truth\ncomes from the data they see, from the information they're fed, and the information people are fed is determined by algorithms, right? And really what we've been doing is sort of engineering what we call high mimetic fitness\npackets of information so that they can spread effectively and carry a message, right? So it's kind of a vector\nto spread the message. And yes, we've been\nusing sort of techniques that are optimal for today's\nalgorithmically-amplified information landscapes. But I think we're reaching\nthe point of, you know, scale where we can have serious\ndebates and serious conversations and, you know, that's why\nwe're considering doing a bunch of debates and having more serious\nlong form discussions. 'Cause I don't think that\nthe timeline is optimal for sort of very serious,\nthoughtful discussions. You get rewarded for sort\nof polarization, right? And so even though we started a movement that is literally trying to\npolarize the tech ecosystem, at the end of the day, it's so that we can have a conversation and find an optimum together. - I mean that's kind of what\nI try to do with this podcast, given the landscape of things, to still have long form conversations. But there is a degree to which\nabsurdity is fully embraced. In fact, this very conversation\nis multi-level absurd. So first of all, I should\nsay that I just very recently had a conversation with Jeff Bezos, and I would love to hear your Beff Jezos opinions of Jeff Bezos. Speaking of intergalactic Jeff Bezos. What do you think of that\nparticular individual whom your name is inspired? - Yeah, I mean, I think\nJeff is really great. I mean, he's built one of the most epic\ncompanies of all time. He's leveraged the technocapital machine and technocapital acceleration to give us what we wanted, right? We want quick delivery,\nvery convenient, at home, low prices, right? He understood how the machine worked and how to harness it, right? Like running the company, not drink, trying to take profits too\nearly, putting it back, letting the system compound\nand keep improving. And you know, arguably I\nthink Amazon's invested some of the most amount of\ncapital in robotics out there. And certainly with the birth of AWS, kind of enabled the sort of\ntech boom we've seen today that has paid the salaries\nof, you know, I guess myself and all of our friends to some extent. And so I think we can all be\ngrateful to, you know, Jeff, and he's one of the great\nentrepreneurs out there, one of the best of all time unarguably. - And of course the work at Blue Origin similar to the work at SpaceX is trying to make humans\na multiplanetary species, which it seems almost like a bigger thing than the capitalist machine, or it's a capitalist machine at a different timescale, perhaps. - Yeah, I think that, you know, companies they tend to optimize, you know, quarter over quarter,\nmaybe a few years out. But individuals that wanna leave a legacy can think on a multi-decadal\nor multi-century timescale. And so the fact that some individuals are such good capital allocators that they unlock the ability\nto allocate capitals to goals that take us much further\nor are much further looking, you know, Elon's doing this with SpaceX, putting all this capital\ntowards getting us to Mars. Jeff is trying to build Blue Origin, and I think he wants to\nbuild O'Neill cylinders and get industry off planet,\nwhich I think is brilliant. I think, you know, just\noverall, I'm for billionaires. I know this is a controversial\nstatement sometimes, but I think that in a sense it's kind of a proof\nof stake voting, right? Like if you've allocated\ncapital efficiently, you unlock more capital to allocate just because clearly you know how to allocate capital more efficiently, which is in contrast to\npoliticians that get elected because they speak the best on TV, right? Not because they have\na proven track record of allocating taxpayer\ncapital most efficiently. And so that's why I'm for capitalism over say, giving all our\nmoney to the government, and letting them figure\nout how to allocate it, so. - Why do you think it's a viral and it's a popular meme\nto criticize billionaires, since you mentioned billionaires. Why do you think there's\nquite a widespread criticism of people with wealth, especially those in the\npublic eye, like Jeff and Elon and Mark Zuckerberg and\nwho else, Bill Gates. - Yeah, I think a lot of people would, instead of trying to understand how the technocapital machine works and realizing they have much\nmore agency than they think, they'd rather have this\nsort of victim mindset. I'm just subjected to this\nmachine, it is oppressing me. And the successful players\nclearly must be evil because they've been\nsuccessful at this game that I'm not successful at. But, you know, I've\nmanaged to get some people that were in that mindset\nand make them realize how the technocapital machine works and how you can harness\nit for your own good and for the good of others. And by creating value, you capture some of the value\nyou create for the world. And that's sort of positive\nsum mindset shift is so potent. And really that's what we're\ntrying to do by scaling e/acc is sort of unlocking that\nhigher level of agency. Like actually you're far more in control of the future than you think, you have agency to change\nthe world, go out and do it. Here's permission. - Each individual has agency. The motto keep building is often heard. What does that mean to you, and what does that have\nto do with Diet Coke? - Well, Diet Coke. - By the way, thank you\nso much for the Red Bulls. It's working pretty well.\nI'm feeling pretty good. - Awesome. Well, so building\ntechnologies and building, it doesn't have to be technologies, just building in general\nmeans, you know, having agency, trying to change the world\nby creating, let's say, a company which is a\nself-sustaining organism, that, you know, accomplishes a function in the broader technocapital machine. To us, that's the way to\nachieve change in the world that you'd like to see, rather than say pressuring politicians or creating nonprofits that, you know, nonprofits, once they run out of money, their function can no\nlonger be accomplished. You're kind of deforming\nthe market artificially compared to sort of subverting\nor coursing the market to, or dancing with the market to convince it that actually this function\nis important, adds value. And here it is, right? And so I think, you know,\nthis is sort of the way between this sort of\ndegrowth like ESG approach versus say, Elon, right? The degrowth approach is like, we're gonna manage our way\nout of a climate crisis. And Elon is like, I'm\ngonna build a company that is self-sustaining,\nprofitable, and growing. And we're gonna innovate our\nway out of this dilemma, right? And we're trying to get\npeople to do the latter rather than the former, at all scales. - Elon is an interesting case. So you are a proponent,\nyou celebrate Elon, but he's also somebody\nwho has for a long time warned about the dangers,\nthe potential dangers, existential risks of\nartificial intelligence. How do you square the two? Is\nthat a contradiction to you? - It is somewhat because he's very much against\nregulation in many aspects. But for AI, he's definitely, you know, a proponent of regulations. I think overall, you know, he saw the dangers of\nsay OpenAI, you know, cornering the market, and then\ngetting to have the monopoly over the cultural priors that\nyou can embed in these LLMs that then, you know, as LLMs now become the source of truth for people, then you can shape the\nculture of the people. And so you can control\npeople by controlling LLMs. And he saw that, just like it\nwas the case for social media, if you shape the function\nof information propagation, you can shape people's opinions. He's sought to make a competitor. So at least like I think\nwe're very aligned there, that, you know, the way to a good future is to maintain sort of\nadversarial equilibria between the various AI players. I'd love to talk to him to\nunderstand sort of his thinking about how to make, you know, how to advance AI going forwards. I mean, he's also hedging\nhis bets, I would say, you know, with Neuralink, right? I think if he can't stop the\nprogress of AI, you know, he's building the technology to merge. So, you know, look at the\nactions, not just the words, but. - Well, I mean, there's some\ndegree where being concerned, maybe using human psychology, being concerned about threats\nall around us is a motivator. Like it's an encouraging thing. I operate much better\nwhen there's a deadline. The fear of the deadline. Like, and I, for myself\ncreate artificial things. Like I want to create in\nmyself this kind of anxiety as if something really\nhorrible will happen if I miss the deadline. I think there's some degree of that here because creating AI\nthat's aligned with humans has a lot of potential benefits. And so a different way to reframe that is if you don't, we're all gonna die. It just seems to be a very powerful psychological formulation of the goal of creating human-aligned AI. - I think that anxiety is good. I think, like I said,\nI want the free market to create aligned AIs that are reliable and I think that's what\nhe's trying to do with xAI. So I'm all for it. What I am against is sort\nof stopping, let's say, the open source ecosystem\nfrom thriving right? By, let's say, in the executive order, claiming that open source\nLLMs or dual use technologies then should be government controlled. Then everybody needs to register their GPU and their big matrices\nwith the government. And I think that extra friction will dissuade a lot of\nhackers from contributing, hackers that could later\nbecome the researchers that make key discoveries\nthat push us forward, right? Including discoveries for AI safety. And so I think I just wanna maintain ubiquity of opportunity\nto contribute to AI, and to own a piece of the future, right? It can't just be legislated,\nyou know, behind some wall where only a few players\nget to play the game. - I mean, so the e/acc movement\nis often sort of caricatured to mean sort of progress\nand innovation at all costs, doesn't matter how unsafe it is, doesn't matter if it\ncause a lot of damage. You just build cool shit\nas fast as possible, stay up all night with a\nDiet Coke, whatever it takes. I think, I guess, I don't know if there's\na question in there, but how important to you and what you've seen the\ndifferent formulations of e/acc is safety, is AI safety? - I think, again, I think like if there\nwas no one working on it, I think I would be a proponent of it. I think again, our goal is\nto sort of bring balance and obviously a sense of\nurgency is a useful tool, right, to make progress. It hacks our dopaminergic systems and gives us energy to\nwork late into the night. I think also having a higher purpose you're contributing to, right? At the end of the day, it's\nlike, what am I contributing to? I'm contributing to the growth\nof this beautiful machine so that we can seek to the stars. That's really inspiring. That's also a sort of neuro hack. - So you're saying AI\nsafety is important to you, but right now the\nlandscape of ideas you see is AI safety as a topic is used more often to\ngain centralized control. So in that sense, you're resisting it as a proxy for centralized,\ngaining centralized control. - Yeah, I just think we have to be careful because, you know, safety\nis just the perfect cover for sort of centralization of power and covering up eventually corruption. I'm not saying it's corrupted now, but it could be down the line. And really, if you let the argument run, like there's no amount of sort\nof centralization of control that will be enough to ensure your safety. There's always more 999s of pSafety that you can gain, you\nknow, 999.99999% safe. Maybe you want another nine? Oh, please give us full\naccess to everything you do. Full surveillance. And frankly, those that\nare proponents of AI safety have proposed like having\na global panopticon, right? Where you have centralized perception of everything going on. And to me, that just opens\nup the door wide open for a sort of big brother,\n1984-like scenario. And that's not a future I wanna live in. - 'Cause we know we have some\nexamples throughout history when that did not lead to a good outcome. You mentioned you founded\na company Extropic that recently announced a\n14.1 million seed round. What's the goal of the company? You're talking about a lot of\ninteresting physics things, so what are you up to over\nthere that you can talk about? - Yeah, I mean, you know, originally we weren't\ngonna announce last week, but I think with the\ndoxxing and disclosure, we got our hand forced. So we had to disclose\nroughly what we were doing, but really Extropic was\nborn from my dissatisfaction and that of my colleagues with the quantum computing roadmap, right? Quantum computing was\nsort of the first path to physics-based computing that, you know, was trying to commercially scale. And I was working on physics-based AI that runs on these\nphysics-based computers. But ultimately our greatest\nenemy was this noise, this pervasive problem\nof noise that, you know, as I mentioned, you have to constantly pump\nout the noise out of the system to maintain this pristine environment where quantum mechanics can take effect. And that constraint was just too much, it's too costly to do that. And so we were wondering, right, as generative AI is sort\nof eating the world, more and more of the world's\ncomputational workloads are focused on generative AI, how could we use physics to engineer the ultimate physical substrate\nfor generative AI, right? From first principles of\nphysics, of information theory, of computation, and ultimately\nof thermodynamics, right? And so what we're seeking to build is a physics-based computing system, and physics-based AI algorithms that are inspired by out of\nequilibrium thermodynamics or harness it directly to do machine learning\nas a physical process. - So what does that mean? Machine learning as a physical process. Is that hardware, is it\nsoftware, is it both? Is it trying to do the full stack in some kind of unique way? - Yes, it is full stack. And so we're folks that\nhave built, you know, differentiable programming into the quantum computing ecosystem with TensorFlow Quantum. One of my co-founders\nof TensorFlow Quantum is the CTO, Trevor McCourt. We have some of the best\nquantum computer architects, those that have designed\nIBM's and AWS's systems, they've left quantum\ncomputing to help us build what we call actually a\nthermodynamic computer. - A thermodynamic computer. Well, actually let's linger\non TensorFlow Quantum. What lessons have you learned\nfrom TensorFlow Quantum, maybe you can speak to like what it takes to create essentially what, like a software API to a quantum computer, - Right, I mean that was\na challenge to build, to invent, to build, and then to get to run\non the real devices. - [Lex] Can you actually\nspeak to what it is? - Yeah, so TensorFlow\nQuantum was an attempt at, well, I mean I guess we succeeded\nat combining deep learning or differentiable classical programming with quantum computing, and turn quantum computing\ninto, or have types of programs that are differentiable\nin quantum computing. And you know, Andrej Karpathy calls differentiable\nprogramming software 2.0, right? It's like gradient descent is\na better programmer than you. And the idea was that in the early days\nof quantum computing, you can only run short quantum programs. And so which quantum\nprograms should you run? Well, just let gradient descent\nfind those programs instead. And so we built sort of\nthe first infrastructure to not only run differentiable\nquantum programs, but combine them as part of\nbroader deep learning graphs, incorporating deep neural\nnetworks, you know, the ones you know and love with what are called\nquantum neural networks. And ultimately it was a very\ncross-disciplinary effort. We had to invent all sorts\nof ways to differentiate, to back propagate through\nthe graph, the hybrid graph. But ultimately it taught me that the way to program matter\nand to program physics is by differentiating\nthrough control parameters. If you have parameters that affects the physics of the system, and you can evaluate some loss function, you can optimize the system\nto accomplish a task, whatever that task may be. And that's a very sort of\nuniversal metaframework for how to program\nphysics-based computers. - So try to parameterize everything, make those parameters\ndifferential, and then optimize. - [Guillaume] Yes. - Okay, so is there some more\npractical engineering lessons from TensorFlow Quantum, just organizationally too,\nlike the humans involved, and how to get to a product, how to create good\ndocumentation, how do you have, I don't know, all these\nlittle subtle things that people might not think about. - I think like working across\ndisciplinary boundaries is always a challenge, and you have to be extremely patient in teaching one another, right? I learned a lot of software\nengineering through the process. My colleagues learned a\nlot of quantum physics and some learned machine learning through the process of\nbuilding this system. And I think if you get some\nsmart people that are passionate and trust each other in a room,\nand you have a small team, and you teach each other your specialties, suddenly you're kind of forming this sort of model soup of expertise, and something special\ncomes out of that, right? It's like combining genes,\nbut for your knowledge bases. And sometimes special\nproducts come out of that. And so I think like, even though it's very\nhigh friction initially to work in an interdisciplinary team. I think the product at the\nend of the day is worth it. And so learned a lot trying\nto bridge the gap there. And I mean, it's still\na challenge to this day. You know, we hire folks\nthat have an AI background, folks that have a pure physics background, and somehow we have to make\nthem talk to one another, right. - Is there a magic, is\nthere some science and art to the hiring process, to building a team that can create magic together? - Yeah, it's really hard to pinpoint that je ne sais quoi, right, the. - [Lex] I didn't know you\nspeak French. That's very nice. - Yeah, I'm actually French Canadian, so. - Oh, you are legitimately\nFrench Canadian. - [Guillaume] I am a legit. - I thought you were just\ndoing that for the cred. - No, no, I'm truly French\nCanadian from Montreal. But yeah, essentially we look for people with very high fluid intelligence that aren't overspecialized, because they're gonna have to\nget out of their comfort zone. They're gonna have to incorporate concepts that they've never seen before, and very quickly get\ncomfortable with them, right. Or learn to work in a team. And so that's sort of what\nwe look for when we hire. We can't hire, you know,\npeople that are just like, you know, optimizing this subsystem for the past three or four years. We need like really general sort of broader\nintelligence and specialty, and people that are open-minded, really. 'Cause if you're pioneering\na new approach from scratch, there is no textbook,\nthere's no reference. It's just us, and people\nthat are hungry to learn. So we have to teach each other, we have to learn the literature, we have to share knowledge\nbases, collaborate, in order to push the boundary\nof knowledge further together. Right, and so people that are used to just getting prescribed what to do, you know, at this stage when you're\nat the pioneering stage, that's not necessarily who\nyou want to hire, you know. - So you mentioned with Extropic you're trying to build\nthe physical substrate for generative AI. What's the difference between\nthat and the AGI, AI itself? So is it possible that in\nthe halls of your company, AGI will be created, or\nwill AGI just be using this as a substrate? - I think our goal is to\nboth run human-like AI or anthropomorphic AI. - Sorry for use of the term AGI, and I know it's triggering for you. - We think that the future\nis actually physics-based AI combined with anthropomorphic AI. So you can imagine, I have a\nsort of world modeling engine through physics-based AI, physics-based AI is better at representing the world at all scales, 'cause it can be quantum\nmechanical, thermodynamic, deterministic, hybrid\nrepresentations of the world. Just like our world at different scales has different regimes of physics. If you inspire yourself from that in the ways you learn\nrepresentations of nature, you can have much more accurate\nrepresentations of nature. So you can have very accurate world models at all scales, right? And so you have the world modeling engine, and then you have the\nsort of anthropomorphic AI that is human-like, so\nyou can have the science, the playground to test your ideas, and you can have a synthetic scientist. And to us, that joint\nsystem of a physics-based AI and an anthropomorphic AI is the closest thing to a fully general artificially intelligent system. - So you can get closer to truth by grounding of the AI to physics, but you can also still have\na anthropomorphic interface to us humans that like\nto talk to other humans or human-like systems. So on that topic, what do you, I suppose that is one\nof the big limitations of current large language models to you is that they're not,\nthey're good bullshitters, they're not really grounded\nto truth necessarily. Would that be fair to say? - Yeah, no, you wouldn't, you know, try to extrapolate the\nstock market with an LMM trained on texts from the internet, right? It's not gonna be a very accurate model. It's not gonna model its priors or its uncertainties about the world very accurately, right? So you need a different type of AI to compliment sort of this\ntext extrapolation AI, yeah. - You mentioned singularity earlier. How far away are we from a singularity? - I don't know if I believe\nin a finite time singularity as a single point in time. I think it's gonna be asymptotic, and sort of a diagonal sort of asymptote. Like, you know, we have the light cone, we have the limits of physics restricting our ability to grow. So obviously can't fully\ndiverge on a finite time. I think my priors are that, you know, I think a lot of people on\nthe other side of the aisle think that once we reach human level AI, there's gonna be an inflection\npoint and a sudden, like foom like suddenly AI is gonna\ngrok how to, you know, manipulate matter at the\nnanoscale and assemble nanobots. And having worked, you\nknow, for nearly a decade in applying AI to engineer matter, it's much harder than they think. And in reality, you need a lot of samples from either a simulation of nature that's very accurate and\ncostly, or nature itself. And that keeps your ability to control the world around us in check. There's a sort of minimal cost computationally and thermodynamically to acquiring information about the world in order to be able to\npredict and control it. And that keeps things in check. - It's funny you mentioned\nthe other side of the aisle. So in the poll I posted\nabout Pdoom yesterday what's the probability of doom. There seems to be a nice like division between people think it's\nvery likely and very unlikely. I wonder if in the future, there'll be the actual Republicans\nversus Democrats division blue versus red, is the AI doomers versus\nthe e/acc-ers, e/acc. - Yeah, so this movement, you know, is not right wing or\nleft wing fundamentally, it's more like up versus\ndown in terms of the scale. - [Lex] Which one is the up, okay. - Civilization, right? - [Lex] All right. - But it seems to be like there is a sort of case of alignment of the existing political parties where those that are\nfor more centralization of power, control, and more regulations are aligning with sort of, aligning themselves with the doomers, because that sort of\ninstilling fear in people is a great way for them\nto give up more control and give the government more power. But fundamentally we're\nnot left or versus right. I think, we've done polls of people's alignment within e/acc. I think it's pretty balanced. So it's a new fundamental\nissue of our time. It's not just centralization\nversus decentralization. It's kind of do we go, it's like tech progressivism\nversus technoconservatism, right? - So e/acc as a movement,\nis often formulated in contrast to EA, effective altruism. What do you think are the pros and cons of effective altruism? What's interesting,\ninsightful to you about them, and what is negative? - Right, I think like\npeople trying to do good from first principles is good. - We should actually say,\nand sorry to interrupt, we should probably say that, and you can correct me if I'm wrong, but effective altruism\nis the kind of movement that's trying to do good optimally where good is probably\nmeasured something like the amount of suffering in the world. You wanna minimize it. And there's ways that that can go wrong, as any optimization can. And so it's interesting to explore like how things can go wrong. - We're both trying to\ndo good to some extent, and we're both trying, we're arguing for which loss\nfunction we should use, right? Their loss function is\nsort of hedons, right? Units of hedonism, like\nhow good do you feel? And for how much time, right? And so suffering would be negative hedons, and they're trying to minimize that. But to us that seems\nlike that loss function has sort of spurious minima, right? You can, you know, start minimizing shrimp farm pain, right? Which seems not that productive to me. Or you can end up with wireheading\nwhere you just, you know, either install a neural link\nor you scroll TikTok forever and you feel good on\na short-term timescale because of your neurochemistry. But on long-term timescale, it causes decay and death, right, 'cause you're not being productive. Whereas sort of e/acc measuring\nprogress of civilization, not in terms of a subjective\nloss function like hedonism, but rather an objective measure, a quantity that cannot be gained, that is physical energy, right? It's very objective, right? And there's not many\nways to game it, right? If you did it in terms\nof like GDP or a currency that's pinned to a certain\nvalue that's moving, right? And so that's not a good\nway to measure our progress. And so, but the thing is we're\nboth trying to make progress and ensure humanity\nflourishes and gets to grow. We just have different loss functions and different ways of\ngoing about doing it. - Is there a degree, and maybe you can educate me, correct me, I get a little bit skeptical when there's an equation involved trying to reduce all of\nthe human civilization, human experience to an equation. Is there a degree that\nwe should be skeptical of the tyranny of an equation, of a loss function over which to optimize? Like having a kind of\nintellectual humility about optimizing over loss functions? - Yeah, so this particular loss function, it's not stiff, it's kind of\nan average of averages, right? It's like distributions\nof states in the future are gonna follow a certain distribution. So it's not deterministic. It's not like we're not on\nlike stiff rails, right? It's just a statistical\nstatement about the future. But at the end of the day, you know, you can believe in\ngravity or not, you know, but it's not necessarily an\noption to obey it, right? And some people try to test\nthat and that goes not so well. So similarly, you know, I\nthink thermodynamics is there whether we like it or not. And we're just trying\nto point out what is, and try to orient ourselves,\nand chart a path forward, given this fundamental truth. - But there's still some uncertainty, there's still lack of information. Humans tend to fill the gap\nof the lack of information with narratives. And so how they interpret, you know, even physics is up to interpretation when there's uncertainty involved. And humans tend to use that\nto further their own means. So it's always, whenever\nthere's an equation, it just seems like until we have really perfect\nunderstanding of the universe, humans will do what humans do and they try to use the\nnarrative of doing good to fool the populace into doing bad. I just, I guess that this is something that should be skeptical\nabout in all movements. - That's right. So we\ninvite skepticism, right. - Do you have an\nunderstanding of what might, to a degree that went wrong, what do you think may have gone wrong with effective altruism that might also go wrong with\neffective accelerationism? - Yeah, I mean I think, you know, I think it provided initially\na sense of community for, you know, engineers and intellectuals and rationalists in the early days. And it seems like the\ncommunity was very healthy. But then, you know, they formed\nall sorts of organizations and started routing capital\nand having actual power, right? They have real power. They influence the government, they influence most AI orgs now. I mean, they were literally controlling the board of OpenAI, right? And look over to Anthropic. I think they all have some\ncontrol over that too. And so I think, you know, the assumption of e/acc\nis more like capitalism is that every agent\norganism and metaorganism is gonna act in its own interests. And we should maintain sort\nof adversarial equilibrium or adversarial competition to keep each other in check\nat all times, at all scales. I think that, yeah, ultimately\nit was the perfect cover to acquire tons of power and capital. And unfortunately sometimes\nthat corrupts people over time. - What does a perfectly productive day, since building is important, what does a perfectly productive day in the life of Guillaume Verdon look like? How much caffeine do you consume? Like what's the perfect day? - Okay, so I have a particular regimen. I would say my favorite\ndays are 12 pm to 4 am, and I would have meetings\nin the early afternoon, usually external meetings,\nsome internal meetings, because I'm CEO, I have to\ninterface with the outside world, whether it's customers or investors or interviewing potential candidates. And usually I'll have\nketones, exogenous ketones. - [Lex] So you on a keto diet or is this. - I've done keto before\nfor football and whatnot, but I like to have a meal after sort of part of my day is done. And so I can just have extreme focus. - You do the social\ninteractions earlier in the day, without food. - Front load them, yeah, yeah. Like right now I'm on\nketones and Red Bull. And it just gives you a clarity of thought that is really next level,\n'cause then when you eat, you're actually allocating\nsome of your energy that could be going to neural\nenergy to your digestion. After I eat, maybe I take\na break an hour or so, an hour and a half. And then usually it's like, ideally one meal a day like\nsteak and eggs and vegetables, animal-based primarily, so fruit and meat. And then I do a second wind, usually, that's deep work, right? 'Cause I'm, you know, I am a\nCEO, but I'm still technical. I'm contributing to most patents. And there I'll just stay\nup late into the night, and work with engineers on\nvery technical problems. - So it's like the, the 9 pm to 4 an, whatever, that range of time. - Yeah, yeah, that's the perfect time. The emails, the things that\nare on fire stop trickling in. You can focus, and then\nyou have your second wind and you know, I think Demis Hassabis has a similar work day to some extent. So I think that that's\ndefinitely inspired my workday. But yeah, that I started this\nworkday when I was at Google, and had to manage a bit of\nthe product during the day and have meetings and then\ndo technical work at night. - Exercise, sleep, those kinds of things. Said football used to play football. - Yeah. I used to play American football. I've done all sorts of sports growing up. And then I was into\npowerlifting for a while. So when I was studying\nmathematics in grad school, I would just, you know, do math and lift, take caffeine and that was my day. It was very pure, the\npurest of monk modes. But it's really interesting how in powerlifting you're\ntrying to cause neural adaptation by having certain driving signals, and you're trying to\nengineer neuroplasticity through all sorts of supplements, and you know, you have\nall sorts of, you know, brain-derived neurotrophic factors that get secreted when you lift. So it's funny to me how\nI was trying to engineer neural adaptation in my\nnervous system more broadly, not just my brain while\nlearning mathematics. I think you can learn much\nfaster if you really care. If you convince yourself to care a lot about what you're learning, and you have some sort of\nassistance, let's say caffeine or some cholinergic supplement\nto increase neuroplasticity. I should chat with Andrew\nHuberman at some point. He's the expert. But yeah, at least to me it's like, you know, you can try to input\nmore tokens into your brain, if you will. And you can try to\nincrease the learning rate so that you can learn much\nfaster on a shorter timescale. So I've learned a lot of things.\nI've followed my curiosity. You're naturally, if you're passionate\nabout what you're doing, you're gonna learn faster, you're gonna become smarter faster. And if you follow your curiosity, you're always gonna be interested. And so I advise people to\nfollow their curiosity, and don't respect the\nboundaries of certain fields or what you've been allocated in terms of lane of\nwhat you're working on. Just go out and explore\nand follow your nose and try to acquire and compress as much information as\nyou can into your brain. Anything that you find interesting. - And caring about a thing, like you said, which is interesting, it\nworks for me really well. Is like tricking yourself\nthat you care about a thing. And then you start to\nreally care about it. So it's funny, the motivation is a really good catalyst for learning. - Right, and so at least part\nof my character as Beff Jezos is kind of like. - [Lex] Yeah, the hype man. - Yeah, just hype. But I'm like hyping myself up, but then I just tweet about it. And it's just when I'm\ntrying to get really hyped up and in like an altered\nstate of consciousness where I'm like ultrafocused,\nin the flow, wired, trying to invent something\nthat's never existed. I need to get to like, unreal\nlevels of like excitement. But your brain has these\nlevels of cognition that you can unlock with like higher levels\nof adrenaline and whatnot. And I mean, I've learned\nthat in powerlifting that actually you can\nengineer a mental switch to like increase your strength, right? Like if you can engineer a\nswitch, maybe you have a prompt, like a certain song or some music where suddenly you're like fully primed, then you're at maximum strength, right? And I've engineered that switch\nthrough years of lifting. If you're gonna get under 500\npounds and it could crush you, if you don't have that switch\nto be wired in, you might die. So that'll wake you right up. And that sort of skill I've\ncarried over to like research when it's go time, when\nthe stakes are high, somehow I just reach another\nlevel of neural performance. - So Beff Jezos is your\nsort of embodiment, representation of your intellectual Hulk. It's your productivity\nHulk that you just turn on. What have you learned about\nthe nature of identity from having these two identities? I think it's interesting for people to be able to put on those\ntwo hats so explicitly. - I think it was interesting\nin the early days, I think in the early days I thought it was truly compartmentalized. Like, oh yeah, this is\na character, you know, I'm Guillaume, Beff is just the character, I like, I like take my thoughts\nand then I extrapolate them to a bit more extreme. But you know, over time it's kind of like both identities were\nstarting to merge mentally. And people were like, no, you\nare, I met you, you are Beff, you are not just Guillaume. And I was like, wait, am I? And now it's like fully merged, but it was already before the doxx, it was already starting\nmentally that, you know, I am this character, it's part of me. - Would you recommend\npeople sort of have an alt. - [Guillaume] Absolutely. - Like young people,\nwould you recommend them to explore different\nidentities by having alts, alt accounts? - It's fun. It's like writing an essay\nand taking a position, right? It's like you do this in debate. It's like you can have\nexperimental thoughts, and by having, by the stakes being so low because you're an anon\naccount with I don't know, 20 followers or something, you can experiment with your thoughts, and in a low stakes environment. And I feel like we've lost that in the era of everything\nbeing under your main name, everything being attributable to you. People just are afraid to speak, explore ideas that aren't fully formed. Right, and I feel like\nwe've lost something there. So I hope, you know,\nplatforms like X and others like really help support people trying to stay synonymous or anonymous, because it's really important for people to share thoughts\nthat aren't fully formed and converge onto maybe hidden truths that were hard to converge upon if it was just through open\nconversation with real names. - Yeah, I really believe in like, not radical but rigorous empathy. It's like really considering\nwhat it's like to be a person of a certain viewpoint, and like taking that\nas a thought experiment farther and farther and farther. And one way of doing\nthat is an alt account. That's a fun, interesting\nway to really explore what it's like to be a person\nthat believes a set of beliefs and taking that across the span of several days, weeks, months. Of course there's always\nthe danger of becoming that. That's the Nietzsche\ngaze long into the abyss, the abyss gazes into you. You have to be careful. - Breaking Beff. - Yeah, right. Breaking Beff. Yeah, you wake up with\na shaved head one day, and it's just like, who am I? What have I become? So you've mentioned quite\na bit of advice already, but what advice would\nyou give to young people of how to, in this\ninteresting world we're in, how to have a career\nand how to have a life they can be proud of? - Hmm, I think to me, the reason I went to theoretical physics was that I had to learn\nthe base of the stack that was gonna stick around no matter how the\ntechnology changes, right? And to me, that was the\nfoundation upon which then I later built engineering\nskills and other skills. And to me the laws of physics, you know, it may seem like the landscape right now is changing so fast, it's disorienting. But certain things like\nfundamental mathematics and physics aren't gonna change. And if you have that knowledge, and knowledge about complex\nsystems and adaptive systems, I think that's gonna carry you very far. And so not everybody has\nto study mathematics, but I think it's really\na huge cognitive unlock to learn math and some\nphysics and engineering. - Get as close to the base\nof the stack as possible. - Yeah, that's right. 'Cause the base of the stack\ndoesn't change everything else. You know, your knowledge might become not as relevant in a few years. Of course there's a sort of\ntransfer learning you can do, but then you have to always\ntransfer learn constantly. - I guess the closer you are\nto the base of the stack, the easier the transfer\nlearning, the shorter the jump. - Right, right, and you'd be surprised, like once you've learned concepts in many physical scenarios, how they can carry over to\nunderstanding other systems that aren't necessarily physics. And I guess like the\ne/acc writings, you know, the principles and tenet posts\nthat was based on physics. That was kind of my experimentation with applying some of the thinking from out of equilibrium thermodynamics to understanding the world around us. And it's led to e/acc and this movement. - If you look at you're\none cog in the machine, in the capitalist machine, one human, and if you look at yourself, do you think mortality\nis a feature or a bug? Like would you want to be immortal? - No, I think fundamentally in thermodynamic dissipative adaptation, there's the word dissipation. Dissipation is important.\nDeath is important, right? We have a saying in physics, physics progresses one funeral at a time. I think the same is true for capitalism. Companies, empires, people, everything, everything must die at some point. I think that we should\nprobably extend our lifespan because we need a longer\nperiod of training, 'cause the world is more\nand more complex, right? We have more and more data to really be able to predict\nand understand the world. And if we have a finite window\nof higher neuroplasticity than we have sort of a hard cap in how much we can\nunderstand about our world. So, you know, I think I am for death, because again, I think\nit's important, you know, if you have like a king\nthat would never die, that would be a problem, right? Like the system wouldn't be\nconstantly adapting, right? You need novelty, you need\nyouth, you need disruption to make sure the system's\nalways adapting and malleable. Otherwise, if things are\nimmortal, you know, if you have, let's say corporations\nthat are there forever and they have them monopoly,\nthey get calcified, they become not as optimal,\nnot as high fitness in a changing time\nvarying landscape, right? And so death gives space for youth and novelty to take its place. And I think it's an important part of every system and nature. So yeah, I am for, I'm for death, but I do think that longer lifespan and longer time for\nneuroplasticity, bigger brains, should be something we should strive for. - Well, and that Jeff Bezos and Beff Jezos agree that all companies die. And for Jeff, the goal is to try to, he calls it day one\nthinking, try to constantly, for as long as possible, reinvent, sort of extend the life of the company. But eventually it too will die, 'cause it's so damn difficult\nto keep reinventing. Are you afraid of your own death? - I think I have ideas and\nthings I'd like to achieve in this world before I have to go, but I don't think I'm\nnecessarily afraid of death. - So you're not attached\nto this particular body and mind that you got? - No, I think I'm sure there's\ngonna be better versions of myself in the future or. - [Lex] Forks. - Forks, right. Genetic\nforks or other, right? I truly, I truly believe that. I think there's a sort of a\nevolutionary-like algorithm happening at every bit or NAT in the world is sort of adapting through this process that we describe in e/acc. And I think maintaining\nthis adaptation malleability is how we have constant\noptimization of the whole machine. And so I don't think I'm\nparticularly, you know, an optimum that needs\nto stick around forever. I think there's gonna be\ngreater optima in many ways. - What do you think is\nthe meaning of it all? What's the why of the\nmachine, the e/acc machine. - The why? Well, the\nwhy is thermodynamics. It's why we're here. It's what has led to the formation of life and of civilization, of\nevolution of technologies, and growth of civilization. But why do we have thermodynamics? Why do we have our particular universe? Why do we have these\nparticular hyperparameters, the constants of nature? Well, then you get into the\nanthropic principle, right, and the landscape of\npotential universes, right? We're in the universe\nthat allows for life. And then why is there\npotentially many universes, I don't know, I don't know that part. But could we potentially\nengineer new universes or create pocket universes\nand set the hyperparameters? So there is some mutual information between our existence and that universe. And we'd be somewhat its parents. I think that's really, I don't\nknow, that'd be very poetic. It's purely conjecture. But again, this is why\nfiguring out quantum gravity would allow us to understand\nif we can do that. - And above that, why is it all seem so\nbeautiful and exciting. The quest to figuring out quantum\ngravity seems so exciting. Why, why is that? Why\nare we drawn to that? Why are we pulled towards that? Just does that puzzle\nsolving creative force that underpins all of it, it seems like. - I think we seek, just like an LLM seeks\nto minimize cross entropy between its internal model and the world. We seek to minimize, yeah,\nthe statistical divergence between our predictions and\nthe world and the world itself. And you know, having\nregimes of energy scales or physical scales in which\nwe have no visibility, no ability to predict or perceive. You know, that's kind of an insult to us. And we want to be able to\nunderstand the world better in order to best steer, steer\nit or steer us through it. And in general, it's the\ncapability that has evolved because the better you\ncan predict the world, the better you can capture\nutility or free energy towards your own sustenance and growth. And I think quantum gravity, again, is kind of the final boss in\nterms of knowledge acquisition. Because once we've mastered that, then we can do a lot potentially. But between here and there, I think there's a lot to\nlearn in the mesoscales. There's a lot of information\nto acquire about our world and a lot of engineering perception, prediction and control to be done, to climb up the Kardashev scale. And to us, that's the great\nchallenge of our times. - And when you're not sure where to go, let the meme pave the way. - That's right. - Guillaume, Beff, thank\nyou for talking today. Thank you for the work you're doing. Thank you for the humor and the wisdom you put into the world. This was awesome. - Thank you so much for having\nme, Lex, it's a pleasure. - Thank you for listening\nto this conversation with Guillaume Verdon. To support this podcast, please check out our\nsponsors in the description. And now let me leave you with some words from Albert Einstein. \"If at first the idea is not absurd, then there is no hope for it.\" Thank you for listening. I\nhope to see you next time."
}