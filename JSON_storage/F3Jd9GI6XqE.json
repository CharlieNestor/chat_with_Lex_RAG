{
  "video_id": "F3Jd9GI6XqE",
  "title": "Edward Gibson: Human Language, Psycholinguistics, Syntax, Grammar &amp; LLMs | Lex Fridman Podcast #426",
  "date": "2024-04-17",
  "transcript": [
    {
      "timestamp": "0:00",
      "section": "Introduction",
      "text": "- Naively, I certainly\nthought that all humans would have words for exact counting, and the Piraha don't, okay? So they don't have any words for even one. There's not a word for\none in their language. And so there's certainly not a\nword for two, three, or four, and so that kind of\nblows people's minds off. - Yeah, that is blowing my mind. - [Edward] That's pretty weird, isn't it? - How are you gonna ask,\n\"I want two of those\"? - You just don't, and so\nthat's just not a thing you can possibly ask in the Piraha. It's not possible. That is, there's no words for that. - The following is a\nconversation with Edward Gibson, or Ted, as everybody calls him. He is a psycholinguistics\nprofessor at MIT. He heads the MIT Language\nLab that investigates why human languages look the way they do, the relationship between cultural language and how people represent,\nprocess, and learn language. Also, he should have a book titled \"Syntax, a Cognitive Approach,\" published by MIT Press,\ncoming out this fall. So look out for that. This is \"Lex Fridman Podcast.\" To support it, please\ncheck out our sponsors in the description. And now, dear friends,\nhere's Edward Gibson."
    },
    {
      "timestamp": "1:13",
      "section": "Human language",
      "text": "When did you first become\nfascinated with human language? - As a kid in school, when\nwe had to structure sentences in English grammar, I found\nthat process interesting. I found it confusing as to\nwhat it was I was told to do. I didn't understand what\nthe theory was behind it, but I found it very interesting. - So when you look at grammar, you're almost thinking\nabout it like a puzzle, like almost like a mathematical puzzle? - Yeah, I think that's right. I didn't know I was gonna work\non this at all at that point. I was really just, I was\nkind of a math geek person, computer scientist. I really liked computer science. And then I found language\nas a neat puzzle to work on from an engineering perspective, actually. That's what, I sort of accidentally, I decided after I finished\nmy undergraduate degree, which was computer science and math and Canada and Queens University, I decided to go to grad school. It's like, that's what I\nalways thought I would do. And I went to Cambridge\nwhere they had a master's in, a master's program in\ncomputational linguistics. And I hadn't taken a single\nlanguage class before. All I had taken was CS,\ncomputer science, math classes, pretty much, mostly as an undergrad. And I just thought this\nwas an interesting thing to do for a year. 'Cause it was a single year program. And then I ended up spending\nmy whole life doing it. - So fundamentally, your\njourney through life was one of a mathematician\nand a computer scientist. And then you kind of\ndiscovered the puzzle, the problem of language and\napproached it from that angle to try to understand it from that angle, almost like a mathematician\nor maybe even an engineer. - As an engineer, I'd\nsay, I mean, to be frank, I had taken an AI class, I\nguess it was '83 or '84, '85, somewhere '84 in there, a long time ago. And there was a natural\nlanguage section in there. And it didn't impress me. I thought there must be more\ninteresting things we can do. It didn't seem very, it seemed\njust a bunch of hacks to me. It didn't seem like a real\ntheory of things in any way. And so I just thought this\nseemed like an interesting area where there wasn't enough good work. - Did you ever come across\nthe philosophy angle of logic? So if you think about the '80s with AI, the expert systems\nwhere you try to kind of maybe sidestep the poetry of language and some of the syntax and the grammar and all that kind of stuff and\ngo to the underlying meaning that language is trying to communicate and try to somehow compress that in a computer-representable way. Do you ever come across\nthat in your studies? - I mean, I probably did, but\nI wasn't as interested in it. I was trying to do the\neasier problems first, the ones I could, thought\nmaybe were handleable, which seems like the syntax is easier, like which is just the forms\nas opposed to the meaning. Like when you're starting\nto talk about the meaning, that's a very hard problem. And it still is a really,\nreally hard problem. But the forms is easier. And so I thought at least figuring out the forms of human language,\nwhich sounds really hard, but is actually maybe more tractable. - So it's interesting. You think there is a big\ndivide, there's a gap, there's a distance\nbetween form and meaning. Because that's a question\nyou have discussed a lot with LLMs, because\nthey're damn good at form. - Yeah, I think that's what\nthey're good at, is form. - Yeah.\n- Exactly. And that's why they're good,\n'cause they can do form. Meaning's hard. - Do you think there's, oh, wow. And I mean, it's an open question, right? How close form and meaning are. We'll discuss it, but\nto me, studying form, maybe it's a romantic notion, gives you, form is like the shadow of the bigger meaning\nthing underlying language. Language is how we communicate ideas. We communicate with each\nother using language. So in understanding the\nstructure of that communication, I think you start to understand\nthe structure of thought and the structure of meaning\nbehind those thoughts and communication, to me. But to you, big gap."
    },
    {
      "timestamp": "5:19",
      "section": "Generalizations in language",
      "text": "- Yeah. - What do you find most\nbeautiful about human language? Maybe the form of human language, the expression of human language. - What I find beautiful\nabout human language is some of the generalizations that happen across the human languages, within and across a language. So let me give you an example of something which I find kind of remarkable, that is if a language,\nif it has a word order such that the verbs tend to\ncome before their objects, and so that's like English does that. So we have the first,\nthe subject comes first in a simple sentence. So I say, \"The dog chased the cat,\" or, \"Mary kicked the ball.\" So the subject's first,\nand then after the subject, there's the verb, and\nthen we have objects. All these things come after in English. So it's generally a verb,\nand most of the stuff that we wanna say comes after the subject. It's the objects, there's a lot of things we wanna say that come after. And there's a lot of languages like that. About 40% of the languages\nof the world look like that. They're subject, verb, object languages. And then these languages\ntend to have prepositions, these little markers on the nouns that connect nouns to other\nnouns or nouns to verbs. So a preposition like in,\nor on, or of, or about, I say I talk about something, the something is the\nobject of that preposition. We have these little markers come, just like verbs, they\ncome before their nouns. So now we look at other\nlanguages like Japanese, or Hindi, these are so-called\nverb final languages. Those, maybe a little more than 40%, maybe 45% of the world's languages, or more, I mean 50% of the\nworld's languages are verb final. Those tend to be postpositions. Those markers, the States\nhave the same kinds of markers as we do in English,\nbut they put 'em after. So, sorry, but they put 'em\nfirst, the markers come first. So you say, instead of, you\nknow, talk about a book, you say a book about,\nthe opposite order there, in Japanese or in Hindi,\nyou do the opposite. And the talk comes at the end. So the verb will come at the end as well. So instead of Mary kicked the\nball, it's Mary ball kicked. And then if it says Mary\nkicked the ball to John, it's John to, the to, the marker there, the preposition, it's a\npostposition in these languages. And so the interesting thing,\nfascinating thing to me, is that within a language, this\norder aligns, it's harmonic. And so if it's one or the other, if it's either verb initial or verb final, but then you'll have\nprepositions, prepositions, or postpositions. And so that, and that's\nacross the languages that we can look at. We've got around 1,000 languages for, there's around 7,000\nlanguages around on the Earth right now, but we have\ninformation about, say, word order on around 1,000 of those,\npretty decent amount of information. And for those 1,000 which we know about, about 95% fit that pattern. So they will have either verb,\nit's about half and half, half a verb initial, like English, and half a verb final, like Japanese. - So just to clarify, verb\ninitial is subject, verb, object. - [Edward] That's correct. - Verb final is still\nsubject, object, verb. - That's correct, yeah, the\nsubject is generally first. - That's so fascinating. I ate an apple, or I apple ate. - [Edward] Yes. - Okay, and it's fascinating that there's a pretty even division in\nthe world amongst those 45%. - Yeah, it's pretty even. And those two are the most common by far. Those two word orders, the\nsubject tends to be first. There's so many interesting things, but these things are, the\nthing I find so fascinating is there are these generalizations within and across a language. And not only those, and there's actually a simple explanation, I\nthink, for a lot of that. And that is, you're trying to minimize dependencies between words. That's basically the story, I think, behind a lot of why word\norder looks the way it is, is we're always connecting. What is the thing I'm telling you? I'm talking to you in sentences, you're talking to me in sentences. These are sequences of\nwords which are connected. And the connections are\ndependencies between the words. And it turns out that\nwhat we're trying to do in a language is actually\nminimize those dependency links. It's easier for me to say things if the words that are\nconnecting for their meaning are close together. It's easier for you in\nunderstanding if that's also true. If they're far away, it's\nhard to produce that, and it's hard for you to understand. And the languages of the world, within a language and across languages, fit that generalization, which is, so it turns out that having verbs initial and then having prepositions ends up making dependencies shorter. And having verbs final\nand having postpositions ends up making dependencies\nshorter than if you cross them. If you cross them, it\nends up, you just end up, it's possible, you can do it.\n- You mean within a language? - Within a language, you can do it. It just ends up with longer dependencies than if you didn't. And so languages tend to go that way. They tend to, they call it harmonic. So it was observed a long time\nago without the explanation by a guy called Joseph Greenberg, who's a famous typologist from Stanford. He observed a lot of generalizations about how word order works, and these are some of the\nharmonic generalizations that he observed. - Harmonic generalizations\nabout word order. There's so many things I wanna ask you. - [Edward] Okay, good. - Let me just, sometimes basics."
    },
    {
      "timestamp": "11:06",
      "section": "Dependency grammar",
      "text": "You mentioned dependencies a few times. What do you mean by dependencies? - Well, what I mean is, in language, there's kind of three structures to, three components to the\nstructure of language. One is the sounds. So cat is cuh, at, and tuh in English. I'm not talking about that part. I'm talking, then there's\ntwo meaning parts, and those are the words. And you were talking\nabout meaning earlier. So words have a form, and they have a meaning\nassociated with them. And so cat is a full form in English, and it has a meaning associated\nwith whatever a cat is. And then the combinations of words, that's what I'll call grammar or syntax. And that's like when I have\na combination like the cat or two cats, okay? So where I take two different words there and put them together, and I get a compositional meaning from putting those two\ndifferent words together. And so that's the syntax. And in any sentence or utterance, whatever I'm talking to\nyou, you're talking to me, we have a bunch of words, and we're putting together in a sequence. It turns out they are connected so that every word is connected to just one other word in that sentence. And so you end up with what's\ncalled technically a tree. It's a tree structure. So where there's a root of that\nutterance, of that sentence, and then there's a bunch of dependents, like branches from that root\nthat go down to the words. The words are the leaves in\nthis metaphor for a tree. - So a tree is also sort of\na mathematical construct. - [Edward] Yeah, yeah, it's\na graph theoretical thing. - It's a graph theory thing. So it's fascinating\nthat you can break down a sentence into a tree, and then every word is\nhanging on to another, it's depending on it. - That's right, and\neveryone agrees on that. So all linguists will agree with that. - [Lex] Oh, so this is\nnot a controversial- - That is not controversial. - There's nobody sitting here-\n- I do not think so. - Mad at you.\n- I don't think so. - Okay, there's no linguists\nsitting there mad at this. - No, I think in every language, I think everyone agrees that all sentences are trees at some level. - Can I pause on that?\n- Sure. - 'Cause to me, just as a layman, it's surprising that you\ncan break down sentences in mostly all languages-\n- All languages, I think. - Into a tree.\n- I think so. I've never heard of anyone\ndisagreeing with that. - That's weird. - The details of the trees are\nwhat people disagree about. - Well, okay, so what's\nat the root of a tree? How do you construct, how hard is it, what is the process of constructing\na tree from a sentence? - Well, this is where,\ndepending on what your, there's different theoretical notions. I'm gonna say the simplest\nthing, dependency grammar. It's like a bunch of people invented this. Tesniere was the first French guy back in, I mean, the paper was published in 1959, but he was working on the '30s and stuff. And it goes back to philologist Pignini was doing this in ancient India, okay? And so, doing something like this. The simplest thing we can think of is that there's just\nconnections between the words to make the utterance. And so, let's just say I\nhave two dogs entered a room. Okay, here's a sentence. And so, we're connecting\ntwo and dogs together. That's like, there's some\ndependency between those words to make some bigger meaning. And then we're connecting\ndogs now to entered, right? And we connect a room somehow to entered. And so, I'm gonna connect to room and then room back to entered. That's the tree is I, the root is entered. That's the thing is\nlike an entering event. That's what we're saying here. And the subject, which\nis whatever that dog is, is two dogs, it was. And the connection goes back to dogs, which goes back to, then\nthat goes back to two. I'm just, that's my tree. It starts at entered,\ngoes to dogs, down to two. And then the other side,\nafter the verb, the object, it goes to room, and then that goes back to the determiner or article, whatever you wanna call that word, a. So, there's a bunch of categories of words here we're noticing. So, there are verbs. Those are these things\nthat typically mark, they refer to events\nand states in the world. And there are nouns, which typically refer to people, places, and\nthings, is what people say. But they can refer to other more, they can refer to events\nthemselves as well. They're marked by how they, the category, the part of speech of a word is how it gets used in language. It's like, that's how you decide what the category of a word is. Not by the meaning, but how it gets used. - How it's used. What's usually the root? Is it gonna be the verb\nthat defines the event? - Usually, usually, yes, yes. - Okay.\n- Yeah. I mean, if I don't say a verb, then there won't be a verb,\nand so it'll be something else. - What if you're messing? Are we talking about language\nthat's like correct language? What if you're doing poetry\nand messing with stuff? Is it, then rules go\nout the window, right? Then it's-\n- No. - You're still-\n- No, no, no, no, no. You're constrained by whatever language you're dealing with. Probably you have other\nconstraints in poetry, such that you're, like usually in poetry, there's multiple constraints\nthat you want to, like you wanna usually\nconvey multiple meanings is the idea, and maybe\nyou have like a rhythm or a rhyming structure\nas well, and depending, but you usually are\nconstrained by the rules of your language for the most part, and so you don't violate those too much. You can violate them\nsomewhat, but not too much, so it has to be recognizable\nas your language. Like in English, I can't say,\n\"Dogs two entered room a.\" I mean, I meant that\ntwo dogs entered a room, and I can't mess with the\norder of the articles, the articles and the nouns,\nyou just can't do that. In some languages, you can mess around with the order of words much more. I mean, you speak Russian. Russian has a much freer\nword order than English, and so in fact, you can\nmove around words in, I told you that English has this subject, verb, object, word order, so does Russian, but Russian is much freer than English, and so you can actually mess\naround with the word order, so probably Russian poetry\nis gonna be quite different from English poetry because the word order is much less constrained. - Yeah, there's a much more\nextensive culture of poetry throughout the history of\nthe last 100 years in Russia, and I always wondered why that is, but it seems that there's more flexibility in the way the language is used. You're morphing the language easier by altering the words, altering\nthe order of the words, and messing with it. - Well, you can just mess\nwith different things in each language, and so in Russian, you have case markers,\nwhich are just these endings on the nouns which tell\nyou how it connects, each noun connects to the verb, right? We don't have that in English, and so when I say Mary kissed John, I don't know who the\nagent or the patient is except by the order of the words, right? In Russian, you actually\nhave a marker on the end if you're using a Russian name, and each of those names, you'll also say, is it agent, it'll be the nominative, which is marking the subject, or an accusative will mark the object, and you could put them\nin the reverse order. You could put accusative first, you could put subject, you\ncould put the patient first, and then the verb, and then the subject, and that would be a perfectly\ngood Russian sentence, and it would still mean, I\ncould say John kissed Mary, meaning Mary kissed John, as long as I use the case\nmarkers in the right way. You can't do that in English, and so- - I love the terminology\nof agent and patient, and the other ones you used. Those are sort of\nlinguistic terms, correct? - Those are, those are\nfor kind of meaning, those are meaning, and subject and object are generally used for position, so subject is just the thing\nthat comes before the verb, and the object is the one\nthat comes after the verb. The agent is kind of like the thing doing, that's kind of what that means, right? The subject is often the\nperson doing the action, right? The thing, so, yeah. - Okay, this is fascinating. So how hard is it to\nform a tree in general? Is there a procedure to it? Like if you look at different languages, is it supposed to be a very natural, like is it automatable, or is there some human genius involved? - I think it's pretty\nautomatable at this point. People can figure out what the words are. They can figure out the morphemes, which are the, technically, morphemes are the minimal meaning units\nwithin a language, okay? And so when you say eats or drinks, it actually has two morphemes in English. There's the root, which is the verb, and then there's some ending on it, which tells you that's\nthis third person singular. - [Lex] Can you say what morphemes are? - Morphemes are just the\nminimal meaning units within a language. And then a word is just kind of the things we put spaces between in English. They have a little bit more. They have the morphology as well. They have the endings,\nthis inflectual morphology on the endings on the roots. - They modify something about the word that adds additional meaning. - They tell you, yeah, yeah, yeah. And so we have a little\nbit of that in English, just very little, much more\nin Russian, for instance. But we have a little bit in English. And so we have a little on the nouns. You can say it's either\nsingular or plural. And you can say, same thing for verbs. Like simple past tense, for example, it's like notice in\nEnglish, we say drinks. He drinks, but everyone else\nsays I drink, you drink, we drink, it's unmarked in a way. But in the past tense, it's just drank. For everyone, there's no\nmorphology at all for past tense. There is morphology,\nit's marking past tense, but it's kind of, it's an irregular now. So we don't even, you\nknow, drink to drank, you know, it's not even a regular word. So in most verbs, many\nverbs, there's an ed, we kind of add, so walk to walked, we add that to say it's the past tense. That, I just happened\nto choose an irregular 'cause it's a high-frequency word. High-frequency words tend to\nhave irregulars in English. - [Lex] What's an irregular? - Irregular is just, there isn't a rule. So drink to drank is an irregular. - Drink, drank, okay, versus walked. - As opposed to walk,\nwalked, talked, talked. - And there's a lot of\nirregulars in English. - There's a lot of irregulars in English. The frequent ones, the common\nwords tend to be irregular. There's many, many more\nlow-frequency words, and those tend to be,\nthose irregular ones. - The evolution of the\nirregulars are fascinating. It is essentially slang that's sticky 'cause you're breaking the rules, and then everybody uses it\nand doesn't follow the rules, and they say screw it to the rules. It's fascinating."
    },
    {
      "timestamp": "21:05",
      "section": "Morphology",
      "text": "So you said morphemes, lots of questions. So morphology is what,\nthe study of morphemes? - Morphology is the connections between the morphemes\nonto the roots, the roots. So in English, we mostly have suffixes. We have endings on the\nwords, not very much, but a little bit, as opposed to prefixes. Some words, depending on your language, can have mostly prefixes,\nmostly suffixes, or both. And then even languages, several languages have things called infixes, where you have some kind of a general form for the root, and you\nput stuff in the middle. You change the vowels, stuff like that. - That's fascinating. That is fascinating. So in general, there's,\nwhat, two morphemes per word? Usually one or two, or three? - Well, in English, it's one or two. In English, it tends to be one or two. There can be more. In other languages, a\nlanguage like Finnish, which has a very elaborate morphology, there may be 10 morphemes\non the end of a root, okay? And so there may be millions\nof forms of a given word, okay? - Okay, I will ask the same\nquestion over and over, but how does the, just\nsometimes to understand things like morphemes, it's\nnice to just ask the question, how do these kinds of things evolve? So you have a great book\nstudying sort of the, how the cognitive processing, how language used for communication, so the mathematical notion\nof how effective language is for communication and what role that plays in the evolution of language. But just high level, like how do we, how does a language evolve with, where English has two morphemes, or one or two morphemes per word, and then Finnish has infinity per word? So what, how does that happen? Is it just people- - That's a really good question. That's a very good question, is like, why do languages have more morphology versus less morphology? And I don't think we\nknow the answer to this. I think there's just like\na lot of good solutions to the problem of communication. So I believe, as you hinted, that language is an\ninvented system by humans for communicating their ideas. And I think it comes down to, we label the things we wanna talk about. Those are the morphemes and words. Those are the things we wanna\ntalk about in the world, and we invent those things. And then we put 'em together in ways that are easy for us\nto convey, to process. But that's like a naive view, and I don't, I mean, I think it's\nprobably right, right? It's naive and probably right, but- - Well, that's the thing is,\nI don't know if it's naive. I think it's simple.\n- Simple, yeah. - I think naive is an\nindication that it's incorrect, and somehow it's a trivial,\ntoo simple, I think. It could very well be correct. But it's interesting how sticky, it feels like two people got together. It just feels like once you figure out certain aspects of a language, that just becomes sticky, and the tribe forms around that language. Maybe the language, maybe\nthe tribe forms first, and then the language evolves. And then you just kind of agree, and then you stick to whatever that is. - I mean, these are very\ninteresting questions. We don't know really about how words, even words, get invented very much, about, we don't really, I mean, assuming they get invented, we don't really know\nhow that process works and how these things evolve. What we have is kind of a current picture, a current picture of a\nfew thousand languages, a few thousand instances. We don't have any pictures of really how these things are evolving, really. And then the evolution is\nmassively confused by contact. So as soon as one language group, one group runs into another, we are smart. Humans are smart, and they take on whatever is useful in the other group. And so any kind of contrast\nwhich you're talking about, which I find useful, I'm\ngonna start using as well. So I worked a little bit\nin specific areas of words, in number words and in color words. And in color words, so\nwe have, in English, we have around 11 words that\neveryone knows for colors. And many more if you happen\nto be interested in color for some reason or other. If you're a fashion designer\nor an artist or something, you may have many, many more words. But we can see millions. Like if you have normal color vision, normal trichromatic color vision, you can see millions of\ndistinctions in color. So we don't have millions of words. The most efficient, no, the\nmost detailed color vocabulary would have over a million terms to distinguish all the different\ncolors that we can see, but of course we don't have that. So it's somehow, it's\nkind of useful for English to have evolved in some way to, so there's 11 terms that people\nfind useful to talk about. You know, black, white,\nred, blue, green, yellow, purple, gray, pink, and I\nprobably missed something there. Anyway, there's 11 that everyone knows. And depending on your, but\nyou go to different cultures, especially the\nnon-industrialized cultures, and there'll be many fewer. So some cultures will have\nonly two, believe it or not. The Dani in Papua New\nGuinea have only two labels that the group uses for color. Those are roughly black and white. They are very, very dark\nand very, very light, which are roughly black and white. And you might think, oh, they're dividing the whole color space into\nlight and dark or something. And that's not really true. They mostly just only label\nthe black and the white things. They just don't talk about\nthe colors for the other ones. And so, and then there's other groups. I worked with a group called\nthe Chimane down in Bolivia, in South America, and\nthey have three words that everyone knows,\nbut there's a few others that several people,\nthat many people know. And so they have, kind of\ndepending on how you count, it's between three and seven\nwords that the group knows. And again, they're black and white. Everyone knows those. And red, red is, that\ntends to be the third word that everyone, that cultures bring in. If there's a word, it's\nalways red, the third one. And then after that, it's\nkind of all bets are off about what they bring in. And so after that, they bring in a sort of a big blue-green group. They have one for that. And then they have, and\nthen different people have different words that they'll use for other parts of the space. And so anyway, it's probably related to what they wanna talk, what they, not what they see, 'cause they see the same colors as we see. So it's not like they have\na weak, a low-color palette in the things they're looking at. They're looking at a lot\nof beautiful scenery, okay? A lot of different colored\nflowers and berries and things. And so there's lots of\nthings of very bright colors. But they just don't label\na color in those cases. And the reason probably,\nwe don't know this, but we think probably what's going on here is that what you do,\nwhy you label something is you need to talk to\nsomeone else about it. And why do I need to talk about a color? Well, if I have two\nthings which are identical and I want you to give me\nthe one that's different, and the only way it varies is color, then I invent a word which tells\nyou this is the one I want. So I want the red sweater off the rack, not the green sweater, right? There's two, and so those\nthings will be identical because these are things\nwe made and they're dyed, and there's nothing different about them. And so in industrialized society, we have everything we've got is pretty much arbitrarily colored. But if you go to a\nnon-industrialized group, that's not true. And so they don't, it's not only that they're\nnot interested in color, if you bring bright-colored\nthings to them, they like them just like we like them. Bright colors are great,\nthey're beautiful. They are, but they just don't need to, no need to talk about\nthem, they don't have. - So probably color\nwords is a good example of how language evolves\nfrom sort of function when you need to communicate\nthe use of something. - [Edward] I think so. - Then you kind of invent\ndifferent variations. And basically, you can\nimagine that the evolution of a language has to do with\nwhat the early tribes doing, like what they wanted, what kind of problems are facing them, and they're quickly figuring out how to efficiently\ncommunicate the solution to those problems, whether\nit's aesthetic or functional, all that kind of stuff, running away from a mammoth or whatever. But it's, so I think\nwhat you're pointing to"
    },
    {
      "timestamp": "29:40",
      "section": "Evolution of languages",
      "text": "is that we don't have data\non the evolution of language, because many languages were\nformed a long time ago, so you don't get the chatter. - We have a little bit of\nold English to modern English because there was a writing system, and we can see how old English looked. So the word order changed, for instance, in old English to middle\nEnglish to modern English, and so we could see things like that. But most languages don't\neven have a writing system. So of the 7,000, only\na small subset of those have a writing system, and even if they have a writing system, it's not a very modern writing system, and so they don't have it. So we just basically have\nfor Mandarin, for Chinese, we have a lot of evidence for a long time, and for English, and not for much else. Not for main German a little bit, but not for a whole lot of\nlong-term language evolution. We don't have a lot. We just have snapshots, is what we've got, of current languages. - Yeah, you get an inkling of that from the rapid communication\non certain platforms, like on Reddit. There's different communities, and they'll come up with different slang, usually, from my perspective, driven by a little bit of humor, or maybe mockery or whatever. You know, just talking shit\nin different kinds of ways. And you could see the\nevolution of language there, because I think a lot of\nthings on the internet, you don't want to be\nthe boring mainstream. So you want to deviate from\nthe proper way of talking, and so you get a lot of\ndeviation, like rapid deviation. Then when communities\ncollide, you get like, just like you said, humans adapt to it, and you can see it\nthrough the lens of humor. I mean, it's very difficult to study, but you can imagine\nlike 100 years from now, if there's a new language\nborn, for example, we'll get really high resolution data. - I mean, English is changing. English changes all the time. All languages change all the time. So there's a famous result\nabout the Queen's English. So if you look at the Queen's vowels, the Queen's English is supposed to be, originally, the proper way for the talk was sort of defined by\nwhoever the Queen talked, or the King, whoever was in charge. And so if you look at\nhow her vowels changed from when she first became\nQueen in 1952 or '53, when she was coronated, the first, I mean, that's Queen Elizabeth, who died recently, of course, until 50 years later, her vowels changed. Her vowels shifted a lot, and so that, even in the sounds of British English, in her, the way she was\ntalking was changing. The vowels were changing slightly. So that's just, in the\nsounds, there's change. I don't know what's, I'm interested, we're all interested in what's\ndriving any of these changes. The word order of English\nchanged a lot over 1,000 years. So it used to look like German. It used to be a verb-final\nlanguage with case marking, and it shifted to a verb-medial language, a lot of contact, so a lot\nof contact with French, and it became a verb-medial\nlanguage with no case marking. And so it became this\nverb-initially thing. So that's-\n- It's evolving. - It totally evolved. I mean, it doesn't evolve\nmaybe very much in 20 years, is maybe what you're talking about. But over 50 and 100 years,\nthings change a lot, I think. - We'll now have good data\non it, which is great."
    },
    {
      "timestamp": "33:00",
      "section": "Noam Chomsky",
      "text": "- [Edward] That's for sure, yeah. - Can you talk to what is\nsyntax and what is grammar? So you wrote a book on syntax. - I did. You were asking me before\nabout how do I figure out what a dependency structure is. I'd say the dependency structures aren't that hard to, generally, I think there's a lot of\nagreement of what they are for almost any sentence in most languages. I think people will\nagree on a lot of that. There are other parameters in the mix such that some people think\nthere's a more complicated grammar than just a dependency structure. And so, you know, like Noam Chomsky, he's the most famous linguist ever. And he is famous for proposing a slightly more complicated syntax. And so he invented\nphrase structure grammar. So he's well known for many, many things, but in the '50s and early\n'60s, like the late '50s, he was basically figuring out what's called formal language theory. So, and he figured out sort of a framework for figuring out how complicated language, a certain type of language might be, so-called phrase structure\ngrammars of language might be. And so his idea was that\nmaybe we can think about the complexity of a language by how complicated the rules are, okay? And the rules will look like this. They will have a left-hand side and they'll have a right-hand side. Something on the\nleft-hand side will expand to the thing on the right-hand side. So say we'll start with an S, which is like the root,\nwhich is a sentence, okay? And then we're gonna expand to things like a noun phrase and a verb phrase is what he would say, for instance, okay? An S goes to an NP and a VP is a kind of a phrase structure rule. And then we figure out what an NP is. An NP is a determiner\nand a noun, for instance. And a verb phrase is something else, is a verb and another noun phrase, and another NP, for instance. Those are the rules of a very\nsimple phrase structure, okay? And so he proposed\nphrase structure grammar as a way to sort of cover human languages. And then he actually\nfigured out that, well, depending on the formalization\nof those grammars, you might get more complicated or less complicated languages. And so he said, well,\nthese are things called context-free languages that rule, that he thought human languages tend to be what he calls context-free languages. But there are simpler languages, which are so-called regular languages, and they have a more\nconstrained form to the rules of the phrase structure\nof these particular rules. So he basically discovered\nand kind of invented ways to describe the language. And those are phrase structure. A human language. And he was mostly interested\nin English initially in his work in the '50s. - So quick questions around all this. So formal language theory is the big field of just studying language formally. - Yes, and it doesn't have\nto be human language there. We can have computer\nlanguages, any kind of system which is generating some set\nof expressions in a language. And those could be like the statements in a computer language, for example. So it could be that, or it\ncould be human language. - So technically, you can\nstudy programming languages. - Yes, and have been. Heavily studied using this formalism. There's a big field of\nprogramming languages within the formal language. - Okay, and then phrase\nstructure, grammar, is this idea that you\ncan break down language into this S-N-P-V-P type of thing. - It's a particular formalism\nfor describing language. Okay, so and Chomsky was the first one. He's the one who figured that\nstuff out back in the '50s. But he, and that's equivalent, actually. The context-free grammar\nis kind of equivalent in the sense that it\ngenerates the same sentences as a dependency grammar would. The dependency grammar is a\nlittle simpler in some way. You just have a root, and it goes, we don't have any of these, the\nrules are implicit, I guess. We just have connections between words. The phrase structure grammar\nis kind of a different way to think about the dependency grammar. It's slightly more complicated, but it's kind of the same in some ways. - So to clarify, dependency grammar is the framework under\nwhich you see language, and you make the case\nthat this is a good way to describe language.\n- I think it's the, that's correct. - And Noam Chomsky's watching. This is very upset right now. So let's, I'm just kidding. But what's the difference between, where's the place of disagreement between phrase structure\ngrammar and dependency grammar? - They're very close. So phrase structure grammar\nand dependency grammar aren't that far apart. I like dependency grammar\nbecause it's more perspicuous, it's more transparent about representing the connections between the words. It's just a little harder to see in phrase structure grammar. The place where Chomsky sort of devolved or went off from this is, he also thought there was\nsomething called movement, okay? And so, and that's\nwhere we disagree, okay? That's the place where\nI would say we disagree. And I mean, maybe we'll\nget into that later, but the idea is, if you wanna, do you want me to explain that? - I would love, can you explain movement? - [Edward] Movement, okay, so Chomsky- - You're saying so many\ninteresting things. - Okay, so movement is, Chomsky basically sees\nEnglish and he says, okay, I said, we had\nthat sentence earlier, it was like two dogs entered the room. It's changed a little bit. Say, two dogs will enter the room. And he notices that, hey, English, if I wanna make a\nquestion, a yes/no question from that same sentence, I say, instead of two dogs will enter the room, I say, will two dogs enter the room? Okay, there's a different\nway to say the same idea. And it's like, well, the\nauxiliary verb, that will thing, it's at the front as opposed\nto in the middle, okay? And so, and he looked,\nif you look at English, you see that that's true\nfor all those modal verbs and for other kinds of\nauxiliary verbs in English, you always do that. You always put an auxiliary\nverb at the front. And when he saw that, so if I say, I can win this bet, can\nI win this bet, right? So I move a can to the front. So actually, that's a theory. I just gave you a theory there. He talks about it as movement. That word in the declarative is the root, is the sort of default way\nto think about the sentence, and you move the auxiliary\nverb to the front. That's a movement theory, okay? And he just thought\nthat was just so obvious that it must be true, that there's nothing\nmore to say about that, that this is how auxiliary\nverbs work in English. There's a movement rule\nsuch that you're moving, like to get from the declarative\nto the interrogative, you're moving the auxiliary to the front. And it's a little more complicated as soon as you go to simple\npresent and simple past, because if I say John\nslept, you have to say, did John sleep, not slept John, right? And so you have to somehow\nget an auxiliary verb, and I guess underlyingly,\nit's like slept is, it's a little more complicated than that, but that's his idea,\nthere's a movement, okay? And so a different way\nto think about that, that isn't, I mean, then\nhe ended up showing later. So he proposed this theory of\ngrammar, which has movement, and there's other places where\nhe thought there's movement, not just auxiliary verbs, but things like the passive in English, and things like questions, WH questions, a bunch of places where he thought there's also movement going on. And in each one of those,\nhe thinks there's words, well, phrases and words are moving around from one structure to another, which he called deep structure\nto surface structure. I mean, there's like\ntwo different structures in his theory, okay? There's a different way\nto think about this, which is there's no movement at all. There's a lexical copying rule, such that the word will, or the word can, these auxiliary verbs,\nthey just have two forms. And one of them is the declarative, and one of them is interrogative. And you basically have\nthe declarative one, and oh, I form the interrogative, or I can form one from the other, doesn't matter which direction you go. And I just have a new entry,\nwhich has the same meaning, which has a slightly\ndifferent argument structure. Argument structure is just a fancy word for the ordering of the words. And so if I say, it was the dogs, two dogs can or will enter the room, there's two forms of will. One is will declarative, and then, okay, I've got my subject to the\nleft, it comes before me, and the verb comes after me in that one. And then the will interrogative,\nit's like, oh, I go first. Interrogative, will is first, and then I have the\nsubject immediately after, and then the verb after that. And so you can just generate\nfrom one of those words another word with a slightly\ndifferent argument structure, with different ordering. - [Lex] And these are just lexical copies. They're not necessarily\nmoving from one to another. - There's no movement. - There's a romantic notion that you have one main way to use a word, and then you could move it around. - [Edward] Right, right, right. - Which is essentially\nwhat movement is implying. - Yeah, but that's the\nlexical copying is similar. So then we do lexical\ncopying for that same idea that maybe the declarative is the source, and then we can copy it. And so an advantage,\nthere's multiple advantages of the lexical copying story. It's not my story. This is like Ivan Sag, linguists, a bunch of linguists have\nbeen proposing these stories as well, in tandem with\nthe movement story. Okay, Ivan Sag died a while ago, but he was one of the\nproponents of the non-movement of the lexical copying story. And so that is that a great advantage is, well, Chomsky, really famously in 1971, showed that the movement story leads to learnability problems. It leads to problems for\nhow language is learned. It's really, really hard to figure out what the underlying\nstructure of a language is if you have both phrase\nstructure and movement. It's really hard to figure\nout what came from what. There's a lot of possibilities there. If you don't have that problem, the learning problem gets a lot easier. - Just say there's lexical copies. - [Edward] Yeah, yeah. - When we say the learning problem, do you mean humans\nlearning a new language? - Yeah, just learning English. So baby is lying around\nlistening to me talk, and how are they learning English? Or maybe it's a two-year-old\nwho's learning interrogatives and stuff, or how are they doing that? Are they doing it from,\nare they figuring out? So Chomsky said it's impossible\nto figure it out, actually. He said it's actually impossible. Not hard, but impossible. And therefore, that's where\nuniversal grammar comes from, is that it has to be built in. And so what they're learning\nis there's some built-in, movement is built in in his story, is absolutely part of\nyour language module. And then you're just setting parameters. You're said, depending on English, it's just sort of a variant\nof the universal grammar, and you're figuring out, oh, which orders does English do these things? That's the non-movement story. It doesn't have this. It's like much more bottom-up. You're learning rules. You're learning rules one by one, and oh, this word is\nconnected to that word. A great advantage, another\nadvantage, it's learnable, another advantage of\nit is that it predicts that not all auxiliaries might move. It might depend on the word,\ndepending on whether you, and that turns out to be true. So there's words that don't\nreally work as auxiliary. They work in declarative\nand not in interrogative. So I can say, I'll give\nyou the opposite first. I can say, aren't I invited to the party? And that's an interrogative form, but it's not from, I aren't\ninvited to the party. There is no, I aren't, right? So that's interrogative only. And then we also have forms like ought. I ought to do this. And I guess some old\nBritish people can say- - Ought I?\n- Exactly. It doesn't sound right, does it? For me, it sounds ridiculous. I don't even think ought is great, but I mean, I totally\nrecognize I ought to. I think it's not too bad, actually. I can say, ought to do this. That sounds pretty good.\n- Ought I? If I'm trying to sound\nsophisticated, maybe. - I don't know. It just sounds completely out to me. - Ought I?\n- Yeah. Anyway, so there are variants here. And a lot of these words just work in one versus the other. And that's fine under the\nlexical copying story. It's like, well, you just learn the usage. Whatever the usage is, is\nwhat you do with this word. But it's a little bit harder\nin the movement story. The movement story, that's an advantage, I think, of lexical copying. In all these different places, there's all these usage variants which make the movement story\na little bit harder to work. - So one of the main divisions here is the movement story versus\nthe lexical copy story. That has to do about the\nauxiliary words and so on. But if you rewind to the\nphrase structure grammar versus dependency grammar. - Those are equivalent in some sense in that for any dependency grammar, I can generate a phrase structure grammar which generates exactly\nthe same sentences. I just like the dependency\ngrammar formalism because it makes something really salient, which is the lengths of\ndependencies between words, which isn't so obvious\nin the phrase structure. In the phrase structure, it's\njust kind of hard to see. It's in there, it's just\nvery, very, it's opaque. - Technically, I think\nphrase structure grammar is mappable to dependency grammar. - And vice versa.\n- And vice versa. But there's these little\nlabels, S and P, V, P. - Yeah, for a particular\ndependency grammar, you can make a phrase structure grammar which generates exactly those\nsame sentences and vice versa. But there are many\nphrase structure grammars which you can't really\nmake a dependency grammar. I mean, you can do a lot more\nin a phrase structure grammar, but you get many more of\nthese extra nodes, basically. You can have more structure in there. And some people like that, and\nmaybe there's value to that. I don't like it. (laughs) - Well, for you, so we should clarify. So dependency grammar, it's just, well, one word depends\non only one other word, and you form these trees, and that makes, it really puts priority\non those dependencies just like as a tree that\nyou can then measure the distance of the dependency\nfrom one word to the other. They can then map to\nthe cognitive processing of these sentences, how\neasy it is to understand, and all that kind of stuff. So it just puts the focus on just the mathematical distance of dependence between words. So it's just a different focus. - Absolutely. - Just continue on the thread of Chomsky, 'cause it's really interesting, 'cause as you're discussing disagreement, to the degree there's disagreement, you're also telling the history\nof the study of language, which is really awesome. So you mentioned\ncontext-free versus regular. Does that distinction come into play for dependency grammars? - No, not at all. I mean, regular languages are too simple for human languages. It's a part of the hierarchy, but human languages in\nthe phrase structure world are definitely, they're\nat least context-free, maybe a little bit more, a\nlittle bit harder than that. So there's something called\ncontext-sensitive as well, where you can have, like this is just the\nformal language description. In a context-free grammar, you have one, this is like a bunch of\nformal language theory we're doing here, but- - [Lex] I love it. - Okay, so you have a\nleft-hand side category, and you're expanding to\nanything on the right, is a, that's a context-free. So the idea is that that\ncategory on the left expands in independent of\ncontext to those things, whatever they are on the\nright, doesn't matter what. And a context-sensitive says, okay, I actually have more\nthan one thing on the left. I can tell you only in this context, you know, maybe you have like\na left and a right context, or just a left context or a right context, I have two or more stuff on the left, tells you how to expand those\nthings in that way, okay? So it's context-sensitive. A regular language is\njust more constrained, and so it doesn't allow\nanything on the right. It allows very, basically\nit's one very complicated rule is kind of what a regular language is. And so it doesn't have any, I was gonna say\nlong-distance dependencies, it doesn't allow recursion, for instance. There's no recursion.\n- Recursion. - Yeah, recursion is where you, which is human languages have recursion, they have embedding, and you can't, well, it doesn't allow\ncenter-embedded recursion, which human languages have, which is what- - Center-embedded recursion,\nso within a sentence? Within a sentence.\n- Yeah, within a sentence. So here we're gonna get to that. But I, you know, the formal language stuff is a little aside. Chomsky wasn't proposing it\nfor human languages even, he was just pointing\nout that human languages are context-free, and then he was most in, for human, 'cause that was kind of stuff he did for formal languages, and what he was most interested\nin was human language, and that's like, the movement is where we, where he sort of set\noff on the, I would say, a very interesting, but wrong foot. It was kind of interesting, it's a very, I agree, it's a very interesting history. So there's this set, so he\nproposed this multiple theories, in '57 and then '65, they all\nhave this framework, though, was phrase structure plus movement, different versions of the phrase structure and the movement in the '57, these are the most famous\noriginal bits of Chomsky's work, and then '71 is when he figured out that those lead to learning problems, that there's cases where a\nkid could never figure out which rule, which set\nof rules was intended. And so, and then he said,\nwell, that means it's innate. It's kind of interesting,\nhe just really thought the movement was just so obviously true that he couldn't, he didn't\neven entertain giving it up, it's just obvious, that's obviously right. And it was later where people figured out that there's all these subtle ways in which things which\nlook like generalizations aren't generalizations, and\nthey, across the category, they're word-specific,\nand they kind of work, but they don't work\nacross various other words in the category, and so it's easier to just think of these\nthings as lexical copies. And I think he was very\nobsessed, I don't know, I'm just guessing, that he just, he really wanted this story\nto be simple in some sense, and language is a little more\ncomplicated in some sense. He didn't like words, he\nnever talks about words, he likes to talk about\ncombinations of words, and words are, you know,\nlook up a dictionary, there's 50 senses for\na common word, right? The word take will have\n30 or 40 senses in it. So there'll be many different\nsenses for common words, and he just doesn't think about that, or he doesn't think that's language. I think he doesn't think that's language. He thinks that words are distinct from combinations of words. I think they're the same. If you look at my brain in the scanner, while I'm listening to\na language I understand, and you compare, I can\nlocalize my language network in a few minutes, in like 15 minutes. And what you do is I listen\nto a language I know, I listen to, you know, maybe\nsome language I don't know, or I listen to muffled speech, or I read sentences, and I read non-words, like I can do anything like this, anything that's sort\nof really like English, and anything that's not very like English. So I've got something like it and not, and I got a control, and the voxels, which is just, you know,\nthe 3D pixels in my brain that are responding\nmost is a language area, and that's this left-lateralized\narea in my head. And wherever I look in that network, if you look for the\ncombinations versus the words, it's everywhere.\n- It's the same. - It's the same.\n- That's fascinating. - And so it's like hard to find, there are no areas that we know, I mean, that's, it's a\nlittle overstated right now. At this point, the technology isn't great, it's not bad, but we have the best way to figure out what's going on in my brain when I'm listening or reading language is to use fMRI, Functional\nMagnetic Resonance Imaging. And that's a very good\nlocalization method, so I can figure out where exactly these signals are coming from, pretty, you know, down to millimeters, you know, cubic millimeters\nor smaller, okay, very small, we can figure those out very well. The problem is the when, okay? It's measuring oxygen, okay? And oxygen takes a little\nwhile to get to those cells, so it takes on the order of seconds. So I talk fast, I probably listen fast, and I can probably understand\nthings really fast, so a lot of stuff happens in two seconds. And so to say that we\nknow what's going on, that the words, right\nnow, in that network, our best guess is that whole network is doing something similar, but maybe different parts of that network are doing different things. And that's probably the case, we just don't have very good methods to figure that out, right,\nat this moment, and so. - Since we're kind of\ntalking about the history of the study of language, what other interesting disagreements, and you're both at MIT,\nor were for a long time, what kind of interesting\ndisagreements there, tension of ideas are there\nbetween you and Noam Chomsky? And we should say that Noam was in the linguistics department, and you're, I guess for a\ntime were affiliated there, but primarily brain and\ncognitive science department, which is another way of studying language, and you've been talking about fMRI. So is there something else interesting to bring to the surface\nabout the disagreement between the two of you? Or other people in the- - Yeah, I mean, I've\nbeen at MIT for 31 years, since 1993, and he, Chomsky's\nbeen there much longer. So I met him, I knew him, I met when I first got there, I guess, and we would interact every now and then. I'd say that, so I'd say\nour biggest difference is, are methods, and so that's\nthe biggest difference between me and Noam, is that\nI gather data from people. I do experiments with people,\nand I gather corpus data, whatever corpus data's available, and we do quantitative methods to evaluate any kind\nof hypothesis we have. He just doesn't do that. So he has never once been associated with any experiment or corpus work ever. And so it's all thought experiments. It's his own intuitions. So I just don't think\nthat's the way to do things. That's a cross the street, there across the street\nfrom us kind of difference between Brain and CogSci and linguistics. I mean, not all linguists,\nsome of the linguists, depending on what you\ndo, more speech-oriented, they do more quantitative stuff, but in the meaning, words and, well, it's combinations of\nwords, syntax, semantics, they tend not to do experiments\nand corpus analyses. - So on the linguistic side, probably, but the method is a symptom\nof a bigger approach, which is sort of a psychology\nphilosophy side on Noam, when for you, it's more\nsort of data-driven, sort of almost like a\nmathematical approach. - Yeah, I mean, I'm a psychologist, so I would say we're in psychology. Brain and Cognitive Science is MIT's old psychology department. It was the psychology\ndepartment up until 1985, and it became the Brain and\nCognitive Science Department. And so, I mean, my training\nis math and computer science, but I'm a psychologist. I mean, I don't know what I am. - So data-driven\npsychologist, well, you are. - I am what I am, but I'm\nhappy to be called a linguist, I'm happy to be called\na computer scientist, I'm happy to be called a psychologist, any of those things. - But in the actual, like\nhow that manifests itself outside of the methodology\nis like these differences, these subtle differences\nabout the movement story versus the lexical copy story. - Yeah, those are theories, right? So the theories are, but I\nthink the reason we differ in part is because of how\nwe evaluate the theories. And so I evaluate theories quantitatively, and Noam doesn't. (laughing) - Got it. Okay, well, let's explore the theories that you explore in your book. Let's return to this\ndependency grammar framework of looking at language. What's a good justification why the dependency grammar framework is a good way to explain language? What's your intuition? - So the reason I like dependency grammar, as I've said before, is\nthat it's very transparent about its representation\nof distance between words. So it's like, all it is is\nyou've got a bunch of words, you're connecting together\nto make a sentence. And a really neat insight,\nwhich turns out to be true, is that the further apart\nthe pair of words are that you're connecting, the harder it is to do the production, the harder it is to do the comprehension. It's harder to produce,\nit's harder to understand when the words are far apart. When they're close together,\nit's easy to produce and it's easy to comprehend. Let me give you an example, okay? So we have, in any language, we have mostly local\nconnections between words, but they're abstract. The connections are abstract, they're between categories of words. And so you can always\nmake things further apart if you add modification,\nfor example, after a noun. So a noun in English comes before a verb, the subject noun comes before a verb, and then there's an\nobject after, for example. So I can say what I said before, the dog entered the room\nor something like that. So I can modify dog. If I say something more\nabout dog after it, then what I'm doing is, indirectly, I'm lengthening the dependence\nbetween dog and entered by adding more stuff to it. So just to make it explicit here, if I say the boy who\nthe cat scratched cried, we're gonna have a mean cat here. And so what I've got here is, the boy cried, it would be a\nvery short, simple sentence, and I just told you\nsomething about the boy, and I told you it was the boy\nwho the cat scratched, okay? - So the cry is connected to the boy. - The boy's gonna cry.\n- The cry at the end is connected to the boy in the beginning. - Right, and so I can do that. I can say that, that's a\nperfectly fine English sentence. And I can say the cat which the dog chased ran away or something, okay? I can do that. But it's really hard now,\nwhatever I have here, I have the boy who the cat, now let's say I try to modify cat, okay? The boy who the cat which the\ndog chased scratched ran away. Oh my God, that's hard, right? I'm sort of just working\nthat through in my head, how to produce, and it's\nreally just horrendous to understand, it's not so bad. At least I've got intonation there to sort of mark the boundaries and stuff, but that's really complicated. That's sort of English, in a way. I mean, that follows the rules of English, but so what's interesting about that is that what I'm doing is\nnesting dependencies here. I'm putting one, I've got a subject connected to a verb there, and then I'm modifying that\nwith a clause, another clause, which happens to have a\nsubject and a verb relation. I'm trying to do that\nagain on the second one. And what that does is it\nlengthens out the dependence, multiple dependents actually\nget lengthened out there. The dependencies get longer, on the outside ones get long, and even the ones in\nbetween get kind of long. And you just, so what's\nfascinating is that, that's bad, that's really\nhorrendous in English. But that's horrendous in any language. So in no matter what language you look at, if you do, just figure out some structure where I'm gonna have some\nmodification following some head, which is connected to some later head, and I do it again, it\nwon't be good, guaranteed. Like 100%, that will be\nuninterpretable in that language, in the same way that was\nuninterpretable in English. - Just to clarify, the\ndistance of the dependencies is whenever the boy cried, there's a dependence between two words, and then you're counting\nthe number of what, morphemes between them? - That's a good question. I'll just say words, your\nwords are morphemes between. We don't know that, actually,\nthat's a very good question. What is the distance metric? But let's just say it's words, sure. - And you're saying\nthe longer the distance of that dependence, the\nmore, no matter the language, except Ligali's, even Ligali,\nokay, we'll talk about it. But that, the people will be very upset that speak that language. Not upset, but they'll\neither not understand it, they'll be like, their brain\nwill be working in overtime. - They will have a hard\ntime either producing or comprehending it. They might tell you\nthat's not their language. It's sort of their language. I mean, it's following their, like they'll agree with\neach of those pieces as part of their language, but somehow that combination\nwill be very, very difficult to produce and understand. - Is that a chicken or the egg issue here? So like, is- - Well, I'm giving you an explanation. - Right. - So the, well, I mean, and then there's, I'm giving you two kinds of explanations. I'm telling you that center\nembedding, that's nesting, those are the same, those are synonyms for the same concept here. And the explanation for\nwhat, those are always hard. Center embedding and\nnesting are always hard. And I gave you an explanation\nfor why they might be hard, which is long-distance connections. There's a, when you do center embedding, when you do nesting, you always have long-distance connections\nbetween the dependents. You just, and so that's not necessarily the right explanation. It just, I can go through reasons why that's probably a good explanation. And it's not really\njust about one of them. So probably it's a pair\nof them or something of these dependents that you get long that drives you to like be\nreally confused in that case. And so what the behavioral\nconsequence there, I mean, this is kind of methods, like how do we get at this? You could try to do experiments to get people to produce these things. They're gonna have a\nhard time producing them. You can try to do experiments\nto get them to understand them and see how well they understand them. Can they understand them? Another method you can do is\ngive people partial materials and ask them to complete them. You know, those center-embedded materials, and they'll fail. (laughs) So I've done that, I've done\nall these kinds of things. - So wait a minute. So central embedding, meaning, like you take a normal\nsentence like boy cried and inject a bunch of crap in the middle that separates the boy and the cried. Okay, that's central embedding. And nesting is on top of that. - No, no, nesting is the same thing. Central embedding, those are\ntotally equivalent terms. I'm sorry, I sometimes use one\nand sometimes use the other. - Oh, got it, got it. Totally equivalent.\n- They don't mean anything different.\n- Got it. And then what you're\nsaying is there's a bunch of different kinds of\nexperiments you can do. I mean, I like the understanding one is like have more embedding,\nmore central embedding. Is it easier or harder to understand? But then you have to measure the level of understanding, I guess. - Yeah, yeah, you could. I mean, there's multiple ways to do that. I mean, there's the simplest\nway is just ask people how good does it sound? How natural is the sound? That's a very blunt,\nbut very good measure. If it's very, very reliable,\npeople will do the same thing. And so it's like, I don't\nknow what it means exactly, but it's doing something\nsuch that we're measuring something about the confusion, the difficulty associated with those. - And those are giving you a signal. That's why you can say that.\n- Yeah, yeah. - What about the completion\nof the central embedding? - So if you give them a partial sentence, say I say the book which the author who, and I ask you to now\nfinish that off for me. I mean, either say it.\n- It breaks people's brains. - Yeah, yeah, but you\ncan just say it's written in front of you, and you can just type and have as much time as you want. They will, even though that\none's not too hard, right? So if I say it's like the book, it's like, oh, the book which the author\nwho I met wrote was good. That's a very simple completion for that. If I give that completion online somewhere to a crowdsourcing platform and\nask people to complete that, they will miss off a verb very regularly, like half the time, maybe 2/3 of the time, they'll just leave off\none of those verb phrases. Even with that simple, so to say the book which the author who, and they'll say was, you need three verbs, right? Who I met wrote was good,\nand they'll give me two. They'll say who was famous was\ngood, or something like that. They'll just give me\ntwo, and that'll happen about 60% of the time, so 40%, maybe 30, they'll do it correctly, correctly, meaning they'll do a three-verb phrase. I don't know what's correct or not. This is hard, it's a hard task. - Yeah, actually, I'm\nstruggling with it in my head. - Well, it's easier-\n- When you stare at it. - If you look, it's a little easier than listening, it's pretty tough, 'cause you have to, 'cause\nthere's no trace of it. You have to remember the\nwords that I'm saying, which is very hard auditorily. We wouldn't do it this way. We do it written, you can\nlook at it and figure it out. It's easier in many\ndimensions in some ways, depending on the person. It's easier to gather written data for, I mean, most, I work in\npsycholinguistics, right? Psychology of language and stuff, and so a lot of our work\nis based on written stuff because it's so easy to gather data from people doing written kinds of tasks. Spoken tasks are just more complicated to administer and analyze, because people do weird\nthings when they speak, and it's harder to analyze what they do, but they generally point to\nthe same kinds of things. - So, okay, so the\nuniversal theory of language by Ted Gibson is that\nyou can form dependency, you can form trees from any sentences, and you can measure the\ndistance in some way of those dependencies, and then you can say that most languages have very short dependencies. - All languages.\n- All languages. - All languages have short dependencies. You can actually measure that, so a next student of mine, this guy's at University\nof California, Irvine, Richard Futrell did a thing\na bunch of years ago now where he looked at all the\nlanguages we could look at, which was about 40 initially, and now I think there's about 60, for which there are dependency structures, so meaning there's gotta\nbe like a big text, bunch of texts, which have been parsed for\ntheir dependency structures, and there's about 60 of those which have been parsed that way, and for all of those, what he did was take any sentence\nin one of those languages, and you can do the dependency structure, and then start at the root, we're talking about dependency structures, that's pretty easy now, and he's trying to figure out what a control way you might say the same sentence is in that language, and so he's just like, all right, there's a root, and it has, let's say as a sentence is, let's go back to two\ndogs entered the room, so entered is the root, and entered has two dependents, it's got dogs, and it has room, and what he does is like, let's scramble that order, that's three things, the root, and the head,\nand the two dependents, in just some random order, just random, and then just do that for all\nthe dependents down the tree, so now look, do it for the, and whatever it was,\ntwo, and dogs, and room, and that's not a, it's\na very short sentence, when sentences get longer, and you have more dependents, there's more scrambling that's possible, and what he found was, so that's one, you can figure out one\nscrambling for that sentence, he did this like 100 times, for every sentence in\nevery one of these texts, every corpus, and then he just compared\nthe dependency lengths in those random scramblings to what actually happened, what the English, or the French, or the German was in\nthe original language, or Chinese, or what all\nthese like 60 languages, and the dependency\nlengths are always shorter in the real language, compared to this kind of a control, and there's another, it's a little more rigid, his control, so the way I described it, you could have crossed dependencies, like by scrambling that way, you could scramble in any way at all, languages don't do that, they tend not to cross\ndependencies very much, like so the dependency structure, they tend to keep things non-crossed, and there's a technical term, they call that projective, but it's just non-crossed\nis all that is projective, and so if you just\nconstrain the scrambling, so that it only gives you projective, sort of non-crossed, the same thing holds, so still human languages are much shorter than this kind of a control, so there's like, what it means is that we're, in every language, we're\ntrying to put things close, relative to this kind of a control, like it doesn't matter\nabout the word order, some of these are verb-final, some of these are verb-media-like English, and some are even verb-initial, there are a few languages of the world which have VSO, word order, verb, subject, object languages, haven't talked about those, it's like 10% of the- - And even in those languages, it's still short dependencies. - Short dependencies is rules. - Okay, so what are some\npossible explanations for that, for why languages have evolved that way? So that's one of the, I suppose, disagreements\nyou might have with Chomsky, so you consider the evolution of language in terms of information theory, and for you, the purpose of language is ease of communication,\nright, and processing. - That's right, that's right, so I mean, the story here\nis just about communication, it is just about production, really, it's about ease of\nproduction, is the story. - When you say production, can you- - Oh, I just mean ease\nof language production, it's easier for me to say things when I'm doing, whenever\nI'm talking to you, so somehow I'm formulating\nsome idea in my head and I'm putting these words together, and it's easier for me to do that, to say something where the words are closely connected in a dependency as opposed to separated, like by putting something in between and over and over again, it's just hard for me\nto keep that in my head, like that's the whole story, like the story is basically, it's like the dependency grammar\nsort of gives that to you, like just like long is bad, short is good, it's like easier to keep in mind because you have to keep it in mind for, probably for production, probably matters in comprehension as well, like also matters in comprehension. - It's on both sides of\nit, the production and the- - But I would guess it's\nprobably evolved for production, like it's about producing, it's what's easier for me to say, that ends up being easier for you also, that's very hard to disentangle, this idea of who is it for, is it for me, the speaker, or is it for you, the listener? I mean, part of my language is for you, like the way I talk to\nyou is gonna be different from how I talk to different people, so I'm definitely angling what I'm saying to who I'm saying, right? It's not like I'm just\ntalking the same way to every single person, and so I am sensitive to my audience, but does that work itself out in the dependency length differences? I don't know, maybe that's\nabout just the words, that part, which words I select. - My initial intuition is\nthat you optimize language for the audience. - [Edward] Yeah, but it's both. - It's just kinda like messing\nwith my head a little bit to say that some of the\noptimization might be, it may be the primary\nobjective of the optimization might be the ease of production. - We have different senses, I guess. I'm very selfish. (laughs) And you're like, I\nthink it's all about me, I'm like, I'm just doing\nwhat's easiest for me. - [Lex] What's easiest for me. - I'm like, I mean, but\nI have to, of course, choose the words that I\nthink you're gonna know. I'm not gonna choose words you don't know. In fact, I'm gonna fix that when I, so there it's about, but\nmaybe for the syntax, for the combinations, it's just about me. I feel like it's, I don't know, though. It's very hard to-\n- Wait, wait, wait, wait, but the purpose of communication\nis to be understood, is to convince others and so on. So like the selfish thing\nis to be understood. - Okay, yeah, it's a little\ncircular there, too, then. Okay.\n- Right. I mean, like the ease of production- - [Edward] Helps me be understood, then. I don't think it's circular. So I want what's-\n- No, I think the primary, I think the primary objective\nis to be understood, is about the listener, 'cause otherwise, if you're optimizing for\nthe ease of production, then you're not gonna have any of the interesting complexity of language. Like you're trying to explain- - Well, let's control for\nwhat it is I wanna say. I'm saying let's control\nfor the thing, the message, control for the message. I wanna tell you-\n- But that means the message needs to be understood. That's the goal. - Oh, but that's the meaning. So I'm still talking about the form. Just the form of the meaning. How do I frame the form of the meaning is all I'm talking about. You're talking about a\nharder thing, I think. It's like how am I, like\ntrying to change the meaning. Let's keep the meaning constant. Like which-\n- Got it. - Yeah, if you keep the meaning constant, how can I phrase whatever\nit is I need to say, like I gotta pick the right words, and I'm gonna pick the\norder so it's easy for me. That's what I think is probably the way. - I think I'm still tying meaning and form together in my head. But you're saying if you keep the meaning of what you're saying constant, would the optimization, yeah, it could be the primary objective that optimization is for production. That's interesting. I'm struggling to keep constant meaning. It's just so, I mean, I'm a human, right? So for me, the form, without\nhaving introspected on this, the form and the meaning are\ntied together, like deeply, because I'm a human. Like for me, when I'm speaking, 'cause I haven't thought about language, like in a rigorous way,\nabout the form of language. - Look, for any event,\nthere's an unbounded, I don't wanna say\ninfinite, but sort of ways that I might communicate that same event. This two dogs entered a room, I can say, in many, many different ways. I can say, hey, there's two dogs. They entered the room. Hey, the room was entered by something. The thing that was entered was two dogs. I mean, it's kind of\nawkward and weird and stuff, but those are all similar\nmessages with different forms, but different ways I might frame. And of course, I use the same\nwords there all the time. I could have referred to\nthe dogs as a Dalmatian and a poodle or something. I could have been more\nspecific or less specific about what they are,\nand I could have said, been more abstract about the number. There's like, so I'm\ntrying to keep the meaning, which is this event, constant, and then how am I gonna describe\nthat to get that to you? It kind of depends on what\nyou need to know, right, and what I think you need to know, but I'm like trying to, let's\ncontrol for all that stuff and not, and it's like,\nI'm just choosing about, I'm doing something\nsimpler than you're doing, which is just forms.\n- Yes. - Just words.\n- So to you, specifying the breed of dog and whether they're cute or\nnot is changing the meaning. - That might be, yeah, yeah,\nthat would be changing, oh, that would be changing\nthe meaning for sure. - Right, so you're just,\nyeah, well, yeah, yeah. - That's changing the meaning, but say, even if we keep that constant, we can still talk about\nwhat's easier or hard for me, right, the listener and the, right? Which phrase structures I use,\nwhich combinations, which. - This is so fascinating, and just like a really powerful window\ninto human language, but I wonder still throughout this how vast the gap between meaning and form. I just have this like\nmaybe romanticized notion that they're close together, that they evolve close, like hand in hand, that you can't just\nsimply optimize for one without the other being\nin the room with us. Like, it's, well, it's\nkind of like an iceberg. Form is the tip of the iceberg, and the rest, the meaning is the iceberg, but you can't like separate. - But I think that's why\nthese large language models are so successful, is\n'cause they're good at form, and form isn't that hard in some sense. And meaning is tough still,\nand that's why they're not, they're, you know, they don't\nunderstand what they're, we're gonna talk about that later maybe, but like we can distinguish in our, forget about large language\nmodels, like humans, maybe you'll talk about that later too, is like the difference between language, which is a communication system, and thinking, which is meaning. So language is a communication\nsystem for the meaning, it's not the meaning. And so that's why, I mean, and there's a lot of interesting evidence we can talk about relevant to that. - Well, I mean, that's a\nreally interesting question."
    },
    {
      "timestamp": "1:17:06",
      "section": "Thinking and language",
      "text": "What is the difference between language, written, communicated, versus thought? What do you use the\ndifference between them? - Well, you or anyone\nhas to think of a task which they think is a good thinking task, and there's lots and lots of tasks which should be good thinking tasks. And whatever those tasks are, let's say it's, you know, playing chess, or that's a good thinking task, or playing some game, we're\ndoing some complex puzzles, maybe remembering some\ndigits, that's thinking, remembering some, a lot of\ndifferent tasks we might think, maybe just listening to music is thinking, or there's a lot of different tasks we might think of as thinking. There's this woman in my\ndepartment, Eve Fedorenko, and she's done a lot of\nwork on this question about what's the connection\nbetween language and thought. And so she uses, I was\nreferring earlier to MRI, fMRI, that's her primary method. And so she has been really\nfascinated by this question about whether, what language is, okay? And so, as I mentioned earlier, you can localize my language area, your language area, in\na few minutes, okay? In like 15 minutes, I\ncan listen to language, listen to non-language, or\nbackward speech, or something, and we'll find areas, left\nlateralized network in my head, which is especially, which is\nvery sensitive to language, as opposed to whatever\nthat control was, okay? - Can you specify what\nyou mean by language, like communicated language? Like what is language?\n- Just sentences. You know, I'm listening\nto English of any kind, story, or I can read sentences, anything at all that I\nunderstand, if I understand it, then it'll activate my language network. So right now, my language\nnetwork is going like crazy when I'm talking and when\nI'm listening to you, because we're both, we're communicating. - And that's pretty stable. - Yeah, it's incredibly stable. So I've, I happen to be\nmarried to this woman, Eve Fedorenko, and so\nI've been scanned by her over and over and over since\n2007, or six, or something. And so my language network\nis exactly the same, you know, like a month ago,\nas it was back in 2007. It's amazingly stable, it's astounding. And with it, it's a really\nfundamentally cool thing. And so my language network\nis like my face, okay? It's not changing much\nover time, inside my head. - Can I ask a quick question? Sorry, it's a small tangent. At which point in the, as you\ngrow up from baby to adult, does it stabilize? - We don't know. Like, that's a very hard question. They're working on that right now, because of the problem\nscanning little kids. Like, doing the, trying to do local, trying to do the localization\non little children in this scanner, or you're\nlying in the fMRI scan. That's the best way to figure out where something's going\non inside our brains. And the scanner is loud, and you're in this tiny little\narea, you're claustrophobic. And it doesn't bother me at all. I can go to sleep in there. But some people are bothered by it, and little kids don't really like it, and they don't like to lie still. And you have to be really still, because if you move around, that messes up the coordinates\nof where everything is. And so, you know, try to get, you know, your question is, how and\nwhen are language developing? You know, how does this\nleft-lateralized system come to play? And it's really hard to get a\ntwo-year-old to do this task. But you can maybe, where they're starting\nto get three, and four, and five-year-olds to do\nthis task for short periods. And it looks like it's there pretty early. - So clearly, when you lead up to, like, a baby's first words, before that, there's a lot of\nfascinating turmoil going on about, like, figuring out, like, what are these people saying? And you're trying to, like, make sense, how does that connect to the world, and all that kind of stuff. Yeah, that might be just\nfascinating development that's happening there. That's hard to introspect. - But anyway, we're back to the scanner. And I can find my network in 15 minutes. And now we can ask, find my network, find\nyours, find, you know, 20 other people do this task. And we can do some other tasks. Anything else you think is\nthinking of some other thing. I can do a spatial memory task. I can do a music perception task. I can do a programming\ntask, if I program, okay? I can do, where I can, like, understand computer programs. And none of those tasks tap\nthe language network at all. Like, at all. There's no overlap. They're highly activated in\nother parts of the brain. There's a bilateral network, which I think she tends to call the multiple demands network, which does anything kind of hard, and anything that's kind\nof difficult in some ways will activate that\nmultiple demands network. I mean, music will be in some music area. You know, there's\nmusic-specific kinds of areas. But none of them are activating\nthe language area at all, unless there's words. Like, so if you have\nmusic, and there's a song, and you can hear the words, then you get the language area. - Oh, are we talking about\nspeaking and listening, or are we also talking about reading? - This is all comprehension of any kind. And so-\n- That is fascinating. - So this network doesn't\nmake any difference if it's written or spoken. So the thing that she calls, Fedorenko calls the language network, is this high-level language. So it's not about the spoken language, and it's not about the written language. It's about either one of them. And so when you do speech, you either listen to speech, and you subtract away some\nlanguage you don't understand, or you subtract away backwards speech, which sounds like speech, but it isn't. And then so you take away\nthe sound part altogether. And then if you do written, you get exactly the same network. So for just reading the language versus reading sort of nonsense words or something like that, you'll find exactly the same network. And so this is about high-level- - Comprehension.\n- Comprehension of language, yeah, in this case. And the same thing happens, production's a little\nharder to run the scanner, but the same thing happens in production. You get the same network. So production's a little harder, right? You have to figure out\nhow do you run a task in the network such that you're doing some kind of production. And I can't remember what, they've done a bunch of\ndifferent kinds of tasks there where you get people to-\n- Control the structure. - Produce things, yeah,\nfigure out how to produce. And the same network goes on there. It's actually the same place. - So if you, wait, wait. So if you read random words? - [Edward] Yeah, if you read things like- - Like gibberish. - Yeah, yeah, Lewis\nCarroll's twas brillig, jabberwocky, right? They call that jabberwocky speech. - [Lex] The network doesn't get activated. - [Edward] Not as much. There are words in there. - [Lex] Yeah, 'cause it's still- - There's function words and stuff, so it's lower activation.\n- Fascinating. - Yeah, yeah. So there's like, basically, the more language-like it is, the higher it goes in\nthe language network. And that network is there from when you speak, as\nsoon as you learn language. And it's there, like, you\nspeak multiple languages, the same network is going\nfor your multiple languages. So you speak English, you speak Russian, both of them are hitting\nthat same network, if you're fluent in those languages. - So programming-\n- Not at all. Isn't that amazing?\n- Oh, God. - Even if you're a really good programmer, that is not a human language. It's just not conveying\nthe same information. And so it is not in the language network. And so-\n- That is mind-blowing as I think. That's weird.\n- It's pretty cool. - That's weird.\n- It is amazing. - That's really weird.\n- And so that's like one set of data. This is hers, like, shows\nthat what you might think is thinking is not language. Language is just this\nconventionalized system that we've worked out in human languages. Oh, another fascinating\nlittle bit, tidbit, is that even if there are\nthese constructed languages, like Klingon, or I\ndon't know the languages from \"Game of Thrones,\" I'm sorry, I don't remember those languages, but maybe you do.\n- There's a lot of people offended right now. - There's people that\nspeak those languages. They really speak those languages because the people that\nwrote the languages for the shows, they did an amazing job of constructing something\nlike a human language. And that lights up the language area. That's like, because they can speak pretty much arbitrary\nthoughts in a human language. It's not a, it's a\nconstructed human language, and probably it's related\nto human languages because the people that\nwere constructing them were making them like human\nlanguages in various ways. But it also activates the same network, which is pretty cool. Anyway.\n- Sorry to go into a place where you may be a\nlittle bit philosophical, but is it possible that\nthis area of the brain is doing some kind of translation into a deeper set of almost like concepts? - It has to be doing. So it's doing in communication, right? It is translating from\nthought, whatever that is. It's more abstract, and it's doing that. That's what it's doing. Like it is, that is kind\nof what it is doing. It's like kind of a\nmeaning network, I guess. - Yeah, like a translation network. But I wonder what is at the\ncore, at the bottom of it, like what are thoughts? Are thoughts, to me like-\n- I don't know. - Thoughts and words, are they neighbors, or is it one turtle sitting\non top of the other? Meaning like, is there a\ndeep set of concepts that we- - Well, there's connections, right, between what these things mean, and then there's probably\nother parts of the brain that what these things mean. And so, when I'm talking\nabout whatever it is I wanna talk about, it'll be\nrepresented somewhere else. That knowledge of whatever that is will be represented somewhere else. - Well, I wonder if\nthere's like some stable, nicely compressed encoding of meanings that's separate from language. I guess the implication here is that that we don't think in language. - That's correct. Isn't that cool? And that's so interesting. So, people, I mean, this is\nlike hard to do experiments on, but there is this idea of inner voice, and a lot of people have an inner voice. And so, if you do a poll on the internet and ask if you hear yourself talking when you're just thinking or whatever, about 70 or 80% of people will say yes. Most people have an inner voice. I don't, and so I always\nfind this strange. So, when people talk about an inner voice, I always thought this was a metaphor, and they hear. I know most of you,\nwhoever's listening to this, thinks I'm crazy now 'cause\nI don't have an inner voice, and I just don't know\nwhat you're listening to. It sounds so kind of annoying to me, but to have this voice going\non while you're thinking, but I guess most people have\nthat, and I don't have that, and we don't really know\nwhat that connects to. - I wonder if the inner voice\nactivates that same network. - I don't know. I don't know. I mean, this could be speechy, right? So, that's like, you hear. Do you have an inner voice? - I don't think so.\n- Oh. A lot of people have this sense\nthat they hear themselves, and then say they read someone's email. I've heard people tell me that they hear that other person's voice when they read other people's emails, and I'm like, wow, that sounds so disruptive. - I do think I vocalize when I'm reading, but I don't think I hear a voice. - Well, you probably\ndon't have an inner voice. - [Lex] Yeah, I don't think\nI have an inner voice. - People have an inner voice. People have this strong\npercept of hearing sound in their heads when they're just thinking. - I refuse to believe that's\nthe majority of people. - Majority, absolutely. - What? - It's like 2/3 or 3/4. It's a lot.\n- What? - I never ask class,\nand I went to internet, they always say that. So, you're in a minority. - [Lex] It could be a self-report flaw. - It could be. - You know, when I'm\nreading inside my head, I'm kind of like saying the words, which is probably the wrong way to read, but I don't hear a voice. There's no percept of a voice. I refuse to believe the\nmajority of people have it. Anyway, it's a fascinating,\nthe human brain is fascinating, but it still blew my mind\nthat language does appear, comprehension does appear to\nbe separate from thinking. - So, that's one set. One set of data from Fedorenko's group is that no matter what task you do, if it doesn't have words and\ncombinations of words in it, then it won't light up\nthe language network. It'll be active somewhere\nelse, but not there. So, that's one. And then, this other piece of evidence relevant to that question is, it turns out there are\nthis group of people who've had a massive\nstroke on the left side and wiped out their language network. And as long as they didn't wipe out everything on the right as well, in that case, they wouldn't\nbe cognitively functionable. But if they just wiped out language, which is pretty tough to do because it's very expansive on the left, but if they have, then\nthere is patients like this, so-called global aphasics, who can do any task just\nfine, but not language. You can't talk to them. I mean, they don't understand you. They can't speak, they can't\nwrite, they can't read, but they can play chess,\nthey can drive their cars, they can do all kinds of other stuff. You can do math. So, math is not in the\nlanguage area, for instance. You do arithmetic and stuff,\nthat's not language area. It's got symbols. So, people sort of confuse some kind of symbolic\nprocessing with language, and symbolic processing is not the same. So, there are symbols\nand they have meaning, but it's not language. It's not a conventionalized\nlanguage system. And so, math isn't there. And so, they can do math. They do just as well as their control, age-matched controls and all these tasks. This is Rosemary Varley over\nin University College London, who has a bunch of patients\nwho she's shown this, that they're just... So, that sort of combination suggests that language isn't\nnecessary for thinking. It doesn't mean that you\ncan't think in language. You could think in language, 'cause language allows\na lot of expression, but it's just, you don't\nneed it for thinking. It suggests that language\nis a separate system. - This is kind of blowing\nmy mind right now. - It's cool, isn't it?\n- I'm trying to load that in. - Yeah, yeah.\n- Because it has implications for large language models. - It sure does, and they've\nbeen working on that. - Well, let's take a stroll there."
    },
    {
      "timestamp": "1:30:36",
      "section": "LLMs",
      "text": "You wrote that the best current\ntheories of human language are arguably large language models. So, this has to do with form. - It's kind of a big theory, but the reason it's arguably the best is that it does the best at predicting what's English, for instance. It's incredibly good, better\nthan any other theory. It's not sort of, there's\nnot enough detail. - Well, it's opaque. Like, you don't know what's going on. It's another black box. But I think it is a theory. - What's your definition of a theory? 'Cause it's a gigantic black box with a very large number of\nparameters controlling it. To me, theory usually\nrequires a simplicity, right? - Well, I don't know. Maybe I'm just being loose there. I think it's not a great\ntheory, but it's a theory. It's a good theory in one sense, in that it covers all the data. Like, anything you wanna\nsay in English, it does, and so that's how it's arguably the best, is that no other theory is as good as a large language model in predicting exactly what's good and\nwhat's bad in English. Now you're saying, is it a good theory? Well, probably not, because I want a smaller theory than that. It's too big. I agree. - You could probably construct a mechanism by which it can generate\na simple explanation of a particular language,\nlike a set of rules. Something like, it could\ngenerate a dependency grammar for a language, right? - [Edward] Yes. - You could probably\njust ask it about itself. - Well, that's, I mean, that presumes, and there's some evidence for this, that some large language\nmodels are implementing something like dependency\ngrammar inside them, and so there's work from\na guy called Chris Manning and colleagues over at\nStanford in natural language, and they looked at, I don't know how many\nlarge language model types, but certainly BERT and some others, where you do some kind of fancy math to figure out exactly\nwhat kind of abstractions of representations are going on, and they were saying, it does look like dependency structure is\nwhat they're constructing. It doesn't, like, so it's\nactually a very, very good map, so they are constructing\nsomething like that. Does it mean that they're\nusing that for meaning? I mean, probably, but we don't know. - You write that the kinds\nof theories of language that LLMs are closest to are called construction-based theories. Can you explain what\nconstruction-based theories are? - It's just a general theory of language such that there's a\nform and a meaning pair for lots of pieces of the language, and so it's primarily usage-based, is the construction grammar. It's just, it's trying\nto deal with the things that people actually say,\nactually say and actually write, and so it's a usage-based idea, and what's a construction? A construction's either a simple word, so like a morpheme plus its meaning, or a combination of words. It's basically combinations\nof words, like the rules, but it's unspecified as to what the form of the grammar is underlyingly, and so I would argue that\nthe dependency grammar is maybe the right form to use for the types of construction grammar. Construction grammar typically\nisn't formalized quite, and so maybe the formalization, a-formalization of that, it\nmight be in dependency grammar. I mean, I would think so,\nbut it's up to people, other researchers in that\narea, if they agree or not. - Do you think that large language models understand language? Are they mimicking language? I guess the deeper question there is, are they just understanding\nthe surface form, or do they understand something\ndeeper about the meaning that then generates the form? - I mean, I would argue\nthey're doing the form. They're doing the form, they're\ndoing it really, really well and are they doing the meaning? No, probably not. I mean, there's lots of these\nexamples from various groups showing that they can be\ntricked in all kinds of ways. They really don't understand\nthe meaning of what's going on, and so there's a lot of examples that he and other groups have given, which show they don't really\nunderstand what's going on. So you know the Monty Hall\nproblem is this silly problem, right, where if you have three door, it's \"Let's Make a Deal,\"\nit's this old game show, and there's three doors, and\nthere's a prize behind one, and there's some junk\nprizes behind the other two, and you're trying to select one, and if you, you know, he knows, Monty, he knows where the target\nitem is, the good thing. He knows everything is back there, and you're supposed to,\nhe gives you a choice. You choose one of the three, and then he opens one of the doors, and it's some junk prize,\nand then the question is, should you trade to get the other one? And the answer is yes, you should trade, because he knew which ones\nyou could turn around, and so now the odds are 2/3, okay? And then if you just\nchange that a little bit to the large language mall, the large language mall\nhas seen that explanation so many times that it just, if you change the story just a little bit, but make it sound like it's\nthe Monty Hall problem, but it's not, you just say,\noh, there's three doors, and one behind them is a good prize, and there's two bad doors. I happen to know it's\nbehind door number one. The good prize, the car,\nis behind door number one, so I'm gonna choose door number one. Monty Hall opens door number three and shows me nothing there. Should I trade for door number two, even though I know the good\nprize is in door number one? And then the large language mall will say, yes, you should trade,\nbecause it just goes through the forms that it's seen\nbefore so many times on these cases where it,\nyes, you should trade, because your odds have\nshifted from one and three now to two out of three to being that thing. It doesn't have any way to remember that actually you have 100% probability behind that door number one. You know that. That's not part of the\nscheme that it's seen hundreds and hundreds of times before, and so you can't, even if\nyou try to explain to it that it's wrong, that they can't do that, it'll just keep giving\nyou back the problem. - But it's also possible\nthe large language model will be aware of the fact\nthat there's sometimes over-representation of a\nparticular kind of formulation, and it's easy to get tricked by that. And so you could see if\nthey get larger and larger, models be a little bit more skeptical. So you see over-representation. So it just feels like form can, training on form can go really far in terms of being able to generate things that look like the\nthing understands deeply the underlying world model of the kind of mathematical\nworld, physical world, psychological world that would generate these kinds of sentences. It just feels like you're creeping close to the meaning part. Easily fooled, all this kind of stuff, but that's humans too. So it just seems really impressive how often it seems like\nit understands concepts. - I mean, you don't have\nto convince me of that. I am very, very impressed. But does, I mean, you're\ngiving a possible world where maybe someone's gonna\ntrain some other versions such that it'll be\nsomehow abstracting away from types of forms. I mean, I don't think that's happened. And so- - Well, no, no, no, I'm not saying that. I think when you just\nlook at anecdotal examples and just showing a large number of them where it doesn't seem to understand and it's easily fooled, that does not seem like a scientific, a data-driven analysis of how many places is damn impressive in terms\nof meaning and understanding and how many places is easily fooled. And like-\n- That's not the inference. So I don't wanna make that, the inference I don't,\nI wouldn't wanna make was that inference. The inference I'm trying\nto push is just that, is it like humans here? It's probably not like humans here. It's different. So humans don't make that error. If you explain that to them, they're not gonna make that error. They don't make that error. And so that's something, it's\ndoing something different from humans that they're\ndoing in that case. - Well, what's the\nmechanism by which humans figure out that it's an error? - I'm just saying the error there is like, if I explain to you there's 100% chance that the car's behind\nthis case, this door, well, do you wanna trade? People say no. But this thing will say\nyes because it's so, that trick, it's so wound up on the form that it's, that's an error\nthat a human doesn't make, which is kind of interesting. - Less likely to make, I should say. - Yeah, less likely.\n- Because like humans are very-\n- Oh yeah. - I mean, you're asking, you know, you're asking humans to, you're asking a system to understand 100%. Like you're asking some\nmathematical concepts. And so like. - Look, the places where\nlarge language models are, the form is amazing. So let's go back to nested structures, center-embedded structures, okay? If you ask a human to complete\nthose, they can't do it. Neither can a large language model. They're just like humans in that. If you ask, if I ask a\nlarge language model- - That's fascinating, by the way. - That's-\n- The central embedding? - Yeah, the center embedding-\n- The central embedding is struggles with-\n- Just like humans, exactly like humans. Exactly the same way as humans. And that's not trained. So they do exactly, so\nthat is a similarity. So but then it's, that's\nnot meaning, right? This is form. But when we get into meaning, this is where they get kind of messed up. Where you start just saying,\noh, what's behind this door? Oh, it's, you know, this\nis the thing I want. Humans don't mess that up as much. You know, here, the\nform is, it's just like. The form matches amazing, similar, without being trained to do that. I mean, it's trained in the sense that it's getting lots of data, which is just like human data. But it's not being trained on, you know, bad sentences and being told what's bad. It just can't do those. It'll actually say things like, those are too hard for me\nto complete, or something, which is kind of interesting, actually. Kind of, how does it know that? I don't know. - But it really often doesn't\njust complete sentences. Very often says stuff that's true. - Mm-hmm. - And sometimes says\nstuff that's not true. And almost always the form is great. - Yeah. - But it's still very surprising that with really great form, it's able to generate a lot\nof things that are true. Based on what it's trained on, and so on. - Yes, yes, yes. - So it's not just form\nthat is generating. It's mimicking true statements. - That's right, that's right. - From the internet. I guess the underlying idea there is that on the internet,\ntruth is over-represented versus falsehoods. - [Edward] Yeah, I think\nthat's probably right, yeah. - But the fundamental\nthing it's trained on, you're saying, is just form. - [Edward] I think so, yeah. Yeah, I think so. - Well, that's a sad, to me that's still a little bit of an open question. I probably lean agreeing with you, especially now you've just blown my mind that there's a separate\nmodule in the brain for language versus thinking. Maybe there's a fundamental part missing from the large language model approach that lacks the thinking,\nthe reasoning capability. - Yeah, that's what this group argues. So the same group, Fedorenko's group, has a recent paper arguing exactly that. There's a guy called Kyle Mahowell who's here in Austin, Texas, actually. He's an old student of mine, but he's a faculty in\nlinguistics at Texas, and he was the first author on that. - That's fascinating. Still, to me, an open question. What do you have the\ninteresting limits of LLMs? - You know, I don't see\nany limits to their form. Their form is perfect.\n- Impressive. - Yeah, yeah, yeah, it's pretty much, I mean, it's close to- - Well, you said ability to\ncomplete central embeddings. - Yeah, it's just the same as humans. It seems the same as humans. - But that's not perfect, right? It should be-\n- That's good. No, but I want it to be like humans. I want a model of humans. - But, oh, wait, wait, wait, wait. Oh, so perfect is as close\nto humans as possible. I got it.\n- Yeah, yeah. - But you should be able\nto, if you're not human, you're superhuman, you should be able to complete central\nembedded sentences, right? - [Lex] I mean, that's the mechanism. If it's modeling something, I think it's kind of really\ninteresting that it can't- - That it's really interesting. - That it's more like,\nI think it's potentially underlyingly modeling something like the way the form is processed. - The form of human language. - The way that-\n- And how humans process the language.\n- Yes, yes. I think that's plausible. - And how they generate language. Processed language and generated language, that's fascinating.\n- Yeah. - So in that sense, they're perfect."
    },
    {
      "timestamp": "1:43:35",
      "section": "Center embedding",
      "text": "If we can just linger on\nthe center embedding thing that's hard for LLMs to produce, and that seems really impressive 'cause that's hard for humans to produce, and how does that connect to the thing we've been talking about before, which is the dependency grammar framework in which you view language and the finding that short dependencies seem to be a universal part of language? So why is it hard to\ncomplete center embeddings? - So what I like about dependency grammar is it makes the cognitive cost associated with longer distance\nconnections very transparent. Basically, there's some, it\nturns out there is a cost associated with producing\nand comprehending connections between words which are\njust not beside each other. The further apart they\nare, the worse it is, according to, well, we can measure that. And there is a cost associated with that. - Can you just linger on what do you mean by cognitive cost?\n- Sure. - And how do you measure it? - Oh, well, you can measure\nit in a lot of ways. The simplest is just asking\npeople to say whether, how good a sentence sounds, which is ask. That's one way to measure, and you can try to triangulate\nthen across sentences and across structures to try to figure out what the source of that is. You can look at reading times\nin controlled materials, in certain kinds of materials, and then we can measure the\ndependency distances there. There's a recent study which looked at, we're talking about the brain here. We could look at the\nlanguage network, okay? We could look at the language network and we could look at the activation in the language network and\nhow big the activation is depending on the length\nof the dependencies. And it turns out in just random sentences that you're listening to,\nif you're listening to, so it turns out there are people\nlistening to stories here, and the bigger, the\nlonger the dependency is, the stronger the activation\nin the language network. And so there's some measure. There's a bunch of different\nmeasures we could do. That's kind of a neat measure, actually, of actual-\n- Activations. - Activation in the brain. - You can somehow, in different ways, convert it to a number. I wonder if there's a beautiful equation connecting cognitive cost\nand length of dependency. E equals MC squared kind of thing. - Yeah, it's complicated,\nbut probably it's doable. I would guess it's doable. I tried to do that a while ago, and I was reasonably successful, but for some reason I\nstopped working on that. I agree with you that it\nwould be nice to figure out. So there's some way to\nfigure out the cost. I mean, it's complicated. Another issue you raised before was like how do you measure distance? Is it words? It probably isn't part of the problem. Is that some words\nmatter more than others, and probably meaning\nlike nouns might matter, and then it maybe depends\non which kind of noun. Is it a noun we've already introduced or a noun that's already been mentioned? Is it a pronoun versus a name? Like all these things probably matter. So probably the simplest thing to do is just like, oh, let's\nforget about all that and just think about words or morphemes. - For sure, but there\nmight be some insight in the kind of function\nthat fits the data, meaning like a quadratic, like what- - I think it's an exponential. So we think it's probably an exponential such that the longer the\ndistance, the less it matters. And so then it's the sum of those. That was our best guess a while ago. So you've got a bunch of dependencies. If you've got a bunch of them that are being connected at some point, at the ends of those, the cost\nis some exponential function of those is my guess. But because the reason it's\nprobably an exponential is like it's not just the\ndistance between two words, 'cause I can make a very, very\nlong subject verb dependency by adding lots and lots of noun phrases and prepositional phrases, and\nit doesn't matter too much. It's when you do nested, when\nI have multiple of these, then things go really bad, go south. - Probably somehow\nconnected to working memory - That's probably a\nfunction of the memory here is the access, is trying to\nfind those earlier things. It's kind of hard to figure out what was referred to earlier. Those are those connections. That's the sort of notion of merking, as opposed to a storagy\nthing, but trying to connect, retrieve those earlier words, depending on what was in between. And then we're talking about interference of similar things in between. The right theory probably has\nthat kind of notion in it, is an interference of similar. And so I'm dealing with an abstraction over the right theory, which is just, let's count words, it's\nnot right, but it's close. And then maybe you're right, though, there's some sort of an\nexponential or something on the, to figure out the total, so we can figure out a\nfunction for any given sentence in any given language. But it's funny, people\nhaven't done that too much, which I do think is, I'm interested that you find that interesting. I really find that interesting, and a lot of people haven't\nfound it interesting, and I don't know why I haven't got people to wanna work on that,\nI really like that too. - No, that's a beautified, and the underlying idea is beautiful, that there's a cognitive cost that correlates with the\nlength of dependency. It just, it feels like, I mean, language is so fundamental\nto the human experience, and this is a nice,\nclean theory of language, where it's like, wow, okay, so we like our words close together, dependent words close together. - Yeah, that's why I like it too. It's so simple. - Yeah, the simplicity of the theory. - It's so simple, and yet it explains some very complicated phenomena. If I write these very\ncomplicated sentences, it's kind of hard to\nknow why they're so hard, and you can like, oh, nail it down. I can give you a math formula for why each one of\nthem is bad, and where, and that's kind of cool. I think that's very neat. - Have you gone through the process? Is there like, if you\ntake a piece of text, and then simplify, sort of like, there's an average length of dependency, and then you like, you know, reduce it, and see comprehension on the entire, not just single sentence,\nbut like, you know, you go from James Joyce to\nHemingway, or something. - No, no, the simple answer is no, that does, there's\nprobably things you can do in that kind of direction. - [Lex] That's fun. - We might, you know, we're gonna talk about legalese at some point, and so maybe we'll talk\nabout that kind of thinking with applied to legalese. - Well, let's talk about legalese, 'cause you mentioned that as an exception. We're just taking it tangent upon tangent. That's an interesting one. You give it as an exception. - It's an exception. - That you say that\nmost natural languages, as we've been talking about,\nhave local dependencies, with one exception, legalese. - That's right. - So what is legalese, first of all? - Oh, well, legalese is\nwhat you think it is. It's just any legal language. - Well, I mean, like, I\nactually know very little about the kind of\nlanguage that lawyers use. - So I'm just talking\nabout language in laws, and language in contracts. - Got it. - So the stuff that you have to run into, we have to run into every\nother day, or every day, and you skip over,\nbecause it reads poorly. And, or, you know, partly\nit's just long, right? There's a lot of text there that we don't really wanna know about. But the thing I'm interested in, so I've been working with\nthis guy called Eric Martinez, who is a, he was a lawyer,\nwho was taking my class. I was teaching a\npsycholinguistics lab class, I have been teaching it\nfor a long time at MIT, and he's a, he was a\nlaw student at Harvard. And he took the class, 'cause he had done some\nlinguistics as an undergrad, and he was interested in the problem of why legalese sounds hard to understand. You know, why, and so why\nis it hard to understand, and why do they write that way, if it is so hard to understand? It seems apparent that\nit's hard to understand. The question is, why is it? And so we didn't know, and we did an evaluation\nof a bunch of contracts, actually we just took a bunch\nof sort of random contracts, 'cause I don't know, you know, there's, contracts and laws\nmight not be exactly the same, but contracts are kind of the things that most people have to\ndeal with most of the time. And so that's kind of\nthe most common thing that humans have, like, that adults in our industrialized society have to deal with a lot. And so that's what we pulled, and we didn't know what\nwas hard about them, but it turns out that\nthe way they're written is very center-embedded, it has nested structures in them. So it has low-frequency words as well, that's not surprising, lots of texts have low, it does have, surprising, slightly lower-frequency words than other kinds of control texts, even sort of academic texts, legalese is even worse, it is the worst that we were able to find. - You just revealed a game\nthat lawyers are playing. - They're not, though.\n- They're optimizing a different, well- - You know, it's interesting, that's a, now you're getting at why, and so, and I don't think, so now you're saying it's,\nthey're doing it intentionally, I don't think they're\ndoing it intentionally. But let's get-\n- It's an emergent phenomenon, okay, all right.\n- Yeah, yeah, yeah, we'll get to that, we'll get to that. And so, but we wanted to see why, so we see what first, as opposed, so like, 'cause it turns\nout that we're not the first to observe that legalese is weird. Like, back to, Nixon had a\nPlain Language Act in 1970, and Obama had one, and\nboy, a lot of these, you know, a lot of presidents have said, oh, we've gotta simplify legal\nlanguage, must simplify it. But if you don't know\nhow it's complicated, it's not easy to simplify it. You need to know what it\nis you're supposed to do before you can fix it, right? And so you need to, like,\nyou need a psycholinguist to analyze the text and\nsee what's wrong with it before you can, like, fix it. You don't know how to fix it. How am I supposed to fix something? I don't know what's wrong with it. And so what we did was\njust, that's what we did. We figured out, let's, okay, we just took a bunch of\ncontracts, had people, and we encoded them for\nthe, so a bunch of features. And so another feature that people, one of them was center embedding. And so that is, like,\nbasically how often a clause would intervene between a\nsubject and a verb, for example. That's one kind of a center\nembedding of a clause, okay? And it turns out they're\nmassively center embedded. Like, so I think in random\ncontracts and in random laws, I think you get about 70%\nor 80, something like 70% of sentences have a center\nembedded clause in them, which is insanely high. If you go to any other text,\nit's down to 20% or something. It's so much higher than any\ncontrol you can think of, including, you think, oh, people think, oh, technical academic text. No, people don't write\ncenter embedded sentences in technical academic text. I mean, they do a little bit, but much, it's on the 20%, 30%\nrealm, as opposed to 70. And so there's that, and\nthere's low-frequency words. And then people, oh, maybe it's passive. People don't like the passive. Passive, for some reason,\nthe passive voice in English has a bad rap, and I'm not really sure where that comes from. And there is a lot of passive in the, there's much more\npassive voice in legalese than there is in other texts. - And the passive voice accounts for some of the low-frequency words. - No, no, no, no, no, those are separate. Those are separate. - Oh, so passive voice sucks,\nlow-frequency word sucks. - Well, sucks is different. - That's a judgment on passive. - Yeah, yeah, yeah, drop the judgment. It's just like, these are frequent. These are things which\nhappen in legalese text. Then we can ask. The dependent measure is\nhow well you understand those things with those features. Okay, and so then, and it turns out the passive makes no difference. So it has zero effect on\nyour comprehension ability, on your recall ability, nothing at all. It has no effect. The words matter a little bit. They do, low-frequency\nwords are gonna hurt you in recall and understanding. But what really hurts is\nthe central embedding. That kills you. That is like, that slows people down. That makes them very\npoor at understanding. That makes them, they\ncan't recall what was said as well, nearly as well. And we did this not only on laypeople. We did it on a lot of laypeople. We ran it on 100 lawyers. We recruited lawyers from a wide range of sort of different levels\nof law firms and stuff. And they have the same pattern. So they also, like when they did this, I did not know what happened. I thought maybe they could process. They're used to legalese. They can process it just as\nwell as if it was normal. No, no, they're much\nbetter than laypeople. So they're much, they\ncan much better recall, much better understanding, but they have the same main effects as laypeople, exactly the same. So they also much prefer the non-center. So we constructed\nnon-center embedded versions of each of these. We constructed versions which\nhave higher frequency words in those places. And we did, we un-passivized. We turned them into active versions. The passive active made no difference. The words made a little difference. And the un-center embedding\nmakes big differences in all the populations. - Un-center embedding. How hard is that process, by the way? - It's not very hard.\n- Don't question. But how hard is it to\ndetect center embedding? - Oh, easy, easy to detect. That's just easy to parse.\n- You're just looking at long dependencies? Or is there a real-\n- Yeah, yeah. You can just, you can, so there's automatic parsers for English, which are pretty good. Very-\n- And they can detect center embedding?\n- Oh, yeah. Very-\n- Or, I guess, nesting. - Perfectly. Yeah, pretty much. - So you're not just looking\nfor long dependencies. You're just literally\nlooking for center embedding. - Yeah, we are in this\ncase, in these cases. But long dependencies,\nthey're highly correlated, these kinds of things.\n- Highly. So like, center embedding is a big bomb you throw inside of a sentence that just blows up the, that makes super- - Can I read a sentence\nfor you from these things? I see, I can find, I mean, this is just like one of the things that, this is just typical.\n- My eyes might glaze over in mid-sentence. No, I understand that. I mean, legalese is hard.\n- So here we go. This is a good one. It goes, in the event that any payment or benefit by the company, all such payments and benefits, including the payments and benefits under section 3A hereof, being here and after referred\nto as a total payment, would be subject to the excise tax, then the cash severance\npayments shall be reduced. So that's something we\npulled from a regular text, from a contract.\n- Wow. - And the center embedded\nbit there is just, for some reason, there's a definition. They throw the definition of\nwhat payments and benefits are in between the subject and the verb. Let's, how about don't do that? How about put the\ndefinition somewhere else as opposed to in the\nmiddle of the sentence? And so that's very,\nvery common, by the way. That's what happens. You just throw your definitions. You use a word, a couple words, and then you define it, and then you continue the sentence. Like, just don't write like that. And you ask, so then we asked lawyers. We thought, oh, maybe lawyers like this. Lawyers don't like this. (laughs) They don't like this. They don't wanna write like this. We asked them to rate materials which are with the same meaning, with un-center-embedded\nand center-embedded, and they much preferred the\nun-center-embedded versions. - On the comprehension,\non the reading side. - Yeah, and we asked them, would you hire someone who\nwrites like this or this? We asked them all kinds of questions, and they always preferred\nthe less complicated version, all of them. So I don't even think\nthey want it this way. - Yeah, but how did it happen? - How did it happen? That's a very good question. And the answer is, I still don't know. But-\n- I have some theories. - Well, our best theory at the moment is that there's actually some\nkind of a performative meaning in the center-embedding in the style which tells you it's legalese. We think that that's the kind of a style which tells you it's legalese. Like, that's a reasonable guess. And maybe it's just, so for instance, if you're, like, it's like a magic spell. So we kinda call this the\nmagic spell hypothesis. So when you tell someone to\nput a magic spell on someone, what do you do? People know what a magic spell is, and they do a lot of rhyming. That's kinda what people will tend to do. They'll do rhyming, and they'll do some kind of poetry kind of thing. - Abracadabra type of thing.\n- Yeah, yeah. And maybe there's a\nsyntactic sort of reflex here of a magic spell, which\nis center-embedding. And so that's like, oh,\nit's trying to tell you this is something which is true, which is what the goal of law is, right? It's telling you something\nthat we want you to believe as certainly true, right? That's what legal contracts are trying to, enforce on you, right? And so maybe that's like a form, which has, this is like an abstract, very abstract form, center-embedding, which has a meaning associated with it. - Well, don't you think\nthere's an incentive for lawyers to generate things\nthat are hard to understand? - That was one of our working hypotheses. We just couldn't find\nany evidence of that. - No, lawyers also don't understand it. - Well, we asked lawyers.\n- You're creating space, why you, I mean, you ask in\na communist Soviet Union, the individual members, their self-report is not going to correctly\nreflect what is broken about the gigantic bureaucracy\nthat leads to Chernobyl or something like this. I think the incentives\nunder which you operate are not always transparent to the members within that system. So it just feels like\na strange coincidence that there is benefit\nif you just zoom out, look at the system, as opposed\nto asking individual lawyers that making something hard to understand is going to make a lot of people money. You're gonna need a\nlawyer to figure that out, I guess, from the perspective\nof the individual, but then that could be\nthe performative aspect. It could be as opposed\nto the incentive-driven to be complicated, it\ncould be performative to where we lawyers speak\nin this sophisticated way and you regular humans\ndon't understand it, so you need to hire a lawyer. Yeah, I don't know which one\nit is, but it's suspicious. Suspicious that it's hard to understand and that everybody's eyes\nglaze over and they don't read. - I'm suspicious as well,\nI'm still suspicious. And I hear what you're\nsaying, it could be kind of, no individual, and even\naverage of individuals, it could just be a few bad apples in a way which are driving the effect in some way. - Influential bad apples\nthat everybody looks up to, whatever, they're like\ncentral figures in how- - But it is kind of interesting\nthat among our 100 lawyers, they did not share that.\n- They didn't want this. That's fascinating.\n- They really didn't like it. And so it gave us hope.\n- And they weren't better than regular people at comprehending it. Or they were, on average, better, but like-\n- But they had the same difference.\n- The same difference. - Exact same difference. But they wanted it fixed. So they also, and so\nthat gave us hope that, because it actually isn't very\nhard to construct a material which is un-center-embedded\nand has the same meaning, it's not very hard to do. You just basically, in that situation, you're just putting definitions outside of the subject-verb relation\nin that particular example. And that's kind of, that's pretty general, what they're doing is just\nthrowing stuff in there which you didn't have to put in there. There's extra words involved, typically. You may need a few extra\nwords sort of to refer to the things that you're\ndefining outside in some way, 'cause if you only use\nit in that one sentence, then there's no reason\nto introduce extra terms. So we might have a few more words, but it'll be easier to understand. So I mean, I have hope that now that maybe we can make legalese less\nconvoluted in this way. - So maybe the next president\nof the United States can, instead of saying\ngeneric things, say- - Say exactly what-\n- I ban center embeddings and make\nTed the language czar of the- - Or he can make Eric. Martinez is the guy you\nshould really put in there. - Eric Martinez, yeah, yeah, yeah. (both laughing) But center embeddings are\nthe bad thing to have. - That's right, yeah.\n- So if you get rid of that- - That'll do a lot of\nit, that'd fix a lot. - That's fascinating.\n- Yeah. - That is so fascinating.\n- Yeah. - And it's just really\nfascinating on many fronts that humans are just not able to deal with this kind of thing. And that language, because of that, evolved in the way you\ndid, it's fascinating. So one of the mathematical\nformulations you have when talking about\nlanguages of communication is this idea of noisy channels. What's a noisy channel? - Well, so that's about communication. And so this is going back to Shannon. So Shannon, Claude Shannon was\na student at MIT in the '40s. And so he wrote this very\ninfluential piece of work about communication theory\nor information theory. And he was interested in\nhuman language, actually. He was interested in this\nproblem of communication, of getting a message from\nmy head to your head. And so he was concerned or interested in what was a robust way to do that. And so assuming we both\nspeak the same language, we both already speak English,\nwhatever the language is, we speak that. What is a way that I can say the language so that it's most likely to get the signal that I want to you? And so, and then the problem\nthere in the communication is the noisy channel, is that there's, I make, there's a lot\nof noise in the system. I don't speak perfectly. I make errors. That's noise. There's background noise. You know that, as we-\n- Like a literal- - Literal background noise. There is like white\nnoise in the background or some other kind of noise. There's some speaking\ngoing on that you're, or just, you're at a party. That's background noise. You're trying to hear someone. It's hard to understand them because there's all this\nother stuff going on in the background. And then there's noise\non the communication, on the receiver side, so that you have some problem\nmaybe understanding me for stuff that's just\ninternal to you in some way. So you've got some other\nproblems, whatever, with understanding for whatever reasons. Maybe you're, maybe you've\nhad too much to drink. You know, who knows why you're not able to pay attention to the signal. So that's the noisy channel. And so that language, if\nit's a communication system, we are trying to optimize, in some sense, the passing of the message\nfrom one side to the other. And so it, I mean, one idea is that maybe, you know, aspects of like\nword order, for example, might have optimized in some way to make language a little more easy to be passed from speaker to listener. And so Shannon's the\nguy that did this stuff way back in the '40s. You know, it's very\ninteresting, historically, he was interested in\nworking in linguistics. He was at MIT, and he did, this was his master's\nthesis, of all things. You know, it's crazy how much he did for his master's thesis in 1948, I think, or '49, something. And he wanted to keep working in language. And it just wasn't a popular communication as a reason, a source\nfor what language was wasn't popular at the time. So Chomsky was becoming,\nit was moving in there. He was, and he just wasn't\nable to get a handle there, I think. And so he moved to Bell Haps\nand worked on communication from a mathematical point of view, and did all kinds of amazing work. And so he's just-\n- More on the signal side versus the language side.\n- Yeah, mm-hmm. - Ah, yeah, it would've\nbeen interesting to see if he pursued the language side. - Yeah.\n- That's really interesting. - Yeah, he was interested in that. His examples in the '40s are kinda like, they're very language-like things. - Yeah. - We can kinda show that\nthere's a noisy channel process going on in when you're listening to me, you can often sort of guess what I meant by what you think I\nmeant given what I said. And I mean, with respect to sort of why language looks the way it does, we might, there might be sort of, as I alluded to, there\nmight be ways in which word order is somewhat optimized because of the noisy channel in some way. - I mean, that's really\ncool to sort of model if you don't hear certain\nparts of a sentence or have some probability\nof missing that part, like how do you construct a language that's resilient to that,\nthat's somewhat robust to that? - [Edward] Yeah, that's the idea. - And then you're kinda\nsaying like the word order and the syntax of a language,\nthe dependency length are all helpful to deal with. - Yeah, well, dependency\nlength is really about memory. I think that's like about sort\nof what's easier or harder to produce in some way. And these other ideas are about sort of robustness to communication, so the problem of potential\nloss of signal due to noise. It's so that there may\nbe aspects of word order, which is somewhat optimized for that. And we have this one\nguess in that direction. These are kind of just so stories, I have to be pretty frank. They're not like, I\ncan't show this is true. All we can do is like look at the current languages of the world. This is like, we can't sort\nof see how languages change or anything because\nwe've got these snapshots of a few hundred or a\nfew thousand languages. We don't really, we\ncan't do the right kinds of modifications to test\nthese things experimentally. And so just take this with\na grain of salt, okay, from here, this stuff. The dependency stuff I can,\nI'm much more solid on. I'm like, here's what the lengths are and here's what's hard,\nhere's what's easy, and this is a reasonable structure. I think I'm pretty reasonable. Here's like why, why does the word order look the way it does? We're now into shaky territory,\nbut it's kind of cool. - But we're talking\nabout, just to be clear, we're talking about maybe just actually the sounds of communication. Like you and I are sitting\nin a bar, it's very loud, and you model with a noisy\nchannel the loudness, the noise, and we have the\nsignal that's coming across. And you're saying word\norder might have something to do with optimizing that,\nwhere there's presence of noise. - [Edward] Yes, yes, yes. - I mean, it's really interesting. I mean, to me, it's interesting\nhow much you can load into the noisy channel. Like how much can you bake in? Well, you said like, you know, cognitive load on the receiver end. - We think that those are, there's three, at least three different kinds\nof things going on there. And we probably don't wanna\ntreat them all as the same. And so I think that the right model, a better model of a noisy\nchannel would treat, would have three different\nsources of noise, which are background noise,\nspeaker inherent noise, and listener inherent noise. And those are not, those\nare all different things. - Sure, but then underneath it, there's a million other\nsubsets of like, what. - [Edward] That's true. - On the receiving end, I mean, I just mentioned cognitive\nload on both sides. Then there's like speech\nimpediments or just everything. Worldview, I mean, the meaning, we start to creep into\nthe meaning realm of like, we have different worldviews. - Well, how about just form still though? Like just what language you know. Like, so how well you know the language. And so if it's second language\nfor you versus first language and how maybe what other\nlanguages you know, these are still just form stuff. And that's like potentially\nvery informative. And you know, how old you are, these things probably matter, right? So like a child learning\na language is, you know, as a noisy representation\nof English grammar, you know, depending on how old they are. So maybe when they're six,\nthey're perfectly formed, but."
    },
    {
      "timestamp": "2:10:02",
      "section": "Learning a new language",
      "text": "- You mentioned one of the things is like a way to measure a\nlanguage is learning problems. So like, what's the correlation between everything\nwe've been talking about and how easy it is to learn a language? So is like short dependencies correlated to ability to learn a language? Is there some kind of, or\nlike the dependency grammar, is there some kind of connection there? How easy it is to learn? - Yeah, well, all the languages\nin the world's language, none is, right now, we know\nis any better than any other with respect to sort of optimizing dependency links, for example. They're all kind of do it well. They all keep low. So I think of every human language as some kind of sort of\nan optimization problem. A complex optimization problem to this communication problem. And so they've like, they've solved it. They're just sort of noisy solutions to this problem of communication. There's just so many ways you can do this. - So they're not optimized for learning. They're probably optimized\nfor communication. - And learning. So yes, one of the factors which is. Yeah, so learning is\nmessing this up a bit. And so, for example, if it were just about\nminimizing dependency links, and that was all that matters, then we might find grammars which didn't have\nregularity in their rules. But languages always have\nregularity in their rules. So what I mean by that is that if I wanted to say something to you in the optimal way to say it was, what really mattered to me, all that mattered was\nkeeping the dependencies as close together as possible, then I would have a very lax set of free structure or dependency rules. I wouldn't have very many of those. I would have very little of that. And I would just put the words as close, the things that refer to the\nthings that are connected right beside each other. But we don't do that. There are word order rules, right? So they're very, and depending on the language, they're more and less strict, right? So you speak Russian, they're less strict than English. English has very rigid word order rules. We order things in a very particular way. And so why do we do that? That's probably not about communication. That's probably about learning. I mean, then we're talking about learning. It's probably easier to\nlearn regular things, things which are very\npredictable and easy to, so that's probably about\nlearning, is our guess, 'cause that can't be about communication. - Can it be just noise? Can it be just the messiness of the development of a language? - Well, if it were just a communication, then we should have languages which have very, very free word order, and we don't have that. We have free-er, but not free, like there's always- - Well, no, but what I mean by noise is like cultural, like\nsticky cultural things, like the way you communicate, just there's a stickiness to it, that it's an imperfect, it's a noisy, it's stochastic, the function over which you're optimizing is very noisy. Because I don't, it feels weird to say that learning is part of\nthe objective function, 'cause some languages are way harder to learn than others, right? Or is that, that's not true? That's interesting. I mean, that's the public\nsort of perception, right? - Yes, that's true. - For a second language. - For a second language. - But that depends on what\nyou started with, right? So it really depends on how close that second language is to\nthe first language you've got. And so, yes, it's very, very hard to learn Arabic if you've\nstarted with English, or it's hard to learn Japanese, or if you've started with, Chinese, I think, is the worst. There's like Defense Language Institute in the United States has like a list of how hard it is to learn\nwhat language from English, and I think Chinese is the worst. - But that's just a second language. You're saying babies don't care. - No, no, there's no evidence that there's anything harder or easier about any baby, any language learned. Like by three or four,\nthey speak that language. And so there's no evidence\nof anything harder or easier about any human language. They're all kind of equal."
    },
    {
      "timestamp": "2:13:54",
      "section": "Nature vs nurture",
      "text": "- To what degree is language, this is returning to Chomsky\na little bit, is innate? You said that for Chomsky, he used the idea that languages, some aspects of language are innate to explain away certain\nthings that are observed. How much are we born with language at the core of our mind, brain? - I mean, the answer is\nI don't know, of course, but the, I mean, I like to, I'm an engineer at heart, I guess, and I sort of think it's fine to postulate that a lot of it's learned, and so I'm guessing that\na lot of it's learned. So I think the reason Chomsky\nwent with the innateness is because he hypothesized\nmovement in his grammar. He was interested in grammar, and movement's hard to learn. I think he's right. Movement is a hard, it's\na hard thing to learn, to learn these two things\ntogether and how they interact, and there's a lot of ways\nin which you might generate exactly the same sentences,\nand it's really hard. And so he's like, oh,\nI guess it's learned. Sorry, I guess it's not\nlearned, it's innate. And if you just throw out the movement and just think about\nthat in a different way, you know, then you get some messiness, but the messiness is human language, which it actually fits better. That messiness isn't a problem. It's actually, it's a\nvaluable asset of the theory. And so I think I don't really see a reason to postulate much innate structure, and that's kind of why I think\nthese large language models are learning so well, is because I think you can learn the form, the forms of human\nlanguage from the input. I think that's like,\nit's likely to be true. - So that part of the brain that lights up when you're doing all the comprehension, that could be learned. That could be just, you don't\nneed, you don't need any. - Yeah, it doesn't have to be innate. So like lots of stuff\nis modular in the brain that's learned. It doesn't have to, you know,\nso there's something called the visual word form area in the back, and so it's in the back of your head, near the visual cortex, okay? And that is very specialized language, sorry, very specialized brain area, which does visual word\nprocessing if you read, if you're a reader, okay? If you don't read, you\ndon't have it, okay? Guess what? You spend some time learning to read, and you develop that brain\narea, which does exactly that. And so the modularization is\nnot evidence for innateness. So the modularization of a language area doesn't mean we're born with it. We could have easily learned that. We might've been born with it. We just don't know at this point. We might very well have been born with this left-lateralized area. I mean, there's like a lot of\nother interesting components here, features of this kind of argument. So some people get a stroke, or something goes really\nwrong on the left side, where the language area would be, and that isn't there. It's not available, and it\ndevelops just fine on the right. So it's not about the left. It goes to the left, like\nthis is a very interesting question, is why are\nany of the brain areas the way that they are, and how\ndid they come to be that way? And there's these natural\nexperiments which happen, where people get these\nstrange events in their brains at very young ages, which wipe\nout sections of their brain, and they behave totally normally, and no one knows anything was wrong. And we find out later, 'cause they happen to be accidentally\nscanned for some reason, it's like, what happened\nto your left hemisphere? It's missing. There's not many people who've missed their whole left hemisphere,\nbut they'll be missing some other section of\ntheir left or their right. And they behave absolutely normally. We'd never know. So that's a very interesting\ncurrent research. You know, this is another project that this person, Eve\nFedorenko, is working on. She's got all these people contacting her, because she's scanned some people who have been missing sections. One person missed a section of her brain and was scanned in her\nlab, and she happened to be a writer for the New York Times, and there was an article\nin the New York Times about the, just about\nthe scanning procedure and about what might be learned about by sort of the general process of MRI and language, and that's her language. And because she's writing\nfor the New York Times, then all these people\nstarted writing to her, who also have similar kinds of deficits, because they've been accidentally scanned for some reason and found out\nthey're missing some section. And they say they volunteer to be scanned. - So these are natural experiments. - Natural experiments,\nthey're kind of messy, but natural experiments, kind of cool. She calls them interesting brains. - The first few hours, days, months of human life are fascinating, 'cause like, well, inside\nthe womb, actually, like that development, that machinery, whatever that is, seems\nto create powerful humans that are able to speak, comprehend, think, all that kind of stuff,\nno matter what happens, not no matter what, but\nrobust to the different ways that the brain might be damaged and so on. That's really interesting. But what would Chomsky say about the fact, the thing you're saying now, that language seems to be happening\nseparate from thought? Because as far as I understand,\nmaybe you can correct me, he thought that language underpins- - Yeah, he thinks so, I\ndon't know what he'd say. - He would be surprised, 'cause for him, the idea is that language is sort of the foundation of thought. - That's right, absolutely. - And it's pretty mind-blowing to think that it could be completely\nseparate from thought. - That's right, but so he's\nbasically a philosopher, philosopher of language, in a way, thinking about these\nthings, it's a fine thought. You can't test it in his methods. You can't do a thought\nexperiment to figure that out. You need a scanner, you\nneed brain-damaged people, you need something, you\nneed ways to measure that, and that's what fMRI\noffers, and patients are a little messier, fMRI is\npretty unambiguous, I'd say. It's very unambiguous,\nthere's no way to say that the language network\nis doing any of these tasks. You should look at those data,\nit's like there's no chance that you can say that those\nnetworks are overlapping. They're not overlapping, they're\njust completely different. And so, you can always make,\noh, it's only two people, it's four people, or\nsomething for the patients, and there's something special\nabout them, we don't know, but these are just random\npeople, and with lots of them, and you find always the same effects, and it's very robust, I'd say. - Well, it's a fascinating effect."
    },
    {
      "timestamp": "2:20:30",
      "section": "Culture and language",
      "text": "You mentioned Bolivia. What's the connection\nbetween culture and language? You've also mentioned that\nmuch of our study of language comes from W-E-I-R-D, WEIRD people, Western-educated, industrialized,\nrich, and democratic. So, when you study remote cultures, such as around the Amazon jungle, what can you learn about language? - So, that term WEIRD is from Joe Henrich. He's at Harvard, he's a\nHarvard evolutionary biologist. And so, he works on lots\nof different topics, and he basically was\npushing that observation that we should be careful\nabout the inferences we wanna make when we're\ntalking in psychology, or most in psychology,\nI guess, about humans, if we're talking about\nundergrads at MIT and Harvard. Those aren't the same, right? These aren't the same things. And so, if you wanna make\ninferences about language, for instance, there's a lot\nof other kinds of languages in the world than English\nand French and Chinese. And so, maybe, for language,\nwe care about how culture, 'cause cultures can be\nvery, I mean, of course, English and Chinese\ncultures are very different, but hunter-gatherers are much\nmore different in some ways. And so, if culture has an\neffect on what language is, then we kind of wanna look\nthere as well as looking. It's not like the industrialized cultures aren't interesting. Of course, they are, but we want to look at non-industrialized cultures as well. And so, I've worked with two. I've worked with the Chimane,\nwhich are in Bolivia, and Amazon, both in the\nAmazon, in these cases. And there are so-called farmer-foragers, which is not hunter-gatherers. It's sort of one-up from hunter-gatherers in that they do a little\nbit of farming as well, a lot of hunting as well, but a little bit of farming. And the kind of farming they\ndo is the kind of farming that I might do if I ever\nwere to grow tomatoes or something in my backyard. So, it's not like big field farming. It's just farming for a family,\na few things you do that. So, that's the kind of farming they do. And the other group I've\nworked with are the Piraha, which are also in the Amazon\nand happen to be in Brazil. And that's with a guy called Dan Everett, who is a linguist, anthropologist, who actually lived and worked in the, I mean, he was a missionary,\nactually, initially, back in the '70s, trying\nto translate languages so they could teach them the Bible, teach them Christianity. - What can you say about that? - Yeah, so, the two\ngroups I've worked with, the Chimane and the Piraha,\nare both isolate languages, meaning there's no known\nconnected languages at all. They're just on their own. There's a lot of those. And most of the isolates\noccur in the Amazon or in Papua New Guinea, in these places where the world has sort of\nstayed still for long enough. And so, there aren't earthquakes. There aren't, well,\ncertainly no earthquakes in the Amazon jungle. And the climate isn't bad,\nso you don't have droughts. And so, in Africa, you've\ngot a lot of moving of people because there's drought problems. And so, they get a lot\nof language contact. When you have, when people have to, if you've gotta move\nbecause you've got no water, then you've gotta get going. And then you run into\ncontact with other tribes, other groups. In the Amazon, that's not the case. And so, people can stay there\nfor hundreds and hundreds and probably thousands of years, I guess. And so, these groups have,\nthe Chimane and the Piraha are both isolates in\nthat, and they can just, I guess they've just lived\nthere for ages and ages with minimal contact with\nother outside groups. And so, I mean, I'm interested\nin them because they are, I mean, in these cases, I'm\ninterested in their words. I would love to study their\nsyntax, their orders of words, but I'm mostly just\ninterested in how languages are connected to their\ncultures in this way. And so, with the Piraha,\ntheir most interesting, I was working on number\nthere, number information. And so, the basic idea is I\nthink language is invented. That's what I get from the words here, is that I think language is invented. We talked about color earlier. It's the same idea, so that\nwhat you need to talk about with someone else is what\nyou're gonna invent words for. And so, we invent labels\nfor colors that I need, not that I can see, but the\nthings I need to tell you about so that I can get objects from you or get you to give me the right objects. And I just don't need a word for teal or a word for aquamarine\nin the Amazon jungle for the most part, because\nI don't have two things which differ on those colors. I just don't have that. And so, numbers are\nreally another fascinating source of information\nhere, where you might, naively, I certainly\nthought that all humans would have words for exact counting, and the Piraha don't, okay? So, they don't have\nany words for even one. There's not a word for\none in their language. And so, there's certainly not a word for two, three, or four. So, that kind of blows people's minds off. - Yeah, that is blowing my mind. - [Edward] That's pretty weird, isn't it? - How are you gonna ask,\nI want two of those? - You just don't. And so, that's just not a\nthing you can possibly ask in the Piraha. It's not possible. There's no words for that. So, here's how we found this out, okay? So, it was thought to be\na one-to-many language. There are three words for\nquantifiers, for sets, but people had thought that\nthose meant one, two, and many. But what they really mean\nis few, some, and many. Many is correct. It's few, some, and many. And so, the way we figured this out, and this is kind of cool, is that we gave people, we\nhad a set of objects, okay? These were having to be spools of thread. Doesn't really matter what they are. Identical objects. And when I sort of started off here, I just give you one of those\nand say, \"What's that?\" Okay, so you're a Piraha speaker, and you tell me what it is. And then I give you two\nand say, \"What's that?\" And nothing's changing in the set except for the number, okay? And then I just ask you\nto label these things. We just do this for a\nbunch of different people. And frankly, I did this task. - This is fascinating.\n- And it's a weird, it's a little bit weird. So, they say the word that\nwe thought was one, it's few, but for the first one. And then maybe they say few, or maybe they say some for the second. And then for the third or the fourth, they start using the\nword many for the set. And then five, six, seven, eight. I go all the way to 10. And it's always the same word. And they look at me like I'm stupid because they told me what the word was for six, seven, eight. And I'm gonna continue\nasking them at nine and 10. I'm like, \"I'm sorry.\" They understand that I\nwanna know their language. That's the point of the task, is I'm trying to learn their\nlanguage, and so that's okay. But it does seem like I'm a little slow 'cause they already told me\nwhat the word for many was, five, six, seven, and I keep asking. So it's a little funny to\ndo this task over and over. We did this with a guy called,\nDan was our translator. He's the only one who really\nspeaks Piraha fluently. He's a good bilingual\nfor a bunch of languages, but also English and Piraha. And then a guy called Mike Frank was also a student with me down there. He and I did these things. And so you do that, okay? And everyone does the same thing. All, you know, we asked like 10 people and they all do exactly the\nsame labeling for one up. And then we just do the same thing down on like random order, actually. We do some of them up, some\nof them down first, okay? And so we do, instead of one\nto 10, we do 10 down to one. And so I give them 10, nine, at eight, they start saying the word for some. And then at down, when you get to four, everyone is saying the word for few, which we thought was one. So it's like the context determined what word, what that\nquantifier they used was. So it's not a count word. They're not count words. They're just approximate words. - And they're gonna be\nnoisy when you interview a bunch of people with\nthe definition of few and there's gonna be a\nthreshold in the context. - Yeah, yeah, I don't\nknow what that means. That's gonna depend on the context. I think it's true in English too, right? If you ask an English\nperson what a few is, I mean, that's gonna depend\ncompletely on the context. - And it might actually be\nat first hard to discover 'cause for a lot of people, the jump from one to\ntwo will be few, right? So it's a jump. - Yeah, it might be. It might still be there, yeah. - I mean, that's fascinating. That's fascinating. I mean, the numbers\ndon't present themselves. - So the words aren't there. And so then we do these other things. Well, if they don't have the words, can they do exact matching kinds of tasks? Can they even do those tasks? And the answer is sort of yes and no. And so yes, they can do them. So here's the tasks that we did. We put out those spools\nof thread again, okay? So we put like three out here and then we gave them some objects and those happened to be\nuninflated red balloons. It doesn't really matter what they are. It's just a bunch of\nexactly the same thing. And it was easy to put down right next to these\nspools of thread, okay? And so then I put out three of these and your task was to just put one against each of my three things. And they could do that perfectly. So I mean, I would actually do that. It was a very easy task to explain to them because I did this with\nthis guy, Mike Frank, and I'd be the experimenter\ntelling him to do this and showing him to do this. And then we just like,\njust do what he did. You'll copy him. All we had to, I didn't\nhave to speak Piraha except for know what, copy him. Like do what he did is like\nall we had to be able to say. And then they would do\nthat just perfectly. And so we'd move it up. We'd do some sort of random\nnumber of items up to 10 and they basically do perfectly on that. They never get that wrong. I mean, that's not a counting task, right? That is just a match. You just put one against,\nit doesn't matter how many. I don't need to know\nhow many there are there to do that correctly. And they would make\nmistakes, but very, very few and no more than MIT undergrads. Just gonna say, like, there's\nno, these are low stakes. So, you know, you make mistakes. - So counting is not required\nto complete the matching task. - That's right, not at all. Okay, and so that's our control. And this guy had gone down there before and said that they couldn't do this task. But I just don't know\nwhat he did wrong there 'cause they can do this\ntask perfectly well. And, you know, I can train\nmy dog to do this task. So of course they can do this task. And so, you know, it's not a hard task. But the other task that was\nsort of more interesting is like, so then we do a bunch of tasks where you need some way to encode the set. So like, one of them is just, I just put a opaque sheet\nin front of the things. I put down a bunch, a set of these things and I put an opaque sheet down. And so you can't see them anymore. And I tell you, do the same\nthing you were doing before. Right, you know, and it's\neasy if it's two or three, it's very easy. But if I don't have the words for eight, it's a little harder. Like maybe, you know, with practice, when, well, no. - 'Cause you have to count.\n- For us it's easy 'cause we just count them. It's just so easy to count them. But they don't, they can't count them because they don't count. They don't have words for this thing. And so they would do approximate. It's totally fascinating. So they would get them\napproximately right, you know, after four or five. 'Cause you can, basically\nyou always get four right, three or four. That looks, that's something\nwe can visually see. But after that, you kind of have, it's an approximate number. And so then, and there's\na bunch of tasks we did and they all failed, I mean, failed. They did approximate after\nfive on all those tasks. And it kind of shows that the words, you kind of need the words, you know, to be able to do these kinds of tasks. - There's a little bit of a\nchicken and egg thing there because if you don't have the words, then maybe they'll limit\nyou in the kind of, like a little baby Einstein there won't be able to come\nup with a counting task. You know what I mean? Like the ability to count enables you to come up with interesting\nthings probably. So yes, you develop counting\nbecause you need it. But then once you have counting, you can probably come up with a bunch of different inventions. Like how to, I don't\nknow, what kind of thing. They do matching really\nwell for building purposes, building some kind of hut\nor something like this. So it's interesting that\nlanguage is a limiter on what you're able to do. - Yeah, here language\nis just, is the words. Here is the words. Like the words for exact count\nis the limiting factor here. They just don't have 'em. - Yeah, yeah. But that's what I mean. That limit is also a limit on the society of what they're able to build. - That's gonna be true, yeah. So it's probably, I mean, this is one of those problems with the snapshot of\njust current languages is that we don't know\nwhat causes a culture to discover/invent a counting system. But the hypothesis is the guess out there is something to do with farming. So if you have a bunch of goats and you wanna keep track of them and you say you have 17 goats and you go to bed at night\nand you get up in the morning, boy, it's easier to have\na count system to do that. That's an abstraction over a set. So they don't have,\nlike, people often ask me when I tell 'em about this kind of work, they say, \"Well, don't\nthese, don't they have kids? \"Don't they have a lot of children?\" I'm like, \"Yeah, they\nhave a lot of children.\" And they do. They often have families of\nthree or four or five kids. And they go, \"Well, don't\nthey need the numbers \"to keep track of their kids?\" And I always ask this\nperson who says this, like, \"Do you have children?\" (laughing) And the answer's always no because that's not how you\nkeep track of your kids. You care about their identities. It's very important to me when I go, \"I think I have five children.\" - [Lex] You don't think\none, two, three, four? - It matters which five. If you replaced one with\nsomeone else, I would care. A goat, maybe not. That's the kind of point. It's an abstraction. Something that looks\nvery similar to the one wouldn't matter to me, probably. - But if you care about goats, you're gonna know them\nactually individually also. - [Edward] Yeah, you will. - I mean, cows, goats, if\nthere's a source of food and milk and all that kind of stuff, you're gonna actually really care. - But I'm saying it is an abstraction such that you don't have to\ncare about their identities to do this thing fast. That's the hypothesis, not mine. From anthropologists as a guessing about where words for counting came from is from farming, maybe. - Yeah."
    },
    {
      "timestamp": "2:34:58",
      "section": "Universal language",
      "text": "Do you have a sense\nwhy universal languages like Esperanto have not taken off? Like, why do we have all\nthese different languages? - Well, my guess is the\nfunction of a language is to do something in a community. I mean, unless there's some function to that language in the community, it's not gonna survive,\nit's not gonna be useful. So here's a great example. So what I'm, like, language\ndeath is super common, okay? Languages are dying all around the world. And here's why they're dying. And it's like, yeah, I see this in, it's not happening right now in either the Chimane or the Piraha, but it probably will. And so there's a neighboring\ngroup called Mositan, which is, I said that it's an isolate. It's actually, there's a dual,\nthere's two of them, okay? So it's actually, there's two languages which are really close, which\nare Mositan and Chimane, which are unrelated to anything else. And Mositan is unlike Chimane in that it has a lot of contact\nwith Spanish and it's dying. So that language is dying. The reason it's dying is\nthere's not a lot of value for the local people in\ntheir native language. So there's much more\nvalue in knowing Spanish, like, because they wanna\nfeed their families. And how do you feed your family? You learn Spanish so you can make money, so you can get a job and do these things, and then you make money. And so they want Spanish things. And so Mositan is in danger and is dying. And that's normal. And so basically the\nproblem is that people, the reason we learn\nlanguage is to communicate. And we use it to make money and to do whatever it\nis to feed our families. And if that's not happening,\nthen it won't take off. It's not like a game or something. This is like something we use. Like, why is English so popular? It's not because it's an\neasy language to learn. Maybe it is. I don't really know. But that's not why it's popular. - But because the United\nStates is a gigantic economy, and therefore-\n- Yeah, yeah. It's big economies that do this. It's all it is. It's all about money. And so there's a motivation\nto learn Mandarin. There's a motivation to learn Spanish. There's a motivation to learn English. These languages are very valuable to know because there's so, so many\nspeakers all over the world. - That's fascinating. - There's less of a value economically. It's like kind of what drives this. It's not just for fun. I mean, there are these\ngroups that do want to learn language just\nfor language's sake. And there's something to that. But those are rarities in general. Those are a few small groups that do that. Most people don't do that. - Well, if that was the primary driver, then everybody was speaking English or speaking one language. There's also attention.\n- That's happening. - Well. - We're moving towards fewer\nand fewer languages, exactly. - We are. I wonder if, you're right. Maybe this is slow, but maybe\nthat's where we're moving. But there is a tension. You're saying a language that the fringes. But if you look at\ngeopolitics and superpowers, it does seem that there's\nanother thing in tension, which is a language is a\nnational identity sometimes. - Oh, yeah.\n- For certain nations. I mean, that's the war in Ukraine. Language, Ukrainian language\nis a symbol of that war in many ways, like a country\nfighting for its own identity. So it's not merely the convenience. I mean, those two things are a tension, is the convenience of\ntrade and the economics and be able to communicate\nwith neighboring countries and trade more efficiently\nwith neighboring countries, all that kind of stuff, but\nalso identity of the group. - [Edward] I completely agree. - As language is the\nway, for every community, like dialects that emerge are\na kind of identity for people. Sometimes a way for people to say F-U to the more powerful people. That's interesting. So in that way, language\ncan be used as that tool. - Yeah, I completely agree. And there's a lot of work to\ntry to create that identity so people want to do that. As a cognitive scientist\nand language expert, I hope that continues because\nI don't want languages to die. I want languages to survive because they're so interesting\nfor so many reasons. But I mean, I find them fascinating just for the language part, but I think there's a lot of\nconnections to culture as well, which is also very important."
    },
    {
      "timestamp": "2:39:21",
      "section": "Language translation",
      "text": "- Do you have hope for machine translation that can break down the\nbarriers of language? So while all these different\ndiverse languages exist, I guess there's many ways\nof asking this question, but basically how hard is it to translate in an automated way from\none language to another? - There's gonna be cases where it's gonna be really hard, right? So there are concepts\nthat are in one language and not in another. Like the most extreme kinds of cases are these cases of number information. So good luck translating a\nlot of English into Piraha. It's just impossible. There's no way to do it because there are no\nwords for these concepts that we're talking about. There's probably the flip side, right? There's probably stuff in Piraha which is gonna be hard to translate into English on the other side. And so I just don't know\nwhat those concepts are. I mean, the world space is\ndifferent from my world space. And so I don't know what, so the things they talk about, things are, it's gonna\nhave to do with their life as opposed to my industrial life, which is gonna be different. And so there's gonna be\nproblems like that always. There's like, maybe it's not so bad in the case of some of these spaces and maybe it's gonna be harder in others. And so it's pretty bad in number. It's like extreme, I'd\nsay, in the number space, exact number space. But in the color dimension, right? So that's not so bad. I mean, but it's a problem that you don't have ways\nto talk about the concepts. - And there might be entire\nconcepts that are missing. So to you, it's more\nabout the space of concept versus the space of form. Like form, you can probably map. - Yeah, but so you were talking\nearlier about translation and about how translations, there's good and bad translations. I mean, now we're talking about\ntranslations of form, right? So what makes a writing good, right? - [Lex] There's a music to the form. - It's not just the content,\nit's how it's written. And translating that,\nthat sounds difficult. - We should say that there is like, I don't hesitate to say meaning, but there's a music and\na rhythm to the form. When you look at the broad picture, like the difference between\nDostoevsky and Tolstoy or Hemingway, Bukowski, James\nJoyce, like I mentioned, there's a beat to it,\nthere's an edge to it that it's like, is in the form. - We can probably get measures of those. - Yeah.\n- I don't know. I'm optimistic that we could\nget measures of those things. And so maybe that's- - Translatable? - I don't know. I don't know though. I have not worked on that. - [Lex] I would love to see- - That sounds totally fascinating. - Translation to Hemingway. I mean, Hemingway's probably the lowest, I would love to see different authors, but the average per\nsentence dependency length for Hemingway is probably the shortest. - That's your sense, huh? It's simple sentences with\nshort, yeah, yeah, yeah, yeah. - I mean, that's when, if you\nhave really long sentences, even if they don't have\ncenter embedding, like- - They can have longer connections, yeah. - [Lex] They can have longer connections. - They don't have to, right? You can't have a long, long sentence with a bunch of local words, yeah. But it is much more likely\nto have the possibility of long dependencies with\nlong sentences, yeah."
    },
    {
      "timestamp": "2:42:36",
      "section": "Animal communication",
      "text": "- I met a guy named Aza Raskin, who does a lot of cool\nstuff, really brilliant. Works with Tristan Harris\non a bunch of stuff. But he was talking to me about\ncommunicating with animals. He co-founded Earth Species Project, where you're trying to\nfind the common language between whales, crows, and humans. And he was saying that there's\na lot of promising work, that even though the\nsignals are very different, like the actual, like,\nif you have embeddings of the languages, they're actually trying to communicate similar type things. Is there something you\ncan comment on that? Like where, is there promise to that? In everything you've seen\nin different cultures, especially like remote cultures, that this is a possibility? Or no, that we can talk to whales? - I would say yes. I think it's not crazy at all. I think it's quite reasonable. There's this sort of weird\nview, well, odd view, I think, that to think that human\nlanguage is somehow special. I mean, it is, maybe it is. We can certainly do more than\nany of the other species. And maybe our language\nsystem is part of that. It's possible. But people have often\ntalked about how human, like Chomsky, in fact, has talked about how only human language has\nthis compositionality thing that he thinks is sort of key in language. And the problem with that argument is he doesn't speak whale. (laughs) And he doesn't speak crow,\nand he doesn't speak monkey. He's like, they say things like, \"Well, they're making a\nbunch of grunts and squeaks.\" And the reasoning is like,\nthat's bad reasoning. Like, I'm pretty sure if you asked a whale what we're saying, they'd say, \"Well, they're making a\nbunch of weird noises.\" - Exactly.\n- And so it's like, this is a very odd reasoning to be making that human language is special because we're the only ones\nwho have human language. I'm like, well, we don't\nknow what those other, we just can't talk to them yet. And so there are probably\na signal in there, and it might very well\nbe something complicated like human language. I mean, sure, with a small\nbrain in lower species, there's probably not a very\ngood communication system, but in these higher species where you have what seems to be abilities\nto communicate something, there might very well be\na lot more signal there than we might have otherwise thought. - But also, if we have a lot\nof intellectual humility here, there's somebody formerly from MIT, Neri Oxman, who I admire very much, has talked a lot about, has worked on communicating with plants. So like, yes, the signal\nthere is even less than, but like, it's not out of\nthe realm of possibility that all nature has a\nway of communicating. And it's a very different language, but they do develop a kind of language through the chemistry, through some way of\ncommunicating with each other. And if you have enough humility\nabout that possibility, I think you can, I think it would be very\ninteresting in a few decades, maybe centuries, hopefully not, a humbling possibility of\nbeing able to communicate not just between humans, effectively, but between all of living things on Earth. - Well, I mean, I think some of them are not gonna have much\ninteresting to say, but some of them will.\n- But you could still- - We don't know. We certainly don't know. - I think if we're humble, there could be some\ninteresting trees out there. - Well, they're probably\ntalking to other trees, right? They're not talking to us. And so to the extent they're talking, they're saying something\ninteresting to some other, conspecific, as opposed to us, right? And so there probably is,\nthere may be some signal there. So there are people out there, actually it's pretty common\nto say that human language is special and different from any other animal\ncommunication system. And I just don't think\nthe evidence is there for that claim. I think it's not obvious. We just don't know, 'cause we don't speak these\nother communication systems until we get better. I do think there are\npeople working on that, as you pointed out, people working on whale\nspeak, for instance. That's really fascinating. - Let me ask you a wild,\nout there sci-fi question. If we make contact with an\nintelligent alien civilization, and you get to meet them, how hard do you think, how surprised would you be about\ntheir way of communicating? Do you think it would be recognizable? Maybe there's some parallels here to when you go to the remote tribes. - I mean, I would want\nDan Everett with me. He is amazing at learning\nforeign languages. And so he, this is an amazing feat, right? To be able to go, this is a language, which has no translators before him. I mean, there were-\n- Oh, wow, he just shows up?\n- He was a missionary. Well, there was a guy that\nhad been there before, but he wasn't very good. And so he learned the language far better than anyone else had learned before him. He's good at, he's a very social person. I think that's a big part of\nit, is being able to interact. So I don't know, it kind\nof depends on this species from outer space, how much\nthey wanna talk to us. - Is there something you could say about the process he follows? How do you show up to\na tribe and socialize? I mean, I guess colors and counting is one of the most basic\nthings to figure out. - Yeah, you start that, you\nactually start with objects and just say, you know,\njust throw a stick down and say, \"Stick,\" and then you\nsay, \"What do you call this?\" And then they'll say\nthe word for whatever. And he says, \"The standard thing to do \"is to throw two sticks at two sticks.\" And then, you know, he\nlearned pretty quick that there weren't any\ncount words in this language because they didn't know,\nthis wasn't interesting. I mean, it was kind of weird. They'd say some or something, the same word over and over again. And so, but that is a standard thing. You just like try to, but you\nhave to be pretty out there socially, like willing\nto talk to random people, which these are, you know, really very different people from you. And he was, and he's very social. And so I think that's a big part of this, is like, that's how, you know, a lot of people know a lot of languages is they're willing to\ntalk to other people. - That's a tough one, where you just show up knowing nothing. - [Edward] Yeah, oh God, yeah, yeah, yeah. - It's beautiful that humans are able to connect in that way. - [Edward] Yeah, yeah. - You've had an incredible career exploring this fascinating topic. What advice would you\ngive to young people? About how to have a career like that, or a life that they can be proud of? - When you see something\ninteresting, just go and do it. Like I do that. Like that's something I do, which is kind of unusual for most people. So like when I saw the Piraha, like if Piraha was\navailable to go and visit, I was like, yes, yes, I'll go. And then when we couldn't go back, we had some trouble with\nthe Brazilian government. There's some corrupt people there. It was very difficult to\nget, go back in there. And so I was like, all right, I gotta find another group. And so we searched around, and we were able to find the Chamonix, because I wanted to keep\nworking on this kind of problem. And so we found the\nChamonix and just go there. I didn't really have,\nwe didn't have content. We had a little bit of\ncontact and brought someone. And that was, you know, we\njust kind of just try things. I say it's like, a lot of\nthat's just like ambition, just try to do something that\nother people haven't done. Just give it a shot is what I, I mean, I do that all the time. - I love it. And I love the fact\nthat your pursuit of fun has landed you here talking to me. This was an incredible conversation, that you're just a\nfascinating human being. Thank you for taking a journey through human language with me today. This is awesome. - [Edward] Thank you very much,\nLex, it's been a pleasure. - Thanks for listening\nto this conversation with Edward Gibson. To support this podcast, please check out our\nsponsors in the description. And now let me leave you with\nsome words from Wittgenstein. \"The limits of my language\nmean the limits of my world.\" Thank you for listening. I hope to see you next time."
    }
  ],
  "full_text": "- Naively, I certainly\nthought that all humans would have words for exact counting, and the Piraha don't, okay? So they don't have any words for even one. There's not a word for\none in their language. And so there's certainly not a\nword for two, three, or four, and so that kind of\nblows people's minds off. - Yeah, that is blowing my mind. - [Edward] That's pretty weird, isn't it? - How are you gonna ask,\n\"I want two of those\"? - You just don't, and so\nthat's just not a thing you can possibly ask in the Piraha. It's not possible. That is, there's no words for that. - The following is a\nconversation with Edward Gibson, or Ted, as everybody calls him. He is a psycholinguistics\nprofessor at MIT. He heads the MIT Language\nLab that investigates why human languages look the way they do, the relationship between cultural language and how people represent,\nprocess, and learn language. Also, he should have a book titled \"Syntax, a Cognitive Approach,\" published by MIT Press,\ncoming out this fall. So look out for that. This is \"Lex Fridman Podcast.\" To support it, please\ncheck out our sponsors in the description. And now, dear friends,\nhere's Edward Gibson. When did you first become\nfascinated with human language? - As a kid in school, when\nwe had to structure sentences in English grammar, I found\nthat process interesting. I found it confusing as to\nwhat it was I was told to do. I didn't understand what\nthe theory was behind it, but I found it very interesting. - So when you look at grammar, you're almost thinking\nabout it like a puzzle, like almost like a mathematical puzzle? - Yeah, I think that's right. I didn't know I was gonna work\non this at all at that point. I was really just, I was\nkind of a math geek person, computer scientist. I really liked computer science. And then I found language\nas a neat puzzle to work on from an engineering perspective, actually. That's what, I sort of accidentally, I decided after I finished\nmy undergraduate degree, which was computer science and math and Canada and Queens University, I decided to go to grad school. It's like, that's what I\nalways thought I would do. And I went to Cambridge\nwhere they had a master's in, a master's program in\ncomputational linguistics. And I hadn't taken a single\nlanguage class before. All I had taken was CS,\ncomputer science, math classes, pretty much, mostly as an undergrad. And I just thought this\nwas an interesting thing to do for a year. 'Cause it was a single year program. And then I ended up spending\nmy whole life doing it. - So fundamentally, your\njourney through life was one of a mathematician\nand a computer scientist. And then you kind of\ndiscovered the puzzle, the problem of language and\napproached it from that angle to try to understand it from that angle, almost like a mathematician\nor maybe even an engineer. - As an engineer, I'd\nsay, I mean, to be frank, I had taken an AI class, I\nguess it was '83 or '84, '85, somewhere '84 in there, a long time ago. And there was a natural\nlanguage section in there. And it didn't impress me. I thought there must be more\ninteresting things we can do. It didn't seem very, it seemed\njust a bunch of hacks to me. It didn't seem like a real\ntheory of things in any way. And so I just thought this\nseemed like an interesting area where there wasn't enough good work. - Did you ever come across\nthe philosophy angle of logic? So if you think about the '80s with AI, the expert systems\nwhere you try to kind of maybe sidestep the poetry of language and some of the syntax and the grammar and all that kind of stuff and\ngo to the underlying meaning that language is trying to communicate and try to somehow compress that in a computer-representable way. Do you ever come across\nthat in your studies? - I mean, I probably did, but\nI wasn't as interested in it. I was trying to do the\neasier problems first, the ones I could, thought\nmaybe were handleable, which seems like the syntax is easier, like which is just the forms\nas opposed to the meaning. Like when you're starting\nto talk about the meaning, that's a very hard problem. And it still is a really,\nreally hard problem. But the forms is easier. And so I thought at least figuring out the forms of human language,\nwhich sounds really hard, but is actually maybe more tractable. - So it's interesting. You think there is a big\ndivide, there's a gap, there's a distance\nbetween form and meaning. Because that's a question\nyou have discussed a lot with LLMs, because\nthey're damn good at form. - Yeah, I think that's what\nthey're good at, is form. - Yeah.\n- Exactly. And that's why they're good,\n'cause they can do form. Meaning's hard. - Do you think there's, oh, wow. And I mean, it's an open question, right? How close form and meaning are. We'll discuss it, but\nto me, studying form, maybe it's a romantic notion, gives you, form is like the shadow of the bigger meaning\nthing underlying language. Language is how we communicate ideas. We communicate with each\nother using language. So in understanding the\nstructure of that communication, I think you start to understand\nthe structure of thought and the structure of meaning\nbehind those thoughts and communication, to me. But to you, big gap. - Yeah. - What do you find most\nbeautiful about human language? Maybe the form of human language, the expression of human language. - What I find beautiful\nabout human language is some of the generalizations that happen across the human languages, within and across a language. So let me give you an example of something which I find kind of remarkable, that is if a language,\nif it has a word order such that the verbs tend to\ncome before their objects, and so that's like English does that. So we have the first,\nthe subject comes first in a simple sentence. So I say, \"The dog chased the cat,\" or, \"Mary kicked the ball.\" So the subject's first,\nand then after the subject, there's the verb, and\nthen we have objects. All these things come after in English. So it's generally a verb,\nand most of the stuff that we wanna say comes after the subject. It's the objects, there's a lot of things we wanna say that come after. And there's a lot of languages like that. About 40% of the languages\nof the world look like that. They're subject, verb, object languages. And then these languages\ntend to have prepositions, these little markers on the nouns that connect nouns to other\nnouns or nouns to verbs. So a preposition like in,\nor on, or of, or about, I say I talk about something, the something is the\nobject of that preposition. We have these little markers come, just like verbs, they\ncome before their nouns. So now we look at other\nlanguages like Japanese, or Hindi, these are so-called\nverb final languages. Those, maybe a little more than 40%, maybe 45% of the world's languages, or more, I mean 50% of the\nworld's languages are verb final. Those tend to be postpositions. Those markers, the States\nhave the same kinds of markers as we do in English,\nbut they put 'em after. So, sorry, but they put 'em\nfirst, the markers come first. So you say, instead of, you\nknow, talk about a book, you say a book about,\nthe opposite order there, in Japanese or in Hindi,\nyou do the opposite. And the talk comes at the end. So the verb will come at the end as well. So instead of Mary kicked the\nball, it's Mary ball kicked. And then if it says Mary\nkicked the ball to John, it's John to, the to, the marker there, the preposition, it's a\npostposition in these languages. And so the interesting thing,\nfascinating thing to me, is that within a language, this\norder aligns, it's harmonic. And so if it's one or the other, if it's either verb initial or verb final, but then you'll have\nprepositions, prepositions, or postpositions. And so that, and that's\nacross the languages that we can look at. We've got around 1,000 languages for, there's around 7,000\nlanguages around on the Earth right now, but we have\ninformation about, say, word order on around 1,000 of those,\npretty decent amount of information. And for those 1,000 which we know about, about 95% fit that pattern. So they will have either verb,\nit's about half and half, half a verb initial, like English, and half a verb final, like Japanese. - So just to clarify, verb\ninitial is subject, verb, object. - [Edward] That's correct. - Verb final is still\nsubject, object, verb. - That's correct, yeah, the\nsubject is generally first. - That's so fascinating. I ate an apple, or I apple ate. - [Edward] Yes. - Okay, and it's fascinating that there's a pretty even division in\nthe world amongst those 45%. - Yeah, it's pretty even. And those two are the most common by far. Those two word orders, the\nsubject tends to be first. There's so many interesting things, but these things are, the\nthing I find so fascinating is there are these generalizations within and across a language. And not only those, and there's actually a simple explanation, I\nthink, for a lot of that. And that is, you're trying to minimize dependencies between words. That's basically the story, I think, behind a lot of why word\norder looks the way it is, is we're always connecting. What is the thing I'm telling you? I'm talking to you in sentences, you're talking to me in sentences. These are sequences of\nwords which are connected. And the connections are\ndependencies between the words. And it turns out that\nwhat we're trying to do in a language is actually\nminimize those dependency links. It's easier for me to say things if the words that are\nconnecting for their meaning are close together. It's easier for you in\nunderstanding if that's also true. If they're far away, it's\nhard to produce that, and it's hard for you to understand. And the languages of the world, within a language and across languages, fit that generalization, which is, so it turns out that having verbs initial and then having prepositions ends up making dependencies shorter. And having verbs final\nand having postpositions ends up making dependencies\nshorter than if you cross them. If you cross them, it\nends up, you just end up, it's possible, you can do it.\n- You mean within a language? - Within a language, you can do it. It just ends up with longer dependencies than if you didn't. And so languages tend to go that way. They tend to, they call it harmonic. So it was observed a long time\nago without the explanation by a guy called Joseph Greenberg, who's a famous typologist from Stanford. He observed a lot of generalizations about how word order works, and these are some of the\nharmonic generalizations that he observed. - Harmonic generalizations\nabout word order. There's so many things I wanna ask you. - [Edward] Okay, good. - Let me just, sometimes basics. You mentioned dependencies a few times. What do you mean by dependencies? - Well, what I mean is, in language, there's kind of three structures to, three components to the\nstructure of language. One is the sounds. So cat is cuh, at, and tuh in English. I'm not talking about that part. I'm talking, then there's\ntwo meaning parts, and those are the words. And you were talking\nabout meaning earlier. So words have a form, and they have a meaning\nassociated with them. And so cat is a full form in English, and it has a meaning associated\nwith whatever a cat is. And then the combinations of words, that's what I'll call grammar or syntax. And that's like when I have\na combination like the cat or two cats, okay? So where I take two different words there and put them together, and I get a compositional meaning from putting those two\ndifferent words together. And so that's the syntax. And in any sentence or utterance, whatever I'm talking to\nyou, you're talking to me, we have a bunch of words, and we're putting together in a sequence. It turns out they are connected so that every word is connected to just one other word in that sentence. And so you end up with what's\ncalled technically a tree. It's a tree structure. So where there's a root of that\nutterance, of that sentence, and then there's a bunch of dependents, like branches from that root\nthat go down to the words. The words are the leaves in\nthis metaphor for a tree. - So a tree is also sort of\na mathematical construct. - [Edward] Yeah, yeah, it's\na graph theoretical thing. - It's a graph theory thing. So it's fascinating\nthat you can break down a sentence into a tree, and then every word is\nhanging on to another, it's depending on it. - That's right, and\neveryone agrees on that. So all linguists will agree with that. - [Lex] Oh, so this is\nnot a controversial- - That is not controversial. - There's nobody sitting here-\n- I do not think so. - Mad at you.\n- I don't think so. - Okay, there's no linguists\nsitting there mad at this. - No, I think in every language, I think everyone agrees that all sentences are trees at some level. - Can I pause on that?\n- Sure. - 'Cause to me, just as a layman, it's surprising that you\ncan break down sentences in mostly all languages-\n- All languages, I think. - Into a tree.\n- I think so. I've never heard of anyone\ndisagreeing with that. - That's weird. - The details of the trees are\nwhat people disagree about. - Well, okay, so what's\nat the root of a tree? How do you construct, how hard is it, what is the process of constructing\na tree from a sentence? - Well, this is where,\ndepending on what your, there's different theoretical notions. I'm gonna say the simplest\nthing, dependency grammar. It's like a bunch of people invented this. Tesniere was the first French guy back in, I mean, the paper was published in 1959, but he was working on the '30s and stuff. And it goes back to philologist Pignini was doing this in ancient India, okay? And so, doing something like this. The simplest thing we can think of is that there's just\nconnections between the words to make the utterance. And so, let's just say I\nhave two dogs entered a room. Okay, here's a sentence. And so, we're connecting\ntwo and dogs together. That's like, there's some\ndependency between those words to make some bigger meaning. And then we're connecting\ndogs now to entered, right? And we connect a room somehow to entered. And so, I'm gonna connect to room and then room back to entered. That's the tree is I, the root is entered. That's the thing is\nlike an entering event. That's what we're saying here. And the subject, which\nis whatever that dog is, is two dogs, it was. And the connection goes back to dogs, which goes back to, then\nthat goes back to two. I'm just, that's my tree. It starts at entered,\ngoes to dogs, down to two. And then the other side,\nafter the verb, the object, it goes to room, and then that goes back to the determiner or article, whatever you wanna call that word, a. So, there's a bunch of categories of words here we're noticing. So, there are verbs. Those are these things\nthat typically mark, they refer to events\nand states in the world. And there are nouns, which typically refer to people, places, and\nthings, is what people say. But they can refer to other more, they can refer to events\nthemselves as well. They're marked by how they, the category, the part of speech of a word is how it gets used in language. It's like, that's how you decide what the category of a word is. Not by the meaning, but how it gets used. - How it's used. What's usually the root? Is it gonna be the verb\nthat defines the event? - Usually, usually, yes, yes. - Okay.\n- Yeah. I mean, if I don't say a verb, then there won't be a verb,\nand so it'll be something else. - What if you're messing? Are we talking about language\nthat's like correct language? What if you're doing poetry\nand messing with stuff? Is it, then rules go\nout the window, right? Then it's-\n- No. - You're still-\n- No, no, no, no, no. You're constrained by whatever language you're dealing with. Probably you have other\nconstraints in poetry, such that you're, like usually in poetry, there's multiple constraints\nthat you want to, like you wanna usually\nconvey multiple meanings is the idea, and maybe\nyou have like a rhythm or a rhyming structure\nas well, and depending, but you usually are\nconstrained by the rules of your language for the most part, and so you don't violate those too much. You can violate them\nsomewhat, but not too much, so it has to be recognizable\nas your language. Like in English, I can't say,\n\"Dogs two entered room a.\" I mean, I meant that\ntwo dogs entered a room, and I can't mess with the\norder of the articles, the articles and the nouns,\nyou just can't do that. In some languages, you can mess around with the order of words much more. I mean, you speak Russian. Russian has a much freer\nword order than English, and so in fact, you can\nmove around words in, I told you that English has this subject, verb, object, word order, so does Russian, but Russian is much freer than English, and so you can actually mess\naround with the word order, so probably Russian poetry\nis gonna be quite different from English poetry because the word order is much less constrained. - Yeah, there's a much more\nextensive culture of poetry throughout the history of\nthe last 100 years in Russia, and I always wondered why that is, but it seems that there's more flexibility in the way the language is used. You're morphing the language easier by altering the words, altering\nthe order of the words, and messing with it. - Well, you can just mess\nwith different things in each language, and so in Russian, you have case markers,\nwhich are just these endings on the nouns which tell\nyou how it connects, each noun connects to the verb, right? We don't have that in English, and so when I say Mary kissed John, I don't know who the\nagent or the patient is except by the order of the words, right? In Russian, you actually\nhave a marker on the end if you're using a Russian name, and each of those names, you'll also say, is it agent, it'll be the nominative, which is marking the subject, or an accusative will mark the object, and you could put them\nin the reverse order. You could put accusative first, you could put subject, you\ncould put the patient first, and then the verb, and then the subject, and that would be a perfectly\ngood Russian sentence, and it would still mean, I\ncould say John kissed Mary, meaning Mary kissed John, as long as I use the case\nmarkers in the right way. You can't do that in English, and so- - I love the terminology\nof agent and patient, and the other ones you used. Those are sort of\nlinguistic terms, correct? - Those are, those are\nfor kind of meaning, those are meaning, and subject and object are generally used for position, so subject is just the thing\nthat comes before the verb, and the object is the one\nthat comes after the verb. The agent is kind of like the thing doing, that's kind of what that means, right? The subject is often the\nperson doing the action, right? The thing, so, yeah. - Okay, this is fascinating. So how hard is it to\nform a tree in general? Is there a procedure to it? Like if you look at different languages, is it supposed to be a very natural, like is it automatable, or is there some human genius involved? - I think it's pretty\nautomatable at this point. People can figure out what the words are. They can figure out the morphemes, which are the, technically, morphemes are the minimal meaning units\nwithin a language, okay? And so when you say eats or drinks, it actually has two morphemes in English. There's the root, which is the verb, and then there's some ending on it, which tells you that's\nthis third person singular. - [Lex] Can you say what morphemes are? - Morphemes are just the\nminimal meaning units within a language. And then a word is just kind of the things we put spaces between in English. They have a little bit more. They have the morphology as well. They have the endings,\nthis inflectual morphology on the endings on the roots. - They modify something about the word that adds additional meaning. - They tell you, yeah, yeah, yeah. And so we have a little\nbit of that in English, just very little, much more\nin Russian, for instance. But we have a little bit in English. And so we have a little on the nouns. You can say it's either\nsingular or plural. And you can say, same thing for verbs. Like simple past tense, for example, it's like notice in\nEnglish, we say drinks. He drinks, but everyone else\nsays I drink, you drink, we drink, it's unmarked in a way. But in the past tense, it's just drank. For everyone, there's no\nmorphology at all for past tense. There is morphology,\nit's marking past tense, but it's kind of, it's an irregular now. So we don't even, you\nknow, drink to drank, you know, it's not even a regular word. So in most verbs, many\nverbs, there's an ed, we kind of add, so walk to walked, we add that to say it's the past tense. That, I just happened\nto choose an irregular 'cause it's a high-frequency word. High-frequency words tend to\nhave irregulars in English. - [Lex] What's an irregular? - Irregular is just, there isn't a rule. So drink to drank is an irregular. - Drink, drank, okay, versus walked. - As opposed to walk,\nwalked, talked, talked. - And there's a lot of\nirregulars in English. - There's a lot of irregulars in English. The frequent ones, the common\nwords tend to be irregular. There's many, many more\nlow-frequency words, and those tend to be,\nthose irregular ones. - The evolution of the\nirregulars are fascinating. It is essentially slang that's sticky 'cause you're breaking the rules, and then everybody uses it\nand doesn't follow the rules, and they say screw it to the rules. It's fascinating. So you said morphemes, lots of questions. So morphology is what,\nthe study of morphemes? - Morphology is the connections between the morphemes\nonto the roots, the roots. So in English, we mostly have suffixes. We have endings on the\nwords, not very much, but a little bit, as opposed to prefixes. Some words, depending on your language, can have mostly prefixes,\nmostly suffixes, or both. And then even languages, several languages have things called infixes, where you have some kind of a general form for the root, and you\nput stuff in the middle. You change the vowels, stuff like that. - That's fascinating. That is fascinating. So in general, there's,\nwhat, two morphemes per word? Usually one or two, or three? - Well, in English, it's one or two. In English, it tends to be one or two. There can be more. In other languages, a\nlanguage like Finnish, which has a very elaborate morphology, there may be 10 morphemes\non the end of a root, okay? And so there may be millions\nof forms of a given word, okay? - Okay, I will ask the same\nquestion over and over, but how does the, just\nsometimes to understand things like morphemes, it's\nnice to just ask the question, how do these kinds of things evolve? So you have a great book\nstudying sort of the, how the cognitive processing, how language used for communication, so the mathematical notion\nof how effective language is for communication and what role that plays in the evolution of language. But just high level, like how do we, how does a language evolve with, where English has two morphemes, or one or two morphemes per word, and then Finnish has infinity per word? So what, how does that happen? Is it just people- - That's a really good question. That's a very good question, is like, why do languages have more morphology versus less morphology? And I don't think we\nknow the answer to this. I think there's just like\na lot of good solutions to the problem of communication. So I believe, as you hinted, that language is an\ninvented system by humans for communicating their ideas. And I think it comes down to, we label the things we wanna talk about. Those are the morphemes and words. Those are the things we wanna\ntalk about in the world, and we invent those things. And then we put 'em together in ways that are easy for us\nto convey, to process. But that's like a naive view, and I don't, I mean, I think it's\nprobably right, right? It's naive and probably right, but- - Well, that's the thing is,\nI don't know if it's naive. I think it's simple.\n- Simple, yeah. - I think naive is an\nindication that it's incorrect, and somehow it's a trivial,\ntoo simple, I think. It could very well be correct. But it's interesting how sticky, it feels like two people got together. It just feels like once you figure out certain aspects of a language, that just becomes sticky, and the tribe forms around that language. Maybe the language, maybe\nthe tribe forms first, and then the language evolves. And then you just kind of agree, and then you stick to whatever that is. - I mean, these are very\ninteresting questions. We don't know really about how words, even words, get invented very much, about, we don't really, I mean, assuming they get invented, we don't really know\nhow that process works and how these things evolve. What we have is kind of a current picture, a current picture of a\nfew thousand languages, a few thousand instances. We don't have any pictures of really how these things are evolving, really. And then the evolution is\nmassively confused by contact. So as soon as one language group, one group runs into another, we are smart. Humans are smart, and they take on whatever is useful in the other group. And so any kind of contrast\nwhich you're talking about, which I find useful, I'm\ngonna start using as well. So I worked a little bit\nin specific areas of words, in number words and in color words. And in color words, so\nwe have, in English, we have around 11 words that\neveryone knows for colors. And many more if you happen\nto be interested in color for some reason or other. If you're a fashion designer\nor an artist or something, you may have many, many more words. But we can see millions. Like if you have normal color vision, normal trichromatic color vision, you can see millions of\ndistinctions in color. So we don't have millions of words. The most efficient, no, the\nmost detailed color vocabulary would have over a million terms to distinguish all the different\ncolors that we can see, but of course we don't have that. So it's somehow, it's\nkind of useful for English to have evolved in some way to, so there's 11 terms that people\nfind useful to talk about. You know, black, white,\nred, blue, green, yellow, purple, gray, pink, and I\nprobably missed something there. Anyway, there's 11 that everyone knows. And depending on your, but\nyou go to different cultures, especially the\nnon-industrialized cultures, and there'll be many fewer. So some cultures will have\nonly two, believe it or not. The Dani in Papua New\nGuinea have only two labels that the group uses for color. Those are roughly black and white. They are very, very dark\nand very, very light, which are roughly black and white. And you might think, oh, they're dividing the whole color space into\nlight and dark or something. And that's not really true. They mostly just only label\nthe black and the white things. They just don't talk about\nthe colors for the other ones. And so, and then there's other groups. I worked with a group called\nthe Chimane down in Bolivia, in South America, and\nthey have three words that everyone knows,\nbut there's a few others that several people,\nthat many people know. And so they have, kind of\ndepending on how you count, it's between three and seven\nwords that the group knows. And again, they're black and white. Everyone knows those. And red, red is, that\ntends to be the third word that everyone, that cultures bring in. If there's a word, it's\nalways red, the third one. And then after that, it's\nkind of all bets are off about what they bring in. And so after that, they bring in a sort of a big blue-green group. They have one for that. And then they have, and\nthen different people have different words that they'll use for other parts of the space. And so anyway, it's probably related to what they wanna talk, what they, not what they see, 'cause they see the same colors as we see. So it's not like they have\na weak, a low-color palette in the things they're looking at. They're looking at a lot\nof beautiful scenery, okay? A lot of different colored\nflowers and berries and things. And so there's lots of\nthings of very bright colors. But they just don't label\na color in those cases. And the reason probably,\nwe don't know this, but we think probably what's going on here is that what you do,\nwhy you label something is you need to talk to\nsomeone else about it. And why do I need to talk about a color? Well, if I have two\nthings which are identical and I want you to give me\nthe one that's different, and the only way it varies is color, then I invent a word which tells\nyou this is the one I want. So I want the red sweater off the rack, not the green sweater, right? There's two, and so those\nthings will be identical because these are things\nwe made and they're dyed, and there's nothing different about them. And so in industrialized society, we have everything we've got is pretty much arbitrarily colored. But if you go to a\nnon-industrialized group, that's not true. And so they don't, it's not only that they're\nnot interested in color, if you bring bright-colored\nthings to them, they like them just like we like them. Bright colors are great,\nthey're beautiful. They are, but they just don't need to, no need to talk about\nthem, they don't have. - So probably color\nwords is a good example of how language evolves\nfrom sort of function when you need to communicate\nthe use of something. - [Edward] I think so. - Then you kind of invent\ndifferent variations. And basically, you can\nimagine that the evolution of a language has to do with\nwhat the early tribes doing, like what they wanted, what kind of problems are facing them, and they're quickly figuring out how to efficiently\ncommunicate the solution to those problems, whether\nit's aesthetic or functional, all that kind of stuff, running away from a mammoth or whatever. But it's, so I think\nwhat you're pointing to is that we don't have data\non the evolution of language, because many languages were\nformed a long time ago, so you don't get the chatter. - We have a little bit of\nold English to modern English because there was a writing system, and we can see how old English looked. So the word order changed, for instance, in old English to middle\nEnglish to modern English, and so we could see things like that. But most languages don't\neven have a writing system. So of the 7,000, only\na small subset of those have a writing system, and even if they have a writing system, it's not a very modern writing system, and so they don't have it. So we just basically have\nfor Mandarin, for Chinese, we have a lot of evidence for a long time, and for English, and not for much else. Not for main German a little bit, but not for a whole lot of\nlong-term language evolution. We don't have a lot. We just have snapshots, is what we've got, of current languages. - Yeah, you get an inkling of that from the rapid communication\non certain platforms, like on Reddit. There's different communities, and they'll come up with different slang, usually, from my perspective, driven by a little bit of humor, or maybe mockery or whatever. You know, just talking shit\nin different kinds of ways. And you could see the\nevolution of language there, because I think a lot of\nthings on the internet, you don't want to be\nthe boring mainstream. So you want to deviate from\nthe proper way of talking, and so you get a lot of\ndeviation, like rapid deviation. Then when communities\ncollide, you get like, just like you said, humans adapt to it, and you can see it\nthrough the lens of humor. I mean, it's very difficult to study, but you can imagine\nlike 100 years from now, if there's a new language\nborn, for example, we'll get really high resolution data. - I mean, English is changing. English changes all the time. All languages change all the time. So there's a famous result\nabout the Queen's English. So if you look at the Queen's vowels, the Queen's English is supposed to be, originally, the proper way for the talk was sort of defined by\nwhoever the Queen talked, or the King, whoever was in charge. And so if you look at\nhow her vowels changed from when she first became\nQueen in 1952 or '53, when she was coronated, the first, I mean, that's Queen Elizabeth, who died recently, of course, until 50 years later, her vowels changed. Her vowels shifted a lot, and so that, even in the sounds of British English, in her, the way she was\ntalking was changing. The vowels were changing slightly. So that's just, in the\nsounds, there's change. I don't know what's, I'm interested, we're all interested in what's\ndriving any of these changes. The word order of English\nchanged a lot over 1,000 years. So it used to look like German. It used to be a verb-final\nlanguage with case marking, and it shifted to a verb-medial language, a lot of contact, so a lot\nof contact with French, and it became a verb-medial\nlanguage with no case marking. And so it became this\nverb-initially thing. So that's-\n- It's evolving. - It totally evolved. I mean, it doesn't evolve\nmaybe very much in 20 years, is maybe what you're talking about. But over 50 and 100 years,\nthings change a lot, I think. - We'll now have good data\non it, which is great. - [Edward] That's for sure, yeah. - Can you talk to what is\nsyntax and what is grammar? So you wrote a book on syntax. - I did. You were asking me before\nabout how do I figure out what a dependency structure is. I'd say the dependency structures aren't that hard to, generally, I think there's a lot of\nagreement of what they are for almost any sentence in most languages. I think people will\nagree on a lot of that. There are other parameters in the mix such that some people think\nthere's a more complicated grammar than just a dependency structure. And so, you know, like Noam Chomsky, he's the most famous linguist ever. And he is famous for proposing a slightly more complicated syntax. And so he invented\nphrase structure grammar. So he's well known for many, many things, but in the '50s and early\n'60s, like the late '50s, he was basically figuring out what's called formal language theory. So, and he figured out sort of a framework for figuring out how complicated language, a certain type of language might be, so-called phrase structure\ngrammars of language might be. And so his idea was that\nmaybe we can think about the complexity of a language by how complicated the rules are, okay? And the rules will look like this. They will have a left-hand side and they'll have a right-hand side. Something on the\nleft-hand side will expand to the thing on the right-hand side. So say we'll start with an S, which is like the root,\nwhich is a sentence, okay? And then we're gonna expand to things like a noun phrase and a verb phrase is what he would say, for instance, okay? An S goes to an NP and a VP is a kind of a phrase structure rule. And then we figure out what an NP is. An NP is a determiner\nand a noun, for instance. And a verb phrase is something else, is a verb and another noun phrase, and another NP, for instance. Those are the rules of a very\nsimple phrase structure, okay? And so he proposed\nphrase structure grammar as a way to sort of cover human languages. And then he actually\nfigured out that, well, depending on the formalization\nof those grammars, you might get more complicated or less complicated languages. And so he said, well,\nthese are things called context-free languages that rule, that he thought human languages tend to be what he calls context-free languages. But there are simpler languages, which are so-called regular languages, and they have a more\nconstrained form to the rules of the phrase structure\nof these particular rules. So he basically discovered\nand kind of invented ways to describe the language. And those are phrase structure. A human language. And he was mostly interested\nin English initially in his work in the '50s. - So quick questions around all this. So formal language theory is the big field of just studying language formally. - Yes, and it doesn't have\nto be human language there. We can have computer\nlanguages, any kind of system which is generating some set\nof expressions in a language. And those could be like the statements in a computer language, for example. So it could be that, or it\ncould be human language. - So technically, you can\nstudy programming languages. - Yes, and have been. Heavily studied using this formalism. There's a big field of\nprogramming languages within the formal language. - Okay, and then phrase\nstructure, grammar, is this idea that you\ncan break down language into this S-N-P-V-P type of thing. - It's a particular formalism\nfor describing language. Okay, so and Chomsky was the first one. He's the one who figured that\nstuff out back in the '50s. But he, and that's equivalent, actually. The context-free grammar\nis kind of equivalent in the sense that it\ngenerates the same sentences as a dependency grammar would. The dependency grammar is a\nlittle simpler in some way. You just have a root, and it goes, we don't have any of these, the\nrules are implicit, I guess. We just have connections between words. The phrase structure grammar\nis kind of a different way to think about the dependency grammar. It's slightly more complicated, but it's kind of the same in some ways. - So to clarify, dependency grammar is the framework under\nwhich you see language, and you make the case\nthat this is a good way to describe language.\n- I think it's the, that's correct. - And Noam Chomsky's watching. This is very upset right now. So let's, I'm just kidding. But what's the difference between, where's the place of disagreement between phrase structure\ngrammar and dependency grammar? - They're very close. So phrase structure grammar\nand dependency grammar aren't that far apart. I like dependency grammar\nbecause it's more perspicuous, it's more transparent about representing the connections between the words. It's just a little harder to see in phrase structure grammar. The place where Chomsky sort of devolved or went off from this is, he also thought there was\nsomething called movement, okay? And so, and that's\nwhere we disagree, okay? That's the place where\nI would say we disagree. And I mean, maybe we'll\nget into that later, but the idea is, if you wanna, do you want me to explain that? - I would love, can you explain movement? - [Edward] Movement, okay, so Chomsky- - You're saying so many\ninteresting things. - Okay, so movement is, Chomsky basically sees\nEnglish and he says, okay, I said, we had\nthat sentence earlier, it was like two dogs entered the room. It's changed a little bit. Say, two dogs will enter the room. And he notices that, hey, English, if I wanna make a\nquestion, a yes/no question from that same sentence, I say, instead of two dogs will enter the room, I say, will two dogs enter the room? Okay, there's a different\nway to say the same idea. And it's like, well, the\nauxiliary verb, that will thing, it's at the front as opposed\nto in the middle, okay? And so, and he looked,\nif you look at English, you see that that's true\nfor all those modal verbs and for other kinds of\nauxiliary verbs in English, you always do that. You always put an auxiliary\nverb at the front. And when he saw that, so if I say, I can win this bet, can\nI win this bet, right? So I move a can to the front. So actually, that's a theory. I just gave you a theory there. He talks about it as movement. That word in the declarative is the root, is the sort of default way\nto think about the sentence, and you move the auxiliary\nverb to the front. That's a movement theory, okay? And he just thought\nthat was just so obvious that it must be true, that there's nothing\nmore to say about that, that this is how auxiliary\nverbs work in English. There's a movement rule\nsuch that you're moving, like to get from the declarative\nto the interrogative, you're moving the auxiliary to the front. And it's a little more complicated as soon as you go to simple\npresent and simple past, because if I say John\nslept, you have to say, did John sleep, not slept John, right? And so you have to somehow\nget an auxiliary verb, and I guess underlyingly,\nit's like slept is, it's a little more complicated than that, but that's his idea,\nthere's a movement, okay? And so a different way\nto think about that, that isn't, I mean, then\nhe ended up showing later. So he proposed this theory of\ngrammar, which has movement, and there's other places where\nhe thought there's movement, not just auxiliary verbs, but things like the passive in English, and things like questions, WH questions, a bunch of places where he thought there's also movement going on. And in each one of those,\nhe thinks there's words, well, phrases and words are moving around from one structure to another, which he called deep structure\nto surface structure. I mean, there's like\ntwo different structures in his theory, okay? There's a different way\nto think about this, which is there's no movement at all. There's a lexical copying rule, such that the word will, or the word can, these auxiliary verbs,\nthey just have two forms. And one of them is the declarative, and one of them is interrogative. And you basically have\nthe declarative one, and oh, I form the interrogative, or I can form one from the other, doesn't matter which direction you go. And I just have a new entry,\nwhich has the same meaning, which has a slightly\ndifferent argument structure. Argument structure is just a fancy word for the ordering of the words. And so if I say, it was the dogs, two dogs can or will enter the room, there's two forms of will. One is will declarative, and then, okay, I've got my subject to the\nleft, it comes before me, and the verb comes after me in that one. And then the will interrogative,\nit's like, oh, I go first. Interrogative, will is first, and then I have the\nsubject immediately after, and then the verb after that. And so you can just generate\nfrom one of those words another word with a slightly\ndifferent argument structure, with different ordering. - [Lex] And these are just lexical copies. They're not necessarily\nmoving from one to another. - There's no movement. - There's a romantic notion that you have one main way to use a word, and then you could move it around. - [Edward] Right, right, right. - Which is essentially\nwhat movement is implying. - Yeah, but that's the\nlexical copying is similar. So then we do lexical\ncopying for that same idea that maybe the declarative is the source, and then we can copy it. And so an advantage,\nthere's multiple advantages of the lexical copying story. It's not my story. This is like Ivan Sag, linguists, a bunch of linguists have\nbeen proposing these stories as well, in tandem with\nthe movement story. Okay, Ivan Sag died a while ago, but he was one of the\nproponents of the non-movement of the lexical copying story. And so that is that a great advantage is, well, Chomsky, really famously in 1971, showed that the movement story leads to learnability problems. It leads to problems for\nhow language is learned. It's really, really hard to figure out what the underlying\nstructure of a language is if you have both phrase\nstructure and movement. It's really hard to figure\nout what came from what. There's a lot of possibilities there. If you don't have that problem, the learning problem gets a lot easier. - Just say there's lexical copies. - [Edward] Yeah, yeah. - When we say the learning problem, do you mean humans\nlearning a new language? - Yeah, just learning English. So baby is lying around\nlistening to me talk, and how are they learning English? Or maybe it's a two-year-old\nwho's learning interrogatives and stuff, or how are they doing that? Are they doing it from,\nare they figuring out? So Chomsky said it's impossible\nto figure it out, actually. He said it's actually impossible. Not hard, but impossible. And therefore, that's where\nuniversal grammar comes from, is that it has to be built in. And so what they're learning\nis there's some built-in, movement is built in in his story, is absolutely part of\nyour language module. And then you're just setting parameters. You're said, depending on English, it's just sort of a variant\nof the universal grammar, and you're figuring out, oh, which orders does English do these things? That's the non-movement story. It doesn't have this. It's like much more bottom-up. You're learning rules. You're learning rules one by one, and oh, this word is\nconnected to that word. A great advantage, another\nadvantage, it's learnable, another advantage of\nit is that it predicts that not all auxiliaries might move. It might depend on the word,\ndepending on whether you, and that turns out to be true. So there's words that don't\nreally work as auxiliary. They work in declarative\nand not in interrogative. So I can say, I'll give\nyou the opposite first. I can say, aren't I invited to the party? And that's an interrogative form, but it's not from, I aren't\ninvited to the party. There is no, I aren't, right? So that's interrogative only. And then we also have forms like ought. I ought to do this. And I guess some old\nBritish people can say- - Ought I?\n- Exactly. It doesn't sound right, does it? For me, it sounds ridiculous. I don't even think ought is great, but I mean, I totally\nrecognize I ought to. I think it's not too bad, actually. I can say, ought to do this. That sounds pretty good.\n- Ought I? If I'm trying to sound\nsophisticated, maybe. - I don't know. It just sounds completely out to me. - Ought I?\n- Yeah. Anyway, so there are variants here. And a lot of these words just work in one versus the other. And that's fine under the\nlexical copying story. It's like, well, you just learn the usage. Whatever the usage is, is\nwhat you do with this word. But it's a little bit harder\nin the movement story. The movement story, that's an advantage, I think, of lexical copying. In all these different places, there's all these usage variants which make the movement story\na little bit harder to work. - So one of the main divisions here is the movement story versus\nthe lexical copy story. That has to do about the\nauxiliary words and so on. But if you rewind to the\nphrase structure grammar versus dependency grammar. - Those are equivalent in some sense in that for any dependency grammar, I can generate a phrase structure grammar which generates exactly\nthe same sentences. I just like the dependency\ngrammar formalism because it makes something really salient, which is the lengths of\ndependencies between words, which isn't so obvious\nin the phrase structure. In the phrase structure, it's\njust kind of hard to see. It's in there, it's just\nvery, very, it's opaque. - Technically, I think\nphrase structure grammar is mappable to dependency grammar. - And vice versa.\n- And vice versa. But there's these little\nlabels, S and P, V, P. - Yeah, for a particular\ndependency grammar, you can make a phrase structure grammar which generates exactly those\nsame sentences and vice versa. But there are many\nphrase structure grammars which you can't really\nmake a dependency grammar. I mean, you can do a lot more\nin a phrase structure grammar, but you get many more of\nthese extra nodes, basically. You can have more structure in there. And some people like that, and\nmaybe there's value to that. I don't like it. (laughs) - Well, for you, so we should clarify. So dependency grammar, it's just, well, one word depends\non only one other word, and you form these trees, and that makes, it really puts priority\non those dependencies just like as a tree that\nyou can then measure the distance of the dependency\nfrom one word to the other. They can then map to\nthe cognitive processing of these sentences, how\neasy it is to understand, and all that kind of stuff. So it just puts the focus on just the mathematical distance of dependence between words. So it's just a different focus. - Absolutely. - Just continue on the thread of Chomsky, 'cause it's really interesting, 'cause as you're discussing disagreement, to the degree there's disagreement, you're also telling the history\nof the study of language, which is really awesome. So you mentioned\ncontext-free versus regular. Does that distinction come into play for dependency grammars? - No, not at all. I mean, regular languages are too simple for human languages. It's a part of the hierarchy, but human languages in\nthe phrase structure world are definitely, they're\nat least context-free, maybe a little bit more, a\nlittle bit harder than that. So there's something called\ncontext-sensitive as well, where you can have, like this is just the\nformal language description. In a context-free grammar, you have one, this is like a bunch of\nformal language theory we're doing here, but- - [Lex] I love it. - Okay, so you have a\nleft-hand side category, and you're expanding to\nanything on the right, is a, that's a context-free. So the idea is that that\ncategory on the left expands in independent of\ncontext to those things, whatever they are on the\nright, doesn't matter what. And a context-sensitive says, okay, I actually have more\nthan one thing on the left. I can tell you only in this context, you know, maybe you have like\na left and a right context, or just a left context or a right context, I have two or more stuff on the left, tells you how to expand those\nthings in that way, okay? So it's context-sensitive. A regular language is\njust more constrained, and so it doesn't allow\nanything on the right. It allows very, basically\nit's one very complicated rule is kind of what a regular language is. And so it doesn't have any, I was gonna say\nlong-distance dependencies, it doesn't allow recursion, for instance. There's no recursion.\n- Recursion. - Yeah, recursion is where you, which is human languages have recursion, they have embedding, and you can't, well, it doesn't allow\ncenter-embedded recursion, which human languages have, which is what- - Center-embedded recursion,\nso within a sentence? Within a sentence.\n- Yeah, within a sentence. So here we're gonna get to that. But I, you know, the formal language stuff is a little aside. Chomsky wasn't proposing it\nfor human languages even, he was just pointing\nout that human languages are context-free, and then he was most in, for human, 'cause that was kind of stuff he did for formal languages, and what he was most interested\nin was human language, and that's like, the movement is where we, where he sort of set\noff on the, I would say, a very interesting, but wrong foot. It was kind of interesting, it's a very, I agree, it's a very interesting history. So there's this set, so he\nproposed this multiple theories, in '57 and then '65, they all\nhave this framework, though, was phrase structure plus movement, different versions of the phrase structure and the movement in the '57, these are the most famous\noriginal bits of Chomsky's work, and then '71 is when he figured out that those lead to learning problems, that there's cases where a\nkid could never figure out which rule, which set\nof rules was intended. And so, and then he said,\nwell, that means it's innate. It's kind of interesting,\nhe just really thought the movement was just so obviously true that he couldn't, he didn't\neven entertain giving it up, it's just obvious, that's obviously right. And it was later where people figured out that there's all these subtle ways in which things which\nlook like generalizations aren't generalizations, and\nthey, across the category, they're word-specific,\nand they kind of work, but they don't work\nacross various other words in the category, and so it's easier to just think of these\nthings as lexical copies. And I think he was very\nobsessed, I don't know, I'm just guessing, that he just, he really wanted this story\nto be simple in some sense, and language is a little more\ncomplicated in some sense. He didn't like words, he\nnever talks about words, he likes to talk about\ncombinations of words, and words are, you know,\nlook up a dictionary, there's 50 senses for\na common word, right? The word take will have\n30 or 40 senses in it. So there'll be many different\nsenses for common words, and he just doesn't think about that, or he doesn't think that's language. I think he doesn't think that's language. He thinks that words are distinct from combinations of words. I think they're the same. If you look at my brain in the scanner, while I'm listening to\na language I understand, and you compare, I can\nlocalize my language network in a few minutes, in like 15 minutes. And what you do is I listen\nto a language I know, I listen to, you know, maybe\nsome language I don't know, or I listen to muffled speech, or I read sentences, and I read non-words, like I can do anything like this, anything that's sort\nof really like English, and anything that's not very like English. So I've got something like it and not, and I got a control, and the voxels, which is just, you know,\nthe 3D pixels in my brain that are responding\nmost is a language area, and that's this left-lateralized\narea in my head. And wherever I look in that network, if you look for the\ncombinations versus the words, it's everywhere.\n- It's the same. - It's the same.\n- That's fascinating. - And so it's like hard to find, there are no areas that we know, I mean, that's, it's a\nlittle overstated right now. At this point, the technology isn't great, it's not bad, but we have the best way to figure out what's going on in my brain when I'm listening or reading language is to use fMRI, Functional\nMagnetic Resonance Imaging. And that's a very good\nlocalization method, so I can figure out where exactly these signals are coming from, pretty, you know, down to millimeters, you know, cubic millimeters\nor smaller, okay, very small, we can figure those out very well. The problem is the when, okay? It's measuring oxygen, okay? And oxygen takes a little\nwhile to get to those cells, so it takes on the order of seconds. So I talk fast, I probably listen fast, and I can probably understand\nthings really fast, so a lot of stuff happens in two seconds. And so to say that we\nknow what's going on, that the words, right\nnow, in that network, our best guess is that whole network is doing something similar, but maybe different parts of that network are doing different things. And that's probably the case, we just don't have very good methods to figure that out, right,\nat this moment, and so. - Since we're kind of\ntalking about the history of the study of language, what other interesting disagreements, and you're both at MIT,\nor were for a long time, what kind of interesting\ndisagreements there, tension of ideas are there\nbetween you and Noam Chomsky? And we should say that Noam was in the linguistics department, and you're, I guess for a\ntime were affiliated there, but primarily brain and\ncognitive science department, which is another way of studying language, and you've been talking about fMRI. So is there something else interesting to bring to the surface\nabout the disagreement between the two of you? Or other people in the- - Yeah, I mean, I've\nbeen at MIT for 31 years, since 1993, and he, Chomsky's\nbeen there much longer. So I met him, I knew him, I met when I first got there, I guess, and we would interact every now and then. I'd say that, so I'd say\nour biggest difference is, are methods, and so that's\nthe biggest difference between me and Noam, is that\nI gather data from people. I do experiments with people,\nand I gather corpus data, whatever corpus data's available, and we do quantitative methods to evaluate any kind\nof hypothesis we have. He just doesn't do that. So he has never once been associated with any experiment or corpus work ever. And so it's all thought experiments. It's his own intuitions. So I just don't think\nthat's the way to do things. That's a cross the street, there across the street\nfrom us kind of difference between Brain and CogSci and linguistics. I mean, not all linguists,\nsome of the linguists, depending on what you\ndo, more speech-oriented, they do more quantitative stuff, but in the meaning, words and, well, it's combinations of\nwords, syntax, semantics, they tend not to do experiments\nand corpus analyses. - So on the linguistic side, probably, but the method is a symptom\nof a bigger approach, which is sort of a psychology\nphilosophy side on Noam, when for you, it's more\nsort of data-driven, sort of almost like a\nmathematical approach. - Yeah, I mean, I'm a psychologist, so I would say we're in psychology. Brain and Cognitive Science is MIT's old psychology department. It was the psychology\ndepartment up until 1985, and it became the Brain and\nCognitive Science Department. And so, I mean, my training\nis math and computer science, but I'm a psychologist. I mean, I don't know what I am. - So data-driven\npsychologist, well, you are. - I am what I am, but I'm\nhappy to be called a linguist, I'm happy to be called\na computer scientist, I'm happy to be called a psychologist, any of those things. - But in the actual, like\nhow that manifests itself outside of the methodology\nis like these differences, these subtle differences\nabout the movement story versus the lexical copy story. - Yeah, those are theories, right? So the theories are, but I\nthink the reason we differ in part is because of how\nwe evaluate the theories. And so I evaluate theories quantitatively, and Noam doesn't. (laughing) - Got it. Okay, well, let's explore the theories that you explore in your book. Let's return to this\ndependency grammar framework of looking at language. What's a good justification why the dependency grammar framework is a good way to explain language? What's your intuition? - So the reason I like dependency grammar, as I've said before, is\nthat it's very transparent about its representation\nof distance between words. So it's like, all it is is\nyou've got a bunch of words, you're connecting together\nto make a sentence. And a really neat insight,\nwhich turns out to be true, is that the further apart\nthe pair of words are that you're connecting, the harder it is to do the production, the harder it is to do the comprehension. It's harder to produce,\nit's harder to understand when the words are far apart. When they're close together,\nit's easy to produce and it's easy to comprehend. Let me give you an example, okay? So we have, in any language, we have mostly local\nconnections between words, but they're abstract. The connections are abstract, they're between categories of words. And so you can always\nmake things further apart if you add modification,\nfor example, after a noun. So a noun in English comes before a verb, the subject noun comes before a verb, and then there's an\nobject after, for example. So I can say what I said before, the dog entered the room\nor something like that. So I can modify dog. If I say something more\nabout dog after it, then what I'm doing is, indirectly, I'm lengthening the dependence\nbetween dog and entered by adding more stuff to it. So just to make it explicit here, if I say the boy who\nthe cat scratched cried, we're gonna have a mean cat here. And so what I've got here is, the boy cried, it would be a\nvery short, simple sentence, and I just told you\nsomething about the boy, and I told you it was the boy\nwho the cat scratched, okay? - So the cry is connected to the boy. - The boy's gonna cry.\n- The cry at the end is connected to the boy in the beginning. - Right, and so I can do that. I can say that, that's a\nperfectly fine English sentence. And I can say the cat which the dog chased ran away or something, okay? I can do that. But it's really hard now,\nwhatever I have here, I have the boy who the cat, now let's say I try to modify cat, okay? The boy who the cat which the\ndog chased scratched ran away. Oh my God, that's hard, right? I'm sort of just working\nthat through in my head, how to produce, and it's\nreally just horrendous to understand, it's not so bad. At least I've got intonation there to sort of mark the boundaries and stuff, but that's really complicated. That's sort of English, in a way. I mean, that follows the rules of English, but so what's interesting about that is that what I'm doing is\nnesting dependencies here. I'm putting one, I've got a subject connected to a verb there, and then I'm modifying that\nwith a clause, another clause, which happens to have a\nsubject and a verb relation. I'm trying to do that\nagain on the second one. And what that does is it\nlengthens out the dependence, multiple dependents actually\nget lengthened out there. The dependencies get longer, on the outside ones get long, and even the ones in\nbetween get kind of long. And you just, so what's\nfascinating is that, that's bad, that's really\nhorrendous in English. But that's horrendous in any language. So in no matter what language you look at, if you do, just figure out some structure where I'm gonna have some\nmodification following some head, which is connected to some later head, and I do it again, it\nwon't be good, guaranteed. Like 100%, that will be\nuninterpretable in that language, in the same way that was\nuninterpretable in English. - Just to clarify, the\ndistance of the dependencies is whenever the boy cried, there's a dependence between two words, and then you're counting\nthe number of what, morphemes between them? - That's a good question. I'll just say words, your\nwords are morphemes between. We don't know that, actually,\nthat's a very good question. What is the distance metric? But let's just say it's words, sure. - And you're saying\nthe longer the distance of that dependence, the\nmore, no matter the language, except Ligali's, even Ligali,\nokay, we'll talk about it. But that, the people will be very upset that speak that language. Not upset, but they'll\neither not understand it, they'll be like, their brain\nwill be working in overtime. - They will have a hard\ntime either producing or comprehending it. They might tell you\nthat's not their language. It's sort of their language. I mean, it's following their, like they'll agree with\neach of those pieces as part of their language, but somehow that combination\nwill be very, very difficult to produce and understand. - Is that a chicken or the egg issue here? So like, is- - Well, I'm giving you an explanation. - Right. - So the, well, I mean, and then there's, I'm giving you two kinds of explanations. I'm telling you that center\nembedding, that's nesting, those are the same, those are synonyms for the same concept here. And the explanation for\nwhat, those are always hard. Center embedding and\nnesting are always hard. And I gave you an explanation\nfor why they might be hard, which is long-distance connections. There's a, when you do center embedding, when you do nesting, you always have long-distance connections\nbetween the dependents. You just, and so that's not necessarily the right explanation. It just, I can go through reasons why that's probably a good explanation. And it's not really\njust about one of them. So probably it's a pair\nof them or something of these dependents that you get long that drives you to like be\nreally confused in that case. And so what the behavioral\nconsequence there, I mean, this is kind of methods, like how do we get at this? You could try to do experiments to get people to produce these things. They're gonna have a\nhard time producing them. You can try to do experiments\nto get them to understand them and see how well they understand them. Can they understand them? Another method you can do is\ngive people partial materials and ask them to complete them. You know, those center-embedded materials, and they'll fail. (laughs) So I've done that, I've done\nall these kinds of things. - So wait a minute. So central embedding, meaning, like you take a normal\nsentence like boy cried and inject a bunch of crap in the middle that separates the boy and the cried. Okay, that's central embedding. And nesting is on top of that. - No, no, nesting is the same thing. Central embedding, those are\ntotally equivalent terms. I'm sorry, I sometimes use one\nand sometimes use the other. - Oh, got it, got it. Totally equivalent.\n- They don't mean anything different.\n- Got it. And then what you're\nsaying is there's a bunch of different kinds of\nexperiments you can do. I mean, I like the understanding one is like have more embedding,\nmore central embedding. Is it easier or harder to understand? But then you have to measure the level of understanding, I guess. - Yeah, yeah, you could. I mean, there's multiple ways to do that. I mean, there's the simplest\nway is just ask people how good does it sound? How natural is the sound? That's a very blunt,\nbut very good measure. If it's very, very reliable,\npeople will do the same thing. And so it's like, I don't\nknow what it means exactly, but it's doing something\nsuch that we're measuring something about the confusion, the difficulty associated with those. - And those are giving you a signal. That's why you can say that.\n- Yeah, yeah. - What about the completion\nof the central embedding? - So if you give them a partial sentence, say I say the book which the author who, and I ask you to now\nfinish that off for me. I mean, either say it.\n- It breaks people's brains. - Yeah, yeah, but you\ncan just say it's written in front of you, and you can just type and have as much time as you want. They will, even though that\none's not too hard, right? So if I say it's like the book, it's like, oh, the book which the author\nwho I met wrote was good. That's a very simple completion for that. If I give that completion online somewhere to a crowdsourcing platform and\nask people to complete that, they will miss off a verb very regularly, like half the time, maybe 2/3 of the time, they'll just leave off\none of those verb phrases. Even with that simple, so to say the book which the author who, and they'll say was, you need three verbs, right? Who I met wrote was good,\nand they'll give me two. They'll say who was famous was\ngood, or something like that. They'll just give me\ntwo, and that'll happen about 60% of the time, so 40%, maybe 30, they'll do it correctly, correctly, meaning they'll do a three-verb phrase. I don't know what's correct or not. This is hard, it's a hard task. - Yeah, actually, I'm\nstruggling with it in my head. - Well, it's easier-\n- When you stare at it. - If you look, it's a little easier than listening, it's pretty tough, 'cause you have to, 'cause\nthere's no trace of it. You have to remember the\nwords that I'm saying, which is very hard auditorily. We wouldn't do it this way. We do it written, you can\nlook at it and figure it out. It's easier in many\ndimensions in some ways, depending on the person. It's easier to gather written data for, I mean, most, I work in\npsycholinguistics, right? Psychology of language and stuff, and so a lot of our work\nis based on written stuff because it's so easy to gather data from people doing written kinds of tasks. Spoken tasks are just more complicated to administer and analyze, because people do weird\nthings when they speak, and it's harder to analyze what they do, but they generally point to\nthe same kinds of things. - So, okay, so the\nuniversal theory of language by Ted Gibson is that\nyou can form dependency, you can form trees from any sentences, and you can measure the\ndistance in some way of those dependencies, and then you can say that most languages have very short dependencies. - All languages.\n- All languages. - All languages have short dependencies. You can actually measure that, so a next student of mine, this guy's at University\nof California, Irvine, Richard Futrell did a thing\na bunch of years ago now where he looked at all the\nlanguages we could look at, which was about 40 initially, and now I think there's about 60, for which there are dependency structures, so meaning there's gotta\nbe like a big text, bunch of texts, which have been parsed for\ntheir dependency structures, and there's about 60 of those which have been parsed that way, and for all of those, what he did was take any sentence\nin one of those languages, and you can do the dependency structure, and then start at the root, we're talking about dependency structures, that's pretty easy now, and he's trying to figure out what a control way you might say the same sentence is in that language, and so he's just like, all right, there's a root, and it has, let's say as a sentence is, let's go back to two\ndogs entered the room, so entered is the root, and entered has two dependents, it's got dogs, and it has room, and what he does is like, let's scramble that order, that's three things, the root, and the head,\nand the two dependents, in just some random order, just random, and then just do that for all\nthe dependents down the tree, so now look, do it for the, and whatever it was,\ntwo, and dogs, and room, and that's not a, it's\na very short sentence, when sentences get longer, and you have more dependents, there's more scrambling that's possible, and what he found was, so that's one, you can figure out one\nscrambling for that sentence, he did this like 100 times, for every sentence in\nevery one of these texts, every corpus, and then he just compared\nthe dependency lengths in those random scramblings to what actually happened, what the English, or the French, or the German was in\nthe original language, or Chinese, or what all\nthese like 60 languages, and the dependency\nlengths are always shorter in the real language, compared to this kind of a control, and there's another, it's a little more rigid, his control, so the way I described it, you could have crossed dependencies, like by scrambling that way, you could scramble in any way at all, languages don't do that, they tend not to cross\ndependencies very much, like so the dependency structure, they tend to keep things non-crossed, and there's a technical term, they call that projective, but it's just non-crossed\nis all that is projective, and so if you just\nconstrain the scrambling, so that it only gives you projective, sort of non-crossed, the same thing holds, so still human languages are much shorter than this kind of a control, so there's like, what it means is that we're, in every language, we're\ntrying to put things close, relative to this kind of a control, like it doesn't matter\nabout the word order, some of these are verb-final, some of these are verb-media-like English, and some are even verb-initial, there are a few languages of the world which have VSO, word order, verb, subject, object languages, haven't talked about those, it's like 10% of the- - And even in those languages, it's still short dependencies. - Short dependencies is rules. - Okay, so what are some\npossible explanations for that, for why languages have evolved that way? So that's one of the, I suppose, disagreements\nyou might have with Chomsky, so you consider the evolution of language in terms of information theory, and for you, the purpose of language is ease of communication,\nright, and processing. - That's right, that's right, so I mean, the story here\nis just about communication, it is just about production, really, it's about ease of\nproduction, is the story. - When you say production, can you- - Oh, I just mean ease\nof language production, it's easier for me to say things when I'm doing, whenever\nI'm talking to you, so somehow I'm formulating\nsome idea in my head and I'm putting these words together, and it's easier for me to do that, to say something where the words are closely connected in a dependency as opposed to separated, like by putting something in between and over and over again, it's just hard for me\nto keep that in my head, like that's the whole story, like the story is basically, it's like the dependency grammar\nsort of gives that to you, like just like long is bad, short is good, it's like easier to keep in mind because you have to keep it in mind for, probably for production, probably matters in comprehension as well, like also matters in comprehension. - It's on both sides of\nit, the production and the- - But I would guess it's\nprobably evolved for production, like it's about producing, it's what's easier for me to say, that ends up being easier for you also, that's very hard to disentangle, this idea of who is it for, is it for me, the speaker, or is it for you, the listener? I mean, part of my language is for you, like the way I talk to\nyou is gonna be different from how I talk to different people, so I'm definitely angling what I'm saying to who I'm saying, right? It's not like I'm just\ntalking the same way to every single person, and so I am sensitive to my audience, but does that work itself out in the dependency length differences? I don't know, maybe that's\nabout just the words, that part, which words I select. - My initial intuition is\nthat you optimize language for the audience. - [Edward] Yeah, but it's both. - It's just kinda like messing\nwith my head a little bit to say that some of the\noptimization might be, it may be the primary\nobjective of the optimization might be the ease of production. - We have different senses, I guess. I'm very selfish. (laughs) And you're like, I\nthink it's all about me, I'm like, I'm just doing\nwhat's easiest for me. - [Lex] What's easiest for me. - I'm like, I mean, but\nI have to, of course, choose the words that I\nthink you're gonna know. I'm not gonna choose words you don't know. In fact, I'm gonna fix that when I, so there it's about, but\nmaybe for the syntax, for the combinations, it's just about me. I feel like it's, I don't know, though. It's very hard to-\n- Wait, wait, wait, wait, but the purpose of communication\nis to be understood, is to convince others and so on. So like the selfish thing\nis to be understood. - Okay, yeah, it's a little\ncircular there, too, then. Okay.\n- Right. I mean, like the ease of production- - [Edward] Helps me be understood, then. I don't think it's circular. So I want what's-\n- No, I think the primary, I think the primary objective\nis to be understood, is about the listener, 'cause otherwise, if you're optimizing for\nthe ease of production, then you're not gonna have any of the interesting complexity of language. Like you're trying to explain- - Well, let's control for\nwhat it is I wanna say. I'm saying let's control\nfor the thing, the message, control for the message. I wanna tell you-\n- But that means the message needs to be understood. That's the goal. - Oh, but that's the meaning. So I'm still talking about the form. Just the form of the meaning. How do I frame the form of the meaning is all I'm talking about. You're talking about a\nharder thing, I think. It's like how am I, like\ntrying to change the meaning. Let's keep the meaning constant. Like which-\n- Got it. - Yeah, if you keep the meaning constant, how can I phrase whatever\nit is I need to say, like I gotta pick the right words, and I'm gonna pick the\norder so it's easy for me. That's what I think is probably the way. - I think I'm still tying meaning and form together in my head. But you're saying if you keep the meaning of what you're saying constant, would the optimization, yeah, it could be the primary objective that optimization is for production. That's interesting. I'm struggling to keep constant meaning. It's just so, I mean, I'm a human, right? So for me, the form, without\nhaving introspected on this, the form and the meaning are\ntied together, like deeply, because I'm a human. Like for me, when I'm speaking, 'cause I haven't thought about language, like in a rigorous way,\nabout the form of language. - Look, for any event,\nthere's an unbounded, I don't wanna say\ninfinite, but sort of ways that I might communicate that same event. This two dogs entered a room, I can say, in many, many different ways. I can say, hey, there's two dogs. They entered the room. Hey, the room was entered by something. The thing that was entered was two dogs. I mean, it's kind of\nawkward and weird and stuff, but those are all similar\nmessages with different forms, but different ways I might frame. And of course, I use the same\nwords there all the time. I could have referred to\nthe dogs as a Dalmatian and a poodle or something. I could have been more\nspecific or less specific about what they are,\nand I could have said, been more abstract about the number. There's like, so I'm\ntrying to keep the meaning, which is this event, constant, and then how am I gonna describe\nthat to get that to you? It kind of depends on what\nyou need to know, right, and what I think you need to know, but I'm like trying to, let's\ncontrol for all that stuff and not, and it's like,\nI'm just choosing about, I'm doing something\nsimpler than you're doing, which is just forms.\n- Yes. - Just words.\n- So to you, specifying the breed of dog and whether they're cute or\nnot is changing the meaning. - That might be, yeah, yeah,\nthat would be changing, oh, that would be changing\nthe meaning for sure. - Right, so you're just,\nyeah, well, yeah, yeah. - That's changing the meaning, but say, even if we keep that constant, we can still talk about\nwhat's easier or hard for me, right, the listener and the, right? Which phrase structures I use,\nwhich combinations, which. - This is so fascinating, and just like a really powerful window\ninto human language, but I wonder still throughout this how vast the gap between meaning and form. I just have this like\nmaybe romanticized notion that they're close together, that they evolve close, like hand in hand, that you can't just\nsimply optimize for one without the other being\nin the room with us. Like, it's, well, it's\nkind of like an iceberg. Form is the tip of the iceberg, and the rest, the meaning is the iceberg, but you can't like separate. - But I think that's why\nthese large language models are so successful, is\n'cause they're good at form, and form isn't that hard in some sense. And meaning is tough still,\nand that's why they're not, they're, you know, they don't\nunderstand what they're, we're gonna talk about that later maybe, but like we can distinguish in our, forget about large language\nmodels, like humans, maybe you'll talk about that later too, is like the difference between language, which is a communication system, and thinking, which is meaning. So language is a communication\nsystem for the meaning, it's not the meaning. And so that's why, I mean, and there's a lot of interesting evidence we can talk about relevant to that. - Well, I mean, that's a\nreally interesting question. What is the difference between language, written, communicated, versus thought? What do you use the\ndifference between them? - Well, you or anyone\nhas to think of a task which they think is a good thinking task, and there's lots and lots of tasks which should be good thinking tasks. And whatever those tasks are, let's say it's, you know, playing chess, or that's a good thinking task, or playing some game, we're\ndoing some complex puzzles, maybe remembering some\ndigits, that's thinking, remembering some, a lot of\ndifferent tasks we might think, maybe just listening to music is thinking, or there's a lot of different tasks we might think of as thinking. There's this woman in my\ndepartment, Eve Fedorenko, and she's done a lot of\nwork on this question about what's the connection\nbetween language and thought. And so she uses, I was\nreferring earlier to MRI, fMRI, that's her primary method. And so she has been really\nfascinated by this question about whether, what language is, okay? And so, as I mentioned earlier, you can localize my language area, your language area, in\na few minutes, okay? In like 15 minutes, I\ncan listen to language, listen to non-language, or\nbackward speech, or something, and we'll find areas, left\nlateralized network in my head, which is especially, which is\nvery sensitive to language, as opposed to whatever\nthat control was, okay? - Can you specify what\nyou mean by language, like communicated language? Like what is language?\n- Just sentences. You know, I'm listening\nto English of any kind, story, or I can read sentences, anything at all that I\nunderstand, if I understand it, then it'll activate my language network. So right now, my language\nnetwork is going like crazy when I'm talking and when\nI'm listening to you, because we're both, we're communicating. - And that's pretty stable. - Yeah, it's incredibly stable. So I've, I happen to be\nmarried to this woman, Eve Fedorenko, and so\nI've been scanned by her over and over and over since\n2007, or six, or something. And so my language network\nis exactly the same, you know, like a month ago,\nas it was back in 2007. It's amazingly stable, it's astounding. And with it, it's a really\nfundamentally cool thing. And so my language network\nis like my face, okay? It's not changing much\nover time, inside my head. - Can I ask a quick question? Sorry, it's a small tangent. At which point in the, as you\ngrow up from baby to adult, does it stabilize? - We don't know. Like, that's a very hard question. They're working on that right now, because of the problem\nscanning little kids. Like, doing the, trying to do local, trying to do the localization\non little children in this scanner, or you're\nlying in the fMRI scan. That's the best way to figure out where something's going\non inside our brains. And the scanner is loud, and you're in this tiny little\narea, you're claustrophobic. And it doesn't bother me at all. I can go to sleep in there. But some people are bothered by it, and little kids don't really like it, and they don't like to lie still. And you have to be really still, because if you move around, that messes up the coordinates\nof where everything is. And so, you know, try to get, you know, your question is, how and\nwhen are language developing? You know, how does this\nleft-lateralized system come to play? And it's really hard to get a\ntwo-year-old to do this task. But you can maybe, where they're starting\nto get three, and four, and five-year-olds to do\nthis task for short periods. And it looks like it's there pretty early. - So clearly, when you lead up to, like, a baby's first words, before that, there's a lot of\nfascinating turmoil going on about, like, figuring out, like, what are these people saying? And you're trying to, like, make sense, how does that connect to the world, and all that kind of stuff. Yeah, that might be just\nfascinating development that's happening there. That's hard to introspect. - But anyway, we're back to the scanner. And I can find my network in 15 minutes. And now we can ask, find my network, find\nyours, find, you know, 20 other people do this task. And we can do some other tasks. Anything else you think is\nthinking of some other thing. I can do a spatial memory task. I can do a music perception task. I can do a programming\ntask, if I program, okay? I can do, where I can, like, understand computer programs. And none of those tasks tap\nthe language network at all. Like, at all. There's no overlap. They're highly activated in\nother parts of the brain. There's a bilateral network, which I think she tends to call the multiple demands network, which does anything kind of hard, and anything that's kind\nof difficult in some ways will activate that\nmultiple demands network. I mean, music will be in some music area. You know, there's\nmusic-specific kinds of areas. But none of them are activating\nthe language area at all, unless there's words. Like, so if you have\nmusic, and there's a song, and you can hear the words, then you get the language area. - Oh, are we talking about\nspeaking and listening, or are we also talking about reading? - This is all comprehension of any kind. And so-\n- That is fascinating. - So this network doesn't\nmake any difference if it's written or spoken. So the thing that she calls, Fedorenko calls the language network, is this high-level language. So it's not about the spoken language, and it's not about the written language. It's about either one of them. And so when you do speech, you either listen to speech, and you subtract away some\nlanguage you don't understand, or you subtract away backwards speech, which sounds like speech, but it isn't. And then so you take away\nthe sound part altogether. And then if you do written, you get exactly the same network. So for just reading the language versus reading sort of nonsense words or something like that, you'll find exactly the same network. And so this is about high-level- - Comprehension.\n- Comprehension of language, yeah, in this case. And the same thing happens, production's a little\nharder to run the scanner, but the same thing happens in production. You get the same network. So production's a little harder, right? You have to figure out\nhow do you run a task in the network such that you're doing some kind of production. And I can't remember what, they've done a bunch of\ndifferent kinds of tasks there where you get people to-\n- Control the structure. - Produce things, yeah,\nfigure out how to produce. And the same network goes on there. It's actually the same place. - So if you, wait, wait. So if you read random words? - [Edward] Yeah, if you read things like- - Like gibberish. - Yeah, yeah, Lewis\nCarroll's twas brillig, jabberwocky, right? They call that jabberwocky speech. - [Lex] The network doesn't get activated. - [Edward] Not as much. There are words in there. - [Lex] Yeah, 'cause it's still- - There's function words and stuff, so it's lower activation.\n- Fascinating. - Yeah, yeah. So there's like, basically, the more language-like it is, the higher it goes in\nthe language network. And that network is there from when you speak, as\nsoon as you learn language. And it's there, like, you\nspeak multiple languages, the same network is going\nfor your multiple languages. So you speak English, you speak Russian, both of them are hitting\nthat same network, if you're fluent in those languages. - So programming-\n- Not at all. Isn't that amazing?\n- Oh, God. - Even if you're a really good programmer, that is not a human language. It's just not conveying\nthe same information. And so it is not in the language network. And so-\n- That is mind-blowing as I think. That's weird.\n- It's pretty cool. - That's weird.\n- It is amazing. - That's really weird.\n- And so that's like one set of data. This is hers, like, shows\nthat what you might think is thinking is not language. Language is just this\nconventionalized system that we've worked out in human languages. Oh, another fascinating\nlittle bit, tidbit, is that even if there are\nthese constructed languages, like Klingon, or I\ndon't know the languages from \"Game of Thrones,\" I'm sorry, I don't remember those languages, but maybe you do.\n- There's a lot of people offended right now. - There's people that\nspeak those languages. They really speak those languages because the people that\nwrote the languages for the shows, they did an amazing job of constructing something\nlike a human language. And that lights up the language area. That's like, because they can speak pretty much arbitrary\nthoughts in a human language. It's not a, it's a\nconstructed human language, and probably it's related\nto human languages because the people that\nwere constructing them were making them like human\nlanguages in various ways. But it also activates the same network, which is pretty cool. Anyway.\n- Sorry to go into a place where you may be a\nlittle bit philosophical, but is it possible that\nthis area of the brain is doing some kind of translation into a deeper set of almost like concepts? - It has to be doing. So it's doing in communication, right? It is translating from\nthought, whatever that is. It's more abstract, and it's doing that. That's what it's doing. Like it is, that is kind\nof what it is doing. It's like kind of a\nmeaning network, I guess. - Yeah, like a translation network. But I wonder what is at the\ncore, at the bottom of it, like what are thoughts? Are thoughts, to me like-\n- I don't know. - Thoughts and words, are they neighbors, or is it one turtle sitting\non top of the other? Meaning like, is there a\ndeep set of concepts that we- - Well, there's connections, right, between what these things mean, and then there's probably\nother parts of the brain that what these things mean. And so, when I'm talking\nabout whatever it is I wanna talk about, it'll be\nrepresented somewhere else. That knowledge of whatever that is will be represented somewhere else. - Well, I wonder if\nthere's like some stable, nicely compressed encoding of meanings that's separate from language. I guess the implication here is that that we don't think in language. - That's correct. Isn't that cool? And that's so interesting. So, people, I mean, this is\nlike hard to do experiments on, but there is this idea of inner voice, and a lot of people have an inner voice. And so, if you do a poll on the internet and ask if you hear yourself talking when you're just thinking or whatever, about 70 or 80% of people will say yes. Most people have an inner voice. I don't, and so I always\nfind this strange. So, when people talk about an inner voice, I always thought this was a metaphor, and they hear. I know most of you,\nwhoever's listening to this, thinks I'm crazy now 'cause\nI don't have an inner voice, and I just don't know\nwhat you're listening to. It sounds so kind of annoying to me, but to have this voice going\non while you're thinking, but I guess most people have\nthat, and I don't have that, and we don't really know\nwhat that connects to. - I wonder if the inner voice\nactivates that same network. - I don't know. I don't know. I mean, this could be speechy, right? So, that's like, you hear. Do you have an inner voice? - I don't think so.\n- Oh. A lot of people have this sense\nthat they hear themselves, and then say they read someone's email. I've heard people tell me that they hear that other person's voice when they read other people's emails, and I'm like, wow, that sounds so disruptive. - I do think I vocalize when I'm reading, but I don't think I hear a voice. - Well, you probably\ndon't have an inner voice. - [Lex] Yeah, I don't think\nI have an inner voice. - People have an inner voice. People have this strong\npercept of hearing sound in their heads when they're just thinking. - I refuse to believe that's\nthe majority of people. - Majority, absolutely. - What? - It's like 2/3 or 3/4. It's a lot.\n- What? - I never ask class,\nand I went to internet, they always say that. So, you're in a minority. - [Lex] It could be a self-report flaw. - It could be. - You know, when I'm\nreading inside my head, I'm kind of like saying the words, which is probably the wrong way to read, but I don't hear a voice. There's no percept of a voice. I refuse to believe the\nmajority of people have it. Anyway, it's a fascinating,\nthe human brain is fascinating, but it still blew my mind\nthat language does appear, comprehension does appear to\nbe separate from thinking. - So, that's one set. One set of data from Fedorenko's group is that no matter what task you do, if it doesn't have words and\ncombinations of words in it, then it won't light up\nthe language network. It'll be active somewhere\nelse, but not there. So, that's one. And then, this other piece of evidence relevant to that question is, it turns out there are\nthis group of people who've had a massive\nstroke on the left side and wiped out their language network. And as long as they didn't wipe out everything on the right as well, in that case, they wouldn't\nbe cognitively functionable. But if they just wiped out language, which is pretty tough to do because it's very expansive on the left, but if they have, then\nthere is patients like this, so-called global aphasics, who can do any task just\nfine, but not language. You can't talk to them. I mean, they don't understand you. They can't speak, they can't\nwrite, they can't read, but they can play chess,\nthey can drive their cars, they can do all kinds of other stuff. You can do math. So, math is not in the\nlanguage area, for instance. You do arithmetic and stuff,\nthat's not language area. It's got symbols. So, people sort of confuse some kind of symbolic\nprocessing with language, and symbolic processing is not the same. So, there are symbols\nand they have meaning, but it's not language. It's not a conventionalized\nlanguage system. And so, math isn't there. And so, they can do math. They do just as well as their control, age-matched controls and all these tasks. This is Rosemary Varley over\nin University College London, who has a bunch of patients\nwho she's shown this, that they're just... So, that sort of combination suggests that language isn't\nnecessary for thinking. It doesn't mean that you\ncan't think in language. You could think in language, 'cause language allows\na lot of expression, but it's just, you don't\nneed it for thinking. It suggests that language\nis a separate system. - This is kind of blowing\nmy mind right now. - It's cool, isn't it?\n- I'm trying to load that in. - Yeah, yeah.\n- Because it has implications for large language models. - It sure does, and they've\nbeen working on that. - Well, let's take a stroll there. You wrote that the best current\ntheories of human language are arguably large language models. So, this has to do with form. - It's kind of a big theory, but the reason it's arguably the best is that it does the best at predicting what's English, for instance. It's incredibly good, better\nthan any other theory. It's not sort of, there's\nnot enough detail. - Well, it's opaque. Like, you don't know what's going on. It's another black box. But I think it is a theory. - What's your definition of a theory? 'Cause it's a gigantic black box with a very large number of\nparameters controlling it. To me, theory usually\nrequires a simplicity, right? - Well, I don't know. Maybe I'm just being loose there. I think it's not a great\ntheory, but it's a theory. It's a good theory in one sense, in that it covers all the data. Like, anything you wanna\nsay in English, it does, and so that's how it's arguably the best, is that no other theory is as good as a large language model in predicting exactly what's good and\nwhat's bad in English. Now you're saying, is it a good theory? Well, probably not, because I want a smaller theory than that. It's too big. I agree. - You could probably construct a mechanism by which it can generate\na simple explanation of a particular language,\nlike a set of rules. Something like, it could\ngenerate a dependency grammar for a language, right? - [Edward] Yes. - You could probably\njust ask it about itself. - Well, that's, I mean, that presumes, and there's some evidence for this, that some large language\nmodels are implementing something like dependency\ngrammar inside them, and so there's work from\na guy called Chris Manning and colleagues over at\nStanford in natural language, and they looked at, I don't know how many\nlarge language model types, but certainly BERT and some others, where you do some kind of fancy math to figure out exactly\nwhat kind of abstractions of representations are going on, and they were saying, it does look like dependency structure is\nwhat they're constructing. It doesn't, like, so it's\nactually a very, very good map, so they are constructing\nsomething like that. Does it mean that they're\nusing that for meaning? I mean, probably, but we don't know. - You write that the kinds\nof theories of language that LLMs are closest to are called construction-based theories. Can you explain what\nconstruction-based theories are? - It's just a general theory of language such that there's a\nform and a meaning pair for lots of pieces of the language, and so it's primarily usage-based, is the construction grammar. It's just, it's trying\nto deal with the things that people actually say,\nactually say and actually write, and so it's a usage-based idea, and what's a construction? A construction's either a simple word, so like a morpheme plus its meaning, or a combination of words. It's basically combinations\nof words, like the rules, but it's unspecified as to what the form of the grammar is underlyingly, and so I would argue that\nthe dependency grammar is maybe the right form to use for the types of construction grammar. Construction grammar typically\nisn't formalized quite, and so maybe the formalization, a-formalization of that, it\nmight be in dependency grammar. I mean, I would think so,\nbut it's up to people, other researchers in that\narea, if they agree or not. - Do you think that large language models understand language? Are they mimicking language? I guess the deeper question there is, are they just understanding\nthe surface form, or do they understand something\ndeeper about the meaning that then generates the form? - I mean, I would argue\nthey're doing the form. They're doing the form, they're\ndoing it really, really well and are they doing the meaning? No, probably not. I mean, there's lots of these\nexamples from various groups showing that they can be\ntricked in all kinds of ways. They really don't understand\nthe meaning of what's going on, and so there's a lot of examples that he and other groups have given, which show they don't really\nunderstand what's going on. So you know the Monty Hall\nproblem is this silly problem, right, where if you have three door, it's \"Let's Make a Deal,\"\nit's this old game show, and there's three doors, and\nthere's a prize behind one, and there's some junk\nprizes behind the other two, and you're trying to select one, and if you, you know, he knows, Monty, he knows where the target\nitem is, the good thing. He knows everything is back there, and you're supposed to,\nhe gives you a choice. You choose one of the three, and then he opens one of the doors, and it's some junk prize,\nand then the question is, should you trade to get the other one? And the answer is yes, you should trade, because he knew which ones\nyou could turn around, and so now the odds are 2/3, okay? And then if you just\nchange that a little bit to the large language mall, the large language mall\nhas seen that explanation so many times that it just, if you change the story just a little bit, but make it sound like it's\nthe Monty Hall problem, but it's not, you just say,\noh, there's three doors, and one behind them is a good prize, and there's two bad doors. I happen to know it's\nbehind door number one. The good prize, the car,\nis behind door number one, so I'm gonna choose door number one. Monty Hall opens door number three and shows me nothing there. Should I trade for door number two, even though I know the good\nprize is in door number one? And then the large language mall will say, yes, you should trade,\nbecause it just goes through the forms that it's seen\nbefore so many times on these cases where it,\nyes, you should trade, because your odds have\nshifted from one and three now to two out of three to being that thing. It doesn't have any way to remember that actually you have 100% probability behind that door number one. You know that. That's not part of the\nscheme that it's seen hundreds and hundreds of times before, and so you can't, even if\nyou try to explain to it that it's wrong, that they can't do that, it'll just keep giving\nyou back the problem. - But it's also possible\nthe large language model will be aware of the fact\nthat there's sometimes over-representation of a\nparticular kind of formulation, and it's easy to get tricked by that. And so you could see if\nthey get larger and larger, models be a little bit more skeptical. So you see over-representation. So it just feels like form can, training on form can go really far in terms of being able to generate things that look like the\nthing understands deeply the underlying world model of the kind of mathematical\nworld, physical world, psychological world that would generate these kinds of sentences. It just feels like you're creeping close to the meaning part. Easily fooled, all this kind of stuff, but that's humans too. So it just seems really impressive how often it seems like\nit understands concepts. - I mean, you don't have\nto convince me of that. I am very, very impressed. But does, I mean, you're\ngiving a possible world where maybe someone's gonna\ntrain some other versions such that it'll be\nsomehow abstracting away from types of forms. I mean, I don't think that's happened. And so- - Well, no, no, no, I'm not saying that. I think when you just\nlook at anecdotal examples and just showing a large number of them where it doesn't seem to understand and it's easily fooled, that does not seem like a scientific, a data-driven analysis of how many places is damn impressive in terms\nof meaning and understanding and how many places is easily fooled. And like-\n- That's not the inference. So I don't wanna make that, the inference I don't,\nI wouldn't wanna make was that inference. The inference I'm trying\nto push is just that, is it like humans here? It's probably not like humans here. It's different. So humans don't make that error. If you explain that to them, they're not gonna make that error. They don't make that error. And so that's something, it's\ndoing something different from humans that they're\ndoing in that case. - Well, what's the\nmechanism by which humans figure out that it's an error? - I'm just saying the error there is like, if I explain to you there's 100% chance that the car's behind\nthis case, this door, well, do you wanna trade? People say no. But this thing will say\nyes because it's so, that trick, it's so wound up on the form that it's, that's an error\nthat a human doesn't make, which is kind of interesting. - Less likely to make, I should say. - Yeah, less likely.\n- Because like humans are very-\n- Oh yeah. - I mean, you're asking, you know, you're asking humans to, you're asking a system to understand 100%. Like you're asking some\nmathematical concepts. And so like. - Look, the places where\nlarge language models are, the form is amazing. So let's go back to nested structures, center-embedded structures, okay? If you ask a human to complete\nthose, they can't do it. Neither can a large language model. They're just like humans in that. If you ask, if I ask a\nlarge language model- - That's fascinating, by the way. - That's-\n- The central embedding? - Yeah, the center embedding-\n- The central embedding is struggles with-\n- Just like humans, exactly like humans. Exactly the same way as humans. And that's not trained. So they do exactly, so\nthat is a similarity. So but then it's, that's\nnot meaning, right? This is form. But when we get into meaning, this is where they get kind of messed up. Where you start just saying,\noh, what's behind this door? Oh, it's, you know, this\nis the thing I want. Humans don't mess that up as much. You know, here, the\nform is, it's just like. The form matches amazing, similar, without being trained to do that. I mean, it's trained in the sense that it's getting lots of data, which is just like human data. But it's not being trained on, you know, bad sentences and being told what's bad. It just can't do those. It'll actually say things like, those are too hard for me\nto complete, or something, which is kind of interesting, actually. Kind of, how does it know that? I don't know. - But it really often doesn't\njust complete sentences. Very often says stuff that's true. - Mm-hmm. - And sometimes says\nstuff that's not true. And almost always the form is great. - Yeah. - But it's still very surprising that with really great form, it's able to generate a lot\nof things that are true. Based on what it's trained on, and so on. - Yes, yes, yes. - So it's not just form\nthat is generating. It's mimicking true statements. - That's right, that's right. - From the internet. I guess the underlying idea there is that on the internet,\ntruth is over-represented versus falsehoods. - [Edward] Yeah, I think\nthat's probably right, yeah. - But the fundamental\nthing it's trained on, you're saying, is just form. - [Edward] I think so, yeah. Yeah, I think so. - Well, that's a sad, to me that's still a little bit of an open question. I probably lean agreeing with you, especially now you've just blown my mind that there's a separate\nmodule in the brain for language versus thinking. Maybe there's a fundamental part missing from the large language model approach that lacks the thinking,\nthe reasoning capability. - Yeah, that's what this group argues. So the same group, Fedorenko's group, has a recent paper arguing exactly that. There's a guy called Kyle Mahowell who's here in Austin, Texas, actually. He's an old student of mine, but he's a faculty in\nlinguistics at Texas, and he was the first author on that. - That's fascinating. Still, to me, an open question. What do you have the\ninteresting limits of LLMs? - You know, I don't see\nany limits to their form. Their form is perfect.\n- Impressive. - Yeah, yeah, yeah, it's pretty much, I mean, it's close to- - Well, you said ability to\ncomplete central embeddings. - Yeah, it's just the same as humans. It seems the same as humans. - But that's not perfect, right? It should be-\n- That's good. No, but I want it to be like humans. I want a model of humans. - But, oh, wait, wait, wait, wait. Oh, so perfect is as close\nto humans as possible. I got it.\n- Yeah, yeah. - But you should be able\nto, if you're not human, you're superhuman, you should be able to complete central\nembedded sentences, right? - [Lex] I mean, that's the mechanism. If it's modeling something, I think it's kind of really\ninteresting that it can't- - That it's really interesting. - That it's more like,\nI think it's potentially underlyingly modeling something like the way the form is processed. - The form of human language. - The way that-\n- And how humans process the language.\n- Yes, yes. I think that's plausible. - And how they generate language. Processed language and generated language, that's fascinating.\n- Yeah. - So in that sense, they're perfect. If we can just linger on\nthe center embedding thing that's hard for LLMs to produce, and that seems really impressive 'cause that's hard for humans to produce, and how does that connect to the thing we've been talking about before, which is the dependency grammar framework in which you view language and the finding that short dependencies seem to be a universal part of language? So why is it hard to\ncomplete center embeddings? - So what I like about dependency grammar is it makes the cognitive cost associated with longer distance\nconnections very transparent. Basically, there's some, it\nturns out there is a cost associated with producing\nand comprehending connections between words which are\njust not beside each other. The further apart they\nare, the worse it is, according to, well, we can measure that. And there is a cost associated with that. - Can you just linger on what do you mean by cognitive cost?\n- Sure. - And how do you measure it? - Oh, well, you can measure\nit in a lot of ways. The simplest is just asking\npeople to say whether, how good a sentence sounds, which is ask. That's one way to measure, and you can try to triangulate\nthen across sentences and across structures to try to figure out what the source of that is. You can look at reading times\nin controlled materials, in certain kinds of materials, and then we can measure the\ndependency distances there. There's a recent study which looked at, we're talking about the brain here. We could look at the\nlanguage network, okay? We could look at the language network and we could look at the activation in the language network and\nhow big the activation is depending on the length\nof the dependencies. And it turns out in just random sentences that you're listening to,\nif you're listening to, so it turns out there are people\nlistening to stories here, and the bigger, the\nlonger the dependency is, the stronger the activation\nin the language network. And so there's some measure. There's a bunch of different\nmeasures we could do. That's kind of a neat measure, actually, of actual-\n- Activations. - Activation in the brain. - You can somehow, in different ways, convert it to a number. I wonder if there's a beautiful equation connecting cognitive cost\nand length of dependency. E equals MC squared kind of thing. - Yeah, it's complicated,\nbut probably it's doable. I would guess it's doable. I tried to do that a while ago, and I was reasonably successful, but for some reason I\nstopped working on that. I agree with you that it\nwould be nice to figure out. So there's some way to\nfigure out the cost. I mean, it's complicated. Another issue you raised before was like how do you measure distance? Is it words? It probably isn't part of the problem. Is that some words\nmatter more than others, and probably meaning\nlike nouns might matter, and then it maybe depends\non which kind of noun. Is it a noun we've already introduced or a noun that's already been mentioned? Is it a pronoun versus a name? Like all these things probably matter. So probably the simplest thing to do is just like, oh, let's\nforget about all that and just think about words or morphemes. - For sure, but there\nmight be some insight in the kind of function\nthat fits the data, meaning like a quadratic, like what- - I think it's an exponential. So we think it's probably an exponential such that the longer the\ndistance, the less it matters. And so then it's the sum of those. That was our best guess a while ago. So you've got a bunch of dependencies. If you've got a bunch of them that are being connected at some point, at the ends of those, the cost\nis some exponential function of those is my guess. But because the reason it's\nprobably an exponential is like it's not just the\ndistance between two words, 'cause I can make a very, very\nlong subject verb dependency by adding lots and lots of noun phrases and prepositional phrases, and\nit doesn't matter too much. It's when you do nested, when\nI have multiple of these, then things go really bad, go south. - Probably somehow\nconnected to working memory - That's probably a\nfunction of the memory here is the access, is trying to\nfind those earlier things. It's kind of hard to figure out what was referred to earlier. Those are those connections. That's the sort of notion of merking, as opposed to a storagy\nthing, but trying to connect, retrieve those earlier words, depending on what was in between. And then we're talking about interference of similar things in between. The right theory probably has\nthat kind of notion in it, is an interference of similar. And so I'm dealing with an abstraction over the right theory, which is just, let's count words, it's\nnot right, but it's close. And then maybe you're right, though, there's some sort of an\nexponential or something on the, to figure out the total, so we can figure out a\nfunction for any given sentence in any given language. But it's funny, people\nhaven't done that too much, which I do think is, I'm interested that you find that interesting. I really find that interesting, and a lot of people haven't\nfound it interesting, and I don't know why I haven't got people to wanna work on that,\nI really like that too. - No, that's a beautified, and the underlying idea is beautiful, that there's a cognitive cost that correlates with the\nlength of dependency. It just, it feels like, I mean, language is so fundamental\nto the human experience, and this is a nice,\nclean theory of language, where it's like, wow, okay, so we like our words close together, dependent words close together. - Yeah, that's why I like it too. It's so simple. - Yeah, the simplicity of the theory. - It's so simple, and yet it explains some very complicated phenomena. If I write these very\ncomplicated sentences, it's kind of hard to\nknow why they're so hard, and you can like, oh, nail it down. I can give you a math formula for why each one of\nthem is bad, and where, and that's kind of cool. I think that's very neat. - Have you gone through the process? Is there like, if you\ntake a piece of text, and then simplify, sort of like, there's an average length of dependency, and then you like, you know, reduce it, and see comprehension on the entire, not just single sentence,\nbut like, you know, you go from James Joyce to\nHemingway, or something. - No, no, the simple answer is no, that does, there's\nprobably things you can do in that kind of direction. - [Lex] That's fun. - We might, you know, we're gonna talk about legalese at some point, and so maybe we'll talk\nabout that kind of thinking with applied to legalese. - Well, let's talk about legalese, 'cause you mentioned that as an exception. We're just taking it tangent upon tangent. That's an interesting one. You give it as an exception. - It's an exception. - That you say that\nmost natural languages, as we've been talking about,\nhave local dependencies, with one exception, legalese. - That's right. - So what is legalese, first of all? - Oh, well, legalese is\nwhat you think it is. It's just any legal language. - Well, I mean, like, I\nactually know very little about the kind of\nlanguage that lawyers use. - So I'm just talking\nabout language in laws, and language in contracts. - Got it. - So the stuff that you have to run into, we have to run into every\nother day, or every day, and you skip over,\nbecause it reads poorly. And, or, you know, partly\nit's just long, right? There's a lot of text there that we don't really wanna know about. But the thing I'm interested in, so I've been working with\nthis guy called Eric Martinez, who is a, he was a lawyer,\nwho was taking my class. I was teaching a\npsycholinguistics lab class, I have been teaching it\nfor a long time at MIT, and he's a, he was a\nlaw student at Harvard. And he took the class, 'cause he had done some\nlinguistics as an undergrad, and he was interested in the problem of why legalese sounds hard to understand. You know, why, and so why\nis it hard to understand, and why do they write that way, if it is so hard to understand? It seems apparent that\nit's hard to understand. The question is, why is it? And so we didn't know, and we did an evaluation\nof a bunch of contracts, actually we just took a bunch\nof sort of random contracts, 'cause I don't know, you know, there's, contracts and laws\nmight not be exactly the same, but contracts are kind of the things that most people have to\ndeal with most of the time. And so that's kind of\nthe most common thing that humans have, like, that adults in our industrialized society have to deal with a lot. And so that's what we pulled, and we didn't know what\nwas hard about them, but it turns out that\nthe way they're written is very center-embedded, it has nested structures in them. So it has low-frequency words as well, that's not surprising, lots of texts have low, it does have, surprising, slightly lower-frequency words than other kinds of control texts, even sort of academic texts, legalese is even worse, it is the worst that we were able to find. - You just revealed a game\nthat lawyers are playing. - They're not, though.\n- They're optimizing a different, well- - You know, it's interesting, that's a, now you're getting at why, and so, and I don't think, so now you're saying it's,\nthey're doing it intentionally, I don't think they're\ndoing it intentionally. But let's get-\n- It's an emergent phenomenon, okay, all right.\n- Yeah, yeah, yeah, we'll get to that, we'll get to that. And so, but we wanted to see why, so we see what first, as opposed, so like, 'cause it turns\nout that we're not the first to observe that legalese is weird. Like, back to, Nixon had a\nPlain Language Act in 1970, and Obama had one, and\nboy, a lot of these, you know, a lot of presidents have said, oh, we've gotta simplify legal\nlanguage, must simplify it. But if you don't know\nhow it's complicated, it's not easy to simplify it. You need to know what it\nis you're supposed to do before you can fix it, right? And so you need to, like,\nyou need a psycholinguist to analyze the text and\nsee what's wrong with it before you can, like, fix it. You don't know how to fix it. How am I supposed to fix something? I don't know what's wrong with it. And so what we did was\njust, that's what we did. We figured out, let's, okay, we just took a bunch of\ncontracts, had people, and we encoded them for\nthe, so a bunch of features. And so another feature that people, one of them was center embedding. And so that is, like,\nbasically how often a clause would intervene between a\nsubject and a verb, for example. That's one kind of a center\nembedding of a clause, okay? And it turns out they're\nmassively center embedded. Like, so I think in random\ncontracts and in random laws, I think you get about 70%\nor 80, something like 70% of sentences have a center\nembedded clause in them, which is insanely high. If you go to any other text,\nit's down to 20% or something. It's so much higher than any\ncontrol you can think of, including, you think, oh, people think, oh, technical academic text. No, people don't write\ncenter embedded sentences in technical academic text. I mean, they do a little bit, but much, it's on the 20%, 30%\nrealm, as opposed to 70. And so there's that, and\nthere's low-frequency words. And then people, oh, maybe it's passive. People don't like the passive. Passive, for some reason,\nthe passive voice in English has a bad rap, and I'm not really sure where that comes from. And there is a lot of passive in the, there's much more\npassive voice in legalese than there is in other texts. - And the passive voice accounts for some of the low-frequency words. - No, no, no, no, no, those are separate. Those are separate. - Oh, so passive voice sucks,\nlow-frequency word sucks. - Well, sucks is different. - That's a judgment on passive. - Yeah, yeah, yeah, drop the judgment. It's just like, these are frequent. These are things which\nhappen in legalese text. Then we can ask. The dependent measure is\nhow well you understand those things with those features. Okay, and so then, and it turns out the passive makes no difference. So it has zero effect on\nyour comprehension ability, on your recall ability, nothing at all. It has no effect. The words matter a little bit. They do, low-frequency\nwords are gonna hurt you in recall and understanding. But what really hurts is\nthe central embedding. That kills you. That is like, that slows people down. That makes them very\npoor at understanding. That makes them, they\ncan't recall what was said as well, nearly as well. And we did this not only on laypeople. We did it on a lot of laypeople. We ran it on 100 lawyers. We recruited lawyers from a wide range of sort of different levels\nof law firms and stuff. And they have the same pattern. So they also, like when they did this, I did not know what happened. I thought maybe they could process. They're used to legalese. They can process it just as\nwell as if it was normal. No, no, they're much\nbetter than laypeople. So they're much, they\ncan much better recall, much better understanding, but they have the same main effects as laypeople, exactly the same. So they also much prefer the non-center. So we constructed\nnon-center embedded versions of each of these. We constructed versions which\nhave higher frequency words in those places. And we did, we un-passivized. We turned them into active versions. The passive active made no difference. The words made a little difference. And the un-center embedding\nmakes big differences in all the populations. - Un-center embedding. How hard is that process, by the way? - It's not very hard.\n- Don't question. But how hard is it to\ndetect center embedding? - Oh, easy, easy to detect. That's just easy to parse.\n- You're just looking at long dependencies? Or is there a real-\n- Yeah, yeah. You can just, you can, so there's automatic parsers for English, which are pretty good. Very-\n- And they can detect center embedding?\n- Oh, yeah. Very-\n- Or, I guess, nesting. - Perfectly. Yeah, pretty much. - So you're not just looking\nfor long dependencies. You're just literally\nlooking for center embedding. - Yeah, we are in this\ncase, in these cases. But long dependencies,\nthey're highly correlated, these kinds of things.\n- Highly. So like, center embedding is a big bomb you throw inside of a sentence that just blows up the, that makes super- - Can I read a sentence\nfor you from these things? I see, I can find, I mean, this is just like one of the things that, this is just typical.\n- My eyes might glaze over in mid-sentence. No, I understand that. I mean, legalese is hard.\n- So here we go. This is a good one. It goes, in the event that any payment or benefit by the company, all such payments and benefits, including the payments and benefits under section 3A hereof, being here and after referred\nto as a total payment, would be subject to the excise tax, then the cash severance\npayments shall be reduced. So that's something we\npulled from a regular text, from a contract.\n- Wow. - And the center embedded\nbit there is just, for some reason, there's a definition. They throw the definition of\nwhat payments and benefits are in between the subject and the verb. Let's, how about don't do that? How about put the\ndefinition somewhere else as opposed to in the\nmiddle of the sentence? And so that's very,\nvery common, by the way. That's what happens. You just throw your definitions. You use a word, a couple words, and then you define it, and then you continue the sentence. Like, just don't write like that. And you ask, so then we asked lawyers. We thought, oh, maybe lawyers like this. Lawyers don't like this. (laughs) They don't like this. They don't wanna write like this. We asked them to rate materials which are with the same meaning, with un-center-embedded\nand center-embedded, and they much preferred the\nun-center-embedded versions. - On the comprehension,\non the reading side. - Yeah, and we asked them, would you hire someone who\nwrites like this or this? We asked them all kinds of questions, and they always preferred\nthe less complicated version, all of them. So I don't even think\nthey want it this way. - Yeah, but how did it happen? - How did it happen? That's a very good question. And the answer is, I still don't know. But-\n- I have some theories. - Well, our best theory at the moment is that there's actually some\nkind of a performative meaning in the center-embedding in the style which tells you it's legalese. We think that that's the kind of a style which tells you it's legalese. Like, that's a reasonable guess. And maybe it's just, so for instance, if you're, like, it's like a magic spell. So we kinda call this the\nmagic spell hypothesis. So when you tell someone to\nput a magic spell on someone, what do you do? People know what a magic spell is, and they do a lot of rhyming. That's kinda what people will tend to do. They'll do rhyming, and they'll do some kind of poetry kind of thing. - Abracadabra type of thing.\n- Yeah, yeah. And maybe there's a\nsyntactic sort of reflex here of a magic spell, which\nis center-embedding. And so that's like, oh,\nit's trying to tell you this is something which is true, which is what the goal of law is, right? It's telling you something\nthat we want you to believe as certainly true, right? That's what legal contracts are trying to, enforce on you, right? And so maybe that's like a form, which has, this is like an abstract, very abstract form, center-embedding, which has a meaning associated with it. - Well, don't you think\nthere's an incentive for lawyers to generate things\nthat are hard to understand? - That was one of our working hypotheses. We just couldn't find\nany evidence of that. - No, lawyers also don't understand it. - Well, we asked lawyers.\n- You're creating space, why you, I mean, you ask in\na communist Soviet Union, the individual members, their self-report is not going to correctly\nreflect what is broken about the gigantic bureaucracy\nthat leads to Chernobyl or something like this. I think the incentives\nunder which you operate are not always transparent to the members within that system. So it just feels like\na strange coincidence that there is benefit\nif you just zoom out, look at the system, as opposed\nto asking individual lawyers that making something hard to understand is going to make a lot of people money. You're gonna need a\nlawyer to figure that out, I guess, from the perspective\nof the individual, but then that could be\nthe performative aspect. It could be as opposed\nto the incentive-driven to be complicated, it\ncould be performative to where we lawyers speak\nin this sophisticated way and you regular humans\ndon't understand it, so you need to hire a lawyer. Yeah, I don't know which one\nit is, but it's suspicious. Suspicious that it's hard to understand and that everybody's eyes\nglaze over and they don't read. - I'm suspicious as well,\nI'm still suspicious. And I hear what you're\nsaying, it could be kind of, no individual, and even\naverage of individuals, it could just be a few bad apples in a way which are driving the effect in some way. - Influential bad apples\nthat everybody looks up to, whatever, they're like\ncentral figures in how- - But it is kind of interesting\nthat among our 100 lawyers, they did not share that.\n- They didn't want this. That's fascinating.\n- They really didn't like it. And so it gave us hope.\n- And they weren't better than regular people at comprehending it. Or they were, on average, better, but like-\n- But they had the same difference.\n- The same difference. - Exact same difference. But they wanted it fixed. So they also, and so\nthat gave us hope that, because it actually isn't very\nhard to construct a material which is un-center-embedded\nand has the same meaning, it's not very hard to do. You just basically, in that situation, you're just putting definitions outside of the subject-verb relation\nin that particular example. And that's kind of, that's pretty general, what they're doing is just\nthrowing stuff in there which you didn't have to put in there. There's extra words involved, typically. You may need a few extra\nwords sort of to refer to the things that you're\ndefining outside in some way, 'cause if you only use\nit in that one sentence, then there's no reason\nto introduce extra terms. So we might have a few more words, but it'll be easier to understand. So I mean, I have hope that now that maybe we can make legalese less\nconvoluted in this way. - So maybe the next president\nof the United States can, instead of saying\ngeneric things, say- - Say exactly what-\n- I ban center embeddings and make\nTed the language czar of the- - Or he can make Eric. Martinez is the guy you\nshould really put in there. - Eric Martinez, yeah, yeah, yeah. (both laughing) But center embeddings are\nthe bad thing to have. - That's right, yeah.\n- So if you get rid of that- - That'll do a lot of\nit, that'd fix a lot. - That's fascinating.\n- Yeah. - That is so fascinating.\n- Yeah. - And it's just really\nfascinating on many fronts that humans are just not able to deal with this kind of thing. And that language, because of that, evolved in the way you\ndid, it's fascinating. So one of the mathematical\nformulations you have when talking about\nlanguages of communication is this idea of noisy channels. What's a noisy channel? - Well, so that's about communication. And so this is going back to Shannon. So Shannon, Claude Shannon was\na student at MIT in the '40s. And so he wrote this very\ninfluential piece of work about communication theory\nor information theory. And he was interested in\nhuman language, actually. He was interested in this\nproblem of communication, of getting a message from\nmy head to your head. And so he was concerned or interested in what was a robust way to do that. And so assuming we both\nspeak the same language, we both already speak English,\nwhatever the language is, we speak that. What is a way that I can say the language so that it's most likely to get the signal that I want to you? And so, and then the problem\nthere in the communication is the noisy channel, is that there's, I make, there's a lot\nof noise in the system. I don't speak perfectly. I make errors. That's noise. There's background noise. You know that, as we-\n- Like a literal- - Literal background noise. There is like white\nnoise in the background or some other kind of noise. There's some speaking\ngoing on that you're, or just, you're at a party. That's background noise. You're trying to hear someone. It's hard to understand them because there's all this\nother stuff going on in the background. And then there's noise\non the communication, on the receiver side, so that you have some problem\nmaybe understanding me for stuff that's just\ninternal to you in some way. So you've got some other\nproblems, whatever, with understanding for whatever reasons. Maybe you're, maybe you've\nhad too much to drink. You know, who knows why you're not able to pay attention to the signal. So that's the noisy channel. And so that language, if\nit's a communication system, we are trying to optimize, in some sense, the passing of the message\nfrom one side to the other. And so it, I mean, one idea is that maybe, you know, aspects of like\nword order, for example, might have optimized in some way to make language a little more easy to be passed from speaker to listener. And so Shannon's the\nguy that did this stuff way back in the '40s. You know, it's very\ninteresting, historically, he was interested in\nworking in linguistics. He was at MIT, and he did, this was his master's\nthesis, of all things. You know, it's crazy how much he did for his master's thesis in 1948, I think, or '49, something. And he wanted to keep working in language. And it just wasn't a popular communication as a reason, a source\nfor what language was wasn't popular at the time. So Chomsky was becoming,\nit was moving in there. He was, and he just wasn't\nable to get a handle there, I think. And so he moved to Bell Haps\nand worked on communication from a mathematical point of view, and did all kinds of amazing work. And so he's just-\n- More on the signal side versus the language side.\n- Yeah, mm-hmm. - Ah, yeah, it would've\nbeen interesting to see if he pursued the language side. - Yeah.\n- That's really interesting. - Yeah, he was interested in that. His examples in the '40s are kinda like, they're very language-like things. - Yeah. - We can kinda show that\nthere's a noisy channel process going on in when you're listening to me, you can often sort of guess what I meant by what you think I\nmeant given what I said. And I mean, with respect to sort of why language looks the way it does, we might, there might be sort of, as I alluded to, there\nmight be ways in which word order is somewhat optimized because of the noisy channel in some way. - I mean, that's really\ncool to sort of model if you don't hear certain\nparts of a sentence or have some probability\nof missing that part, like how do you construct a language that's resilient to that,\nthat's somewhat robust to that? - [Edward] Yeah, that's the idea. - And then you're kinda\nsaying like the word order and the syntax of a language,\nthe dependency length are all helpful to deal with. - Yeah, well, dependency\nlength is really about memory. I think that's like about sort\nof what's easier or harder to produce in some way. And these other ideas are about sort of robustness to communication, so the problem of potential\nloss of signal due to noise. It's so that there may\nbe aspects of word order, which is somewhat optimized for that. And we have this one\nguess in that direction. These are kind of just so stories, I have to be pretty frank. They're not like, I\ncan't show this is true. All we can do is like look at the current languages of the world. This is like, we can't sort\nof see how languages change or anything because\nwe've got these snapshots of a few hundred or a\nfew thousand languages. We don't really, we\ncan't do the right kinds of modifications to test\nthese things experimentally. And so just take this with\na grain of salt, okay, from here, this stuff. The dependency stuff I can,\nI'm much more solid on. I'm like, here's what the lengths are and here's what's hard,\nhere's what's easy, and this is a reasonable structure. I think I'm pretty reasonable. Here's like why, why does the word order look the way it does? We're now into shaky territory,\nbut it's kind of cool. - But we're talking\nabout, just to be clear, we're talking about maybe just actually the sounds of communication. Like you and I are sitting\nin a bar, it's very loud, and you model with a noisy\nchannel the loudness, the noise, and we have the\nsignal that's coming across. And you're saying word\norder might have something to do with optimizing that,\nwhere there's presence of noise. - [Edward] Yes, yes, yes. - I mean, it's really interesting. I mean, to me, it's interesting\nhow much you can load into the noisy channel. Like how much can you bake in? Well, you said like, you know, cognitive load on the receiver end. - We think that those are, there's three, at least three different kinds\nof things going on there. And we probably don't wanna\ntreat them all as the same. And so I think that the right model, a better model of a noisy\nchannel would treat, would have three different\nsources of noise, which are background noise,\nspeaker inherent noise, and listener inherent noise. And those are not, those\nare all different things. - Sure, but then underneath it, there's a million other\nsubsets of like, what. - [Edward] That's true. - On the receiving end, I mean, I just mentioned cognitive\nload on both sides. Then there's like speech\nimpediments or just everything. Worldview, I mean, the meaning, we start to creep into\nthe meaning realm of like, we have different worldviews. - Well, how about just form still though? Like just what language you know. Like, so how well you know the language. And so if it's second language\nfor you versus first language and how maybe what other\nlanguages you know, these are still just form stuff. And that's like potentially\nvery informative. And you know, how old you are, these things probably matter, right? So like a child learning\na language is, you know, as a noisy representation\nof English grammar, you know, depending on how old they are. So maybe when they're six,\nthey're perfectly formed, but. - You mentioned one of the things is like a way to measure a\nlanguage is learning problems. So like, what's the correlation between everything\nwe've been talking about and how easy it is to learn a language? So is like short dependencies correlated to ability to learn a language? Is there some kind of, or\nlike the dependency grammar, is there some kind of connection there? How easy it is to learn? - Yeah, well, all the languages\nin the world's language, none is, right now, we know\nis any better than any other with respect to sort of optimizing dependency links, for example. They're all kind of do it well. They all keep low. So I think of every human language as some kind of sort of\nan optimization problem. A complex optimization problem to this communication problem. And so they've like, they've solved it. They're just sort of noisy solutions to this problem of communication. There's just so many ways you can do this. - So they're not optimized for learning. They're probably optimized\nfor communication. - And learning. So yes, one of the factors which is. Yeah, so learning is\nmessing this up a bit. And so, for example, if it were just about\nminimizing dependency links, and that was all that matters, then we might find grammars which didn't have\nregularity in their rules. But languages always have\nregularity in their rules. So what I mean by that is that if I wanted to say something to you in the optimal way to say it was, what really mattered to me, all that mattered was\nkeeping the dependencies as close together as possible, then I would have a very lax set of free structure or dependency rules. I wouldn't have very many of those. I would have very little of that. And I would just put the words as close, the things that refer to the\nthings that are connected right beside each other. But we don't do that. There are word order rules, right? So they're very, and depending on the language, they're more and less strict, right? So you speak Russian, they're less strict than English. English has very rigid word order rules. We order things in a very particular way. And so why do we do that? That's probably not about communication. That's probably about learning. I mean, then we're talking about learning. It's probably easier to\nlearn regular things, things which are very\npredictable and easy to, so that's probably about\nlearning, is our guess, 'cause that can't be about communication. - Can it be just noise? Can it be just the messiness of the development of a language? - Well, if it were just a communication, then we should have languages which have very, very free word order, and we don't have that. We have free-er, but not free, like there's always- - Well, no, but what I mean by noise is like cultural, like\nsticky cultural things, like the way you communicate, just there's a stickiness to it, that it's an imperfect, it's a noisy, it's stochastic, the function over which you're optimizing is very noisy. Because I don't, it feels weird to say that learning is part of\nthe objective function, 'cause some languages are way harder to learn than others, right? Or is that, that's not true? That's interesting. I mean, that's the public\nsort of perception, right? - Yes, that's true. - For a second language. - For a second language. - But that depends on what\nyou started with, right? So it really depends on how close that second language is to\nthe first language you've got. And so, yes, it's very, very hard to learn Arabic if you've\nstarted with English, or it's hard to learn Japanese, or if you've started with, Chinese, I think, is the worst. There's like Defense Language Institute in the United States has like a list of how hard it is to learn\nwhat language from English, and I think Chinese is the worst. - But that's just a second language. You're saying babies don't care. - No, no, there's no evidence that there's anything harder or easier about any baby, any language learned. Like by three or four,\nthey speak that language. And so there's no evidence\nof anything harder or easier about any human language. They're all kind of equal. - To what degree is language, this is returning to Chomsky\na little bit, is innate? You said that for Chomsky, he used the idea that languages, some aspects of language are innate to explain away certain\nthings that are observed. How much are we born with language at the core of our mind, brain? - I mean, the answer is\nI don't know, of course, but the, I mean, I like to, I'm an engineer at heart, I guess, and I sort of think it's fine to postulate that a lot of it's learned, and so I'm guessing that\na lot of it's learned. So I think the reason Chomsky\nwent with the innateness is because he hypothesized\nmovement in his grammar. He was interested in grammar, and movement's hard to learn. I think he's right. Movement is a hard, it's\na hard thing to learn, to learn these two things\ntogether and how they interact, and there's a lot of ways\nin which you might generate exactly the same sentences,\nand it's really hard. And so he's like, oh,\nI guess it's learned. Sorry, I guess it's not\nlearned, it's innate. And if you just throw out the movement and just think about\nthat in a different way, you know, then you get some messiness, but the messiness is human language, which it actually fits better. That messiness isn't a problem. It's actually, it's a\nvaluable asset of the theory. And so I think I don't really see a reason to postulate much innate structure, and that's kind of why I think\nthese large language models are learning so well, is because I think you can learn the form, the forms of human\nlanguage from the input. I think that's like,\nit's likely to be true. - So that part of the brain that lights up when you're doing all the comprehension, that could be learned. That could be just, you don't\nneed, you don't need any. - Yeah, it doesn't have to be innate. So like lots of stuff\nis modular in the brain that's learned. It doesn't have to, you know,\nso there's something called the visual word form area in the back, and so it's in the back of your head, near the visual cortex, okay? And that is very specialized language, sorry, very specialized brain area, which does visual word\nprocessing if you read, if you're a reader, okay? If you don't read, you\ndon't have it, okay? Guess what? You spend some time learning to read, and you develop that brain\narea, which does exactly that. And so the modularization is\nnot evidence for innateness. So the modularization of a language area doesn't mean we're born with it. We could have easily learned that. We might've been born with it. We just don't know at this point. We might very well have been born with this left-lateralized area. I mean, there's like a lot of\nother interesting components here, features of this kind of argument. So some people get a stroke, or something goes really\nwrong on the left side, where the language area would be, and that isn't there. It's not available, and it\ndevelops just fine on the right. So it's not about the left. It goes to the left, like\nthis is a very interesting question, is why are\nany of the brain areas the way that they are, and how\ndid they come to be that way? And there's these natural\nexperiments which happen, where people get these\nstrange events in their brains at very young ages, which wipe\nout sections of their brain, and they behave totally normally, and no one knows anything was wrong. And we find out later, 'cause they happen to be accidentally\nscanned for some reason, it's like, what happened\nto your left hemisphere? It's missing. There's not many people who've missed their whole left hemisphere,\nbut they'll be missing some other section of\ntheir left or their right. And they behave absolutely normally. We'd never know. So that's a very interesting\ncurrent research. You know, this is another project that this person, Eve\nFedorenko, is working on. She's got all these people contacting her, because she's scanned some people who have been missing sections. One person missed a section of her brain and was scanned in her\nlab, and she happened to be a writer for the New York Times, and there was an article\nin the New York Times about the, just about\nthe scanning procedure and about what might be learned about by sort of the general process of MRI and language, and that's her language. And because she's writing\nfor the New York Times, then all these people\nstarted writing to her, who also have similar kinds of deficits, because they've been accidentally scanned for some reason and found out\nthey're missing some section. And they say they volunteer to be scanned. - So these are natural experiments. - Natural experiments,\nthey're kind of messy, but natural experiments, kind of cool. She calls them interesting brains. - The first few hours, days, months of human life are fascinating, 'cause like, well, inside\nthe womb, actually, like that development, that machinery, whatever that is, seems\nto create powerful humans that are able to speak, comprehend, think, all that kind of stuff,\nno matter what happens, not no matter what, but\nrobust to the different ways that the brain might be damaged and so on. That's really interesting. But what would Chomsky say about the fact, the thing you're saying now, that language seems to be happening\nseparate from thought? Because as far as I understand,\nmaybe you can correct me, he thought that language underpins- - Yeah, he thinks so, I\ndon't know what he'd say. - He would be surprised, 'cause for him, the idea is that language is sort of the foundation of thought. - That's right, absolutely. - And it's pretty mind-blowing to think that it could be completely\nseparate from thought. - That's right, but so he's\nbasically a philosopher, philosopher of language, in a way, thinking about these\nthings, it's a fine thought. You can't test it in his methods. You can't do a thought\nexperiment to figure that out. You need a scanner, you\nneed brain-damaged people, you need something, you\nneed ways to measure that, and that's what fMRI\noffers, and patients are a little messier, fMRI is\npretty unambiguous, I'd say. It's very unambiguous,\nthere's no way to say that the language network\nis doing any of these tasks. You should look at those data,\nit's like there's no chance that you can say that those\nnetworks are overlapping. They're not overlapping, they're\njust completely different. And so, you can always make,\noh, it's only two people, it's four people, or\nsomething for the patients, and there's something special\nabout them, we don't know, but these are just random\npeople, and with lots of them, and you find always the same effects, and it's very robust, I'd say. - Well, it's a fascinating effect. You mentioned Bolivia. What's the connection\nbetween culture and language? You've also mentioned that\nmuch of our study of language comes from W-E-I-R-D, WEIRD people, Western-educated, industrialized,\nrich, and democratic. So, when you study remote cultures, such as around the Amazon jungle, what can you learn about language? - So, that term WEIRD is from Joe Henrich. He's at Harvard, he's a\nHarvard evolutionary biologist. And so, he works on lots\nof different topics, and he basically was\npushing that observation that we should be careful\nabout the inferences we wanna make when we're\ntalking in psychology, or most in psychology,\nI guess, about humans, if we're talking about\nundergrads at MIT and Harvard. Those aren't the same, right? These aren't the same things. And so, if you wanna make\ninferences about language, for instance, there's a lot\nof other kinds of languages in the world than English\nand French and Chinese. And so, maybe, for language,\nwe care about how culture, 'cause cultures can be\nvery, I mean, of course, English and Chinese\ncultures are very different, but hunter-gatherers are much\nmore different in some ways. And so, if culture has an\neffect on what language is, then we kind of wanna look\nthere as well as looking. It's not like the industrialized cultures aren't interesting. Of course, they are, but we want to look at non-industrialized cultures as well. And so, I've worked with two. I've worked with the Chimane,\nwhich are in Bolivia, and Amazon, both in the\nAmazon, in these cases. And there are so-called farmer-foragers, which is not hunter-gatherers. It's sort of one-up from hunter-gatherers in that they do a little\nbit of farming as well, a lot of hunting as well, but a little bit of farming. And the kind of farming they\ndo is the kind of farming that I might do if I ever\nwere to grow tomatoes or something in my backyard. So, it's not like big field farming. It's just farming for a family,\na few things you do that. So, that's the kind of farming they do. And the other group I've\nworked with are the Piraha, which are also in the Amazon\nand happen to be in Brazil. And that's with a guy called Dan Everett, who is a linguist, anthropologist, who actually lived and worked in the, I mean, he was a missionary,\nactually, initially, back in the '70s, trying\nto translate languages so they could teach them the Bible, teach them Christianity. - What can you say about that? - Yeah, so, the two\ngroups I've worked with, the Chimane and the Piraha,\nare both isolate languages, meaning there's no known\nconnected languages at all. They're just on their own. There's a lot of those. And most of the isolates\noccur in the Amazon or in Papua New Guinea, in these places where the world has sort of\nstayed still for long enough. And so, there aren't earthquakes. There aren't, well,\ncertainly no earthquakes in the Amazon jungle. And the climate isn't bad,\nso you don't have droughts. And so, in Africa, you've\ngot a lot of moving of people because there's drought problems. And so, they get a lot\nof language contact. When you have, when people have to, if you've gotta move\nbecause you've got no water, then you've gotta get going. And then you run into\ncontact with other tribes, other groups. In the Amazon, that's not the case. And so, people can stay there\nfor hundreds and hundreds and probably thousands of years, I guess. And so, these groups have,\nthe Chimane and the Piraha are both isolates in\nthat, and they can just, I guess they've just lived\nthere for ages and ages with minimal contact with\nother outside groups. And so, I mean, I'm interested\nin them because they are, I mean, in these cases, I'm\ninterested in their words. I would love to study their\nsyntax, their orders of words, but I'm mostly just\ninterested in how languages are connected to their\ncultures in this way. And so, with the Piraha,\ntheir most interesting, I was working on number\nthere, number information. And so, the basic idea is I\nthink language is invented. That's what I get from the words here, is that I think language is invented. We talked about color earlier. It's the same idea, so that\nwhat you need to talk about with someone else is what\nyou're gonna invent words for. And so, we invent labels\nfor colors that I need, not that I can see, but the\nthings I need to tell you about so that I can get objects from you or get you to give me the right objects. And I just don't need a word for teal or a word for aquamarine\nin the Amazon jungle for the most part, because\nI don't have two things which differ on those colors. I just don't have that. And so, numbers are\nreally another fascinating source of information\nhere, where you might, naively, I certainly\nthought that all humans would have words for exact counting, and the Piraha don't, okay? So, they don't have\nany words for even one. There's not a word for\none in their language. And so, there's certainly not a word for two, three, or four. So, that kind of blows people's minds off. - Yeah, that is blowing my mind. - [Edward] That's pretty weird, isn't it? - How are you gonna ask,\nI want two of those? - You just don't. And so, that's just not a\nthing you can possibly ask in the Piraha. It's not possible. There's no words for that. So, here's how we found this out, okay? So, it was thought to be\na one-to-many language. There are three words for\nquantifiers, for sets, but people had thought that\nthose meant one, two, and many. But what they really mean\nis few, some, and many. Many is correct. It's few, some, and many. And so, the way we figured this out, and this is kind of cool, is that we gave people, we\nhad a set of objects, okay? These were having to be spools of thread. Doesn't really matter what they are. Identical objects. And when I sort of started off here, I just give you one of those\nand say, \"What's that?\" Okay, so you're a Piraha speaker, and you tell me what it is. And then I give you two\nand say, \"What's that?\" And nothing's changing in the set except for the number, okay? And then I just ask you\nto label these things. We just do this for a\nbunch of different people. And frankly, I did this task. - This is fascinating.\n- And it's a weird, it's a little bit weird. So, they say the word that\nwe thought was one, it's few, but for the first one. And then maybe they say few, or maybe they say some for the second. And then for the third or the fourth, they start using the\nword many for the set. And then five, six, seven, eight. I go all the way to 10. And it's always the same word. And they look at me like I'm stupid because they told me what the word was for six, seven, eight. And I'm gonna continue\nasking them at nine and 10. I'm like, \"I'm sorry.\" They understand that I\nwanna know their language. That's the point of the task, is I'm trying to learn their\nlanguage, and so that's okay. But it does seem like I'm a little slow 'cause they already told me\nwhat the word for many was, five, six, seven, and I keep asking. So it's a little funny to\ndo this task over and over. We did this with a guy called,\nDan was our translator. He's the only one who really\nspeaks Piraha fluently. He's a good bilingual\nfor a bunch of languages, but also English and Piraha. And then a guy called Mike Frank was also a student with me down there. He and I did these things. And so you do that, okay? And everyone does the same thing. All, you know, we asked like 10 people and they all do exactly the\nsame labeling for one up. And then we just do the same thing down on like random order, actually. We do some of them up, some\nof them down first, okay? And so we do, instead of one\nto 10, we do 10 down to one. And so I give them 10, nine, at eight, they start saying the word for some. And then at down, when you get to four, everyone is saying the word for few, which we thought was one. So it's like the context determined what word, what that\nquantifier they used was. So it's not a count word. They're not count words. They're just approximate words. - And they're gonna be\nnoisy when you interview a bunch of people with\nthe definition of few and there's gonna be a\nthreshold in the context. - Yeah, yeah, I don't\nknow what that means. That's gonna depend on the context. I think it's true in English too, right? If you ask an English\nperson what a few is, I mean, that's gonna depend\ncompletely on the context. - And it might actually be\nat first hard to discover 'cause for a lot of people, the jump from one to\ntwo will be few, right? So it's a jump. - Yeah, it might be. It might still be there, yeah. - I mean, that's fascinating. That's fascinating. I mean, the numbers\ndon't present themselves. - So the words aren't there. And so then we do these other things. Well, if they don't have the words, can they do exact matching kinds of tasks? Can they even do those tasks? And the answer is sort of yes and no. And so yes, they can do them. So here's the tasks that we did. We put out those spools\nof thread again, okay? So we put like three out here and then we gave them some objects and those happened to be\nuninflated red balloons. It doesn't really matter what they are. It's just a bunch of\nexactly the same thing. And it was easy to put down right next to these\nspools of thread, okay? And so then I put out three of these and your task was to just put one against each of my three things. And they could do that perfectly. So I mean, I would actually do that. It was a very easy task to explain to them because I did this with\nthis guy, Mike Frank, and I'd be the experimenter\ntelling him to do this and showing him to do this. And then we just like,\njust do what he did. You'll copy him. All we had to, I didn't\nhave to speak Piraha except for know what, copy him. Like do what he did is like\nall we had to be able to say. And then they would do\nthat just perfectly. And so we'd move it up. We'd do some sort of random\nnumber of items up to 10 and they basically do perfectly on that. They never get that wrong. I mean, that's not a counting task, right? That is just a match. You just put one against,\nit doesn't matter how many. I don't need to know\nhow many there are there to do that correctly. And they would make\nmistakes, but very, very few and no more than MIT undergrads. Just gonna say, like, there's\nno, these are low stakes. So, you know, you make mistakes. - So counting is not required\nto complete the matching task. - That's right, not at all. Okay, and so that's our control. And this guy had gone down there before and said that they couldn't do this task. But I just don't know\nwhat he did wrong there 'cause they can do this\ntask perfectly well. And, you know, I can train\nmy dog to do this task. So of course they can do this task. And so, you know, it's not a hard task. But the other task that was\nsort of more interesting is like, so then we do a bunch of tasks where you need some way to encode the set. So like, one of them is just, I just put a opaque sheet\nin front of the things. I put down a bunch, a set of these things and I put an opaque sheet down. And so you can't see them anymore. And I tell you, do the same\nthing you were doing before. Right, you know, and it's\neasy if it's two or three, it's very easy. But if I don't have the words for eight, it's a little harder. Like maybe, you know, with practice, when, well, no. - 'Cause you have to count.\n- For us it's easy 'cause we just count them. It's just so easy to count them. But they don't, they can't count them because they don't count. They don't have words for this thing. And so they would do approximate. It's totally fascinating. So they would get them\napproximately right, you know, after four or five. 'Cause you can, basically\nyou always get four right, three or four. That looks, that's something\nwe can visually see. But after that, you kind of have, it's an approximate number. And so then, and there's\na bunch of tasks we did and they all failed, I mean, failed. They did approximate after\nfive on all those tasks. And it kind of shows that the words, you kind of need the words, you know, to be able to do these kinds of tasks. - There's a little bit of a\nchicken and egg thing there because if you don't have the words, then maybe they'll limit\nyou in the kind of, like a little baby Einstein there won't be able to come\nup with a counting task. You know what I mean? Like the ability to count enables you to come up with interesting\nthings probably. So yes, you develop counting\nbecause you need it. But then once you have counting, you can probably come up with a bunch of different inventions. Like how to, I don't\nknow, what kind of thing. They do matching really\nwell for building purposes, building some kind of hut\nor something like this. So it's interesting that\nlanguage is a limiter on what you're able to do. - Yeah, here language\nis just, is the words. Here is the words. Like the words for exact count\nis the limiting factor here. They just don't have 'em. - Yeah, yeah. But that's what I mean. That limit is also a limit on the society of what they're able to build. - That's gonna be true, yeah. So it's probably, I mean, this is one of those problems with the snapshot of\njust current languages is that we don't know\nwhat causes a culture to discover/invent a counting system. But the hypothesis is the guess out there is something to do with farming. So if you have a bunch of goats and you wanna keep track of them and you say you have 17 goats and you go to bed at night\nand you get up in the morning, boy, it's easier to have\na count system to do that. That's an abstraction over a set. So they don't have,\nlike, people often ask me when I tell 'em about this kind of work, they say, \"Well, don't\nthese, don't they have kids? \"Don't they have a lot of children?\" I'm like, \"Yeah, they\nhave a lot of children.\" And they do. They often have families of\nthree or four or five kids. And they go, \"Well, don't\nthey need the numbers \"to keep track of their kids?\" And I always ask this\nperson who says this, like, \"Do you have children?\" (laughing) And the answer's always no because that's not how you\nkeep track of your kids. You care about their identities. It's very important to me when I go, \"I think I have five children.\" - [Lex] You don't think\none, two, three, four? - It matters which five. If you replaced one with\nsomeone else, I would care. A goat, maybe not. That's the kind of point. It's an abstraction. Something that looks\nvery similar to the one wouldn't matter to me, probably. - But if you care about goats, you're gonna know them\nactually individually also. - [Edward] Yeah, you will. - I mean, cows, goats, if\nthere's a source of food and milk and all that kind of stuff, you're gonna actually really care. - But I'm saying it is an abstraction such that you don't have to\ncare about their identities to do this thing fast. That's the hypothesis, not mine. From anthropologists as a guessing about where words for counting came from is from farming, maybe. - Yeah. Do you have a sense\nwhy universal languages like Esperanto have not taken off? Like, why do we have all\nthese different languages? - Well, my guess is the\nfunction of a language is to do something in a community. I mean, unless there's some function to that language in the community, it's not gonna survive,\nit's not gonna be useful. So here's a great example. So what I'm, like, language\ndeath is super common, okay? Languages are dying all around the world. And here's why they're dying. And it's like, yeah, I see this in, it's not happening right now in either the Chimane or the Piraha, but it probably will. And so there's a neighboring\ngroup called Mositan, which is, I said that it's an isolate. It's actually, there's a dual,\nthere's two of them, okay? So it's actually, there's two languages which are really close, which\nare Mositan and Chimane, which are unrelated to anything else. And Mositan is unlike Chimane in that it has a lot of contact\nwith Spanish and it's dying. So that language is dying. The reason it's dying is\nthere's not a lot of value for the local people in\ntheir native language. So there's much more\nvalue in knowing Spanish, like, because they wanna\nfeed their families. And how do you feed your family? You learn Spanish so you can make money, so you can get a job and do these things, and then you make money. And so they want Spanish things. And so Mositan is in danger and is dying. And that's normal. And so basically the\nproblem is that people, the reason we learn\nlanguage is to communicate. And we use it to make money and to do whatever it\nis to feed our families. And if that's not happening,\nthen it won't take off. It's not like a game or something. This is like something we use. Like, why is English so popular? It's not because it's an\neasy language to learn. Maybe it is. I don't really know. But that's not why it's popular. - But because the United\nStates is a gigantic economy, and therefore-\n- Yeah, yeah. It's big economies that do this. It's all it is. It's all about money. And so there's a motivation\nto learn Mandarin. There's a motivation to learn Spanish. There's a motivation to learn English. These languages are very valuable to know because there's so, so many\nspeakers all over the world. - That's fascinating. - There's less of a value economically. It's like kind of what drives this. It's not just for fun. I mean, there are these\ngroups that do want to learn language just\nfor language's sake. And there's something to that. But those are rarities in general. Those are a few small groups that do that. Most people don't do that. - Well, if that was the primary driver, then everybody was speaking English or speaking one language. There's also attention.\n- That's happening. - Well. - We're moving towards fewer\nand fewer languages, exactly. - We are. I wonder if, you're right. Maybe this is slow, but maybe\nthat's where we're moving. But there is a tension. You're saying a language that the fringes. But if you look at\ngeopolitics and superpowers, it does seem that there's\nanother thing in tension, which is a language is a\nnational identity sometimes. - Oh, yeah.\n- For certain nations. I mean, that's the war in Ukraine. Language, Ukrainian language\nis a symbol of that war in many ways, like a country\nfighting for its own identity. So it's not merely the convenience. I mean, those two things are a tension, is the convenience of\ntrade and the economics and be able to communicate\nwith neighboring countries and trade more efficiently\nwith neighboring countries, all that kind of stuff, but\nalso identity of the group. - [Edward] I completely agree. - As language is the\nway, for every community, like dialects that emerge are\na kind of identity for people. Sometimes a way for people to say F-U to the more powerful people. That's interesting. So in that way, language\ncan be used as that tool. - Yeah, I completely agree. And there's a lot of work to\ntry to create that identity so people want to do that. As a cognitive scientist\nand language expert, I hope that continues because\nI don't want languages to die. I want languages to survive because they're so interesting\nfor so many reasons. But I mean, I find them fascinating just for the language part, but I think there's a lot of\nconnections to culture as well, which is also very important. - Do you have hope for machine translation that can break down the\nbarriers of language? So while all these different\ndiverse languages exist, I guess there's many ways\nof asking this question, but basically how hard is it to translate in an automated way from\none language to another? - There's gonna be cases where it's gonna be really hard, right? So there are concepts\nthat are in one language and not in another. Like the most extreme kinds of cases are these cases of number information. So good luck translating a\nlot of English into Piraha. It's just impossible. There's no way to do it because there are no\nwords for these concepts that we're talking about. There's probably the flip side, right? There's probably stuff in Piraha which is gonna be hard to translate into English on the other side. And so I just don't know\nwhat those concepts are. I mean, the world space is\ndifferent from my world space. And so I don't know what, so the things they talk about, things are, it's gonna\nhave to do with their life as opposed to my industrial life, which is gonna be different. And so there's gonna be\nproblems like that always. There's like, maybe it's not so bad in the case of some of these spaces and maybe it's gonna be harder in others. And so it's pretty bad in number. It's like extreme, I'd\nsay, in the number space, exact number space. But in the color dimension, right? So that's not so bad. I mean, but it's a problem that you don't have ways\nto talk about the concepts. - And there might be entire\nconcepts that are missing. So to you, it's more\nabout the space of concept versus the space of form. Like form, you can probably map. - Yeah, but so you were talking\nearlier about translation and about how translations, there's good and bad translations. I mean, now we're talking about\ntranslations of form, right? So what makes a writing good, right? - [Lex] There's a music to the form. - It's not just the content,\nit's how it's written. And translating that,\nthat sounds difficult. - We should say that there is like, I don't hesitate to say meaning, but there's a music and\na rhythm to the form. When you look at the broad picture, like the difference between\nDostoevsky and Tolstoy or Hemingway, Bukowski, James\nJoyce, like I mentioned, there's a beat to it,\nthere's an edge to it that it's like, is in the form. - We can probably get measures of those. - Yeah.\n- I don't know. I'm optimistic that we could\nget measures of those things. And so maybe that's- - Translatable? - I don't know. I don't know though. I have not worked on that. - [Lex] I would love to see- - That sounds totally fascinating. - Translation to Hemingway. I mean, Hemingway's probably the lowest, I would love to see different authors, but the average per\nsentence dependency length for Hemingway is probably the shortest. - That's your sense, huh? It's simple sentences with\nshort, yeah, yeah, yeah, yeah. - I mean, that's when, if you\nhave really long sentences, even if they don't have\ncenter embedding, like- - They can have longer connections, yeah. - [Lex] They can have longer connections. - They don't have to, right? You can't have a long, long sentence with a bunch of local words, yeah. But it is much more likely\nto have the possibility of long dependencies with\nlong sentences, yeah. - I met a guy named Aza Raskin, who does a lot of cool\nstuff, really brilliant. Works with Tristan Harris\non a bunch of stuff. But he was talking to me about\ncommunicating with animals. He co-founded Earth Species Project, where you're trying to\nfind the common language between whales, crows, and humans. And he was saying that there's\na lot of promising work, that even though the\nsignals are very different, like the actual, like,\nif you have embeddings of the languages, they're actually trying to communicate similar type things. Is there something you\ncan comment on that? Like where, is there promise to that? In everything you've seen\nin different cultures, especially like remote cultures, that this is a possibility? Or no, that we can talk to whales? - I would say yes. I think it's not crazy at all. I think it's quite reasonable. There's this sort of weird\nview, well, odd view, I think, that to think that human\nlanguage is somehow special. I mean, it is, maybe it is. We can certainly do more than\nany of the other species. And maybe our language\nsystem is part of that. It's possible. But people have often\ntalked about how human, like Chomsky, in fact, has talked about how only human language has\nthis compositionality thing that he thinks is sort of key in language. And the problem with that argument is he doesn't speak whale. (laughs) And he doesn't speak crow,\nand he doesn't speak monkey. He's like, they say things like, \"Well, they're making a\nbunch of grunts and squeaks.\" And the reasoning is like,\nthat's bad reasoning. Like, I'm pretty sure if you asked a whale what we're saying, they'd say, \"Well, they're making a\nbunch of weird noises.\" - Exactly.\n- And so it's like, this is a very odd reasoning to be making that human language is special because we're the only ones\nwho have human language. I'm like, well, we don't\nknow what those other, we just can't talk to them yet. And so there are probably\na signal in there, and it might very well\nbe something complicated like human language. I mean, sure, with a small\nbrain in lower species, there's probably not a very\ngood communication system, but in these higher species where you have what seems to be abilities\nto communicate something, there might very well be\na lot more signal there than we might have otherwise thought. - But also, if we have a lot\nof intellectual humility here, there's somebody formerly from MIT, Neri Oxman, who I admire very much, has talked a lot about, has worked on communicating with plants. So like, yes, the signal\nthere is even less than, but like, it's not out of\nthe realm of possibility that all nature has a\nway of communicating. And it's a very different language, but they do develop a kind of language through the chemistry, through some way of\ncommunicating with each other. And if you have enough humility\nabout that possibility, I think you can, I think it would be very\ninteresting in a few decades, maybe centuries, hopefully not, a humbling possibility of\nbeing able to communicate not just between humans, effectively, but between all of living things on Earth. - Well, I mean, I think some of them are not gonna have much\ninteresting to say, but some of them will.\n- But you could still- - We don't know. We certainly don't know. - I think if we're humble, there could be some\ninteresting trees out there. - Well, they're probably\ntalking to other trees, right? They're not talking to us. And so to the extent they're talking, they're saying something\ninteresting to some other, conspecific, as opposed to us, right? And so there probably is,\nthere may be some signal there. So there are people out there, actually it's pretty common\nto say that human language is special and different from any other animal\ncommunication system. And I just don't think\nthe evidence is there for that claim. I think it's not obvious. We just don't know, 'cause we don't speak these\nother communication systems until we get better. I do think there are\npeople working on that, as you pointed out, people working on whale\nspeak, for instance. That's really fascinating. - Let me ask you a wild,\nout there sci-fi question. If we make contact with an\nintelligent alien civilization, and you get to meet them, how hard do you think, how surprised would you be about\ntheir way of communicating? Do you think it would be recognizable? Maybe there's some parallels here to when you go to the remote tribes. - I mean, I would want\nDan Everett with me. He is amazing at learning\nforeign languages. And so he, this is an amazing feat, right? To be able to go, this is a language, which has no translators before him. I mean, there were-\n- Oh, wow, he just shows up?\n- He was a missionary. Well, there was a guy that\nhad been there before, but he wasn't very good. And so he learned the language far better than anyone else had learned before him. He's good at, he's a very social person. I think that's a big part of\nit, is being able to interact. So I don't know, it kind\nof depends on this species from outer space, how much\nthey wanna talk to us. - Is there something you could say about the process he follows? How do you show up to\na tribe and socialize? I mean, I guess colors and counting is one of the most basic\nthings to figure out. - Yeah, you start that, you\nactually start with objects and just say, you know,\njust throw a stick down and say, \"Stick,\" and then you\nsay, \"What do you call this?\" And then they'll say\nthe word for whatever. And he says, \"The standard thing to do \"is to throw two sticks at two sticks.\" And then, you know, he\nlearned pretty quick that there weren't any\ncount words in this language because they didn't know,\nthis wasn't interesting. I mean, it was kind of weird. They'd say some or something, the same word over and over again. And so, but that is a standard thing. You just like try to, but you\nhave to be pretty out there socially, like willing\nto talk to random people, which these are, you know, really very different people from you. And he was, and he's very social. And so I think that's a big part of this, is like, that's how, you know, a lot of people know a lot of languages is they're willing to\ntalk to other people. - That's a tough one, where you just show up knowing nothing. - [Edward] Yeah, oh God, yeah, yeah, yeah. - It's beautiful that humans are able to connect in that way. - [Edward] Yeah, yeah. - You've had an incredible career exploring this fascinating topic. What advice would you\ngive to young people? About how to have a career like that, or a life that they can be proud of? - When you see something\ninteresting, just go and do it. Like I do that. Like that's something I do, which is kind of unusual for most people. So like when I saw the Piraha, like if Piraha was\navailable to go and visit, I was like, yes, yes, I'll go. And then when we couldn't go back, we had some trouble with\nthe Brazilian government. There's some corrupt people there. It was very difficult to\nget, go back in there. And so I was like, all right, I gotta find another group. And so we searched around, and we were able to find the Chamonix, because I wanted to keep\nworking on this kind of problem. And so we found the\nChamonix and just go there. I didn't really have,\nwe didn't have content. We had a little bit of\ncontact and brought someone. And that was, you know, we\njust kind of just try things. I say it's like, a lot of\nthat's just like ambition, just try to do something that\nother people haven't done. Just give it a shot is what I, I mean, I do that all the time. - I love it. And I love the fact\nthat your pursuit of fun has landed you here talking to me. This was an incredible conversation, that you're just a\nfascinating human being. Thank you for taking a journey through human language with me today. This is awesome. - [Edward] Thank you very much,\nLex, it's been a pleasure. - Thanks for listening\nto this conversation with Edward Gibson. To support this podcast, please check out our\nsponsors in the description. And now let me leave you with\nsome words from Wittgenstein. \"The limits of my language\nmean the limits of my world.\" Thank you for listening. I hope to see you next time."
}