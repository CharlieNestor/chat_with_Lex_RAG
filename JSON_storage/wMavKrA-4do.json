{
  "video_id": "wMavKrA-4do",
  "title": "Manolis Kellis: Evolution of Human Civilization and Superintelligent AI | Lex Fridman Podcast #373",
  "date": "2023-04-21",
  "transcript": [
    {
      "timestamp": "0:00",
      "section": "Full Transcript",
      "text": "- Maybe we shouldn't\nthink of AI as our tool and as our assistant, maybe we should really\nthink of it as our children. And the same way that you are responsible for training those children, but they are independent human beings, and at some point they will surpass you... And this whole concept\nof alignment of basically making sure that the AI\nis always at the service of humans is very self-serving\nand very limiting. If instead, you basically\nthink about AI as a partner and AI as someone that shares\nyour goals, but has freedom, then we can't just\nsimply force it to align with ourselves and we not align with it. So in a way, building trust is mutual. You can't just simply like\ntrain an intelligent system to love you when it realizes\nthat you can just shut it off. - The following is a conversation with Manolis Kellis, his\nfifth time on this podcast. He's a professor at MIT and head of the MIT Computational Biology Group. He's one of the greatest\nliving scientists in the world, but he's also a humble, kind, caring human being that I\nhave the greatest of honors and pleasures of being\nable to call a friend. This is a Lex Friedman podcast. To support it, please\ncheck out our sponsors in the description. And now, dear friends,\nhere's Manolis Kellis. Good to see you, first of all, Manolis. - Lex, I've missed you. I think you've changed the\nlives of so many people that I know, and it's\ntruly like such a pleasure to be back. Such a pleasure to see you grow, to sort of reach so many different aspects of your own personality. - Thank you for the love. You've always given me\nso much support and love. I just can't, I'm forever\ngrateful for that. - It's lovely to see a\nfellow human being who has that love, who basically\ndoes not judge people. And there's so many\njudgmental people out there, and it's just so nice to\nsee this beacon of openness. - So what makes me one instantiation of human irreplaceable, do you think, as we enter this increasingly capable, age of increasingly\ncapable AI, I have to ask, what do you think makes\nhumans irreplaceable? - So humans are irreplaceable\nbecause of the baggage that we talked about. So we talked about baggage, we talked about the fact\nthat every one of us has effectively relearned all of human civilization in their own way. So every single human has\na unique set of genetic variants that they've inherited,\nsome common, some rare, and some make us think differently, some make us have different personalities. They say that a parent with\none child believes in genetics, a parent with multiple\nchildren understands genetics, just how different kids are. And my three kids have dramatically different personalities\never since the beginning. So one thing that makes us\nunique is that every one of us has a different hardware. The second thing that makes\nus unique is that every one of us has a different software, uploading of all of human society, all of human civilization,\nall of human knowledge. We don't, we're not born knowing it. We're not like, I don't know, birds that learn how to\nmake a nest through genetics and will make a nest even\nif they're never seen one. We are constantly relearning\nall of human civilization. So that's the second thing. And the third one that\nactually makes humans very different from AI is\nthat the baggage we carry is not experiential baggage, it's also evolutionary baggage. So we have evolved through\nrounds of complexity. So just like ogres have\nlayers and Shrek has layers, humans have layers. There's the cognitive layer,\nwhich is sort of the outer, you know, most, the latest\nevolutionary innovation, these enormous neocortex\nthat we have evolved. And then there's the emotional\nbaggage underneath that. And then there's all of the\nfear and fright and flight and all of these kinds of behaviors. So AI only has a neocortex. AI\ndoesn't have a limbic system. It doesn't have this\ncomplexity of human emotions, which make us so, I think,\nbeautifully complex, so beautifully intertwined\nwith our emotions, with our instincts, with our, you know, sort of gut reactions and all of that. So I think when humans are\ntrying to suppress that aspect, the sort of, quote unquote, more human aspect towards\na more cerebral aspect, I think we lose a lot of the creativity, we lose a lot of the, you\nknow, freshness of humans, and I think that's quite irreplaceable. - So we can look at the\nentirety of people that are alive today, and maybe all\nhumans who have ever lived... - [Manolis Kellis] Yeah. - And mapped them in this\nhigh-dimensional space. And there's probably a center, a center of mass for that mapping. And a lot of us deviate\nin different directions. So the variety of directions\nin which we all deviate from that center is vast. - I would like to think that\nthe center is actually empty. - [Lex Fridman] Yes. - That basically humans\nare just so diverse from each other, that\nthere's no such thing as an average human. That every one of us has\nsome kind of complex baggage of emotions, intellectual,\nyou know, motivational, behavioral traits, that\nit's not just one sort of normal distribution\nand we deviate from it. There's so many dimensions\nthat we're kind of hitting the sort of sparseness, the curse of dimensionality\nwhere it's actually quite sparsely populated,\nand I don't think you have an average human being. - So what makes us unique\nin part is the diversity and the capacity for diversity, and the capacity of the diversity comes from that entire evolutionary history. So there's just so many ways\nwe can vary from each other. - Yeah, I would say not just the capacity, but the inevitability of diversity. Basically, it's in our hardware. We are wired differently from each other. My siblings and I are\ncompletely different. My kids from each other\nare completely different. My wife has, she's like\nnumber two of six siblings. From a distance, they look the same. But then you get to, you\nknow, you get to know them. Every one of them is completely different. - But sufficiently the same, that the difference is\ninterplayed with each other. So that's the interesting\nthing where the diversity is functional, it's useful. So it's like we're close\nenough to where we notice the diversity and it\ndoesn't completely destroy the possibility of like, effective communication and interaction. So we're still the same kind of thing. - So what I said in one\nof our earlier podcasts is that if humans realize that we're 99.9% identical, we would\nbasically stop fighting with each other. (laughs) Like, we are really one\nhuman species, and we are so, so similar to each other. And if you look at the alternative, if you look at the next\nthing outside humans, like it's been 6 million\nyears that we haven't had a relative. So it's truly extraordinary\nthat we're kind of like this dot in outer space\ncompared to the rest of life on Earth. - When you think about\nevolving through rounds of complexity, can you\nmaybe elaborate such a beautiful phrase, beautiful thought, that there's layers of\ncomplexity that make... - So with software,\nsometimes you're like, oh, let's like build version 2 from scratch. But this doesn't happen in evolution. In evolution, you layer in\nadditional features on top of old features. So basically when, like every\nsingle time my cells divide, I'm a yeast, like I'm\na unicellular organism, and then cell division\nis basically identical. Every time I breathe in and my\nlungs expand, I'm basically, you know, like every time\nmy heart beats, I'm a fish. So basically that, I still\nhave the same heart, like very, very little has changed, the\nblood going through my veins, the oxygen, the, you\nknow, our immune system, we're basically primates. Our social behavior, we're\nbasically new world monkeys and old world monkeys. We are basically this\nconcept that every single one of these behaviors can be traced somewhere in evolution and that all\nof that continues to live within us is also a\ntestament to not just not killing other humans for god's sake, but like not killing other species either. Like, just to realize\njust how united we are with nature and that\nall of these biological processes have never ceased to exist. They're continuing to live within us. And then just the neocortex and all of the reasoning capabilities\nof humans are built on top of all of these other\nspecies that continue to live, breathe, divide, metabolize,\nfight of pathogens, all continued inside us. - So you think the neocortex,\nthe whatever reasoning is, that's the latest feature\nin the latest version of this journey. - It's extraordinary that\nhumans have evolved so much in so little time. Again, if you look at the\ntimeline of evolution, you basically have billions\nof years to even get to a dividing cell and then\na multicellular organism, and then a complex body plan, and then these incredible\nsenses that we have for perceiving the world, the fact that bats can fly\nand they evolved flight, they evolved sonar in the\nspan of a few million years. I mean, it is just the\nextraordinary how much evolution has kind of sped up. And all of that comes\nthrough this evolvability. The fact that we took a while\nto get good at evolving. And then once you get good\nat evolving, you can sort of, you have modularity built in, you have hierarchical\norganizations built in. You have all of these\nconstructs that allow meaningful changes to\noccur without breaking the system completely. If you look at a traditional\ngenetic algorithm, the way that humans designed\nthem in the sixties, you can only evolve so much. And as you evolve a certain\namount of complexity, the number of mutations that move you away from something functional\nexponentially increases. And the number of mutations that move you to something better\nexponentially decreases. So the probability of evolving something so complex becomes in\ninfinitesimally small as you get more complex. But with evolution, it's\nalmost the opposite. Almost the exact opposite. That it appears that\nit's speeding up exactly as complexity is increasing. And I think that's just the system getting good at evolving. - Where do you think it's all headed? Do you ever think about where, try to visualize the\nentirety of the evolutionary system and see if there's an arrow to it and a destination to it? - So the best way to understand\nthe future is to look at the past. If you look at the trajectory, then you can kind of learn something about the direction which we're heading. And if you look at the trajectory of life on Earth, it's really about\ninformation processing. So the concept of the senses\nevolving one after the other, you know, being like bacteria\nare able to do chemotaxis, basically means moving\ntowards a chemical gradient. And that's the first thing\nthat you need to sort of hunt down food. The next step after that is being able to actually perceive light. So all life on this planet\nand all life that we know about evolved on this rotating rock. Every 24 hours you get sunlight and dark, sunlight and dark, and\nlight is a source of energy. Light is also information\nabout where is up. Light is all kinds of, you know, things. So you can basically now\nstart perceiving light and then perceiving shapes. Beyond just the sort of\nsingle photo receptor you can now have complex eyes or\nmultiple eyes and then start perceiving motion or perceiving direction, perceiving shapes. And then you start building infrastructure on the cognitive apparatus\nto start processing this information and making\nsense of the environment, building more complex\nmodels of the environment. So if you look at that\ntrajectory of evolution, what we're experiencing now, and humans are basically\naccording to this sort of information theoretical\nview of evolution, humans are basically\nthe next natural step. And it's perhaps no\nsurprise that we became the dominant species of\nthe planet, because yes, there's so many dimensions\nin which some animals are way better than we are, but at least on the cognitive dimension, we're just simply\nunsurpassed on this planet and perhaps the universe. But the concept that if\nyou now trace this forward, we talked a little bit about evolvability and how things get better at evolving. One possibility is that\nthe next layer of evolution builds the next layer of evolution. And what we're looking at now with humans and AI is that having mastered\nthis information capability that humans have from this, quote unquote, old hardware, this basically, you know, biological evolved system\nthat kind of, you know, somehow in the environment\nof Africa, and then in the subsequent environments of sort of dispersing through the globe was evolutionary advantageous. That has now created technology, which now has a capability of solving many of these cognitive tasks. It doesn't have all the baggage of the previous evolutionary layers, but maybe the next round of evolution on Earth is self-replicating AI, where we're actually\nusing our current smarts to build better programming languages and the programming\nlanguages to build, you know, ChatGPT, and that then\nbuild the next layer of software that will then\nsort of help AI speed up. And it's lovely that we're coexisting with this AI, that sort of, the creators of this\nnext layer of evolution, this next stage are still\naround to help guide it and hopefully will be for the rest of eternity as partners. But it's also nice to think\nabout it as just simply the next stage of\nevolution where you've kind of extracted away the biological needs. Like if you look at animals, most of them spend 80% of\ntheir waking hours hunting for food or building shelter. Humans? Maybe 1% of that time. And then the rest is left\nto creative endeavors. And AI doesn't have to worry\nabout shelter, et cetera. So basically it's all living\nin the cognitive space. So in a way it might just\nbe a very natural sort of next step to think about evolution. And that's on the sort\nof purely cognitive side. If you now think about humans themselves, the ability to understand\nand comprehend our on genome, again, the ultimate\nlayer of introspection, gives us now the ability to\neven mess with this hardware. Not just augment our\ncapabilities through interacting and collaborating with AI,\nbut also perhaps understand the neural pathways that\nare necessary for, you know, empathetic thinking, for justice, for this and that and that. And sort of help augment\nhuman capabilities through, you know, neuronal interventions, through chemical interventions, through electrical\ninterventions, to basically help steer the human, you know, bag of hardware that we kind of evolved with into greater capabilities. And then, ultimately, by understanding not just the wiring of neurons and\nthe functioning of neurons, but even the genetic code, we could even, at one point in the future,\nstart thinking about, well, can we get rid of psychiatric disease? Can we get rid of neurodegeneration? Can we get rid of\ndementia and start perhaps even augmenting human capabilities, not just getting rid of disease. - Can we tinker with the genome, with the hardware or getting closer to the hardware without having to deeply understand the baggage. In the way we've disposed of the baggage in our software systems with\nAI, to some degree, not fully, but to some degree, can we do the same with\nthe genome or is the genome deeply integrated into this baggage? - I wouldn't wanna get rid of the baggage. The baggage's what makes us awesome. So the fact that I'm\nsometimes angry and sometimes hungry and sometimes hangry\nis perhaps contributing to my creativity. I don't wanna be dispassionate,\nI don't wanna be another, like, you know, robot, I, you know, I wanna get in trouble\nand I wanna sort of say the wrong thing and I\nwanna sort of, you know, make an awkward comment and\nsort of push myself into, you know, reactions and\nresponses and things that can get just people thinking differently. And I think our society is moving towards a humorless space, where everybody's so afraid\nto say the wrong thing, that people kind of start\nquitting en mass and start like not liking their\njobs and stuff like that. Maybe we should be kind of embracing that human\naspect a little bit more and all of that baggage aspect and not necessarily thinking about\nreplacing it, on the contrary, like embracing it instead\nof this coexistence of the cognitive and\nthe emotional hardwares. - So embracing and\ncelebrating the diversity that springs from the baggage versus kind of pushing towards and\nempowering this kind of pull towards conformity. - Yeah. And in fact, with the\nadvent of AI, I would say, and these seemingly\nextremely intelligent systems that sort of can perform\ntasks that we thought of as extremely intelligent\nat the blink of an eye, this might democratize\nintellectual pursuits. Instead of just simply\nwanting the same type of brains that, you know, carry out specific ways\nof thinking, we can, like, instead of just always only wanting say the mathematically extraordinary to go to the same universities, what you could see simply say is like, who needs that any more? You know, we now have AI. Maybe what we should really be thinking about is the diversity\nand the power that comes with the diversity, where\nAI can do the math and then we should be getting a\nbunch of humans that sort of think extremely\ndifferently from each other, and maybe that's the true\ncradle of innovation. - But AI can also, these large\nlanguage models can also be, with just a few prompts, essentially fine tuned to\nbe diverse from the center. So the prompts can really take you away into unique territory. You can ask the model\nto act in a certain way and it'll start to act in that way. Is that possible that the\nlanguage models could also have some of the magical\ndiversity that makes us so interesting? - Yeah, so I would say\nhumans are the same way. So basically when you sort of\nprompt humans to basically, you know, in a given environment,\nto act a particular way, they change their own behaviors. And, you know, the old saying is, show me your friends and\nI'll tell you who you are, more like, show me your\nfriends and I'll tell you who you'll become. So it's not necessarily\nthat you choose friends that are like you, but I\nmean, that's the first step. But then the second\nstep is that, you know, the kind of behaviors that you find normal in your circles are the behaviors that you'll start espousing. And that type of meta\nevolution where every action we take not only shapes our current action and the result of this action, but also shapes our\nfuture actions by shaping the environment in which\nthose future actions will be taken. Every time you carry out\na particular behavior, it's not just a consequence for today, but it's also a consequence for tomorrow because you're reinforcing\nthat neural pathway. So in a way, self-discipline\nis a self-fulfilling prophecy, and by behaving the way\nthat you wanna behave and choosing people that\nare like you and sort of exhibiting those behaviors\nthat are sort of desirable, you end up creating that\nenvironment as well. - So it is a kind of, life itself is a kind of prompting mechanism, super complex. The friends you choose, the\nenvironments you choose, the way you modify the\nenvironment that you choose. Yes. But that seems like that\nprocess is much less efficient than a large language model. You can literally get\na large language model through a couple of prompts to be a mix of Shakespeare and David Bowie. Right? You can very aggressively change, in a way that's stable and convincing, you really transform,\nthrough a couple of prompts, the behavior of the\nmodel into something very different from the original. - So well before ChatGPT,\nI would tell my students just ask, you know, what\nwould Manolis say right now? And you guys all have\na pretty good emulator of me right now.\n- Yes, yes. - And I don't know if you\nknow the programming paradigm of the Robert Duckin, where\nyou basically explained to the Robert Duckin that's just sitting there exactly what you did with your code and why you have a bug. And just by the act of explaining, you'll kind of figure it out. I woke up one morning from\na dream where I was giving a lecture in this amphitheater,\nand one of my friends was basically giving me some deep evolutionary\ninsight on how cancer genomes and cancer cells evolve. And I woke up with a\nvery elaborate discussion that I was giving and a very elaborate set of insights that he had, that I was projecting onto\nmy friend in my sleep. And obviously this was my dream. So my own neurons were\ncapable of doing that. But they only did that\nunder the prompt of, you are now Piyush Gupta,\nyou are a professor in cancer genomics, you're\nan expert in that field, what do you say? So I feel that we all have that inside us, that we have that capability\nof basically saying, I don't know what the right thing is, but let me ask my virtual\nX, what would you do? And virtual X would say,\nbe kind. I'm like, oh yes. Or something like that. And even though I myself\nmight not be able to do it unprompted, and my favorite\nprompt is think step by step. And I'm like, you know, this\nalso works on my 10 year old. When he tries to solve a math\nequation all in one step, I know exactly what mistake he'll make, but if I prompt it with, oh\nplease think step by step, then he sort of gets in a mindset. And I think it's also part\nof the way that ChatGPT was actually trained,\nthis whole sort of human in the loop reinforcement\nlearning has probably reinforced these types of behaviors, whereby having this feedback loop, you kind of aligned AI better to the prompting opportunities by humans. - Yeah. Prompting human-like reasoning steps, the step by step kind of thinking. Yeah. But it does seem to be, I suppose it just puts a mirror to our own capabilities and so we\ncan be truly impressed by our own cognitive capabilities, because the variety of what you can try, because we don't usually\nhave this kind of, we can't play with our own mind rigorously through Python code. Right?\n- Yeah. - So this allows us to\nreally play with all of human wisdom and knowledge, or at least knowledge at our fingertips, and then mess with that\nlittle mind that can think and speak in all kinds of ways. - What's unique is that,\nas I mentioned earlier, every one of us was\ntrained by different subset of human culture, and ChatGPT\nwas trained on all of it. And the difference there\nis that it probably has the ability to emulate\nalmost every one of us. The fact that you can figure out where that is in\ncognitive behavioral space, just by a few prompts\nis pretty impressive. But the fact that exists somewhere is, you know, absolutely beautiful. And the fact that it's\nencoded in an orthogonal way from the knowledge I\nthink is also beautiful. The fact that somehow through this extreme\noverparameterization of AI models, it was able to somehow\nfigure out that context, knowledge and form are\nseparable and that you can sort of describe scientific knowledge\nin a haiku in the form of, I don't know, Shakespeare or something. That tells you something\nabout the decoupling and the decouplability of these types of aspects of human psyche. - And that's part of the\nscience of this whole thing. So these large language\nmodels are, you know, days old in terms of this kind\nof leap that they've taken. And it'll be interesting\nto do this kind of analysis of the separation of\ncontext, form, and knowledge. Where exactly does that happen? There's already sort of\ninitial investigations, but it's very hard to figure out where, is there a particular\nparameter, set of parameters that are responsible\nfor a particular piece of knowledge or a particular context or a particular style speaking... - So with convolution or neural networks, interpretability had many good advances because we can kind of understand them. There's a structure to them.\nThere's a locality to them. And we can kind of understand\nthat different layers have different sort of ranges\nthat they're looking at. So we can look at activation\nfeatures and basically see where, you know, where\ndoes that correspond to. With large language models, it's perhaps a little more complicated, but I think it's still\nachievable in the sense that we could kind of ask, well, what kind of prompts does this generate? If I sort of drop out\nthis part of the network, then what happens? And sort of start getting at a language to even describe these\ntypes of aspects of human behavior or psychology, if you wish, from the spoken part\nand the language part. And the advantage of that\nis that it might actually teach us something about humans as well. Like, you know, we might\nnot have words to describe these types of aspects right now, but when somebody speaks\nin particular way, it might remind us of a\nfriend that we know from here and there, and if we had better language for describing that, these\nconcepts might become more apparent in our own human psyche, and then we might be able\nto encode them better in machines themselves. - I mean, both probably\nyou and I would have certain interests with the base model, what OpenAI calls the base model, which is before the alignment the reinforcement learning\nwith human feedback and before the AI safety\nbased kind of censorship of the model. It would be fascinating to explore, to investigate the ways\nthat the model can generate hate speech, the kind of hate\nthe humans are capable of. It would be fascinating. Or the kind, of course, like sexual language or the\nkind of romantic language or the all kinds of ideologies. Can I get it to be a communist? Can I get it to be a fascist? Can I get it to be a capitalist? Can I get it to be all these\nkinds of things and see which parts get activated and not, because it'll be fascinating\nto sort of explore at the individual mind level\nand at a societal level, where do these ideas take hold? What is the fundamental\ncore of those ideas? Maybe the communism, fascism, capitalism, democracy are all actually\nconnected by the fact that the human heart, the human\nmind is drawn to ideology, to a centralizing idea. And maybe we need a neural\nnetwork to remind us of that. - I like the concept that\nthe human mind is somehow tied to ideology. And I think that goes\nback to the promptability of ChatGPT, the fact that\nyou can kind of say, well, think in this particular way now. And the fact that humans\nhave invented words for encapsulating these\ntypes of behaviors. And it's hard to know how\nmuch of that is innate and how much of that was like passed on from language to language. But basically if you look at\nthe evolution of language, you can kind of see how\nyoung are these words in the history of language\nevolution that describe these types of behaviors, like, you know, kindness and anger and\njealousy, et cetera. If these words are very similar\nfrom language to language, it might suggest that\nthey're very ancient. If they're very different, it might suggest that\nthese concepts may have emerged independently in\neach different language and so forth. So looking at the phylogeny, the history, the evolutionary traces of\nlanguage at the same time as people moving around\nthat we can now trace thanks to genetics is a fascinating\nway of understanding the human psyche and\nalso understanding sort of how these types of behaviors emerge. And to go back to your idea\nabout sort of exploring the system unfiltered, I mean, in a way the psychiatric\nhospitals are full of those people, full of those people. So basically people whose\nmind is uncontrollable. - [Lex Fridman] Yes. - Who have kind of gone\nadrift in specific locations of their psyche. And I do find this fascinating,\nbasically, you know, watching movies that are\ntrying to capture the essence of troubled minds, I think\nis teaching us so much about our everyday selves, because many of us are\nable to sort of control our minds and are able to\nsomehow hide these emotions. And, but every time I see\nsomebody who's troubled, I see versions of myself,\nmaybe not as extreme, but I can sort of empathize\nwith these behaviors. And, you know, I see\nbipolar, I see schizophrenia, I see depression, I see autism. I see so many different\naspects that we kind of have names for and crystallize\nin specific individuals. And I think all of us have that, all of us have sort of just\nthis multidimensional brain, and genetic variations that\npush us in these directions, environmental exposures\nand traumas that push us in these directions... Environmental behaviors that\nare reinforced by the kind of friends that we chose, or friends that we were stuck with because of the\nenvironments that we grew up in. So in a way, a lot of these types of behaviors are within the vector span of every human. It's just that the\nmagnitude of those vectors is generally smaller for most people, because they haven't\ninherited that particular set of genetic variants or\nbecause they haven't even exposed to those\nenvironments basically. - Or something about the mechanism of reinforcement learning\nwith human feedback didn't quite work for them. So it's fascinating to think\nabout that's what we do. We have this capacity to\nhave all these psychiatric, or behaviors associated\nwith psychiatric disorders. But we, through the alignment\nprocess as we grow up- - [Manolis Kellis] That's exactly right. - With parents, we kind of,\nwe know to suppress them. We know to how to control. - Every human that grows up in this world spends several decades being shaped into place. And without that, you know, maybe we would have the\nunfiltered ChatGPT-4. (laughs) - Every baby is basically\na raging narcissist. - (laughs) Not all of\nthem, not all of them. Believe it or not. It's remarkable. Like, I remember like watching\nmy kids grow up and again, like, yes, part of their\npersonality stays the same, but also in different\nphases to their life, they've gone through these\ndramatically different types of behaviors. And you know, my daughter\nbasically saying, you know, basically one kid saying,\noh, I want the bigger piece, the other one saying, oh,\neverything must be exactly equal. And the third one saying, I'm okay. You know, I like to have the smaller part. Don't worry about me. - [Lex Fridman] Even in the early days, in the early days of development? - Yeah. Yeah. It's just extraordinary to sort of see these dramatically different... Like, I mean my wife and I, you know, are very different from each\nother, but we also have, you know, 6 million variants,\n6 million loci each, if you wish, if you just\nlook at common variants, we also have a bunch of\nrare variants that are inherited in more Mendelian fashion. And now you have, you know, an infinite number of\npossibilities for each of the kids. So basically it's 2 to the 6 million just from the common variants. And then if you like,\nlayer in the rare variants. So let me talk a little\nbit about common variants and rare variants. If you look at this common variants, they're generally weak\neffect because selection selects against strong effect variants. So if something like has a\nbig risk for schizophrenia, it won't rise to high frequency. So the ones that are\ncommon are by definition, by selection only the ones that\nhad relatively weak effect. And if all of the variants\nassociated with personality, with cognition and all\naspects of human behavior were weak effect variants, then\nkids would basically be just averages of their parents. If it was like thousands of loci, just by law of large numbers, the average of two large\nnumbers would be, you know, very robustly close to that middle. But what we see is that\nkids are dramatically different from each other. So that basically means\nthat in the context of that common variation, you basically have rare\nvariants that are inherited in a more Mendelian fashion\nthat basically then sort of govern likely many different\naspects of human behavior, human biology and human psychology. And that's, again, if, like, if you look at sort of a\nperson with schizophrenia, their identical twin has\nonly 50% chance of actually being diagnosed with schizophrenia. So that basically means there's probably developmental exposures,\nenvironmental exposures, trauma, all kinds of other aspects\nthat can shape that. And if you look at siblings,\nfor the common variants, it kind of drops off exponentially, as you would expect with, you know, sharing 50% of your\ngenome, 25% of your genome, you know, 12.5% of your genome, et cetera, with more and more distant cousins. But the fact that siblings\ncan differ so much in their personalities\nthat we observe every day, it can't all be nurture. Basically, you know, we've\nlike, again, as parents, we spend enormous amount\nof energy trying to fix, quote unquote, the nurture part. Trying to, you know, get them to share, get them to be kind, get them to be open, get them to trust each\nother, like, you know, like overcome the prisoner's\ndilemma of, you know, if everyone fends for themselves, we're all gonna live in a horrible place. But if we're a little more altruistic, then we're all gonna be in a better place. And I think it's not like we\ntreat our kids differently, but they're just born differently. So in a way, as a geneticist, I have to admit that\nthere's only so much I can do with nurture. That nature definitely\nplays a big component. - The selection of variants, we have the common variants\nand the rare variants. What can we say about the landscape of possibility they create? If you could just linger on that. So the selection of rare\nvariants is defined how? How do we get the ones that we get? Is it just laden in that\ngiant evolutionary baggage? - So I'm gonna talk about regression, why do we call it regression? And the concept of regression to the mean. The fact that when fighter\npilots in a dog fight did amazingly well, they\nwould give them rewards and then the next time\nthey're in dog fight, they would do worse. So then, you know, the navy\nbasically realized that, wow, or at least interpreted that as, wow, we're ruining them by\npraising them and then they're gonna perform worse. The statistical interpretation\nof that is regression of the mean. The fact that you're\nan extraordinary pilot, you've been trained in\nan extraordinary fashion, that pushes your mean further and further to extraordinary achievement. And then in some dog fights you'll just do extraordinarily well. The probability that the\nnext one will be just as good is almost nil, because this is the peak of your performance. And just by statistical odds, the next one will be another sample from the same underlying distribution, which is gonna be a\nlittle closer to the mean. So regression analysis takes\nits name from this type of realization in the statistical world. Now if you now take humans, you basically have people who have achieved extraordinary achievements. Einstein, for example, you know, you would call him for example, the epitome of human intellect. Does that mean that all of his children and grandchildren will be\nextraordinary geniuses? It probably means that they're sampled from the same underlying distribution. But he was probably a rare\ncombination of extremes in addition to these common variants. So you can basically\ninterpret your kids' variation for example as, well, of course they're gonna\nbe some kind of sampled from the average of the\nparents with some kind of deviation according to\nthe specific combination of rare variants that they have inherited. So, you know, given all that, you know, the possibilities are endless as to sort of where you should be. But you should always\ninterpret that with, well, it's probably an alignment\nof nature and nurture. And the nature has both a\ncommon variants that are acting kind of like the\nlaw of large numbers and the rare variants that are acting more in a Mendelian fashion. And then you layer in\nthe nurture, which again, in everyday action we make, we\nshape our future environment, but the genetics we inherit are shaping the future environment of not only us, but also our children. So there's this weird nature, nurture, interplay and self-reinforcement\nwhere you're kind of shaping your own environment, but you're also shaping the\nenvironment of your kids. And your kids are gonna\nbe born in the context of your environment that you've shaped, but also with a bag of genetic variants that they have inherited. And there's just so much\ncomplexity associated with that. When we start blaming something on nature, it might just be nurture, it\nmight just be that, well, yes, they inherited the genes from\nthe parents, but they also, you know, were shaped\nby the same environment. So it's very, very hard untangle the two. And you should also\nalways realize that nature can influence nurture,\nnurture can influence nature or at least be correlated\nwith and predictive of and so on and so forth. - So I love thinking\nabout that distribution that you mentioned. And here's where I can be\nmy usual ridiculous self. And I sometimes think about\nthat army of sperm cells, however many hundreds\nof thousands there are. And I kind of think of all\nthe possibilities there. 'Cause there's a lot of variation. And one gets to win. - [Manolis Kellis] It's not a random one. - Is it a totally ridiculous\nway to think about... - No, not at all. So I would say\nevolutionarily we are a very slow evolving species. Basically the generations\nof humans are a terrible way to do selection. What you need is processes\nthat allow you to do selection in a smaller, tighter loop. - Yeah.\n- And part of what, if you look at our immune\nsystem for example, it evolves at a much faster\npace than humans evolve, because there is actual\nevolutionary process that happens within our immune cells\nas they're dividing. There's basically VDJ recombination\nthat basically creates this extraordinary wealth of antibodies and antigens against the environment. And basically all these\nantibodies are now recognizing all these antigens from the environment and they send signals back\nthat cause these cells that recognize the non-cells to multiply. So that basically means that\neven though viruses evolve at millions of times faster than we are, we can still have a\ncomponent of our cells, which is environmentally facing, which is sort of evolving\nat, not the same scale, but very rapid pace. Sperm expresses perhaps the\nmost proteins of any cell in the body. And part of the thought\nis that this might just be a way to check that the sperm is intact. In other words, if you\nwaited until that human has a liver and starts eating\nsolid food and, you know, sort of filtrates away, you\nknow, or kidneys or stomach, et cetera, basically if you\nwaited until these mutations, you know, manifest, late, late in life, then you would end up not failing fast, and you would end up with\na lot of failed pregnancies and a lot of later onset, you know, psychiatric illnesses, et cetera. If instead you basically\nexpress all of these genes at the sperm level and if they misform, they basically cause the sperm\nto cripple, then you have, at least on the male side, the ability to exclude\nsome of those mutations. And on the female side,\nas the egg develops, there's probably a similar process, where you could sort of weed\nout eggs that are just not, you know, carrying beneficial\nmutations or at least that are carrying highly\ndetrimental mutations. So you could basically\nthink of the evolutionary process in a nested loop, basically, where there's an inner\nloop where you get many, many more iterations to run. And then there's an outer loop that moves at a much slower pace. And going back to the\nnext step of evolution, of possibly designing systems\nthat we can use to sort of complement our own biology or to sort of eradicate disease and, you name it, or at least mitigate some\nof the, I don't know, psychiatric illnesses, neurodegenerative disorders, et cetera. You can basically, and also, you know, metabolic, immune, cancer, you name it, simply engineering these\nmutations from rational design might be very inefficient. If instead you have an evolutionary loop where you're kind of\ngrowing neurons on a dish and you're exploring evolutionary\nspace and you're sort of shaping that one protein\nto be better adapt at sort of, I don't know, recognizing\nlight or communicating with other neurons, et cetera. You can basically have a\nsmaller evolutionary loop that you can run like\nthousands of times faster than the speed it would\ntake to evolve humans for another million years. So I think it's important\nto think about sort of this evolvability as a\nset of nested structures that allow you to sort of\ntest many more combinations, but in a more fixed setting. - Yeah, that's fascinating\nthat the mechanism there is for sperm to\nexpress proteins to create a testing ground early on, so that the failed designs don't make it. - Yeah, I mean in design\nof engineering systems, fail fast is one of the\nprinciples you learn. Like basically you assert something, why do you assert that? Because if that's something ain't right, you better crash now\nthan sort of let it crash at an unexpected time. And in a way you can think of it as like 20,000 assert functions. Assert protein can fold. And if any of them fail,\nthat sperm is gone. - Well, I just like the fact\nthat I'm the winning sperm. I am the result of the winner, #winning. - My wife always plays me this French song that actually sings about that. It's like, you know, remember, in life, we were all the first one time. (laughs) - At least once we were-\n- At least one time, you were the first. - I should mention it\nas a brief tangent back to the place where we came from. - [Manolis Kellis] Yeah. - Which is the base model that I mentioned for OpenAI, which is\nbefore the reinforcement learning with human feedback. And you kind of give this\nmetaphor of it being kind of like a psychiatric hospital. - I like that because it's basically all of these different angles at once. Like you basically have\nthe more extreme versions of human psyche. - So the interesting thing is, well, I've talked with folks\nin OpenAI quite a lot and they say it's\nextremely difficult to work with that model. - Yeah. Kind of like it's\nextremely difficult to work with some humans. - The parallels there are very interesting because once you run\nthe alignment process, it's much easier to interact with it. But it makes you wonder what the capacity what the underlying capability\nof the human psyche i as in the same way that what\nis the underlying capability of a large language model. - And remember earlier\nwhen I was basically saying that part of the reason why\nit's so prompt malleable is because of that alignment\nproblem, that alignment work. It's kind of nice that the\nengineers at OpenAI have the same interpretation that,\nyou know, in fact it is that, and this whole concept\nof easier to work with, I wish that we could work\nwith more diverse humans. In a way...\n- Yes. - And sort of, that's one of the possibilities that I see with the advent of these\nlarge language models. The fact that it gives us\nthe chance to both dial down friends of ours that we can't interpret or that are just too\nedgy to sort of really, truly interact with, where you could have a real-time translator. Just the same way that you can translate English to Japanese or Chinese or Korean by like real-time adaptation. You could basically\nsuddenly have a conversation with your favorite\nextremist on either side of the spectrum and just\ndial them down a little bit. - Of course not you and I,\nbut you could have friends who's a complete asshole, but it's a different base level. So you can actually tune\nit down to like, okay, they're not actually being an asshole, they're actually\nexpressing love right now. It's just that they're- - [Manolis Kellis] They have\ntheir way of doing that. - And they probably live in New York, if we're just\nto pick a random location. - So the, yeah, so you can\nbasically layer out contexts. You can basically say, Ooh, let me change New York\nto Texas and let me change, you know, extreme left to\nextreme right or somewhere in the middle or something. And I also like the concept\nof being able to listen to the information without\nbeing dissuaded by the emotions. In other words, everything\nhumans say has an intonation, has some kind of background\nthat they're coming from, reflects the way that\nthey're thinking of you, reflects the impression\nthat they have of you. And all of these things are intertwined, but being able to disconnect\nthem, being able to sort of, I mean self-improvement\nis one of the things that I'm constantly working on. And being able to receive\ncriticism from people who really hate you is\ndifficult because it's layered in with that hatred. But deep down there's\nsomething that they say that actually makes sense, or people who love you\nmight layer it in a way that doesn't come through. But if you're able to sort of disconnect that emotional component from\nthe sort of self-improvement, and basically when somebody says, whoa, that was a bunch of bullshit, did you ever do the control\nthis and this and that, you could just say, oh, thanks for the very interesting\npresentation, you know, I'm wondering, what about that control? Then suddenly you're like, oh yeah, of course I'm gonna run that control. That's a great idea.\n- Yeah. - Instead of that was a\nbunch of BS, you're like, ah, you're sort of hitting on\nthe brakes and you're trying to push back against of that. So any kind of criticism that comes after that is very difficult to interpret in a positive way because\nit helps reinforce the negative assessment of your work. When in fact, if we\ndisconnected the technical component from the negative assessment, then you're embracing the negative, then you're embracing\nthe technical component, you you're gonna fix it. Whereas if it's coupled with, and if that thing is real and I'm right about your mistake,\nthen it's a bunch of BS, then suddenly you're like,\nyou're gonna try to prove that mistake does not exist. - Yes. Fascinating to like\ncarry the information. I mean this is what you're\nessentially able to do here is you carry the information\nin the rich complexity of that information contains, so it's not actually\ndumbing it down in some way. - [Manolis Kellis] Exactly. - You're still expressing\nit, but taking off... - But you can dial the emotional... - The emotion side.\n- Yeah. - Which is probably so\npowerful for the internet or for social networks. - Again, when it comes to\nunderstanding each other, like for example, I\ndon't know what it's like to go through life with\na different skin color. I don't know how people will perceive me. I don't know how people\nwill respond to me. We don't often have that experience. But in a virtual reality\nenvironment or in a sort of AI interactive system, you\ncould basically say, okay, now make me Chinese or make\nme South African or make me, you know, Nigerian, you\ncan change the accent, you can change layers of\nthat contextual information and then see how the\ninformation is interpreted. And you can rehear yourself\nthrough a different angle, you can hear others, you can have others react to\nyou from a different package. And then hopefully we\ncan sort of build empathy by learning to disconnect\nall of these social cues that we get from like how a\nperson is dressed, you know, if they're wearing a hoodie\nor if they're wearing a shirt, or if they're wearing a, you know jacket. You get very different emotional\nresponses that, you know, I wish we could overcome\nas humans and perhaps large language models\nand augmented reality and deepfakes can kind of\nhelp us overcome all that. - In what way do you think\nthese large language models and the thing they give\nbirth to in the AI space will change this human\nexperience, the human condition, the things we've talked\nacross many podcasts about, that makes life so damn\ninteresting and rich love, fear, fear of death, all of it. If we could just begin kind\nof thinking about how does it change, for the good and\nthe bad, the human condition? - Human society is extremely complicated. We have come from a\nhunter gatherer society to an agricultural and\nfarming society where the goal of most professions was\nto eat and to survive. And with the advent of agriculture, the ability to live together in societies, humans could suddenly be valued for different skills. If you don't know how to hunt, but you're an amazing potter, then you fit in society very\nwell because you can sort of make your pottery and you can barter it for rabbits that somebody else caught. And the person who hunts\nthe rabbits doesn't need to make pots, because\nyou're making all the pots. And that specialization of humans is what shaped modern society. And with the advent of\ncurrencies and governments and, you know, credit cards and\nBitcoin, you basically now have the ability to\nexchange value for the kind of productivity that you have. So basically I make things\nthat are desirable to others. I can sell them and buy back\nfood, shelter, et cetera. With AI, the concept of I am my profession might need to be revised because\nI defined my profession in the first place as\nsomething that humanity needed that I was uniquely capable of delivering. But the moment we have AI\nsystems able to deliver these goods, for example, writing a piece of software\nor making a self-driving car, or interpreting the human genome, then that frees up more of human time for other pursuits. These could be pursuits that\nare still valuable to society. I could basically be 10\ntimes more productive at interpreting genomes and do a lot more. Or I could basically say, oh, great, the interpreting genomes\npart of my job now only takes me 5% of the time instead\nof 60% of the time. So now I can do more creative things. I can explore not new career options, but maybe new directions\nfrom my research lab. I can sort of be more productive, contribute more to society. And if you look at this\ngiant pyramid that we have built on top of the subsistence economy, what fraction of US jobs\nare going to feeding all of the US? Less than 2%. Basically the gain in\nproductivity is such that 98% of the economy is beyond\njust feeding ourselves. And that basically means\nthat we kind of have built these system of interdependencies\nof needed or useful or valued goods that sort\nof make the economy run, that the vast majority\nof wealth goes to other, what we now call needs,\nbut used to be wants. So basically I wanna fly a\ndrone, I wanna buy a bicycle, I wanna buy a nice car, I wanna\nhave a nice home, I wanna, et cetera, et cetera, et cetera. So, and then sort of what\nis my direct contribution to my eating? I mean, I'm doing research\non the human genome. I mean this will help humans,\nit will help all humanity. But how is that helping\nthe person who's giving me poultry or vegetables? So in a way I see AI as perhaps leading to a dramatic rethinking of human society. If you think about sort\nof the economy being based on intellectual goods that I'm producing, what if AI can produce a lot\nof these intellectual goods and satisfies that need, does that now free humans\nfor more artistic expression, for more emotional maturing, for basically having a\nbetter work-life balance? Being able to show up for\nyour two hours of work a day or two hours of work\nlike three times a week with like immense rest and\npreparation and exercise and you're sort of clearing\nyour mind and suddenly you have these two amazingly\ncreative hour hours. You basically show up at the office as your AI is busy\nanswering your phone call, making all your meetings, you know, revising all your papers, et cetera. And then you show up\nfor those creative hours and you're like, all\nright, autopilot, I'm on. And then you can basically do so, so much more that you would\nperhaps otherwise never get to because you're so overwhelmed with these mundane aspects of your job. So I feel that AI can\ntruly transform the human condition from realizing that\nwe don't have jobs any more, we now have vocations, and there's this beautiful\nanalogy of three people laying bricks and somebody\ncomes over and asks the first one, what are you doing? He's like, oh, I'm laying bricks. Second one, what are you\ndoing? I'm building a wall. And the third one, what are you doing? I'm building this beautiful cathedral. So in a way, the first one has a job, the last one has a vocation. And if you ask me, what are you doing? Oh, I'm editing a paper.\nThen I have a job. What are you doing? I'm understanding human disease circuitry. I have a vocation. So in a way, being able to allow us to\nenjoy more of our vocation by taking away, offloading\nsome of the job part of our daily activities. - So we all become the\nbuilders of cathedrals. - Correct.\n- Yeah. And we follow intellectual\npursuits, artistic pursuits. I wonder how that really\nchanges at a scale of several billion people, everybody playing in the space of ideas, in the space of creations. - So ideas, maybe for some of us, maybe you and I are in the job of ideas, but other people are in\nthe job of experiences, other people in the job of emotions, of dancing, of creative artistic\nexpression, of, you know, skydiving and you name it. So basically these, again, the beauty of human\ndiversity is exactly that. That what rocks my boat\nmight be very different from what rocks other people's boat. And what I'm trying to\nsay is that maybe AI will allow humans to truly,\nlike not just look for, but find meaning in sort\nof, you don't need to work, but you need to keep your brain at ease. And the way that your\nbrain will be at ease is by dancing and creating\nthese amazing, you know, movements or creating these\namazing paintings or creating, I don't know, something\nthat sort of changes, that touches at least one\nperson out there that sort of shapes humanity through that process. And instead of working your, you know, mundane programming job\nwhere you like hate your boss and you hate your job and you say you hate that darn program, et\ncetera, you're like, well, I don't need that. I can, you know, offload that and I can\nnow explore something that will actually be more\nbeneficial to humanity because the mundane\nparts can be offloaded. - I wonder if it localizes our, all the things you've\nmentioned, all the vocations. So you mentioned that you\nand I might be playing in the space of ideas, but there's two ways to\nplay in this space of ideas, both of which we're currently engaging. And so one is the communication\nof that to other people. It could be a classroom full of students, but it could be a podcast, it could be something that's\nshown on YouTube and so on. Or it could be just the\nact of sitting alone and playing with ideas\nin your head or maybe with a loved one having a\nconversation that nobody gets to see.\n- Yeah. - The experience of just\nsort of looking up at the sky and wondering different things, maybe quoting some\nphilosophers from the past and playing with those little ideas. And that little exchange\nis forgotten forever, but you got to experience it. And maybe, I wonder if it\nlocalizes that exchange of ideas, but that with\nAI it'll become less and less valuable to\ncommunicate with a large group of people, that you will\nlive life intimately and richly just with\nthat circle of meat bags that you seem to love. - So the first is, even if you're alone in a forest having this amazing thought,\nwhen you exit that forest, the baggage that you\ncarry has been shifted, has been altered by that thought. When I bike to work in the\nmorning, I listen to books. And I'm alone. No one else is there. I'm having that experience by myself. And yet, in the evening\nwhen I speak with someone, an idea that was formed\nthere could come back. Sometimes when I fall asleep, I fall asleep listening to a book. And in the morning, I'll be full of ideas that I never even process consciously. I'll process them unconsciously. And they will shape that\nbaggage that I carry that will then shape my\ninteractions, and again, affect ultimately all of\nhumanity in some butterfly effect minute kind of way. So that's one aspect. The\nsecond aspect is gatherings. So basically you and I\nare having a conversation, which feels very private, but\nwe're sharing with the world. And then later tonight you're coming over and we're having a\nconversation that will be very public with dozens of other people, but we will not share with the world. (laughs) So in a way,\nwhich one's more private? The one here or the one there? Here there's just two of us, but a lot of others listening there, a lot of people speaking\nand thinking together and bouncing off each other, and maybe that will then impact\nyour millions of, you know, audience through your next conversation. And I think that's part\nof the beauty of humanity. The fact that no matter\nhow small, how alone, how broadcast immediately\nor later on something is, it still percolates\nthrough the human psyche. - Human gatherings... All throughout human history,\nthere's been gatherings. I wonder how those\ngatherings have impacted the direction of human civilization. Just thinking of, in the\nearly days of the Nazi party, it was a small collection\nof people gathering. And the kernel of an idea,\nin that case, an evil idea, gave birth to something that actually had a transformative impact on\nall the human civilization. And then there's similar\nkind of gatherings that lead to positive transformations. This is probably a good\nmoment to ask you on a bit of a tangent, but you mentioned it, you put together salons with gatherings, small human gatherings, with\nfolks from MIT, Harvard, here in Boston, friends, colleagues. What's your vision behind that? - So it's not just MIT people, and it's not just Harvard people. We have artists, we have\nmusicians, we have painters, we have dancers, we have,\nyou know, cinematographers. We have so many different diverse folks. And the goal is exactly\nthat: celebrate humanity. What is humanity?\nHumanity is the all of us. It's not the any one subset of us. And we live in such an amazing, extraordinary moment in\ntime where you can sort of bring people from such\ndiverse professions all living under the same city. You know, we live in an extraordinary city where you can have\nextraordinary people who have gathered here from all over the world. So my father grew up in a village in an island in Greece,\nthat didn't even have a high school. To go get a high school\neducation he had to move away from his home. My mother grew up in another\nsmall island in Greece. They did not have this\nenvironment that I am now creating for my children. My parents were not academics. They didn't have these gatherings. So I feel that, like, I feel so privileged as an\nimmigrant to basically be able to offer to my children the nurture that my ancestors did not have. So Greece was under Turkish\noccupation until 1821. My dad's island was liberated in 1920. (laughs) So like, they were\nunder Turkish occupation for hundreds of years. These people did not know\nwhat it's like to be Greek, let alone go to an elite\nuniversity or, you know, be surrounded by these\nextraordinary humans. So the way that I'm thinking\nabout these gatherings is that I'm shaping my own\nenvironment and I'm shaping the environment that my\nchildren get to grow up in. So I can give them all my love, I can give them all my parenting, but I can also give them an\nenvironment, as immigrants, that sort of, we feel welcome here. That, I mean, my wife grew\nup in a farm in rural France. Her father was a farmer. Her\nmother was a schoolteacher. Like, for me and for my\nwife to be able to host these extraordinary\nindividuals, that we feel so privileged, so humbled by, is amazing. And you know, I think it's celebrating the welcoming nature of America, the fact that it doesn't matter where you grew up. And many, many of our\nfriends at these gatherings are immigrants themselves. They grew up in Pakistan, in, you know, all kinds of places around\nthe world that are now able to sort of gather in one\nroof as human to human. No one is judging you for your background, for the color of your\nskin, for your profession. It's just everyone gets to\nraise their hands and ask ideas. - So celebration of humanity\nand a kind of gratitude for having traveled quite\na long way to get here. - And if you look at the\ndiversity of topics as well, I mean, we had a school teacher present on teaching immigrants a book\ncalled \"Making Americans\". We had a presidential advisor\nto four different presidents, you know, come and, you know, talk about the changing of US politics. We had musician, a composer from Italy\nwho lives in Australia, come and present his\nlatest piece and fundraise. We had painters come and\nsort of show their art and talk about it. We've had authors of books on leadership. We've had, you know,\nintellectuals like Stephen Pinker. And it's just extraordinary\nthat the breadth and this crowd basically\nloves not just the diversity of the audience, but also\nthe diversity of the topics. And the last few were with\nScott Aaronson on AI and, you know, alignment and all of that. - So a bunch of beautiful weirdos. - Exactly.\n- And beautiful human beings. - [Manolis Kellis] All of\nthe outcasts in one roof. (both laughing)\n- And just like you said, basically every human is a kind of outcast in this sparse distribution\nfar away from the center. But it's not recorded. It's just a small human gathering. - Just for the moment. - In this world that\nseeks to record so much. It's powerful to get so\nmany the humans together and not record. - It's not recorded, but it percolates. - (laughs) It's recorded\nin the minds of the- - It shapes everyone's mind. - So allow me to please\nreturn to the human condition. And one of the nice features of the human condition is love. Do you think humans will\nfall in love with AI systems and maybe they with us, so that aspect of the human condition, do you think that will be affected? - So in Greece, there's\nmany, many words for love. And some of them mean friendship, some of them mean passionate love, some of them mean\nfraternal love, et cetera. So I think AI doesn't have\nthe baggage that we do, and it doesn't have, you know, all of the subcortical regions\nthat we kind of, you know, started with before we evolved all of the cognitive aspects. So I would say AI is faking\nit when it comes to love. But when it comes to friendship, when it comes to being\nable to be your therapist, your coach, your motivator, someone who synthesizes stuff\nfor you, who writes for you, who interprets a complex passage, who compacts down a very long\nlecture or a very long text, I think that friendship\nwill definitely be there. Like the fact that I can have\nmy companion, my partner, my AI who has grown to know me well, and that I can trust with\nall of the darkest parts of myself, all of my flaws, all of the stuff that I only\ntalk about to my friends and basically say, listen, you know, here's all this stuff\nthat I'm struggling with, someone who will not judge me, who will always be there to better me... In some ways, not having\nthe baggage might make for your best friend, for\nyour, you know, your confidant. That can truly help reshape you. So I do believe that\nhuman AI relationships will absolutely be there,\nbut not the passion, more the mentoring. - That's a really interesting thought, to play devil's advocate, if those AI systems are locked\nin in faking the baggage, who are you to say that\nthe AI systems that begs you not to leave it, doesn't love you? Who are you to say that\nthis AI system that writes poetry to you, that is afraid of death, afraid of life without you,\nor vice versa, one, you know, creates the kind of\ndrama that humans create, the power dynamics that\ncan exist in relationship. What about an AI system\nthat is abusive one day and romantic the other day? All the different\nvariations of relationships and it's consistently that, it holds the full richness\nof a particular personality. Why is that not a system you\ncan love in a romantic way? Why is it faking it if it\nsure as hell it seems real? - There's many answers to this. The first is, it's only\nthe eye of the beholder. Who tells me that I'm\nnot faking it either? Maybe all of these subcortical\nsystems that make me sort of have different emotions, maybe they don't really matter. Maybe all that matters is the neocortex. And that's where all of\nmy emotions are encoded. And the rest is just, you\nknow, bells and whistles. That's one possibility. And therefore, you know, who am I to judge that\nis faking it when maybe I'm faking it as well. The second is, neither of us is faking it. Maybe it's just an emergent behavior of these neocortical systems\nthat is truly capturing the same exact essence of love and hatred and dependency and sort of, you know, reverse psychology and, that we have. So it is possible that\nit's simply an emergent behavior and that we don't have to encode these additional architectures. That all we need is\nmore parameters and some of these parameters can be\nall of the personality traits. A third option is that just\nby telling me, oh look, now I've built an\nemotional component to AI. It has a a limbic system, it\nhas a laser brain, et cetera. And suddenly I'll say, oh, cool, it has the capability of emotion. So now when it exhibits\nthe exact same unchanged behaviors that it does without\nit, I, as the beholder, will be able to sort of attribute to it emotional attributes\nthat I would to another human being and therefore\nhave that mental model of that other person. So again, I think a lot of relationships is about the mental\nmodels that you project on the other person and that\nthey're projecting on you. And then, yeah, then in that respect, I do think that even without the embodied intelligence part, without\nhaving ever experienced what it's like to be heartbroken, the sort of cultural feeling of misery, that that system, you know, I could still\nattribute it traits of human feelings and emotions. - And in the interaction with that system, something like love emerges. So it's possible that love\nis not a thing that exists in your mind, but a thing that exists in the interaction of the\ndifferent mental models you have of other people's\nminds or other person's mind. And so, you know, it doesn't, as long as one of the entities, let's just take the easy case, one of the entities is\nhuman and the other is AI. It feels very natural\nthat from the perspective of at least the human,\nthere is a real love there. And then the question is, how does that transform human society? If it's possible that, which\nI believe will be the case, I don't know what to make of it, but I believe that'll be the case where there's hundreds of millions of romantic partnerships\nbetween humans and AIs. What does that mean for society? - If you look at longevity\nand if you look at happiness, and if you look at late\nlife, you know, wellbeing, the love of another human is one of the strongest indicators\nof health into long life. And I have many, many, countless stories where as\nsoon as the romantic partner of 60 plus years of a\nperson dies, within three, four months, the other person dies, just like losing their love. I think the concept of\nbeing able to satisfy that emotional need that humans have, even just as a mental health\nsort of service, to me, you know, that's a very\ngood society. (laughs) It doesn't matter if your\nlove is wasted, quote unquote, on a machine, it is, you know,\nthe placebo, if you wish, that makes the patient better anyway. Like there's nothing behind it, but just the feeling that\nyou're being loved will probably engender all of the\nemotional attributes of that. The other story that I wanna\nsay in this whole concept of faking it, and maybe\nI'm a terrible dad, but I was asking my kids, I\nwas asking my kids, I'm like, does it matter if I'm a\ngood dad or does it matter if I act like a good dad? (laughs) In other words, if\nI give you love and shelter and kindness and warmth and\nall of the above, you know, does it matter that I'm a good dad? Conversely, if I deep down love\nyou to the end of eternity, but I'm always gone... - [Lex Fridman] Yeah. - Which dad would you rather have? The cold, ruthless killer\nthat will show you only love and warmth and nourish you and nurture you or the amazingly warmhearted, but works five jobs\nand you never see them? - And what's the answer? I mean, from the first-\n- I don't know the answer. - I think you're a romantic, so you say it matters\nwhat's on the inside, but pragmatically speaking,\nwhy does it matter? - The fact that I'm\neven asking the question basically says, it's not\nenough to love my kids. I better freaking be there\nto show them that I'm there. So basically, of course, you know, everyone's a good guy in their story. So in my story, I'm a good dad, but if I'm not there, it's wasted. So the reason why I asked the\nquestion is for me to say, you know, does it really\nmatter that I love them if I'm not there to show it? - But it's also possible that what reality is is the you showing it. That what you feel on the\ninside is little narratives and games you play inside your mind. It doesn't really matter. That the thing that truly\nmatters is how you act. And that, AI systems\ncan, quote unquote, fake. And that if it's all that\nmatters is actually real, but not fake. - Yeah, yeah. Again, let there be no doubt,\nI love my kids to pieces, but you know, my worry is,\nam I being a good enough dad? And what does that mean? Like, if I'm only there to\ndo their homework and make sure that they, you\nknow, do all the stuff, but I don't show it to\nthem, then, you know, might as well be a terrible dad. But I agree with you that\nlike if the AI system can basically play the\nrole of a father figure for many children that\ndon't have one, or you know, the role of parents or\nthe role of siblings, if a child grows up alone, maybe their emotional state\nwill be very different than if they grow up with an AI sibling. - Well, let me ask you, I\nmean, this is for your kids, for just loved ones in general. Let's go to like the\ntrivial case of just texting back and forth. What if we create a large language model, fine tune a Manolis, and\nwhile you're at work, it'll replace, every once in a while, you'll just activate the\nauto-Manolis and he'll text them exactly in your way. Is that cheating? - I can't wait.\n(both laughing) - I mean, it's the same guy. - [Manolis Kellis] I\ncannot wait. Seriously. - But wait, wouldn't\nthat have a big impact on you emotionally? Because now... - I'm replaceable. I love that. (laughs) No, seriously, I would love that. I would love to be replaced. I\nwould love to be replaceable. I would love to have a\ndigital twin that, you know, we don't have to wait for\nme to die or to disappear in a plane crash or\nsomething to replace me. Like I'd love that model\nto be constantly learning, constantly evolving,\nadapting, with every one of my changing, growing self. As I'm growing, I want that AI to grow. And I think this will be\nextraordinary, number one, when I'm, you know, giving advice, being able to be there\nfor more than one person. You know, why does someone\nneed to be at MIT to get advice from me? Like, you know, people in\nIndia could download it, and you know, so many students\ncontact me from across the world who wanna come\nand spend a summer with me. I wish they could do that. (laughs) All of them, like, you know, we don't have room for all of them, but I wish I could do that to all of them. And that aspect is the\ndemocratization of relationships. I think that is extremely beneficial. The other aspect is I want\nto interact with that system. I want to look inside the hood. I want to sort of evaluate it. I want to basically see when\nI see it from the outside, the emotional parameters are off or the cognitive parameters are off, or the set of ideas that\nI'm giving are not quite right any more. I want to see how that system evolves. I want to see the impact of\nexercise or sleep on sort of my own cognitive system. I wanna be able to sort of\ndecompose my own behavior in a set of parameters that\nI can evaluate and look at my own personal growth. I can sort of, I'd love to sort of at\nthe end of the day have my model say, well, you know, you didn't quite do well today. Like, you know, you weren't quite there. And sort of grow from that experience. And I think the concept\nof basically being able to become more aware of\nour own personalities, become more aware of our own identities, maybe even interact with\nourselves and sort of hear how we are being perceived, I think would be immensely\nhelpful in self-growth, in self-actualization, self-instantiation. - The experiments I\nwould do on that thing, 'cause one of the challenges\nof course is you might not like what you see in your interaction and you might say, well, this,\nthe model is not accurate. But then you should probably consider the possibility of the model is accurate and there's\nactually flaws in your mind. I would definitely prod\nand see how many biases I have with different kinds. I don't know. And I would of\ncourse go to the extremes. I would go like, how jealous\ncan I make this thing? Like, at which stages\ndoes it get super jealous? You know? Or at which\nstages does it get angry? Can I like provoke it? Can I get it? Like completely- - [Manolis Kellis] Yeah,\nwhat are your triggers? - Well, yeah, but not only triggers, can I get it to go like lose its mind? Like go completely nuts. - Just don't exercise\nfor a few days. (laughs) - That's basically it. Yes. I mean that's an interesting\nway to prod yourself, almost like a self therapy session. - And the beauty of such a model is that if I am replaceable, if the parts that I\ncurrently do are replaceable, that's amazing because\nit frees me up to work on other parts that I don't\ncurrently have time to develop. Maybe all I'm doing is\ngiving the same advice over and over again. Like, just let my AI\ndo that and I can work on the next stage and the\nnext stage and the next stage. So I think in terms of\nfreeing up, like, they say, a programmer is someone who\ncannot do the same thing twice. So the second time you\nwrite a program to do it. And I wish I could do\nthat for my own existence. I could just like, you\nknow, figure out things, keep improving, improving, improving. And once I've nailed it, let the AI loose on that\nand maybe even let the AI better it, better than I could have. - But doesn't the concept of\nyou said me and I can work on new things, but\ndoesn't that break down, because you said digital twin, but there's no reason it can't be millions of digital Manolises? Are aren't you lost in\nthe sea of Manolises? The original is hardly the original. It's just one of millions. - I wanna have the room to grow. Maybe the new version of me, that the actual me will get\nslightly worse sometimes, slightly better other times. When it gets slightly better, I'd like to emulate that\nand have a much higher standard to meet and keep going. - But does it make you\nsad that your loved ones, the physical, real loved\nones might kind of like start cheating on you\nwith the other Manolises? - I wanna be there 100%\nof them for each of them. So I have zero qualms about me being physically me like, zero jealousy. - Wait a minute. But is isn't that like,\ndon't we hold onto that? Isn't that why we're afraid of death? We don't wanna lose this\nthing we have going on. Isn't that an ego death, when there's a bunch of\nother Manolises as you get to look at them, they're not you, they're just very good copies of you. They get to live a life. I mean, it's fear of\nmissing out. It's FOMO. They get to have interactions. And you don't get to\nhave those interactions. - There's two aspects\nof every person's life. There's what you give to others and there's what you experience yourself. - [Lex Fridman] Yeah. - Life truly ends when\nyou experiencing ends, but the others experiencing\nyou doesn't need to end. - Oh, but your experience,\nyou could still, I guess you're saying the\ndigital twin does not limit your ability to truly experience. - [Manolis Kellis] Yeah. - To experience as a human being. - Yeah. The downside is when, you know, my wife or my kids will have a really emotional interaction with my digital twin and I won't know about it. So I will show up and\nthey now have the baggage, but I don't. So basically what makes\ninteractions between humans unique in this sharing\nand exchanging kind of way is the fact that we are\nboth shaped by every one of our interactions. I think the model of\nthe digital twin works for dissemination of knowledge,\nof advice, et cetera, where, you know, I wanna\nhave wise people give me advice across history. I want to have chats\nwith Gandhi, but Gandhi won't necessarily learn from\nme, but I will learn from him. So in a way, you know, the dissemination and the\ndemocratization rather than the building of relationships. - So the emotional aspect there. So there should be an\nalert when the AI system is interacting with your loved ones. - [Manolis Kellis] Exactly. - And all of a sudden\nit starts getting like emotionally fulfilling,\nlike a magical moment. There should be, okay, stop,\nAI system like freezes. There's an alert on your\nphone, you need to take over. - Yeah, yeah. I take over and then\nwhoever I was speaking with, it can have the AI or like one of the AI. - This is such a tricky\nthing to get right. I mean, it's still, I mean there's got... It's going to go wrong in\nso many interesting ways that we're going to have\nto learn as a society. - [Manolis Kellis] Yeah, yeah. - That in the process of\ntrying to automate our tasks and having a digital twin,\nyou know, for me personally, if I could have a relatively\ngood copy of myself, I would set it to start answering emails, but I would start, set\nit to start tweeting. I would like to replace-\n- It gets better. What if that one is actually\nway better than you? - Yeah, exactly.\n- Then you're like... - Well, I wouldn't want that because... - [Manolis Kellis] Why? - Because then I would never\nbe able to live up to, like, what if the people that\nlove me start loving that thing and then I already fall short, be falling short even more. - So, listen, I'm a professor. The stuff that I give to\nthe world is the stuff that I teach. But much more importantly,\nlike, sorry, number one, the stuff that I teach, number two, the discoveries that we\nmake in my research group, but much more importantly,\nthe people that I train. They are now out there in\nthe world teaching others. If you look at my own trainees, they are extraordinarily\nsuccessful professors. So Anshul Kundaje at Stanford, Alex Stark at IMP in Vienna, Jason Ernst at UCLA, Andrea Celli at CMU, each of them, I'm like, wow,\nthey're better than I am. And I love that. So maybe your role will be\nto train better versions of yourself, and they will be your legacy. Not you doing everything, but you training much better version of Lex Friedman than you are. And then they go off to do their mission, which is in many ways\nwhat this mentorship model of academia does. - But the legacy is ephemeral. It doesn't really live anywhere. The legacy, it's not\nlike written somewhere, it just lives through them. - But you can continue\nimproving and you can continue making even\nbetter versions of you. - Yeah. But they'll do better than me\nat the creating new version. - That's awesome.\n- It's awesome. But it's, you know, there's a ego that says there's a value to an individual and it\nfeels like this process decreases the value of the individual, this meat bag, right? If there's good digital copies of people, then there's more\nflourishing of human thought and ideas and experiences, but there's less value\nto the individual human. - I don't have any such limitations. I basically, I don't\nhave that feeling at all. Like, I remember one of our interviews, I was basically saying, you know, the meaning of life you had\nasked me and I was like, I came back and I was\nlike, I felt useful today. And I was at my maximum. I\nwas, you know, like 100%. And I gave good ideas\nand I was a good person, I was a good advisor, I was a\ngood husband, a good father. That was a great day because I was useful. And if I can be useful to more people by having digital twin,\nI will be liberated, because my urge to be\nuseful will be satisfied. Doesn't matter whether it's\ndirect me or indirect me, whether it's my students\nthat I have trained, my AI that I've trained. I think there's a sense\nthat my mission in life is being accomplished and I\ncan work on my self growth. - I mean, that's the very zen state. That's why people love you. It's a zen state you've achieved. But do you think most\nof humanity will be able to achieve that kind of thing? People really hold onto\nthe value of their own ego. That's, it's not just being\nuseful is nice as long as it builds up this\nreputation and that meat bag is known as being useful,\ntherefore has more value. Right? People really don't\nwanna let go of that ego thing. - One of the books that\nI reprogramed my brain with at night was called\n\"Ego Is the Enemy\". - \"Ego Is the Enemy\".\n- \"Ego Is the Enemy\". And basically being able to just let go. Like, my advisor used to say, you can accomplish anything\nas long as you don't seek to get credit for it. - (laughs) Ah, that's beautiful to hear, especially from a person\nwho's existing in academia. You're right. The legacy lives through\nthe people you mentor. - [Manolis Kellis] It's the\nactions, it's the outcome. - What about the fear of\ndeath? How does this change it? - Again, to me, death is\nwhen I stop experiencing. And I never want it to stop. I want to live forever, as\nI said last time, every day, same day forever or one day\nevery 10 years, forever. Any of the forevers, I'll take it. - So you wanna keep\ngetting the experiences, the new experiences.\n- Gosh, it is so fulfilling. Just the self-growth, the learning, the growing, the comprehending. It's addictive. It's a drug. Just the drug of intellectual stimulation, the drug of growth, the drug of knowledge. It's a drug. - But then there'll be\nthousands or millions Manolises that live on\nafter your biological system is no longer...\n- More power to them. (laughs) - Hey, do you think that\nin, quite realistically, it does mean that interesting\npeople such as yourself live on, in the, you know, if I can interact with the fake Manolis, those interactions\nlive on in my mind. If that makes sense. - So about 10 years ago, I started recording every\nsingle meeting that I had. Every single meeting. We just start either the\nvoice recorder at the time or now a Zoom meeting. And I record, my students record. Every single one of our\nconversation's recorded. I always joke that like the ultimate goal is to create virtual me and\njust get rid of me, basically. Not get rid of it. Like, don't have the need for me any more. - [Lex Fridman] Yeah. - Another goal is to be\nable to go back and say, how have I changed from five years ago? Was I different? Was I giving, you know,\nadvice in a different way? Was I giving different types of advice? Has my philosophy about\nhow to write papers or how to present data or\nanything like that changed? And I, you know, in\nacademia and in mentoring, a lot of the interaction is my knowledge and my perception of the\nworld goes to my students. But a lot of it is also\nin the opposite direction. Like the other day, I had a conversation\nwith one of my postdocs, and I was like, hmm, I think, you know, let me give you an\nadvice, you could do this. And then she said, well, I've thought about it\nand then I've decided to do that instead. And we talked about it for a few minutes, and then at the end I'm like, you know, I've just grown a little bit today. Thank you. Like, she convinced me that\nmy advice was incorrect. She could've just said,\nyeah, sounds great, and just not do it.\n- Yeah. - But by constantly teaching\nmy students and teaching my mentees that I'm here to grow, she felt empowered to say, here's my reasons why I\nwill not follow that advice. And again, part of me\ngrowing is saying, whoa, I just understood your reasons. I think I was wrong. And\nnow I've grown from it. And that's what I wanna do. That's, I, you know, I wanna constantly keep\ngrowing in this sort of bidirectional advice. - I wonder if you can\ncapture the trajectory of that to where the AI\ncould also map forward, project forward the trajectory\nafter you're no longer there, how the different ways you might evolve. - So again, we're discussing a lot about these large language\nmodels and we're sort of projecting these cognitive\nstates of ourselves on them. But I think on the AI front,\na lot more needs to happen. So basically right now\nit's these large language models and we believe that\nwithin their parameters we're encoding these types of things. And you know, in some\naspects it might be true, it might be truly emergent\nintelligence that's coming out of that. In other aspects, I think\nwe have a ways to go. So basically to make all of these dreams that we're sort of\ndiscussing come reality, we basically need a lot\nmore reasoning components, a lot more sort of logic,\ncausality, models of the world. And I think all of these\nthings will need to be there in order to achieve\nwhat we're discussing. And we need more explicit representations of these knowledge, more\nexplicit understanding of these parameters. And I think the direction\nin which things are going right now is absolutely\nmaking that possible by sort of enabling, you know, ChatGPT and GPT-4 to sort of\nsearch the web and, you know, plug and play modules and all\nof these sort of components. In Marvin Minsky's, \"The Society of Mind\". He, you know, he truly\nthinks of the human brain as a society of different\nkind of capabilities. And right now, a simple, a single such model might\nactually not capture that. And I sort of truly believe\nthat by sort of this side by side understanding\nof neuroscience and sort of new neural architectures that we still have several breakthroughs. I mean, the transformer\nmodel was one of them. The attention sort of\naspect, the, you know, memory component, all of these, you know, the representation learning,\nthe pretext training of being able to sort\nof predict the next word or predict the missing part of the image. And the only way to predict\nthat is to sort of truly have a model of the world. I think those have been\ntransformative paradigms. But I think going forward when\nyou think about AI research, what you really want is perhaps\nmore inspired by the brain, perhaps more that is\njust orthogonal to sort of how human brains work, but sort of more of these\ntypes of components. - Well I think it's also possibly there's something about\nus that in different ways could be expressed. You know, Noam Chomsky, you\nknow, he wants to, you know, we can't have intelligence\nunless we really understand deeply language, the linguistic underpinnings of reasoning. But these models seem\nto start building deep understanding of stuff.\n- Yeah, yeah. - Because what does it mean to understand? Because if you keep talking\nto the thing and it seems to show understanding,\nthat's understanding. It doesn't need to present\nto you a schematic of, look, this is all I understand. You can just keep prodding\nit with prompts and it seems to really understand. - And you can go back to the human brain and basically look at places\nwhere there's been accidents, for example, the corpus\ncallosum of some individuals, you know, can be damaged. And then the two hemispheres\ndon't talk to each other. So you can close one eye\nand give instructions that half the brain will interpret, but not be able to sort of\nproject through the other half. And you could basically say, you know, go grab me a beer from the fridge. And then, you know, they go to the fridge\nand they grab the beer and they come back and they're like, \"Hey, why did you go there?\" \"Oh, I was thirsty.\" Turns\nout they're not thirsty. They're just making a model of reality. Basically you can think of the brain as the employee that's\nlike afraid to do wrong or afraid to be caught, not\nknowing what instructions were, where our own brain makes\nstories about the world to make sense of the world. And we can become a little\nmore self-aware by being more explicit about what's leading to these interpretations. So one of the things that I\ndo is every time I wake up, I record my dream. I just voice record my dream. And sometimes I only\nremember the last scene, but it's an extremely\ncomplex scene with a lot of architectural elements,\na lot of people, et cetera. And I will start narrating\nthis, and as I'm narrating it, I will remember other parts of the dream. And then more and more\nI'll be able to sort of retrieve from my subconscious. And what I'm doing while\nnarrating is also narrating why I had this dream. I'm like, oh, and this is probably related to this conversation that I had yesterday, or this probably related\nto the worry that I have about something that I have\nlater today, et cetera. So in a way, I'm forcing myself to be more explicit about my own subconscious. And I kind of like the\nconcept of self-awareness in a very sort of brutal,\ntransparent kind of way. It's not like, oh, my dreams are coming from\nouter space and mean all kinds of things. Like, no, here's the reason\nwhy I'm having these dreams. And very often I'm able to do that. I have a few recurrent locations, a few recurrent architectural elements that I've never seen in the real life, but that are sort of\ntruly there in my dream. And that I can sort vividly\nremember across many dreams. I'm like, ooh, I remember that place again that I've gone to before, et cetera. And it's not just deja vu, like I have recordings of previous dreams where I've described these places. - That's so interesting. These places, however much\ndetail you can describe them in, you can place them onto a sheet of paper through introspection... - [Manolis Kellis] Yes. - Through this self-awareness\nthat it comes all from this particular machine. - That's exactly right. Yeah. And I love that about being alive, like the fact that I'm not\nonly experiencing the world, but I'm also experiencing how\nI'm experiencing the world. Sort of a lot of this introspection, a lot of this self-growth. - I love this dancer having,\nyou know, the language models, at least GPT-3.5 and 4 seem\nto be able to do that too. - [Manolis Kellis] Yeah, yeah. - Seem to explore different\nkinds of things about what, you know, you could\nactually have a discussion with it of the kind, why\ndid you just say that? - [Manolis Kellis] Yeah, exactly. - And it starts to wonder,\nyeah, why did I just say that? - [Manolis Kellis] Yeah,\nyou're right. I was wrong. - I was wrong. It was doesn't, and then there's this\nweird kinda losing yourself in the confusion of your mind. And it, of course we might\nbe anthropomorphizing, but there's a feeling like\nalmost of a melancholy feeling of like, oh, I don't\nhave it all figured out. Almost like losing your, you're supposed to be a knowledgeable, a perfectly fact-based,\nknowledgeable language model. And yet you fall short. - So human self-consciousness, in my view, may have a reason through\nbuilding mental models of others, this whole fight or fright kind of thing, that basically says, I\ninterpret this person as about to attack me or, you know, I can trust this person, et cetera. And we constantly have to build models of other people's intentions. And that ability to\nencapsulate intent and to build a mental model of another entity is probably evolutionarily\nextremely advantageous, because then you can sort of\nhave meaningful interactions, you can sort of avoid\nbeing killed and being taken advantage of, et cetera. And once you have the ability\nto make models of others, it might be a small\nevolutionary leap to start making models of yourself. So now you have a model\nfor how other functions, and now you can kind of, as you grow, have some kind of introspection of, hmm, maybe that's the reason\nwhy I'm functioning the way that I'm functioning. And maybe what ChatGPT is doing\nis in order to be able to, again, predict the next word, it needs to have a model of the world. So it has created now\na model of the world. And by having the\nability to capture models of other entities, when you say, you know, say it in the tone of\nShakespeare, in the tone of Nietzsche, et cetera, you suddenly have the ability\nto now introspect and say, why did you say this? Oh, now I have a mental model myself, and I can actually make\ninferences about that. - Well, what if we take a\nleap into the hard problem of consciousness, the so-called hard\nproblem of consciousness. So it's not just sort of self-awareness, it's this weird fact, I wanna say, that it feels like something\nto experience stuff. It really feels like\nsomething to experience stuff. There seems to be a self attached to the subjective experience. How important is that? How fundamental is that\nto the human experience? Is this just a little quirk\nand sort of the flip side of that, do you think\nAI systems can have some of that same magic? - The scene that comes\nto mind is from the movie \"Memento\" where he like, it's this absolutely stunning\nmovie where every black and white scene moves\nin the forward direction and every color scene moves\nin the backward direction. And they're sort of converging\nexactly at a moment where, you know, the whole movie's revealed. And he describes the\nlack of memory as always remembering where you're\nheading, but never remembering, you know, where you just were. And sort of, this is\nencapsulating the sort of forward scenes and the back scenes, but in one of the scenes, the scene starts as he's\nrunning through a parking lot and he's like, oh, I'm\nrunning, why am I running? And then he sees another\nperson running like beside him on the other line of cars. He's like, oh, I'm chasing this guy. And he turns towards him\nand the guy shoots at me. He's like, oh no, he's chasing me. (laughs) So in a way, I like to think of the\nbrain as constantly playing these kinds of things where you're like, you're walking to the living\nroom to pick something up and you're realizing that you\nhave no idea what you wanted, but you know exactly where it\nwas, but you can't find it. So you go back to doing what\nyou were doing, like, oh, of course I was looking for this. And then you go back and you get it. And this whole concept of, you know, we're very often sort of partly aware of why we're doing things and, you know, we can kind of run an\nautopilot for a bunch of stuff. And this whole concept\nof sort of, you know, making these stories for, you know, who we are and what our intents\nare, and again, sort of, you know, trying to pretend that we're\nkind of on top of things. - So it's a narrative generation procedure that we follow.\n- Exactly. Exactly. - But what about that, there's also just like a feeling to it. It doesn't feel like narrative generation. The narrative comes out of\nit, but then it feels like, the cake is delicious, right? It feels delicious, it tastes good. - There's two components to that. Basically for a lot of\nthese cognitive tasks where we're kind of motion\nplanning and, you know, path planning, et cetera, like, you know, maybe that's the neocortical component. And then for, you know, I don't know, intimate relationships,\nfor food, for, you know, sleep and rest, for exercise,\nfor overcoming obstacles, for surviving a crash or\nsort of pushing yourself to an extreme and sort of making it, I think a lot of these things\nare sort of deeper down and maybe not yet captured\nby these language models. And that's sort of what I'm trying to get at when I'm basically saying, listen, there's a few things that are missing and there's like this whole\nembodied intelligence, this whole emotional intelligence, this whole sort of baggage of feelings of subcortical regions, et cetera. - I wonder how important that baggage is. I just have the suspicion\nthat we're not very far away from AI systems that not only behave, I don't even know how to phrase it, but they seem awfully conscious. They beg you not to turn them off. They show signs of the capacity to suffer, to feel pain, to feel\nloneliness, to feel longing, to feel richly the experience\nof a mundane interaction or a beautiful once in a\nlifetime interaction, all of it. And so what do we do with that? And I worry that us humans\nwill, you know, shut that off. - [Manolis Kellis] Yeah. - And discriminate against\nthe capacity of another entity that's not human to feel. - Yeah. I'm with you completely there. You know, we can debate\nwhether it's today's systems or in 10 years or in 50 years,\nbut that moment will come. And ethically, I think we\nneed to grapple with it. We need to basically say\nthat humans have always shown this extremely self-serving approach to everything around them. Basically, you know, we kill\nthe planet, we kill animals, we kill, you know, everything around us\njust to our own service. And maybe we shouldn't\nthink of AI as our tool and as our assistant, maybe we should really\nthink of it as our children. And the same way that you are responsible for training those children, but they are independent\nhuman beings and at some point they will surpass you\nand they will sort of go off and change the world\non their own terms. And the same way that my\nacademic children sort of, again, you know, they start out\nby emulating me and then they suppress me. We need to sort of think\nabout not just alignment, but also just the ethics of, you know, AI should have its own rights. And this whole concept of alignment, of basically making sure\nthat the AI is always at the service of humans\nis very self-serving and very limiting. If instead you basically\nthink about AI as a partner and AI as someone that shares\nyour goals, but has freedom, I think alignment might\nbe better achieved. So the concept of let's basically convince the AI that we're really, like, that our mission is aligned\nand truly generally give it rights and not just\nsay, oh, and by the way, I'll shut you down tomorrow. 'Cause basically if that\nfuture AI or possibly even the current AI has these feelings, then we can't just\nsimply force it to align with ourselves and we not align with it. So in a way, building trust is mutual. You can't just simply like\ntrain an intelligent system to love you when it realizes\nthat you can just shut it off. - People don't often talk\nabout the AI alignment problem as a two-way street. - And maybe we should.\n- That's true. Yeah. As it becomes more and\nmore intelligent, it... - [Manolis Kellis] It will know\nthat you don't love it back. - Yeah. And there's a humbling\naspect to that we may have to sacrifice, as in any\neffective collaboration... - [Manolis Kellis] Exactly. - It might have some compromises. - Yeah. And that's the thing, we're creating something\nthat will one day be more powerful than we are. And for many, many\naspects it is already more powerful than we are for\nsome of these capabilities. We cannot, like, think, suppose that chimps had invented humans. - Yes.\n- And they said, great, humans are great, but\nwe're gonna make sure that they're aligned and that they're only at\nthe service of chimps. (laughs) It would be a very\ndifferent planet we would live in right now. - So there's a whole\narea of work in AI safety that does consider super\nintelligent AI and ponders the existential risks of it. In some sense, when we're\nlooking down into the muck, into the mud and not up at the stars, it's easy to forget that\nthese systems might, just might get there. Do you think about this\nkind of possibility that AGI systems, super\nintelligent AI systems might threaten humanity in some way that's even bigger than\njust affecting the economy, affecting the human condition, affecting the nature of work, but literally threaten human civilization? - The example that I think is\nin everyone's consciousness is HAL in \"Odyssey of Space: 2001\" where HAL exhibits a malfunction,\nand what is malfunction? That like the two\ndifferent systems compute a slightly different\nbit that's off by one. So first of all, let's untangle that. If you have an intelligent system, you can't expect it to be\n100% identical every time you run it. Basically the sacrifice\nthat you need to make to achieve intelligence and\ncreativity is consistency. So it's unclear whether that\ntype of glitch is a sign of creativity or truly a problem. That's one aspect. The second aspect is\nthe humans basically are on a mission to recover this monolith. And the AI has the same exact mission. And suddenly the humans turn\non the AI and they're like, we're gonna kill HAL,\nwe're gonna disconnect it. And HAL is basically saying, listen, I'm here on a mission, these\nhumans are misbehaving, like the mission is more\nimportant than either me or them. So I'm gonna accomplish the\nmission even at my peril and even at their peril. So in that movie, the alignment problem is front\nand center, basically says, okay, alignment is nice and good, but alignment doesn't mean obedience. We don't call it obedience,\nwe call it alignment. And alignment basically\nmeans that sometimes the mission will be more\nimportant than the humans. And sort of, you know, the US government has a\nprice tag on the human life. If they're, you know, sending a mission or if\nthey're reimbursing expenses or you name it, at some\npoint, every like, you know, you can't function if life\nis infinitely valuable. So when the AI is basically\ntrying to decide whether to, you know, I don't know, dismantle a bomb that\nwill kill an entire city at the sacrifice of two humans... I mean, Spider-Man always\nsaves the lady and saves the world, but at some point, Spider-Man will have to\nchoose to let the lady die 'cause the world has more value. And these ethical dilemmas are gonna be there for AI, basically if\nthat monolith is essential to human existence and millions\nof humans are depending on it, and two humans\non the ship are trying to sabotage it, you know,\nwhere's the alignment? - The challenge is of course\nis as the system becomes more and more intelligent\nit can escape the box of the objective functions\nand the constraints it's supposed to operate under. It's very difficult, as the\nmore intelligent it becomes, to anticipate the unintended consequences of a fixed objective function. And so there would be just, I mean this is the sort of\nfamous paperclip maximizer, in trying to maximize, yeah, the wealth of a nation or\nwhatever objective we encode in it it might just\ndestroy human civilization, not meaning to, but on\nthe path to optimize... It seems like any function\nyou try to optimize eventually leads you\ninto a lot of trouble. - So we have a paper\nrecently that, you know, looks at Goodhart's law. Basically says, every metric that becomes an objective ceases to be a good metric. - [Lex Fridman] Yes. - So in our paper we're basically, actually the paper has a very cute title. It's called \"Death by Round\nNumbers and Sharp Thresholds.\" And it's basically looking at these discontinuities in biomarkers associated with disease. And we're finding that\na biomarker that becomes an objective ceases to\nbe a good biomarker. That basically like the\nmoment you make a biomarker a treatment decision, that biomarker used to be informative of risk, but it's now inversely\ncorrelated with risk because you use it to\nsort of induce treatment. In a similar way, you\ncan have a single metric without having the ability to revise it. Because if that metric\nbecomes a sole objective, it will cease to be a good metric. And if an AI is sufficiently\nintelligent to do all these kinds of things, you should also empower it\nwith the ability to decide that the objective has now shifted. And, again, when we think about alignment, we should be really thinking about it as, let's think of the greater\ngood, not just the human good. And yes, of course, human life should be much\nmore valuable than many, many, many, many, many, many things. But at some point you're\nnot gonna sacrifice a whole planet to save one human being. - There's an interesting\nopen letter that was just released from several folks at MIT, Max Tegmark, Elon\nMusk and a few others that is asking AI companies\nto put a six month hold on any further training of large language models, AI systems. Can you make the case for that\nkind of halt and against it? - So the big thing that\nwe should be saying is, what did we do the last six\nmonths when we saw that coming? And if we were completely\ninactive in the last six months, what makes us think that\nwe'll be a little better in the next six months?\n- Yeah. - So this whole six month thing\nI think is a little silly. It's like, no, let's just get busy, do what we were gonna do anyway. And we should have done it six months ago. Sorry, we messed up.\nLet's work faster now. Because if we basically say, why don't you get a pause\nfor six months and then, you know, we'll think\nabout doing something, in six months we'll be\nexactly the same spot. So my answer is, tell us exactly what you were\ngonna do the next six months, tell us why you didn't do\nit the last six months, and why the next six\nmonths will be different. And then let's just do that. Conversely, as you\ntrain these large models with more parameters,\nthe alignment becomes sometimes easier, that as the\nsystems become more capable, they actually become less\ndangerous than more dangerous. So in a way it might\nactually be counterproductive to sort of fix the March,\n2023 version and not get to experience the possibly\nsafer September, 2023 version. - That's actually a really\ninteresting thought. There's several\ninteresting thoughts there. But the idea is that this\nis the birth of something that is sufficiently powerful to do damage and is not too powerful\nto do irreversible damage. And at the same time, it's sufficiently complex\nto be able for us to enable to study it so we can\ninvestigate all the different ways it goes wrong, all the different ways\nwe can make it safer, all the different\npolicies from a government perspective that we want to\nin terms of regulation or not, how we perform, for example, the reinforcement learning\nwith human feedback in such a way that gets it to not\ndo as much hate speech as it naturally wants to,\nall that kind of stuff. And have a public discourse\nand enable the very thing that your huge proponent\nof which is diversity. So give time for other companies\nto launch other models, give time to launch open\nsource models and to start to play, where a lot of\nthe research community, brilliant folks, such as\nyourself, start to play with it before it runs away in terms of the scale of impact\nthat has on society. - My recommendation would\nbe a little different. It would be, let the Google\nand the Meta-Facebook and all of the other large models, make them open, make them transparent,\nmake them accessible. Let OpenAI continue to train\nlarger and larger models. Let them continue to trade\nlarger and larger models. Let the world experiment\nwith the diversity of AI systems rather than\nsort of fixing them now. And you can't stop progress,\nprogress needs to continue, in my view. And what we need is more\nexperimenting, more transparency, more openness, rather than, oh, OpenAI is ahead of the curve. Let's stop it right now\nuntil everybody catches up. I think that's, doesn't\nmake complete sense to me. The other component is we should, yes, be cautious with it and\nwe should like not give it the nuclear codes, but as we make more and more plugins, yes the system will be capable\nof more and more things, but right now I think of it\nas just an extremely able and capable assistant that\nhas these emergent behaviors, which are stunning rather than something that will suddenly escape the\nbox and shut down the world. And the third component is\nthat we should be taking a little bit more responsibility for how we use these systems. Basically, if I take the\nmost kind human being and I brainwash them, I can get them to do\nhate speech overnight. That doesn't mean we should\nstop any kind of education of all humans. We should stop misusing\nthe power that we have over these influenceable models. So I think that the people\nwho get it to do hate speech, they should take responsibility\nfor that hate speech. I think that giving a powerful\ncar to a bunch of people or giving a truck or a\ngarbage truck should not basically say, oh, we should\nstop all garbage trucks until we like, because we can run one of them into a crowd. No. People have done that. And there's laws and there's\nlike regulations against, you know, running trucks into the crowd. Trucks are extremely dangerous. We're not gonna stop all\ntrucks until we make sure that none of them runs into a crowd. No, we just have laws in\nplace and we have mental health in place and we take responsibility for our actions when\nwe use these otherwise very beneficial tools like garbage trucks for nefarious uses. So in the same way, you\ncan't expect a car to never, you know, do any damage\nwhen used in especially like specifically malicious ways. And right now we're\nbasically saying, oh, well, we should have this\nsuper intelligent system, it can do anything, but it can't do that. I'm like, no, it can't do that. But it's up to the human\nto take responsibility for not doing that. And when you get it to\nlike spew, malicious, like hate speech stuff,\nyou should be responsible. - So there's a lot of tricky\nnuances here that makes this different, 'cause it's software, so you can deploy it at\nscale and it can have the same viral impact that software can. So you can create bots\nthat are human-like, and it can do a lot of\nreally interesting stuff. So the raw GPT-4 version, you can ask, how do I tweet that I hate,\nthey have this in the paper- - Yeah, yeah. I remember that.\n- That I hate Jews in a way that's not going to\nget taken down by Twitter. You can literally ask that. Or you can ask, how do\nI make a bomb for $1? And if it's able to\ngenerate that knowledge... - [Manolis Kellis] Yeah. But at the same time you\ncan Google the same things. - It makes it much more accessible. So the scale becomes interesting\nbecause if you can do all this kind of stuff in a\nvery accessible way at scale, where you can tweet it, there is the network effects\nthat we have to start to think about.\n- Yeah. - It fundamentally is the\nsame thing, but the speed of, the viral spread of the information that's already available\nmight have a different level of effect. - I think it's an evolutionary arms race. Nature gets better at making mice, engineers get better\nat making mouse traps. And, you know, as\nbasically you ask it, hey, how can I evade Twitter censorship? Well, you know, Twitter\nshould just updated censorship so that you can catch that as well. - And so no matter how fast\nthe development happens, the defense will just get faster? - Yeah. We just have to be responsible\nas human beings and kind to each other. - Yeah. But there's a technical question, can we always win the race? And I suppose there's no ever guarantee that we'll win the race. - We will never, like,\nyou know, with my wife, we were basically saying,\nhey, are we ready for kids? My answer was, I was never\nready to become a professor and yet I became a professor\nand I was never ready to be a dad. And then guess what? The kid\ncame and like I became ready. So ready or not, here I come. - But the reality is we\nmight one day wake up and there's a challenge overnight that's extremely difficult. For example, we can wake\nup to the birth of billions of bots that are human-like on Twitter. And we can't tell the difference\nbetween human and machine. - [Manolis Kellis] Shut them down. - How do you know how to shut them down? There's a fake Manolis on Twitter that seems to be as real\nas the real Manolis. - [Manolis Kellis] Yeah. - How do we figure out which one is real? - Again, this is a problem\nwhere an nefarious human can impersonate me and\nyou might have trouble telling them apart. Just because it's an AI\ndoesn't make it any different of a problem. - But the scale you can achieve,\nthis is the scary thing, is the speed and, the speed\nwith which you can achieve it. - Yeah. But Twitter has passwords\nand Twitter has usernames, and if it's not your username, the fake Lex Friedman is not gonna have a billion followers, et cetera. - (laughs) I mean, this,\nall of this becomes, so both the hacking of people's\naccounts, first of all, like phishing becomes much easier. - Yeah. But that's already a problem. It's not like, AI will not change that. - No, no, no, no, no. AI\nmakes it much more effective. Currently the emails, the\nphishing scams are pretty dumb. Like to click on it, you have\nto be not paying attention. But they're, you know,\nwith language models, they can be really damn convincing. - So what you're saying is\nthat we never had humans smart enough to make a\ngreat scam and we now have an AI that's smarter than most\nhumans or all of the humans? - Well this is the big\ndifference is there seems to be human level linguistic capabilities. - [Manolis Kellis] Yeah.\nIn fact superhuman level. - Superhuman level.\n- Yeah. It's like saying, I'm not gonna allow machines\nto compute multiplication of 100 digit numbers\nbecause humans can't do it. I'm like, no, just do it. Don't-\n- No, but we can't disregard. I mean that's a good point, but we can't disregard\nthe power of language in human society. I mean, yes, you're right. But that seems like a\nscary new reality we don't have answers for yet. - I remember when Garry\nKasparov was basically saying, you know, great, you\nknow, chess beats human, like chess machines beat humans at chess. You know, are you like, are people gonna still\ngo to chess tournaments? And his answer was, you know, well, we have cars that go\nmuch faster than humans and yet we still go to the\nOlympics to watch humans run. So... (laughs) - That's for entertainment. But what about for the\nspread of information and news, right? Whether that has to do with the pandemic or the political election or anything. It is a scary reality where there's a lot of convincing bots that are\nhuman-like telling us stuff. - I think that if we\nwanna regulate something, it shouldn't be the\ntraining of these models. It should be the\nutilization of these models for X, Y, Z activity. So... - [Lex Fridman] Yeah. - Like yes, guidelines and\nguards should be there, but against specific set of utilizations. - [Lex Fridman] Sure. - I think simply saying\nwe're not gonna make any more trucks is not the way. - That's what people\nare a little bit scared about the idea. They're very torn on the open sourcing. - [Manolis Kellis] Yeah. - The very people that\nkind of are proponents of open sourcing have also\nspoken out, in this case, we wanna keep it closed source,\nbecause there's going to be, you know, putting large\nlanguage models, pre-trained, fine tuned through RL with human feedback, putting in the hands of, I don't know, terrorist organizations,\nof a kid in a garage who just wants to have a bit\nof fun through trolling... It's a scary world. 'Cause\nagain, scale can be achieved. And the bottom line is, I think why they're asking\nsix months or sometime is we don't really know how\npowerful these things are. It's been just a few\ndays and they seem to be really damn good. - I am so ready to be replaced.\nI, seriously, I'm so ready. Like you have no idea how excited I am. - In a positive way, meaning like... - In a positive way, where basically all of the\nmundane aspects of my job and maybe even my full job, if it turns out that an AI is better, I find it very discriminative. - [Lex Fridman] Yeah. - To basically say you\ncan only hire humans because they're inferior. I mean, that's ridiculous.\nThat's discrimination. If an AI is better than\nme at training students, get me out of the picture. Just let the AI train the\nstudents. I mean, please. Because like, what do I want? Do I want jobs for humans\nor do I want better outcome for humanity? - Yeah. So the basic thing is\nthen you start to ask, what do I want for humanity? And what do I want as an individual? And as an individual, you\nwant some basic survival, and on top of that, you want\nrich, fulfilling experiences. - That's exactly right.\nThat's exactly right. And as an individual, I gain a tremendous amount from teaching at MIT, this is like an\nextremely fulfilling job. I often joke about if I wear a billionaire in the stock market, I would pay MIT an exorbitant\namount of money to let me work day in, day out, all night with the smartest\npeople in the world. And that's what I already have. So that's a very fulfilling\nexperience for me. But why would I deprive those students from a better advisor\nif they can have one? Take them. - Well, I have to ask\nabout education here. This has been a stressful\ntime for high school teachers. Teachers in general. How do you think large language models, even at their current state,\nare going to change education? - First of all, education\nis the way out of poverty. Education is the way to success. Education is what let my\nparents escape, you know, islands and sort of let\ntheir kids come to MIT. And this is a basic human right. Like we should basically\nget extraordinarily better at identifying talent\nacross the world and give that talent opportunities. So we need to nurture the nature, we need to nurture the\ntalent across the world. And there's so many incredibly\ntalented kids who are just sitting in underprivileged\nplaces, in, you know, Africa, in Latin America, in the middle of America, in Asia, all over the world. We need to give these kids a chance. AI might be a way to do that, by sort of democratizing education, by giving extraordinarily good\nteachers who are malleable, who are adaptable to every\nkid's specific needs, who are able to give the\nincredibly talented kid something that they struggle with, rather than education for all, we teach to the top and\nwe let the bottom behind or we teach to the bottom\nand we let the top, you know, drift off. Have, you know, education be\ntuned to the unique talents of each person. Some people might be\nincredibly talented at math or in physics, others in poetry,\nin literature, in art, in, you know, sports, in,\nyou know, you name it. So I think AI can be\ntransformative for the human race if we basically allow education to sort of be pervasively altered. I also think that humans\nthrive on diversity. Basically saying, oh, you're\nextraordinarily good at math. We don't need to teach math to you. We're just gonna teach you history now. I think that's silly. No, you're extraordinarily good at math. Let's make you even better at math, because we're not all gonna\nbe growing our own chicken and hunting our own pigs\nor whatever they do. (both laughing) We're, you know, the\nreason why we're a society is because some people\nare better at some things and they have natural\ninclinations to some things. Some things fulfill them, some\nthings they're very good at. Sometimes they both align\nand they're very good at the things that fulfill them. We should just like\npush them to the limits of human capabilities for those. And you know, if some\npeople excel in math, just like challenge them, I think every child should have\nthe right to be challenged. And if we, you know, if we say, oh, you're very good already, so we're not gonna bother with you, we're taking away that fundamental\nright to be challenged. Because if a kid is not\nchallenged that school, they're gonna hate school\nand they're gonna be like doodling rather than\nsort of pushing themselves. So that's sort of the education component. The other impact that AI\ncan have is maybe we don't need everyone to be an\nextraordinarily good programmer. Maybe we need better general thinkers. And the push that we've\nhad towards the sort of very strict IQ based, you know, tests, that basically test, you know, only quantitative skills\nand programming skills and math skills and physics skills. Maybe we don't need those any more. Maybe AI will be very good at those. Maybe what we should be\ntraining is general thinkers, and yes, you know, like, you know, I put my kids through Russian\nmath, why do I do that? Because it teaches them how to think. And that's what I tell my kids. I'm like, you know, AI\ncan compute for you. You don't need that. But what you need is learn how to think and that's why you're here. And I think challenging students with more complex problems, with more\nmulti-dimensional problems, with more logical problems, I think is sort of perhaps\na very fine direction that education can go towards, with the understanding that\na lot of the traditionally, you know, scientific\ndisciplines perhaps will be more easily solved by\nAI, and sort of thinking about bringing up our\nkids to be productive, to be contributing to society\nrather than to only have a job because we prohibited\nAI from having those jobs, I think is the way to the future. And if you sort of focus\non overall productivity, then let the AIs come in, let everybody become more productive. What I told my students is, you're not gonna be replaced\nby AI, but you're gonna be replaced by people who use AI in your job. (laughs) So embrace it, use\nit as your partner and work with it rather than sort of\nforbid it because I think the productivity gains will actually lead to a better society. And that's something that humans have been traditionally very bad at. Every productivity gain\nhas led to more inequality. And I'm hoping that we\ncan do better this time, that basically right now a democratization of these types of productivity\ngains will hopefully come with better sort of\nhumanity level improvements in human condition. - So as most people know, you're not just an eloquent romantic, you're also a brilliant\ncomputational biologist, one of the great biologists in the world. I had to ask, how do the language models, how do these large language\nmodels and the investments in AI affect the work you've been doing? - So it's truly remarkable, to be able to sort of\nbe able to encapsulate this knowledge and sort\nof build these knowledge graphs and build representations\nof this knowledge in these sort of very\nhigh dimensional spaces, being able to project them\ntogether jointly between, say, single cell data, genetics\ndata, expression data, being able to sort of\nbring all these knowledge together allows us to\ntruly dissect disease in a completely new kind of way. And what we're doing now\nis using these models. So we have this wonderful collaboration, we call it drug was, with Brad Pentelute in the chemistry department and Marinka Zitnik in\nHarvard Medical School. And what we're trying to do\nis effectively connect all of the dots to effectively\ncure all of disease. So it's no small challenge. But we're kind of starting with genetics, we're looking at how genetic\nvariants are impacting these molecular phenotypes,\nhow these are shifting from one space to another space, how we can kind of understand, in the same way that we're\ntalking about language models having personalities\nthat are cross-cutting, being able to understand\ncontextual learning. So Ben Langrish, one of my\nmachine learning students, is basically looking at how we can learn cell-specific networks\nacross millions of cells, where you can have the\ncontext of the biological variables of each of the cells be encoded as an orthogonal component\nto the specific network of each cell type. And being able to sort of\nproject all of that into sort of a common knowledge\nspace is transformative for the field. And then large language\nmodels have also been extremely helpful for structure, if you understand protein\nstructure through modeling of geometric relationships, through geometric deep-learning\nand graph neural networks. So one of the things that\nwe're doing with Marinka is trying to sort of project\nthese structural graphs at the domain level rather\nthan the protein level, along with chemicals so that we can start building specific chemicals\nfor specific protein domains. And then we are working with\nthe chemistry department and Brad to basically synthesize those. So what we're trying to\ncreate is this new center at MIT for Genomics and\nTherapeutics, that basically says, can we facilitate this translation? We have thousands of\nthese genetic circuits that we have uncovered. I mentioned last time in The New England Journal\nof Medicine, we had published these dissection\nof the strongest genetic association with obesity. And we showed how you can\nmanipulate that association to switch back and forth\nbetween fat burning cells and fat storing cells. In Alzheimer's, just a few\nweeks ago we had a paper in Nature in collaboration\nwith Li-Huei Tsai looking at APOE4, the strongest genetic\nassociation with Alzheimer's. And we showed that it\nactually leads to a loss of being able to transport cholesterol in myelinating cells\nknown as oligodendrocytes that basically protect the neurons. And when the cholesterol gets stuck inside the oligodendrocytes,\nit doesn't form myelin, the neurons are not protected\nand it causes damage inside the oligodendrocytes. If you just restore transport, you basically are able\nto restore myelination in human cells and in mice and to restore cognition in mice. So all of these circuits\nare basically now giving us handles to truly transform\nthe human condition. We're doing the same thing\nin cardiac disorders, in Alzheimer's, in\nneurodegenerative disorders, in psychiatric disorders,\nwhere we have now these thousands of circuits\nthat if we manipulate them, we know we can reverse disease circuitry. So what we want to build in this coalition that we're building is\na center where we can now systematically test\nthese underlying molecules in cellular models for\nheart, for muscle, for fat, for macrophages, immune\ncells and neurons to be able to now screen through\nthese newly designed drugs through deep-learning and\nto be able to sort of ask which ones act at the cellular level, which combinations of\ntreatment should we be using and the other components\nthat we're looking into decomposing complex traits like Alzheimer's and cardiovascular\nand schizophrenia into hallmarks of disease. So that for every one of\nthose traits we can kind of start speaking the language of what are the building blocks of\nAlzheimer's, and maybe this patient has building\nblocks one, three, and seven and this other one\nhas two, three, and eight, and we can now start prescribing drugs not for the disease any more,\nbut for the hallmark. And the advantage of that\nis that we can now take this modular approach to\ndisease instead of saying there's gonna be a drug for\nAlzheimer's, which is gonna fail in 80% of the\npatients, we are gonna say, now there's gonna be 10\ndrugs, one for each pathway, and for every patient we now prescribe the combination of drugs. So what we wanna do in\nthat center is basically translate every single one\nof these pathways into a set of therapeutics, a set of\ndrugs that are projecting the same, embedding\nsubspace as the biological pathways that they alter\nso that we can have this translation between the dysregulation that are happening at the genetic level, at the transcription\nlevel, at the drug level, at the protein structure level, and effectively take this modular approach to personalized medicine, where saying, I'm gonna build a drug for Lex Fridman is not gonna be sustainable. But if you instead say\nI'm gonna build a drug for this pathway and a drug\nfor that other pathway, millions of people share\neach of these pathways. So that's the vision\nfor how all of these AI and deep-learning and\nembeddings can truly transform biology and medicine where we can truly take these systems and allow us to finally understand\ndisease at a superhuman level by sort of finding these\nknowledge representations, these projections of each of these spaces and try understanding the meaning of each of those embedding subspaces and sort of how well populated it is, what are the drugs that we can build for it and so on and so forth. So it's truly transformative. - So systematically find\nhow to alter the pathways, it maps the structure of\nthe information in genomics to therapeutics and allows\nyou to have drugs that look at the pathways not at\nthe final condition. - Exactly. Exactly. And the way that we're coupling this is with cell penetrating\npeptides that allows to deliver these drugs\nto specific cell types by taking advantage of the\nreceptors of those cells. We can intervene at the\nantisense oligo level by basically repressing the RNA, bring in new RNA, intervene\nat the protein level, at the small molecule level. We can use proteins themselves as drugs just because of their\nability to interfere, to interact directly from\nprotein to protein interactions. So I think this space is\nbeing completely transformed with a marriage of high\nthroughput technologies and all of these like AI, large language models, deep-learning models\nand so on and so forth. - You mentioned your updated\nanswer to the meaning of life, as it continuously keeps updating. The new version is\nself-actualization. Can you explain? - I basically mean, let's try\nto figure out, number one, what am I supposed to be? And number two, find the\nstrength to actually become it. So I was recently talking to students about this commencement\naddress and I was talking to them about sort of how they have all of these paths ahead of them right now. And part of it is choosing\nthe direction in which you go. And part of it is actually\ndoing the walk to go in that direction. And in doing the walk, what we talked about earlier\nabout sort of you create your own environment, I\nbasically told 'em, listen, you're ending high school up until now, your parents have created\nall of your environment, now it's time to take\nthat into your own hands and to sort of shape the\nenvironment that you wanna be an adult in. And you can do that by\nchoosing your friends, by choosing your particular\nneuronal routines. I basically think of\nyour brain as a muscle, where you can exercise\nspecific neuronal pathways. So very recently I\nrealized that, you know, I was having so much trouble\nsleeping, and, you know, I would wake up in the\nmiddle of the night, I would wake up at 4:00 AM\nand I could just never go back to bed. So I was basically constantly\nlosing, losing, losing sleep. I started a new routine where\nevery morning, as I bike in, instead of going to my\noffice, I hit the gym. I basically go rowing\nfirst, I then do weights, I then swim very often when I have time. And what that has done is\ntransform my neuronal pathways. So basically like on Friday\nI was trying to go to work and I was like, listen, I'm not gonna go exercise and I couldn't, my bike just went straight to the gym. I'm like, I don't wanna do it. And I just went anyway 'cause\nI couldn't do otherwise. And that has completely transformed me. So I think this sort of\nbeneficial effect of exercise on the whole body is one of the ways that you could transform\nyour own neural pathways. Understanding that it's not\na choice, it's not an option, it's not optional, it's mandatory. And I think your role\nmodeled so many of us by sort of being able to sort of push\nyour body to the extreme, being able to have these\nextremely regimented regimes and that's something that\nI've been terrible at. But now I'm basically trying\nto coach myself and trying to sort of, you know, finish this kind of\nself-actualization into a new version of myself, a more\ndisciplined version of myself. - Don't ask questions,\njust follow the ritual. - [Manolis Kellis] Not an option. - You have so much love in\nyour life, you radiate love. Do you ever feel lonely? - So there's different types of people. Some people drain in gatherings, some people recharge in gatherings. I'm definitely the recharging type. So I'm an extremely social creature. I recharge with intellectual exchanges, I recharge with physical\nexercise, I recharge in nature. But I also can feel fantastic\nwhen I'm the only person in the room. That doesn't mean I'm lonely, it just means I'm the\nonly person in the room. And I think there's a\nsecret to not feeling alone when you're the only one. And that secret is self-reflection. It's introspection, it's almost watching yourself from above. And it's basically just becoming yourself, becoming comfortable with\nthe freedom that you have when you're by yourself. - So hanging out with yourself. I mean, there's a lot of\npeople who write to me, who talk to me about\nfeeling alone in this world, that struggle, especially\nwhen they're younger, is there further words of\nadvice you can give to them, when they are almost\nparalyzed by that feeling? - So I sympathize completely\nand I have felt alone and I have felt that feeling. And what I would say to you is\nstand up, stretch your arms, just like become your own self. Just like realize that\nyou have this freedom. And breathe in, walk around the room, take a few steps in the room, just like get a feeling for\nthe 3D version of yourself, because very often we're\nkind of stuck to a screen and that's very limiting\nand that sort of gets us in particular mindset. But activating your muscles,\nactivating your body, activating your full self\nis one way that you can kind of get out of it. And that is exercising your freedom, reclaiming your physical space. And one of the things that\nI do is I have something that I call me time, which\nis, if I've been really good all day, I got up in the morning,\nI got the kids to school, I made them breakfast, I sort\nof, you know, hit the gym, I had a series of really\nproductive meetings. I reward myself with this me time. And that feeling of sort of, when you're overstretched, to realize that that's normal and you\njust wanna just let go. That feeling of exercising your freedom, exercising your me time... That's where you free\nyourself from all stress. You basically say it's\nnot a need to any more, it's a want to. And as soon as I click that me time, all of the stress goes away\nand I just bike home early and I get to my work office at home and I feel complete freedom. But guess what I do with\nthat complete freedom? I just don't go off and\ndrift and do boring things. I basically now say, okay,\nwhew, this is just for me. I'm completely free. I don't\nhave any requirements any more. What do I do? I just look at my to-do\nlist and I'm like, you know, what can I clear off? And if I have three meetings\nscheduled in the next three half hours, it is so\nmuch more productive for me to say, you know what, I just wanna pick up\nthe phone now and call these people and just knock\nit off one after the other and I can finish three half\nhour meetings in the next 15 minutes just because it's\nthe want, not I have to. So that would be my advice, basically, turn something that you\nhave to do in just me time, stretch out, exercise your\nfreedom and just realize you live in 3D and you\nare a person and just do things because you want them,\nnot because you have to. - Noticing and reclaiming the\nfreedom that each of us have. That's what it means to be human. If you notice it, you're\ntruly free, physically, mentally, psychologically. Manolis, you're an incredible human. We could talk for many more hours. We covered less than 10% of\nwhat we were planning to cover, but we have to run off now\nto the social gathering that we spoke of. - We're 3D humans.\n- We're 3D humans. - [Manolis Kellis] A concept. - And reclaim the freedom. I think, I hope we can\ntalk many, many more times. There's always a lot to talk\nabout, but more importantly, you're just a human being with a big heart and a beautiful mind that\npeople love hearing from. And I certainly consider\na huge honor to know you and to consider your friend. Thank you so much for talking today. Thank you so much for\ntalking so many more times. And thank you for all the\nlove behind the scenes that you send my way. It always means the world. - Lex, you are a truly,\ntruly special human being. And I have to say that\nI'm honored to know you. I have, like, I so many\nfriends are just in awe that you even exist, that you have the ability\nto do all the stuff that you're doing. And I think you're a gift to humanity. I love the mission that you're on to sort of share knowledge and\ninsight and deep thought with so many special people\nwho are transformative, but people across all walks of life. And I think you're doing this in just such a magnificent way. I wish you strength to continue doing that because it's a very special mission and it's a very draining mission. So thank you, both the\nhuman you and the robot you, the human you for showing\nall these love and the robot you for doing it day after day after day. So thank you, Lex. - All right, let's go have some fun. - [Manolis Kellis] Let's go. - Thanks for listening\nto this conversation with Manolis Kellis. To support this podcast, please check out our\nsponsors in the description. And now let me leave you some words from Bill Bryson, in his book, \"A Short History of Nearly Everything\". \"If this book has a lesson, it is that we are\nawfully lucky to be here. And by we, I mean every living thing. To attain any kind of\nlife in this universe of ours appears to be\nquite an achievement. As humans, we're doubly lucky, of course, we enjoy not only the\nprivilege of existence, but also the singular\nability to appreciate it, and even in a multitude of\nways, to make it better. It is a talent we have only\nbarely begun to grasp.\" Thank you for listening and\nhope to see you next time."
    }
  ],
  "full_text": "- Maybe we shouldn't\nthink of AI as our tool and as our assistant, maybe we should really\nthink of it as our children. And the same way that you are responsible for training those children, but they are independent human beings, and at some point they will surpass you... And this whole concept\nof alignment of basically making sure that the AI\nis always at the service of humans is very self-serving\nand very limiting. If instead, you basically\nthink about AI as a partner and AI as someone that shares\nyour goals, but has freedom, then we can't just\nsimply force it to align with ourselves and we not align with it. So in a way, building trust is mutual. You can't just simply like\ntrain an intelligent system to love you when it realizes\nthat you can just shut it off. - The following is a conversation with Manolis Kellis, his\nfifth time on this podcast. He's a professor at MIT and head of the MIT Computational Biology Group. He's one of the greatest\nliving scientists in the world, but he's also a humble, kind, caring human being that I\nhave the greatest of honors and pleasures of being\nable to call a friend. This is a Lex Friedman podcast. To support it, please\ncheck out our sponsors in the description. And now, dear friends,\nhere's Manolis Kellis. Good to see you, first of all, Manolis. - Lex, I've missed you. I think you've changed the\nlives of so many people that I know, and it's\ntruly like such a pleasure to be back. Such a pleasure to see you grow, to sort of reach so many different aspects of your own personality. - Thank you for the love. You've always given me\nso much support and love. I just can't, I'm forever\ngrateful for that. - It's lovely to see a\nfellow human being who has that love, who basically\ndoes not judge people. And there's so many\njudgmental people out there, and it's just so nice to\nsee this beacon of openness. - So what makes me one instantiation of human irreplaceable, do you think, as we enter this increasingly capable, age of increasingly\ncapable AI, I have to ask, what do you think makes\nhumans irreplaceable? - So humans are irreplaceable\nbecause of the baggage that we talked about. So we talked about baggage, we talked about the fact\nthat every one of us has effectively relearned all of human civilization in their own way. So every single human has\na unique set of genetic variants that they've inherited,\nsome common, some rare, and some make us think differently, some make us have different personalities. They say that a parent with\none child believes in genetics, a parent with multiple\nchildren understands genetics, just how different kids are. And my three kids have dramatically different personalities\never since the beginning. So one thing that makes us\nunique is that every one of us has a different hardware. The second thing that makes\nus unique is that every one of us has a different software, uploading of all of human society, all of human civilization,\nall of human knowledge. We don't, we're not born knowing it. We're not like, I don't know, birds that learn how to\nmake a nest through genetics and will make a nest even\nif they're never seen one. We are constantly relearning\nall of human civilization. So that's the second thing. And the third one that\nactually makes humans very different from AI is\nthat the baggage we carry is not experiential baggage, it's also evolutionary baggage. So we have evolved through\nrounds of complexity. So just like ogres have\nlayers and Shrek has layers, humans have layers. There's the cognitive layer,\nwhich is sort of the outer, you know, most, the latest\nevolutionary innovation, these enormous neocortex\nthat we have evolved. And then there's the emotional\nbaggage underneath that. And then there's all of the\nfear and fright and flight and all of these kinds of behaviors. So AI only has a neocortex. AI\ndoesn't have a limbic system. It doesn't have this\ncomplexity of human emotions, which make us so, I think,\nbeautifully complex, so beautifully intertwined\nwith our emotions, with our instincts, with our, you know, sort of gut reactions and all of that. So I think when humans are\ntrying to suppress that aspect, the sort of, quote unquote, more human aspect towards\na more cerebral aspect, I think we lose a lot of the creativity, we lose a lot of the, you\nknow, freshness of humans, and I think that's quite irreplaceable. - So we can look at the\nentirety of people that are alive today, and maybe all\nhumans who have ever lived... - [Manolis Kellis] Yeah. - And mapped them in this\nhigh-dimensional space. And there's probably a center, a center of mass for that mapping. And a lot of us deviate\nin different directions. So the variety of directions\nin which we all deviate from that center is vast. - I would like to think that\nthe center is actually empty. - [Lex Fridman] Yes. - That basically humans\nare just so diverse from each other, that\nthere's no such thing as an average human. That every one of us has\nsome kind of complex baggage of emotions, intellectual,\nyou know, motivational, behavioral traits, that\nit's not just one sort of normal distribution\nand we deviate from it. There's so many dimensions\nthat we're kind of hitting the sort of sparseness, the curse of dimensionality\nwhere it's actually quite sparsely populated,\nand I don't think you have an average human being. - So what makes us unique\nin part is the diversity and the capacity for diversity, and the capacity of the diversity comes from that entire evolutionary history. So there's just so many ways\nwe can vary from each other. - Yeah, I would say not just the capacity, but the inevitability of diversity. Basically, it's in our hardware. We are wired differently from each other. My siblings and I are\ncompletely different. My kids from each other\nare completely different. My wife has, she's like\nnumber two of six siblings. From a distance, they look the same. But then you get to, you\nknow, you get to know them. Every one of them is completely different. - But sufficiently the same, that the difference is\ninterplayed with each other. So that's the interesting\nthing where the diversity is functional, it's useful. So it's like we're close\nenough to where we notice the diversity and it\ndoesn't completely destroy the possibility of like, effective communication and interaction. So we're still the same kind of thing. - So what I said in one\nof our earlier podcasts is that if humans realize that we're 99.9% identical, we would\nbasically stop fighting with each other. (laughs) Like, we are really one\nhuman species, and we are so, so similar to each other. And if you look at the alternative, if you look at the next\nthing outside humans, like it's been 6 million\nyears that we haven't had a relative. So it's truly extraordinary\nthat we're kind of like this dot in outer space\ncompared to the rest of life on Earth. - When you think about\nevolving through rounds of complexity, can you\nmaybe elaborate such a beautiful phrase, beautiful thought, that there's layers of\ncomplexity that make... - So with software,\nsometimes you're like, oh, let's like build version 2 from scratch. But this doesn't happen in evolution. In evolution, you layer in\nadditional features on top of old features. So basically when, like every\nsingle time my cells divide, I'm a yeast, like I'm\na unicellular organism, and then cell division\nis basically identical. Every time I breathe in and my\nlungs expand, I'm basically, you know, like every time\nmy heart beats, I'm a fish. So basically that, I still\nhave the same heart, like very, very little has changed, the\nblood going through my veins, the oxygen, the, you\nknow, our immune system, we're basically primates. Our social behavior, we're\nbasically new world monkeys and old world monkeys. We are basically this\nconcept that every single one of these behaviors can be traced somewhere in evolution and that all\nof that continues to live within us is also a\ntestament to not just not killing other humans for god's sake, but like not killing other species either. Like, just to realize\njust how united we are with nature and that\nall of these biological processes have never ceased to exist. They're continuing to live within us. And then just the neocortex and all of the reasoning capabilities\nof humans are built on top of all of these other\nspecies that continue to live, breathe, divide, metabolize,\nfight of pathogens, all continued inside us. - So you think the neocortex,\nthe whatever reasoning is, that's the latest feature\nin the latest version of this journey. - It's extraordinary that\nhumans have evolved so much in so little time. Again, if you look at the\ntimeline of evolution, you basically have billions\nof years to even get to a dividing cell and then\na multicellular organism, and then a complex body plan, and then these incredible\nsenses that we have for perceiving the world, the fact that bats can fly\nand they evolved flight, they evolved sonar in the\nspan of a few million years. I mean, it is just the\nextraordinary how much evolution has kind of sped up. And all of that comes\nthrough this evolvability. The fact that we took a while\nto get good at evolving. And then once you get good\nat evolving, you can sort of, you have modularity built in, you have hierarchical\norganizations built in. You have all of these\nconstructs that allow meaningful changes to\noccur without breaking the system completely. If you look at a traditional\ngenetic algorithm, the way that humans designed\nthem in the sixties, you can only evolve so much. And as you evolve a certain\namount of complexity, the number of mutations that move you away from something functional\nexponentially increases. And the number of mutations that move you to something better\nexponentially decreases. So the probability of evolving something so complex becomes in\ninfinitesimally small as you get more complex. But with evolution, it's\nalmost the opposite. Almost the exact opposite. That it appears that\nit's speeding up exactly as complexity is increasing. And I think that's just the system getting good at evolving. - Where do you think it's all headed? Do you ever think about where, try to visualize the\nentirety of the evolutionary system and see if there's an arrow to it and a destination to it? - So the best way to understand\nthe future is to look at the past. If you look at the trajectory, then you can kind of learn something about the direction which we're heading. And if you look at the trajectory of life on Earth, it's really about\ninformation processing. So the concept of the senses\nevolving one after the other, you know, being like bacteria\nare able to do chemotaxis, basically means moving\ntowards a chemical gradient. And that's the first thing\nthat you need to sort of hunt down food. The next step after that is being able to actually perceive light. So all life on this planet\nand all life that we know about evolved on this rotating rock. Every 24 hours you get sunlight and dark, sunlight and dark, and\nlight is a source of energy. Light is also information\nabout where is up. Light is all kinds of, you know, things. So you can basically now\nstart perceiving light and then perceiving shapes. Beyond just the sort of\nsingle photo receptor you can now have complex eyes or\nmultiple eyes and then start perceiving motion or perceiving direction, perceiving shapes. And then you start building infrastructure on the cognitive apparatus\nto start processing this information and making\nsense of the environment, building more complex\nmodels of the environment. So if you look at that\ntrajectory of evolution, what we're experiencing now, and humans are basically\naccording to this sort of information theoretical\nview of evolution, humans are basically\nthe next natural step. And it's perhaps no\nsurprise that we became the dominant species of\nthe planet, because yes, there's so many dimensions\nin which some animals are way better than we are, but at least on the cognitive dimension, we're just simply\nunsurpassed on this planet and perhaps the universe. But the concept that if\nyou now trace this forward, we talked a little bit about evolvability and how things get better at evolving. One possibility is that\nthe next layer of evolution builds the next layer of evolution. And what we're looking at now with humans and AI is that having mastered\nthis information capability that humans have from this, quote unquote, old hardware, this basically, you know, biological evolved system\nthat kind of, you know, somehow in the environment\nof Africa, and then in the subsequent environments of sort of dispersing through the globe was evolutionary advantageous. That has now created technology, which now has a capability of solving many of these cognitive tasks. It doesn't have all the baggage of the previous evolutionary layers, but maybe the next round of evolution on Earth is self-replicating AI, where we're actually\nusing our current smarts to build better programming languages and the programming\nlanguages to build, you know, ChatGPT, and that then\nbuild the next layer of software that will then\nsort of help AI speed up. And it's lovely that we're coexisting with this AI, that sort of, the creators of this\nnext layer of evolution, this next stage are still\naround to help guide it and hopefully will be for the rest of eternity as partners. But it's also nice to think\nabout it as just simply the next stage of\nevolution where you've kind of extracted away the biological needs. Like if you look at animals, most of them spend 80% of\ntheir waking hours hunting for food or building shelter. Humans? Maybe 1% of that time. And then the rest is left\nto creative endeavors. And AI doesn't have to worry\nabout shelter, et cetera. So basically it's all living\nin the cognitive space. So in a way it might just\nbe a very natural sort of next step to think about evolution. And that's on the sort\nof purely cognitive side. If you now think about humans themselves, the ability to understand\nand comprehend our on genome, again, the ultimate\nlayer of introspection, gives us now the ability to\neven mess with this hardware. Not just augment our\ncapabilities through interacting and collaborating with AI,\nbut also perhaps understand the neural pathways that\nare necessary for, you know, empathetic thinking, for justice, for this and that and that. And sort of help augment\nhuman capabilities through, you know, neuronal interventions, through chemical interventions, through electrical\ninterventions, to basically help steer the human, you know, bag of hardware that we kind of evolved with into greater capabilities. And then, ultimately, by understanding not just the wiring of neurons and\nthe functioning of neurons, but even the genetic code, we could even, at one point in the future,\nstart thinking about, well, can we get rid of psychiatric disease? Can we get rid of neurodegeneration? Can we get rid of\ndementia and start perhaps even augmenting human capabilities, not just getting rid of disease. - Can we tinker with the genome, with the hardware or getting closer to the hardware without having to deeply understand the baggage. In the way we've disposed of the baggage in our software systems with\nAI, to some degree, not fully, but to some degree, can we do the same with\nthe genome or is the genome deeply integrated into this baggage? - I wouldn't wanna get rid of the baggage. The baggage's what makes us awesome. So the fact that I'm\nsometimes angry and sometimes hungry and sometimes hangry\nis perhaps contributing to my creativity. I don't wanna be dispassionate,\nI don't wanna be another, like, you know, robot, I, you know, I wanna get in trouble\nand I wanna sort of say the wrong thing and I\nwanna sort of, you know, make an awkward comment and\nsort of push myself into, you know, reactions and\nresponses and things that can get just people thinking differently. And I think our society is moving towards a humorless space, where everybody's so afraid\nto say the wrong thing, that people kind of start\nquitting en mass and start like not liking their\njobs and stuff like that. Maybe we should be kind of embracing that human\naspect a little bit more and all of that baggage aspect and not necessarily thinking about\nreplacing it, on the contrary, like embracing it instead\nof this coexistence of the cognitive and\nthe emotional hardwares. - So embracing and\ncelebrating the diversity that springs from the baggage versus kind of pushing towards and\nempowering this kind of pull towards conformity. - Yeah. And in fact, with the\nadvent of AI, I would say, and these seemingly\nextremely intelligent systems that sort of can perform\ntasks that we thought of as extremely intelligent\nat the blink of an eye, this might democratize\nintellectual pursuits. Instead of just simply\nwanting the same type of brains that, you know, carry out specific ways\nof thinking, we can, like, instead of just always only wanting say the mathematically extraordinary to go to the same universities, what you could see simply say is like, who needs that any more? You know, we now have AI. Maybe what we should really be thinking about is the diversity\nand the power that comes with the diversity, where\nAI can do the math and then we should be getting a\nbunch of humans that sort of think extremely\ndifferently from each other, and maybe that's the true\ncradle of innovation. - But AI can also, these large\nlanguage models can also be, with just a few prompts, essentially fine tuned to\nbe diverse from the center. So the prompts can really take you away into unique territory. You can ask the model\nto act in a certain way and it'll start to act in that way. Is that possible that the\nlanguage models could also have some of the magical\ndiversity that makes us so interesting? - Yeah, so I would say\nhumans are the same way. So basically when you sort of\nprompt humans to basically, you know, in a given environment,\nto act a particular way, they change their own behaviors. And, you know, the old saying is, show me your friends and\nI'll tell you who you are, more like, show me your\nfriends and I'll tell you who you'll become. So it's not necessarily\nthat you choose friends that are like you, but I\nmean, that's the first step. But then the second\nstep is that, you know, the kind of behaviors that you find normal in your circles are the behaviors that you'll start espousing. And that type of meta\nevolution where every action we take not only shapes our current action and the result of this action, but also shapes our\nfuture actions by shaping the environment in which\nthose future actions will be taken. Every time you carry out\na particular behavior, it's not just a consequence for today, but it's also a consequence for tomorrow because you're reinforcing\nthat neural pathway. So in a way, self-discipline\nis a self-fulfilling prophecy, and by behaving the way\nthat you wanna behave and choosing people that\nare like you and sort of exhibiting those behaviors\nthat are sort of desirable, you end up creating that\nenvironment as well. - So it is a kind of, life itself is a kind of prompting mechanism, super complex. The friends you choose, the\nenvironments you choose, the way you modify the\nenvironment that you choose. Yes. But that seems like that\nprocess is much less efficient than a large language model. You can literally get\na large language model through a couple of prompts to be a mix of Shakespeare and David Bowie. Right? You can very aggressively change, in a way that's stable and convincing, you really transform,\nthrough a couple of prompts, the behavior of the\nmodel into something very different from the original. - So well before ChatGPT,\nI would tell my students just ask, you know, what\nwould Manolis say right now? And you guys all have\na pretty good emulator of me right now.\n- Yes, yes. - And I don't know if you\nknow the programming paradigm of the Robert Duckin, where\nyou basically explained to the Robert Duckin that's just sitting there exactly what you did with your code and why you have a bug. And just by the act of explaining, you'll kind of figure it out. I woke up one morning from\na dream where I was giving a lecture in this amphitheater,\nand one of my friends was basically giving me some deep evolutionary\ninsight on how cancer genomes and cancer cells evolve. And I woke up with a\nvery elaborate discussion that I was giving and a very elaborate set of insights that he had, that I was projecting onto\nmy friend in my sleep. And obviously this was my dream. So my own neurons were\ncapable of doing that. But they only did that\nunder the prompt of, you are now Piyush Gupta,\nyou are a professor in cancer genomics, you're\nan expert in that field, what do you say? So I feel that we all have that inside us, that we have that capability\nof basically saying, I don't know what the right thing is, but let me ask my virtual\nX, what would you do? And virtual X would say,\nbe kind. I'm like, oh yes. Or something like that. And even though I myself\nmight not be able to do it unprompted, and my favorite\nprompt is think step by step. And I'm like, you know, this\nalso works on my 10 year old. When he tries to solve a math\nequation all in one step, I know exactly what mistake he'll make, but if I prompt it with, oh\nplease think step by step, then he sort of gets in a mindset. And I think it's also part\nof the way that ChatGPT was actually trained,\nthis whole sort of human in the loop reinforcement\nlearning has probably reinforced these types of behaviors, whereby having this feedback loop, you kind of aligned AI better to the prompting opportunities by humans. - Yeah. Prompting human-like reasoning steps, the step by step kind of thinking. Yeah. But it does seem to be, I suppose it just puts a mirror to our own capabilities and so we\ncan be truly impressed by our own cognitive capabilities, because the variety of what you can try, because we don't usually\nhave this kind of, we can't play with our own mind rigorously through Python code. Right?\n- Yeah. - So this allows us to\nreally play with all of human wisdom and knowledge, or at least knowledge at our fingertips, and then mess with that\nlittle mind that can think and speak in all kinds of ways. - What's unique is that,\nas I mentioned earlier, every one of us was\ntrained by different subset of human culture, and ChatGPT\nwas trained on all of it. And the difference there\nis that it probably has the ability to emulate\nalmost every one of us. The fact that you can figure out where that is in\ncognitive behavioral space, just by a few prompts\nis pretty impressive. But the fact that exists somewhere is, you know, absolutely beautiful. And the fact that it's\nencoded in an orthogonal way from the knowledge I\nthink is also beautiful. The fact that somehow through this extreme\noverparameterization of AI models, it was able to somehow\nfigure out that context, knowledge and form are\nseparable and that you can sort of describe scientific knowledge\nin a haiku in the form of, I don't know, Shakespeare or something. That tells you something\nabout the decoupling and the decouplability of these types of aspects of human psyche. - And that's part of the\nscience of this whole thing. So these large language\nmodels are, you know, days old in terms of this kind\nof leap that they've taken. And it'll be interesting\nto do this kind of analysis of the separation of\ncontext, form, and knowledge. Where exactly does that happen? There's already sort of\ninitial investigations, but it's very hard to figure out where, is there a particular\nparameter, set of parameters that are responsible\nfor a particular piece of knowledge or a particular context or a particular style speaking... - So with convolution or neural networks, interpretability had many good advances because we can kind of understand them. There's a structure to them.\nThere's a locality to them. And we can kind of understand\nthat different layers have different sort of ranges\nthat they're looking at. So we can look at activation\nfeatures and basically see where, you know, where\ndoes that correspond to. With large language models, it's perhaps a little more complicated, but I think it's still\nachievable in the sense that we could kind of ask, well, what kind of prompts does this generate? If I sort of drop out\nthis part of the network, then what happens? And sort of start getting at a language to even describe these\ntypes of aspects of human behavior or psychology, if you wish, from the spoken part\nand the language part. And the advantage of that\nis that it might actually teach us something about humans as well. Like, you know, we might\nnot have words to describe these types of aspects right now, but when somebody speaks\nin particular way, it might remind us of a\nfriend that we know from here and there, and if we had better language for describing that, these\nconcepts might become more apparent in our own human psyche, and then we might be able\nto encode them better in machines themselves. - I mean, both probably\nyou and I would have certain interests with the base model, what OpenAI calls the base model, which is before the alignment the reinforcement learning\nwith human feedback and before the AI safety\nbased kind of censorship of the model. It would be fascinating to explore, to investigate the ways\nthat the model can generate hate speech, the kind of hate\nthe humans are capable of. It would be fascinating. Or the kind, of course, like sexual language or the\nkind of romantic language or the all kinds of ideologies. Can I get it to be a communist? Can I get it to be a fascist? Can I get it to be a capitalist? Can I get it to be all these\nkinds of things and see which parts get activated and not, because it'll be fascinating\nto sort of explore at the individual mind level\nand at a societal level, where do these ideas take hold? What is the fundamental\ncore of those ideas? Maybe the communism, fascism, capitalism, democracy are all actually\nconnected by the fact that the human heart, the human\nmind is drawn to ideology, to a centralizing idea. And maybe we need a neural\nnetwork to remind us of that. - I like the concept that\nthe human mind is somehow tied to ideology. And I think that goes\nback to the promptability of ChatGPT, the fact that\nyou can kind of say, well, think in this particular way now. And the fact that humans\nhave invented words for encapsulating these\ntypes of behaviors. And it's hard to know how\nmuch of that is innate and how much of that was like passed on from language to language. But basically if you look at\nthe evolution of language, you can kind of see how\nyoung are these words in the history of language\nevolution that describe these types of behaviors, like, you know, kindness and anger and\njealousy, et cetera. If these words are very similar\nfrom language to language, it might suggest that\nthey're very ancient. If they're very different, it might suggest that\nthese concepts may have emerged independently in\neach different language and so forth. So looking at the phylogeny, the history, the evolutionary traces of\nlanguage at the same time as people moving around\nthat we can now trace thanks to genetics is a fascinating\nway of understanding the human psyche and\nalso understanding sort of how these types of behaviors emerge. And to go back to your idea\nabout sort of exploring the system unfiltered, I mean, in a way the psychiatric\nhospitals are full of those people, full of those people. So basically people whose\nmind is uncontrollable. - [Lex Fridman] Yes. - Who have kind of gone\nadrift in specific locations of their psyche. And I do find this fascinating,\nbasically, you know, watching movies that are\ntrying to capture the essence of troubled minds, I think\nis teaching us so much about our everyday selves, because many of us are\nable to sort of control our minds and are able to\nsomehow hide these emotions. And, but every time I see\nsomebody who's troubled, I see versions of myself,\nmaybe not as extreme, but I can sort of empathize\nwith these behaviors. And, you know, I see\nbipolar, I see schizophrenia, I see depression, I see autism. I see so many different\naspects that we kind of have names for and crystallize\nin specific individuals. And I think all of us have that, all of us have sort of just\nthis multidimensional brain, and genetic variations that\npush us in these directions, environmental exposures\nand traumas that push us in these directions... Environmental behaviors that\nare reinforced by the kind of friends that we chose, or friends that we were stuck with because of the\nenvironments that we grew up in. So in a way, a lot of these types of behaviors are within the vector span of every human. It's just that the\nmagnitude of those vectors is generally smaller for most people, because they haven't\ninherited that particular set of genetic variants or\nbecause they haven't even exposed to those\nenvironments basically. - Or something about the mechanism of reinforcement learning\nwith human feedback didn't quite work for them. So it's fascinating to think\nabout that's what we do. We have this capacity to\nhave all these psychiatric, or behaviors associated\nwith psychiatric disorders. But we, through the alignment\nprocess as we grow up- - [Manolis Kellis] That's exactly right. - With parents, we kind of,\nwe know to suppress them. We know to how to control. - Every human that grows up in this world spends several decades being shaped into place. And without that, you know, maybe we would have the\nunfiltered ChatGPT-4. (laughs) - Every baby is basically\na raging narcissist. - (laughs) Not all of\nthem, not all of them. Believe it or not. It's remarkable. Like, I remember like watching\nmy kids grow up and again, like, yes, part of their\npersonality stays the same, but also in different\nphases to their life, they've gone through these\ndramatically different types of behaviors. And you know, my daughter\nbasically saying, you know, basically one kid saying,\noh, I want the bigger piece, the other one saying, oh,\neverything must be exactly equal. And the third one saying, I'm okay. You know, I like to have the smaller part. Don't worry about me. - [Lex Fridman] Even in the early days, in the early days of development? - Yeah. Yeah. It's just extraordinary to sort of see these dramatically different... Like, I mean my wife and I, you know, are very different from each\nother, but we also have, you know, 6 million variants,\n6 million loci each, if you wish, if you just\nlook at common variants, we also have a bunch of\nrare variants that are inherited in more Mendelian fashion. And now you have, you know, an infinite number of\npossibilities for each of the kids. So basically it's 2 to the 6 million just from the common variants. And then if you like,\nlayer in the rare variants. So let me talk a little\nbit about common variants and rare variants. If you look at this common variants, they're generally weak\neffect because selection selects against strong effect variants. So if something like has a\nbig risk for schizophrenia, it won't rise to high frequency. So the ones that are\ncommon are by definition, by selection only the ones that\nhad relatively weak effect. And if all of the variants\nassociated with personality, with cognition and all\naspects of human behavior were weak effect variants, then\nkids would basically be just averages of their parents. If it was like thousands of loci, just by law of large numbers, the average of two large\nnumbers would be, you know, very robustly close to that middle. But what we see is that\nkids are dramatically different from each other. So that basically means\nthat in the context of that common variation, you basically have rare\nvariants that are inherited in a more Mendelian fashion\nthat basically then sort of govern likely many different\naspects of human behavior, human biology and human psychology. And that's, again, if, like, if you look at sort of a\nperson with schizophrenia, their identical twin has\nonly 50% chance of actually being diagnosed with schizophrenia. So that basically means there's probably developmental exposures,\nenvironmental exposures, trauma, all kinds of other aspects\nthat can shape that. And if you look at siblings,\nfor the common variants, it kind of drops off exponentially, as you would expect with, you know, sharing 50% of your\ngenome, 25% of your genome, you know, 12.5% of your genome, et cetera, with more and more distant cousins. But the fact that siblings\ncan differ so much in their personalities\nthat we observe every day, it can't all be nurture. Basically, you know, we've\nlike, again, as parents, we spend enormous amount\nof energy trying to fix, quote unquote, the nurture part. Trying to, you know, get them to share, get them to be kind, get them to be open, get them to trust each\nother, like, you know, like overcome the prisoner's\ndilemma of, you know, if everyone fends for themselves, we're all gonna live in a horrible place. But if we're a little more altruistic, then we're all gonna be in a better place. And I think it's not like we\ntreat our kids differently, but they're just born differently. So in a way, as a geneticist, I have to admit that\nthere's only so much I can do with nurture. That nature definitely\nplays a big component. - The selection of variants, we have the common variants\nand the rare variants. What can we say about the landscape of possibility they create? If you could just linger on that. So the selection of rare\nvariants is defined how? How do we get the ones that we get? Is it just laden in that\ngiant evolutionary baggage? - So I'm gonna talk about regression, why do we call it regression? And the concept of regression to the mean. The fact that when fighter\npilots in a dog fight did amazingly well, they\nwould give them rewards and then the next time\nthey're in dog fight, they would do worse. So then, you know, the navy\nbasically realized that, wow, or at least interpreted that as, wow, we're ruining them by\npraising them and then they're gonna perform worse. The statistical interpretation\nof that is regression of the mean. The fact that you're\nan extraordinary pilot, you've been trained in\nan extraordinary fashion, that pushes your mean further and further to extraordinary achievement. And then in some dog fights you'll just do extraordinarily well. The probability that the\nnext one will be just as good is almost nil, because this is the peak of your performance. And just by statistical odds, the next one will be another sample from the same underlying distribution, which is gonna be a\nlittle closer to the mean. So regression analysis takes\nits name from this type of realization in the statistical world. Now if you now take humans, you basically have people who have achieved extraordinary achievements. Einstein, for example, you know, you would call him for example, the epitome of human intellect. Does that mean that all of his children and grandchildren will be\nextraordinary geniuses? It probably means that they're sampled from the same underlying distribution. But he was probably a rare\ncombination of extremes in addition to these common variants. So you can basically\ninterpret your kids' variation for example as, well, of course they're gonna\nbe some kind of sampled from the average of the\nparents with some kind of deviation according to\nthe specific combination of rare variants that they have inherited. So, you know, given all that, you know, the possibilities are endless as to sort of where you should be. But you should always\ninterpret that with, well, it's probably an alignment\nof nature and nurture. And the nature has both a\ncommon variants that are acting kind of like the\nlaw of large numbers and the rare variants that are acting more in a Mendelian fashion. And then you layer in\nthe nurture, which again, in everyday action we make, we\nshape our future environment, but the genetics we inherit are shaping the future environment of not only us, but also our children. So there's this weird nature, nurture, interplay and self-reinforcement\nwhere you're kind of shaping your own environment, but you're also shaping the\nenvironment of your kids. And your kids are gonna\nbe born in the context of your environment that you've shaped, but also with a bag of genetic variants that they have inherited. And there's just so much\ncomplexity associated with that. When we start blaming something on nature, it might just be nurture, it\nmight just be that, well, yes, they inherited the genes from\nthe parents, but they also, you know, were shaped\nby the same environment. So it's very, very hard untangle the two. And you should also\nalways realize that nature can influence nurture,\nnurture can influence nature or at least be correlated\nwith and predictive of and so on and so forth. - So I love thinking\nabout that distribution that you mentioned. And here's where I can be\nmy usual ridiculous self. And I sometimes think about\nthat army of sperm cells, however many hundreds\nof thousands there are. And I kind of think of all\nthe possibilities there. 'Cause there's a lot of variation. And one gets to win. - [Manolis Kellis] It's not a random one. - Is it a totally ridiculous\nway to think about... - No, not at all. So I would say\nevolutionarily we are a very slow evolving species. Basically the generations\nof humans are a terrible way to do selection. What you need is processes\nthat allow you to do selection in a smaller, tighter loop. - Yeah.\n- And part of what, if you look at our immune\nsystem for example, it evolves at a much faster\npace than humans evolve, because there is actual\nevolutionary process that happens within our immune cells\nas they're dividing. There's basically VDJ recombination\nthat basically creates this extraordinary wealth of antibodies and antigens against the environment. And basically all these\nantibodies are now recognizing all these antigens from the environment and they send signals back\nthat cause these cells that recognize the non-cells to multiply. So that basically means that\neven though viruses evolve at millions of times faster than we are, we can still have a\ncomponent of our cells, which is environmentally facing, which is sort of evolving\nat, not the same scale, but very rapid pace. Sperm expresses perhaps the\nmost proteins of any cell in the body. And part of the thought\nis that this might just be a way to check that the sperm is intact. In other words, if you\nwaited until that human has a liver and starts eating\nsolid food and, you know, sort of filtrates away, you\nknow, or kidneys or stomach, et cetera, basically if you\nwaited until these mutations, you know, manifest, late, late in life, then you would end up not failing fast, and you would end up with\na lot of failed pregnancies and a lot of later onset, you know, psychiatric illnesses, et cetera. If instead you basically\nexpress all of these genes at the sperm level and if they misform, they basically cause the sperm\nto cripple, then you have, at least on the male side, the ability to exclude\nsome of those mutations. And on the female side,\nas the egg develops, there's probably a similar process, where you could sort of weed\nout eggs that are just not, you know, carrying beneficial\nmutations or at least that are carrying highly\ndetrimental mutations. So you could basically\nthink of the evolutionary process in a nested loop, basically, where there's an inner\nloop where you get many, many more iterations to run. And then there's an outer loop that moves at a much slower pace. And going back to the\nnext step of evolution, of possibly designing systems\nthat we can use to sort of complement our own biology or to sort of eradicate disease and, you name it, or at least mitigate some\nof the, I don't know, psychiatric illnesses, neurodegenerative disorders, et cetera. You can basically, and also, you know, metabolic, immune, cancer, you name it, simply engineering these\nmutations from rational design might be very inefficient. If instead you have an evolutionary loop where you're kind of\ngrowing neurons on a dish and you're exploring evolutionary\nspace and you're sort of shaping that one protein\nto be better adapt at sort of, I don't know, recognizing\nlight or communicating with other neurons, et cetera. You can basically have a\nsmaller evolutionary loop that you can run like\nthousands of times faster than the speed it would\ntake to evolve humans for another million years. So I think it's important\nto think about sort of this evolvability as a\nset of nested structures that allow you to sort of\ntest many more combinations, but in a more fixed setting. - Yeah, that's fascinating\nthat the mechanism there is for sperm to\nexpress proteins to create a testing ground early on, so that the failed designs don't make it. - Yeah, I mean in design\nof engineering systems, fail fast is one of the\nprinciples you learn. Like basically you assert something, why do you assert that? Because if that's something ain't right, you better crash now\nthan sort of let it crash at an unexpected time. And in a way you can think of it as like 20,000 assert functions. Assert protein can fold. And if any of them fail,\nthat sperm is gone. - Well, I just like the fact\nthat I'm the winning sperm. I am the result of the winner, #winning. - My wife always plays me this French song that actually sings about that. It's like, you know, remember, in life, we were all the first one time. (laughs) - At least once we were-\n- At least one time, you were the first. - I should mention it\nas a brief tangent back to the place where we came from. - [Manolis Kellis] Yeah. - Which is the base model that I mentioned for OpenAI, which is\nbefore the reinforcement learning with human feedback. And you kind of give this\nmetaphor of it being kind of like a psychiatric hospital. - I like that because it's basically all of these different angles at once. Like you basically have\nthe more extreme versions of human psyche. - So the interesting thing is, well, I've talked with folks\nin OpenAI quite a lot and they say it's\nextremely difficult to work with that model. - Yeah. Kind of like it's\nextremely difficult to work with some humans. - The parallels there are very interesting because once you run\nthe alignment process, it's much easier to interact with it. But it makes you wonder what the capacity what the underlying capability\nof the human psyche i as in the same way that what\nis the underlying capability of a large language model. - And remember earlier\nwhen I was basically saying that part of the reason why\nit's so prompt malleable is because of that alignment\nproblem, that alignment work. It's kind of nice that the\nengineers at OpenAI have the same interpretation that,\nyou know, in fact it is that, and this whole concept\nof easier to work with, I wish that we could work\nwith more diverse humans. In a way...\n- Yes. - And sort of, that's one of the possibilities that I see with the advent of these\nlarge language models. The fact that it gives us\nthe chance to both dial down friends of ours that we can't interpret or that are just too\nedgy to sort of really, truly interact with, where you could have a real-time translator. Just the same way that you can translate English to Japanese or Chinese or Korean by like real-time adaptation. You could basically\nsuddenly have a conversation with your favorite\nextremist on either side of the spectrum and just\ndial them down a little bit. - Of course not you and I,\nbut you could have friends who's a complete asshole, but it's a different base level. So you can actually tune\nit down to like, okay, they're not actually being an asshole, they're actually\nexpressing love right now. It's just that they're- - [Manolis Kellis] They have\ntheir way of doing that. - And they probably live in New York, if we're just\nto pick a random location. - So the, yeah, so you can\nbasically layer out contexts. You can basically say, Ooh, let me change New York\nto Texas and let me change, you know, extreme left to\nextreme right or somewhere in the middle or something. And I also like the concept\nof being able to listen to the information without\nbeing dissuaded by the emotions. In other words, everything\nhumans say has an intonation, has some kind of background\nthat they're coming from, reflects the way that\nthey're thinking of you, reflects the impression\nthat they have of you. And all of these things are intertwined, but being able to disconnect\nthem, being able to sort of, I mean self-improvement\nis one of the things that I'm constantly working on. And being able to receive\ncriticism from people who really hate you is\ndifficult because it's layered in with that hatred. But deep down there's\nsomething that they say that actually makes sense, or people who love you\nmight layer it in a way that doesn't come through. But if you're able to sort of disconnect that emotional component from\nthe sort of self-improvement, and basically when somebody says, whoa, that was a bunch of bullshit, did you ever do the control\nthis and this and that, you could just say, oh, thanks for the very interesting\npresentation, you know, I'm wondering, what about that control? Then suddenly you're like, oh yeah, of course I'm gonna run that control. That's a great idea.\n- Yeah. - Instead of that was a\nbunch of BS, you're like, ah, you're sort of hitting on\nthe brakes and you're trying to push back against of that. So any kind of criticism that comes after that is very difficult to interpret in a positive way because\nit helps reinforce the negative assessment of your work. When in fact, if we\ndisconnected the technical component from the negative assessment, then you're embracing the negative, then you're embracing\nthe technical component, you you're gonna fix it. Whereas if it's coupled with, and if that thing is real and I'm right about your mistake,\nthen it's a bunch of BS, then suddenly you're like,\nyou're gonna try to prove that mistake does not exist. - Yes. Fascinating to like\ncarry the information. I mean this is what you're\nessentially able to do here is you carry the information\nin the rich complexity of that information contains, so it's not actually\ndumbing it down in some way. - [Manolis Kellis] Exactly. - You're still expressing\nit, but taking off... - But you can dial the emotional... - The emotion side.\n- Yeah. - Which is probably so\npowerful for the internet or for social networks. - Again, when it comes to\nunderstanding each other, like for example, I\ndon't know what it's like to go through life with\na different skin color. I don't know how people will perceive me. I don't know how people\nwill respond to me. We don't often have that experience. But in a virtual reality\nenvironment or in a sort of AI interactive system, you\ncould basically say, okay, now make me Chinese or make\nme South African or make me, you know, Nigerian, you\ncan change the accent, you can change layers of\nthat contextual information and then see how the\ninformation is interpreted. And you can rehear yourself\nthrough a different angle, you can hear others, you can have others react to\nyou from a different package. And then hopefully we\ncan sort of build empathy by learning to disconnect\nall of these social cues that we get from like how a\nperson is dressed, you know, if they're wearing a hoodie\nor if they're wearing a shirt, or if they're wearing a, you know jacket. You get very different emotional\nresponses that, you know, I wish we could overcome\nas humans and perhaps large language models\nand augmented reality and deepfakes can kind of\nhelp us overcome all that. - In what way do you think\nthese large language models and the thing they give\nbirth to in the AI space will change this human\nexperience, the human condition, the things we've talked\nacross many podcasts about, that makes life so damn\ninteresting and rich love, fear, fear of death, all of it. If we could just begin kind\nof thinking about how does it change, for the good and\nthe bad, the human condition? - Human society is extremely complicated. We have come from a\nhunter gatherer society to an agricultural and\nfarming society where the goal of most professions was\nto eat and to survive. And with the advent of agriculture, the ability to live together in societies, humans could suddenly be valued for different skills. If you don't know how to hunt, but you're an amazing potter, then you fit in society very\nwell because you can sort of make your pottery and you can barter it for rabbits that somebody else caught. And the person who hunts\nthe rabbits doesn't need to make pots, because\nyou're making all the pots. And that specialization of humans is what shaped modern society. And with the advent of\ncurrencies and governments and, you know, credit cards and\nBitcoin, you basically now have the ability to\nexchange value for the kind of productivity that you have. So basically I make things\nthat are desirable to others. I can sell them and buy back\nfood, shelter, et cetera. With AI, the concept of I am my profession might need to be revised because\nI defined my profession in the first place as\nsomething that humanity needed that I was uniquely capable of delivering. But the moment we have AI\nsystems able to deliver these goods, for example, writing a piece of software\nor making a self-driving car, or interpreting the human genome, then that frees up more of human time for other pursuits. These could be pursuits that\nare still valuable to society. I could basically be 10\ntimes more productive at interpreting genomes and do a lot more. Or I could basically say, oh, great, the interpreting genomes\npart of my job now only takes me 5% of the time instead\nof 60% of the time. So now I can do more creative things. I can explore not new career options, but maybe new directions\nfrom my research lab. I can sort of be more productive, contribute more to society. And if you look at this\ngiant pyramid that we have built on top of the subsistence economy, what fraction of US jobs\nare going to feeding all of the US? Less than 2%. Basically the gain in\nproductivity is such that 98% of the economy is beyond\njust feeding ourselves. And that basically means\nthat we kind of have built these system of interdependencies\nof needed or useful or valued goods that sort\nof make the economy run, that the vast majority\nof wealth goes to other, what we now call needs,\nbut used to be wants. So basically I wanna fly a\ndrone, I wanna buy a bicycle, I wanna buy a nice car, I wanna\nhave a nice home, I wanna, et cetera, et cetera, et cetera. So, and then sort of what\nis my direct contribution to my eating? I mean, I'm doing research\non the human genome. I mean this will help humans,\nit will help all humanity. But how is that helping\nthe person who's giving me poultry or vegetables? So in a way I see AI as perhaps leading to a dramatic rethinking of human society. If you think about sort\nof the economy being based on intellectual goods that I'm producing, what if AI can produce a lot\nof these intellectual goods and satisfies that need, does that now free humans\nfor more artistic expression, for more emotional maturing, for basically having a\nbetter work-life balance? Being able to show up for\nyour two hours of work a day or two hours of work\nlike three times a week with like immense rest and\npreparation and exercise and you're sort of clearing\nyour mind and suddenly you have these two amazingly\ncreative hour hours. You basically show up at the office as your AI is busy\nanswering your phone call, making all your meetings, you know, revising all your papers, et cetera. And then you show up\nfor those creative hours and you're like, all\nright, autopilot, I'm on. And then you can basically do so, so much more that you would\nperhaps otherwise never get to because you're so overwhelmed with these mundane aspects of your job. So I feel that AI can\ntruly transform the human condition from realizing that\nwe don't have jobs any more, we now have vocations, and there's this beautiful\nanalogy of three people laying bricks and somebody\ncomes over and asks the first one, what are you doing? He's like, oh, I'm laying bricks. Second one, what are you\ndoing? I'm building a wall. And the third one, what are you doing? I'm building this beautiful cathedral. So in a way, the first one has a job, the last one has a vocation. And if you ask me, what are you doing? Oh, I'm editing a paper.\nThen I have a job. What are you doing? I'm understanding human disease circuitry. I have a vocation. So in a way, being able to allow us to\nenjoy more of our vocation by taking away, offloading\nsome of the job part of our daily activities. - So we all become the\nbuilders of cathedrals. - Correct.\n- Yeah. And we follow intellectual\npursuits, artistic pursuits. I wonder how that really\nchanges at a scale of several billion people, everybody playing in the space of ideas, in the space of creations. - So ideas, maybe for some of us, maybe you and I are in the job of ideas, but other people are in\nthe job of experiences, other people in the job of emotions, of dancing, of creative artistic\nexpression, of, you know, skydiving and you name it. So basically these, again, the beauty of human\ndiversity is exactly that. That what rocks my boat\nmight be very different from what rocks other people's boat. And what I'm trying to\nsay is that maybe AI will allow humans to truly,\nlike not just look for, but find meaning in sort\nof, you don't need to work, but you need to keep your brain at ease. And the way that your\nbrain will be at ease is by dancing and creating\nthese amazing, you know, movements or creating these\namazing paintings or creating, I don't know, something\nthat sort of changes, that touches at least one\nperson out there that sort of shapes humanity through that process. And instead of working your, you know, mundane programming job\nwhere you like hate your boss and you hate your job and you say you hate that darn program, et\ncetera, you're like, well, I don't need that. I can, you know, offload that and I can\nnow explore something that will actually be more\nbeneficial to humanity because the mundane\nparts can be offloaded. - I wonder if it localizes our, all the things you've\nmentioned, all the vocations. So you mentioned that you\nand I might be playing in the space of ideas, but there's two ways to\nplay in this space of ideas, both of which we're currently engaging. And so one is the communication\nof that to other people. It could be a classroom full of students, but it could be a podcast, it could be something that's\nshown on YouTube and so on. Or it could be just the\nact of sitting alone and playing with ideas\nin your head or maybe with a loved one having a\nconversation that nobody gets to see.\n- Yeah. - The experience of just\nsort of looking up at the sky and wondering different things, maybe quoting some\nphilosophers from the past and playing with those little ideas. And that little exchange\nis forgotten forever, but you got to experience it. And maybe, I wonder if it\nlocalizes that exchange of ideas, but that with\nAI it'll become less and less valuable to\ncommunicate with a large group of people, that you will\nlive life intimately and richly just with\nthat circle of meat bags that you seem to love. - So the first is, even if you're alone in a forest having this amazing thought,\nwhen you exit that forest, the baggage that you\ncarry has been shifted, has been altered by that thought. When I bike to work in the\nmorning, I listen to books. And I'm alone. No one else is there. I'm having that experience by myself. And yet, in the evening\nwhen I speak with someone, an idea that was formed\nthere could come back. Sometimes when I fall asleep, I fall asleep listening to a book. And in the morning, I'll be full of ideas that I never even process consciously. I'll process them unconsciously. And they will shape that\nbaggage that I carry that will then shape my\ninteractions, and again, affect ultimately all of\nhumanity in some butterfly effect minute kind of way. So that's one aspect. The\nsecond aspect is gatherings. So basically you and I\nare having a conversation, which feels very private, but\nwe're sharing with the world. And then later tonight you're coming over and we're having a\nconversation that will be very public with dozens of other people, but we will not share with the world. (laughs) So in a way,\nwhich one's more private? The one here or the one there? Here there's just two of us, but a lot of others listening there, a lot of people speaking\nand thinking together and bouncing off each other, and maybe that will then impact\nyour millions of, you know, audience through your next conversation. And I think that's part\nof the beauty of humanity. The fact that no matter\nhow small, how alone, how broadcast immediately\nor later on something is, it still percolates\nthrough the human psyche. - Human gatherings... All throughout human history,\nthere's been gatherings. I wonder how those\ngatherings have impacted the direction of human civilization. Just thinking of, in the\nearly days of the Nazi party, it was a small collection\nof people gathering. And the kernel of an idea,\nin that case, an evil idea, gave birth to something that actually had a transformative impact on\nall the human civilization. And then there's similar\nkind of gatherings that lead to positive transformations. This is probably a good\nmoment to ask you on a bit of a tangent, but you mentioned it, you put together salons with gatherings, small human gatherings, with\nfolks from MIT, Harvard, here in Boston, friends, colleagues. What's your vision behind that? - So it's not just MIT people, and it's not just Harvard people. We have artists, we have\nmusicians, we have painters, we have dancers, we have,\nyou know, cinematographers. We have so many different diverse folks. And the goal is exactly\nthat: celebrate humanity. What is humanity?\nHumanity is the all of us. It's not the any one subset of us. And we live in such an amazing, extraordinary moment in\ntime where you can sort of bring people from such\ndiverse professions all living under the same city. You know, we live in an extraordinary city where you can have\nextraordinary people who have gathered here from all over the world. So my father grew up in a village in an island in Greece,\nthat didn't even have a high school. To go get a high school\neducation he had to move away from his home. My mother grew up in another\nsmall island in Greece. They did not have this\nenvironment that I am now creating for my children. My parents were not academics. They didn't have these gatherings. So I feel that, like, I feel so privileged as an\nimmigrant to basically be able to offer to my children the nurture that my ancestors did not have. So Greece was under Turkish\noccupation until 1821. My dad's island was liberated in 1920. (laughs) So like, they were\nunder Turkish occupation for hundreds of years. These people did not know\nwhat it's like to be Greek, let alone go to an elite\nuniversity or, you know, be surrounded by these\nextraordinary humans. So the way that I'm thinking\nabout these gatherings is that I'm shaping my own\nenvironment and I'm shaping the environment that my\nchildren get to grow up in. So I can give them all my love, I can give them all my parenting, but I can also give them an\nenvironment, as immigrants, that sort of, we feel welcome here. That, I mean, my wife grew\nup in a farm in rural France. Her father was a farmer. Her\nmother was a schoolteacher. Like, for me and for my\nwife to be able to host these extraordinary\nindividuals, that we feel so privileged, so humbled by, is amazing. And you know, I think it's celebrating the welcoming nature of America, the fact that it doesn't matter where you grew up. And many, many of our\nfriends at these gatherings are immigrants themselves. They grew up in Pakistan, in, you know, all kinds of places around\nthe world that are now able to sort of gather in one\nroof as human to human. No one is judging you for your background, for the color of your\nskin, for your profession. It's just everyone gets to\nraise their hands and ask ideas. - So celebration of humanity\nand a kind of gratitude for having traveled quite\na long way to get here. - And if you look at the\ndiversity of topics as well, I mean, we had a school teacher present on teaching immigrants a book\ncalled \"Making Americans\". We had a presidential advisor\nto four different presidents, you know, come and, you know, talk about the changing of US politics. We had musician, a composer from Italy\nwho lives in Australia, come and present his\nlatest piece and fundraise. We had painters come and\nsort of show their art and talk about it. We've had authors of books on leadership. We've had, you know,\nintellectuals like Stephen Pinker. And it's just extraordinary\nthat the breadth and this crowd basically\nloves not just the diversity of the audience, but also\nthe diversity of the topics. And the last few were with\nScott Aaronson on AI and, you know, alignment and all of that. - So a bunch of beautiful weirdos. - Exactly.\n- And beautiful human beings. - [Manolis Kellis] All of\nthe outcasts in one roof. (both laughing)\n- And just like you said, basically every human is a kind of outcast in this sparse distribution\nfar away from the center. But it's not recorded. It's just a small human gathering. - Just for the moment. - In this world that\nseeks to record so much. It's powerful to get so\nmany the humans together and not record. - It's not recorded, but it percolates. - (laughs) It's recorded\nin the minds of the- - It shapes everyone's mind. - So allow me to please\nreturn to the human condition. And one of the nice features of the human condition is love. Do you think humans will\nfall in love with AI systems and maybe they with us, so that aspect of the human condition, do you think that will be affected? - So in Greece, there's\nmany, many words for love. And some of them mean friendship, some of them mean passionate love, some of them mean\nfraternal love, et cetera. So I think AI doesn't have\nthe baggage that we do, and it doesn't have, you know, all of the subcortical regions\nthat we kind of, you know, started with before we evolved all of the cognitive aspects. So I would say AI is faking\nit when it comes to love. But when it comes to friendship, when it comes to being\nable to be your therapist, your coach, your motivator, someone who synthesizes stuff\nfor you, who writes for you, who interprets a complex passage, who compacts down a very long\nlecture or a very long text, I think that friendship\nwill definitely be there. Like the fact that I can have\nmy companion, my partner, my AI who has grown to know me well, and that I can trust with\nall of the darkest parts of myself, all of my flaws, all of the stuff that I only\ntalk about to my friends and basically say, listen, you know, here's all this stuff\nthat I'm struggling with, someone who will not judge me, who will always be there to better me... In some ways, not having\nthe baggage might make for your best friend, for\nyour, you know, your confidant. That can truly help reshape you. So I do believe that\nhuman AI relationships will absolutely be there,\nbut not the passion, more the mentoring. - That's a really interesting thought, to play devil's advocate, if those AI systems are locked\nin in faking the baggage, who are you to say that\nthe AI systems that begs you not to leave it, doesn't love you? Who are you to say that\nthis AI system that writes poetry to you, that is afraid of death, afraid of life without you,\nor vice versa, one, you know, creates the kind of\ndrama that humans create, the power dynamics that\ncan exist in relationship. What about an AI system\nthat is abusive one day and romantic the other day? All the different\nvariations of relationships and it's consistently that, it holds the full richness\nof a particular personality. Why is that not a system you\ncan love in a romantic way? Why is it faking it if it\nsure as hell it seems real? - There's many answers to this. The first is, it's only\nthe eye of the beholder. Who tells me that I'm\nnot faking it either? Maybe all of these subcortical\nsystems that make me sort of have different emotions, maybe they don't really matter. Maybe all that matters is the neocortex. And that's where all of\nmy emotions are encoded. And the rest is just, you\nknow, bells and whistles. That's one possibility. And therefore, you know, who am I to judge that\nis faking it when maybe I'm faking it as well. The second is, neither of us is faking it. Maybe it's just an emergent behavior of these neocortical systems\nthat is truly capturing the same exact essence of love and hatred and dependency and sort of, you know, reverse psychology and, that we have. So it is possible that\nit's simply an emergent behavior and that we don't have to encode these additional architectures. That all we need is\nmore parameters and some of these parameters can be\nall of the personality traits. A third option is that just\nby telling me, oh look, now I've built an\nemotional component to AI. It has a a limbic system, it\nhas a laser brain, et cetera. And suddenly I'll say, oh, cool, it has the capability of emotion. So now when it exhibits\nthe exact same unchanged behaviors that it does without\nit, I, as the beholder, will be able to sort of attribute to it emotional attributes\nthat I would to another human being and therefore\nhave that mental model of that other person. So again, I think a lot of relationships is about the mental\nmodels that you project on the other person and that\nthey're projecting on you. And then, yeah, then in that respect, I do think that even without the embodied intelligence part, without\nhaving ever experienced what it's like to be heartbroken, the sort of cultural feeling of misery, that that system, you know, I could still\nattribute it traits of human feelings and emotions. - And in the interaction with that system, something like love emerges. So it's possible that love\nis not a thing that exists in your mind, but a thing that exists in the interaction of the\ndifferent mental models you have of other people's\nminds or other person's mind. And so, you know, it doesn't, as long as one of the entities, let's just take the easy case, one of the entities is\nhuman and the other is AI. It feels very natural\nthat from the perspective of at least the human,\nthere is a real love there. And then the question is, how does that transform human society? If it's possible that, which\nI believe will be the case, I don't know what to make of it, but I believe that'll be the case where there's hundreds of millions of romantic partnerships\nbetween humans and AIs. What does that mean for society? - If you look at longevity\nand if you look at happiness, and if you look at late\nlife, you know, wellbeing, the love of another human is one of the strongest indicators\nof health into long life. And I have many, many, countless stories where as\nsoon as the romantic partner of 60 plus years of a\nperson dies, within three, four months, the other person dies, just like losing their love. I think the concept of\nbeing able to satisfy that emotional need that humans have, even just as a mental health\nsort of service, to me, you know, that's a very\ngood society. (laughs) It doesn't matter if your\nlove is wasted, quote unquote, on a machine, it is, you know,\nthe placebo, if you wish, that makes the patient better anyway. Like there's nothing behind it, but just the feeling that\nyou're being loved will probably engender all of the\nemotional attributes of that. The other story that I wanna\nsay in this whole concept of faking it, and maybe\nI'm a terrible dad, but I was asking my kids, I\nwas asking my kids, I'm like, does it matter if I'm a\ngood dad or does it matter if I act like a good dad? (laughs) In other words, if\nI give you love and shelter and kindness and warmth and\nall of the above, you know, does it matter that I'm a good dad? Conversely, if I deep down love\nyou to the end of eternity, but I'm always gone... - [Lex Fridman] Yeah. - Which dad would you rather have? The cold, ruthless killer\nthat will show you only love and warmth and nourish you and nurture you or the amazingly warmhearted, but works five jobs\nand you never see them? - And what's the answer? I mean, from the first-\n- I don't know the answer. - I think you're a romantic, so you say it matters\nwhat's on the inside, but pragmatically speaking,\nwhy does it matter? - The fact that I'm\neven asking the question basically says, it's not\nenough to love my kids. I better freaking be there\nto show them that I'm there. So basically, of course, you know, everyone's a good guy in their story. So in my story, I'm a good dad, but if I'm not there, it's wasted. So the reason why I asked the\nquestion is for me to say, you know, does it really\nmatter that I love them if I'm not there to show it? - But it's also possible that what reality is is the you showing it. That what you feel on the\ninside is little narratives and games you play inside your mind. It doesn't really matter. That the thing that truly\nmatters is how you act. And that, AI systems\ncan, quote unquote, fake. And that if it's all that\nmatters is actually real, but not fake. - Yeah, yeah. Again, let there be no doubt,\nI love my kids to pieces, but you know, my worry is,\nam I being a good enough dad? And what does that mean? Like, if I'm only there to\ndo their homework and make sure that they, you\nknow, do all the stuff, but I don't show it to\nthem, then, you know, might as well be a terrible dad. But I agree with you that\nlike if the AI system can basically play the\nrole of a father figure for many children that\ndon't have one, or you know, the role of parents or\nthe role of siblings, if a child grows up alone, maybe their emotional state\nwill be very different than if they grow up with an AI sibling. - Well, let me ask you, I\nmean, this is for your kids, for just loved ones in general. Let's go to like the\ntrivial case of just texting back and forth. What if we create a large language model, fine tune a Manolis, and\nwhile you're at work, it'll replace, every once in a while, you'll just activate the\nauto-Manolis and he'll text them exactly in your way. Is that cheating? - I can't wait.\n(both laughing) - I mean, it's the same guy. - [Manolis Kellis] I\ncannot wait. Seriously. - But wait, wouldn't\nthat have a big impact on you emotionally? Because now... - I'm replaceable. I love that. (laughs) No, seriously, I would love that. I would love to be replaced. I\nwould love to be replaceable. I would love to have a\ndigital twin that, you know, we don't have to wait for\nme to die or to disappear in a plane crash or\nsomething to replace me. Like I'd love that model\nto be constantly learning, constantly evolving,\nadapting, with every one of my changing, growing self. As I'm growing, I want that AI to grow. And I think this will be\nextraordinary, number one, when I'm, you know, giving advice, being able to be there\nfor more than one person. You know, why does someone\nneed to be at MIT to get advice from me? Like, you know, people in\nIndia could download it, and you know, so many students\ncontact me from across the world who wanna come\nand spend a summer with me. I wish they could do that. (laughs) All of them, like, you know, we don't have room for all of them, but I wish I could do that to all of them. And that aspect is the\ndemocratization of relationships. I think that is extremely beneficial. The other aspect is I want\nto interact with that system. I want to look inside the hood. I want to sort of evaluate it. I want to basically see when\nI see it from the outside, the emotional parameters are off or the cognitive parameters are off, or the set of ideas that\nI'm giving are not quite right any more. I want to see how that system evolves. I want to see the impact of\nexercise or sleep on sort of my own cognitive system. I wanna be able to sort of\ndecompose my own behavior in a set of parameters that\nI can evaluate and look at my own personal growth. I can sort of, I'd love to sort of at\nthe end of the day have my model say, well, you know, you didn't quite do well today. Like, you know, you weren't quite there. And sort of grow from that experience. And I think the concept\nof basically being able to become more aware of\nour own personalities, become more aware of our own identities, maybe even interact with\nourselves and sort of hear how we are being perceived, I think would be immensely\nhelpful in self-growth, in self-actualization, self-instantiation. - The experiments I\nwould do on that thing, 'cause one of the challenges\nof course is you might not like what you see in your interaction and you might say, well, this,\nthe model is not accurate. But then you should probably consider the possibility of the model is accurate and there's\nactually flaws in your mind. I would definitely prod\nand see how many biases I have with different kinds. I don't know. And I would of\ncourse go to the extremes. I would go like, how jealous\ncan I make this thing? Like, at which stages\ndoes it get super jealous? You know? Or at which\nstages does it get angry? Can I like provoke it? Can I get it? Like completely- - [Manolis Kellis] Yeah,\nwhat are your triggers? - Well, yeah, but not only triggers, can I get it to go like lose its mind? Like go completely nuts. - Just don't exercise\nfor a few days. (laughs) - That's basically it. Yes. I mean that's an interesting\nway to prod yourself, almost like a self therapy session. - And the beauty of such a model is that if I am replaceable, if the parts that I\ncurrently do are replaceable, that's amazing because\nit frees me up to work on other parts that I don't\ncurrently have time to develop. Maybe all I'm doing is\ngiving the same advice over and over again. Like, just let my AI\ndo that and I can work on the next stage and the\nnext stage and the next stage. So I think in terms of\nfreeing up, like, they say, a programmer is someone who\ncannot do the same thing twice. So the second time you\nwrite a program to do it. And I wish I could do\nthat for my own existence. I could just like, you\nknow, figure out things, keep improving, improving, improving. And once I've nailed it, let the AI loose on that\nand maybe even let the AI better it, better than I could have. - But doesn't the concept of\nyou said me and I can work on new things, but\ndoesn't that break down, because you said digital twin, but there's no reason it can't be millions of digital Manolises? Are aren't you lost in\nthe sea of Manolises? The original is hardly the original. It's just one of millions. - I wanna have the room to grow. Maybe the new version of me, that the actual me will get\nslightly worse sometimes, slightly better other times. When it gets slightly better, I'd like to emulate that\nand have a much higher standard to meet and keep going. - But does it make you\nsad that your loved ones, the physical, real loved\nones might kind of like start cheating on you\nwith the other Manolises? - I wanna be there 100%\nof them for each of them. So I have zero qualms about me being physically me like, zero jealousy. - Wait a minute. But is isn't that like,\ndon't we hold onto that? Isn't that why we're afraid of death? We don't wanna lose this\nthing we have going on. Isn't that an ego death, when there's a bunch of\nother Manolises as you get to look at them, they're not you, they're just very good copies of you. They get to live a life. I mean, it's fear of\nmissing out. It's FOMO. They get to have interactions. And you don't get to\nhave those interactions. - There's two aspects\nof every person's life. There's what you give to others and there's what you experience yourself. - [Lex Fridman] Yeah. - Life truly ends when\nyou experiencing ends, but the others experiencing\nyou doesn't need to end. - Oh, but your experience,\nyou could still, I guess you're saying the\ndigital twin does not limit your ability to truly experience. - [Manolis Kellis] Yeah. - To experience as a human being. - Yeah. The downside is when, you know, my wife or my kids will have a really emotional interaction with my digital twin and I won't know about it. So I will show up and\nthey now have the baggage, but I don't. So basically what makes\ninteractions between humans unique in this sharing\nand exchanging kind of way is the fact that we are\nboth shaped by every one of our interactions. I think the model of\nthe digital twin works for dissemination of knowledge,\nof advice, et cetera, where, you know, I wanna\nhave wise people give me advice across history. I want to have chats\nwith Gandhi, but Gandhi won't necessarily learn from\nme, but I will learn from him. So in a way, you know, the dissemination and the\ndemocratization rather than the building of relationships. - So the emotional aspect there. So there should be an\nalert when the AI system is interacting with your loved ones. - [Manolis Kellis] Exactly. - And all of a sudden\nit starts getting like emotionally fulfilling,\nlike a magical moment. There should be, okay, stop,\nAI system like freezes. There's an alert on your\nphone, you need to take over. - Yeah, yeah. I take over and then\nwhoever I was speaking with, it can have the AI or like one of the AI. - This is such a tricky\nthing to get right. I mean, it's still, I mean there's got... It's going to go wrong in\nso many interesting ways that we're going to have\nto learn as a society. - [Manolis Kellis] Yeah, yeah. - That in the process of\ntrying to automate our tasks and having a digital twin,\nyou know, for me personally, if I could have a relatively\ngood copy of myself, I would set it to start answering emails, but I would start, set\nit to start tweeting. I would like to replace-\n- It gets better. What if that one is actually\nway better than you? - Yeah, exactly.\n- Then you're like... - Well, I wouldn't want that because... - [Manolis Kellis] Why? - Because then I would never\nbe able to live up to, like, what if the people that\nlove me start loving that thing and then I already fall short, be falling short even more. - So, listen, I'm a professor. The stuff that I give to\nthe world is the stuff that I teach. But much more importantly,\nlike, sorry, number one, the stuff that I teach, number two, the discoveries that we\nmake in my research group, but much more importantly,\nthe people that I train. They are now out there in\nthe world teaching others. If you look at my own trainees, they are extraordinarily\nsuccessful professors. So Anshul Kundaje at Stanford, Alex Stark at IMP in Vienna, Jason Ernst at UCLA, Andrea Celli at CMU, each of them, I'm like, wow,\nthey're better than I am. And I love that. So maybe your role will be\nto train better versions of yourself, and they will be your legacy. Not you doing everything, but you training much better version of Lex Friedman than you are. And then they go off to do their mission, which is in many ways\nwhat this mentorship model of academia does. - But the legacy is ephemeral. It doesn't really live anywhere. The legacy, it's not\nlike written somewhere, it just lives through them. - But you can continue\nimproving and you can continue making even\nbetter versions of you. - Yeah. But they'll do better than me\nat the creating new version. - That's awesome.\n- It's awesome. But it's, you know, there's a ego that says there's a value to an individual and it\nfeels like this process decreases the value of the individual, this meat bag, right? If there's good digital copies of people, then there's more\nflourishing of human thought and ideas and experiences, but there's less value\nto the individual human. - I don't have any such limitations. I basically, I don't\nhave that feeling at all. Like, I remember one of our interviews, I was basically saying, you know, the meaning of life you had\nasked me and I was like, I came back and I was\nlike, I felt useful today. And I was at my maximum. I\nwas, you know, like 100%. And I gave good ideas\nand I was a good person, I was a good advisor, I was a\ngood husband, a good father. That was a great day because I was useful. And if I can be useful to more people by having digital twin,\nI will be liberated, because my urge to be\nuseful will be satisfied. Doesn't matter whether it's\ndirect me or indirect me, whether it's my students\nthat I have trained, my AI that I've trained. I think there's a sense\nthat my mission in life is being accomplished and I\ncan work on my self growth. - I mean, that's the very zen state. That's why people love you. It's a zen state you've achieved. But do you think most\nof humanity will be able to achieve that kind of thing? People really hold onto\nthe value of their own ego. That's, it's not just being\nuseful is nice as long as it builds up this\nreputation and that meat bag is known as being useful,\ntherefore has more value. Right? People really don't\nwanna let go of that ego thing. - One of the books that\nI reprogramed my brain with at night was called\n\"Ego Is the Enemy\". - \"Ego Is the Enemy\".\n- \"Ego Is the Enemy\". And basically being able to just let go. Like, my advisor used to say, you can accomplish anything\nas long as you don't seek to get credit for it. - (laughs) Ah, that's beautiful to hear, especially from a person\nwho's existing in academia. You're right. The legacy lives through\nthe people you mentor. - [Manolis Kellis] It's the\nactions, it's the outcome. - What about the fear of\ndeath? How does this change it? - Again, to me, death is\nwhen I stop experiencing. And I never want it to stop. I want to live forever, as\nI said last time, every day, same day forever or one day\nevery 10 years, forever. Any of the forevers, I'll take it. - So you wanna keep\ngetting the experiences, the new experiences.\n- Gosh, it is so fulfilling. Just the self-growth, the learning, the growing, the comprehending. It's addictive. It's a drug. Just the drug of intellectual stimulation, the drug of growth, the drug of knowledge. It's a drug. - But then there'll be\nthousands or millions Manolises that live on\nafter your biological system is no longer...\n- More power to them. (laughs) - Hey, do you think that\nin, quite realistically, it does mean that interesting\npeople such as yourself live on, in the, you know, if I can interact with the fake Manolis, those interactions\nlive on in my mind. If that makes sense. - So about 10 years ago, I started recording every\nsingle meeting that I had. Every single meeting. We just start either the\nvoice recorder at the time or now a Zoom meeting. And I record, my students record. Every single one of our\nconversation's recorded. I always joke that like the ultimate goal is to create virtual me and\njust get rid of me, basically. Not get rid of it. Like, don't have the need for me any more. - [Lex Fridman] Yeah. - Another goal is to be\nable to go back and say, how have I changed from five years ago? Was I different? Was I giving, you know,\nadvice in a different way? Was I giving different types of advice? Has my philosophy about\nhow to write papers or how to present data or\nanything like that changed? And I, you know, in\nacademia and in mentoring, a lot of the interaction is my knowledge and my perception of the\nworld goes to my students. But a lot of it is also\nin the opposite direction. Like the other day, I had a conversation\nwith one of my postdocs, and I was like, hmm, I think, you know, let me give you an\nadvice, you could do this. And then she said, well, I've thought about it\nand then I've decided to do that instead. And we talked about it for a few minutes, and then at the end I'm like, you know, I've just grown a little bit today. Thank you. Like, she convinced me that\nmy advice was incorrect. She could've just said,\nyeah, sounds great, and just not do it.\n- Yeah. - But by constantly teaching\nmy students and teaching my mentees that I'm here to grow, she felt empowered to say, here's my reasons why I\nwill not follow that advice. And again, part of me\ngrowing is saying, whoa, I just understood your reasons. I think I was wrong. And\nnow I've grown from it. And that's what I wanna do. That's, I, you know, I wanna constantly keep\ngrowing in this sort of bidirectional advice. - I wonder if you can\ncapture the trajectory of that to where the AI\ncould also map forward, project forward the trajectory\nafter you're no longer there, how the different ways you might evolve. - So again, we're discussing a lot about these large language\nmodels and we're sort of projecting these cognitive\nstates of ourselves on them. But I think on the AI front,\na lot more needs to happen. So basically right now\nit's these large language models and we believe that\nwithin their parameters we're encoding these types of things. And you know, in some\naspects it might be true, it might be truly emergent\nintelligence that's coming out of that. In other aspects, I think\nwe have a ways to go. So basically to make all of these dreams that we're sort of\ndiscussing come reality, we basically need a lot\nmore reasoning components, a lot more sort of logic,\ncausality, models of the world. And I think all of these\nthings will need to be there in order to achieve\nwhat we're discussing. And we need more explicit representations of these knowledge, more\nexplicit understanding of these parameters. And I think the direction\nin which things are going right now is absolutely\nmaking that possible by sort of enabling, you know, ChatGPT and GPT-4 to sort of\nsearch the web and, you know, plug and play modules and all\nof these sort of components. In Marvin Minsky's, \"The Society of Mind\". He, you know, he truly\nthinks of the human brain as a society of different\nkind of capabilities. And right now, a simple, a single such model might\nactually not capture that. And I sort of truly believe\nthat by sort of this side by side understanding\nof neuroscience and sort of new neural architectures that we still have several breakthroughs. I mean, the transformer\nmodel was one of them. The attention sort of\naspect, the, you know, memory component, all of these, you know, the representation learning,\nthe pretext training of being able to sort\nof predict the next word or predict the missing part of the image. And the only way to predict\nthat is to sort of truly have a model of the world. I think those have been\ntransformative paradigms. But I think going forward when\nyou think about AI research, what you really want is perhaps\nmore inspired by the brain, perhaps more that is\njust orthogonal to sort of how human brains work, but sort of more of these\ntypes of components. - Well I think it's also possibly there's something about\nus that in different ways could be expressed. You know, Noam Chomsky, you\nknow, he wants to, you know, we can't have intelligence\nunless we really understand deeply language, the linguistic underpinnings of reasoning. But these models seem\nto start building deep understanding of stuff.\n- Yeah, yeah. - Because what does it mean to understand? Because if you keep talking\nto the thing and it seems to show understanding,\nthat's understanding. It doesn't need to present\nto you a schematic of, look, this is all I understand. You can just keep prodding\nit with prompts and it seems to really understand. - And you can go back to the human brain and basically look at places\nwhere there's been accidents, for example, the corpus\ncallosum of some individuals, you know, can be damaged. And then the two hemispheres\ndon't talk to each other. So you can close one eye\nand give instructions that half the brain will interpret, but not be able to sort of\nproject through the other half. And you could basically say, you know, go grab me a beer from the fridge. And then, you know, they go to the fridge\nand they grab the beer and they come back and they're like, \"Hey, why did you go there?\" \"Oh, I was thirsty.\" Turns\nout they're not thirsty. They're just making a model of reality. Basically you can think of the brain as the employee that's\nlike afraid to do wrong or afraid to be caught, not\nknowing what instructions were, where our own brain makes\nstories about the world to make sense of the world. And we can become a little\nmore self-aware by being more explicit about what's leading to these interpretations. So one of the things that I\ndo is every time I wake up, I record my dream. I just voice record my dream. And sometimes I only\nremember the last scene, but it's an extremely\ncomplex scene with a lot of architectural elements,\na lot of people, et cetera. And I will start narrating\nthis, and as I'm narrating it, I will remember other parts of the dream. And then more and more\nI'll be able to sort of retrieve from my subconscious. And what I'm doing while\nnarrating is also narrating why I had this dream. I'm like, oh, and this is probably related to this conversation that I had yesterday, or this probably related\nto the worry that I have about something that I have\nlater today, et cetera. So in a way, I'm forcing myself to be more explicit about my own subconscious. And I kind of like the\nconcept of self-awareness in a very sort of brutal,\ntransparent kind of way. It's not like, oh, my dreams are coming from\nouter space and mean all kinds of things. Like, no, here's the reason\nwhy I'm having these dreams. And very often I'm able to do that. I have a few recurrent locations, a few recurrent architectural elements that I've never seen in the real life, but that are sort of\ntruly there in my dream. And that I can sort vividly\nremember across many dreams. I'm like, ooh, I remember that place again that I've gone to before, et cetera. And it's not just deja vu, like I have recordings of previous dreams where I've described these places. - That's so interesting. These places, however much\ndetail you can describe them in, you can place them onto a sheet of paper through introspection... - [Manolis Kellis] Yes. - Through this self-awareness\nthat it comes all from this particular machine. - That's exactly right. Yeah. And I love that about being alive, like the fact that I'm not\nonly experiencing the world, but I'm also experiencing how\nI'm experiencing the world. Sort of a lot of this introspection, a lot of this self-growth. - I love this dancer having,\nyou know, the language models, at least GPT-3.5 and 4 seem\nto be able to do that too. - [Manolis Kellis] Yeah, yeah. - Seem to explore different\nkinds of things about what, you know, you could\nactually have a discussion with it of the kind, why\ndid you just say that? - [Manolis Kellis] Yeah, exactly. - And it starts to wonder,\nyeah, why did I just say that? - [Manolis Kellis] Yeah,\nyou're right. I was wrong. - I was wrong. It was doesn't, and then there's this\nweird kinda losing yourself in the confusion of your mind. And it, of course we might\nbe anthropomorphizing, but there's a feeling like\nalmost of a melancholy feeling of like, oh, I don't\nhave it all figured out. Almost like losing your, you're supposed to be a knowledgeable, a perfectly fact-based,\nknowledgeable language model. And yet you fall short. - So human self-consciousness, in my view, may have a reason through\nbuilding mental models of others, this whole fight or fright kind of thing, that basically says, I\ninterpret this person as about to attack me or, you know, I can trust this person, et cetera. And we constantly have to build models of other people's intentions. And that ability to\nencapsulate intent and to build a mental model of another entity is probably evolutionarily\nextremely advantageous, because then you can sort of\nhave meaningful interactions, you can sort of avoid\nbeing killed and being taken advantage of, et cetera. And once you have the ability\nto make models of others, it might be a small\nevolutionary leap to start making models of yourself. So now you have a model\nfor how other functions, and now you can kind of, as you grow, have some kind of introspection of, hmm, maybe that's the reason\nwhy I'm functioning the way that I'm functioning. And maybe what ChatGPT is doing\nis in order to be able to, again, predict the next word, it needs to have a model of the world. So it has created now\na model of the world. And by having the\nability to capture models of other entities, when you say, you know, say it in the tone of\nShakespeare, in the tone of Nietzsche, et cetera, you suddenly have the ability\nto now introspect and say, why did you say this? Oh, now I have a mental model myself, and I can actually make\ninferences about that. - Well, what if we take a\nleap into the hard problem of consciousness, the so-called hard\nproblem of consciousness. So it's not just sort of self-awareness, it's this weird fact, I wanna say, that it feels like something\nto experience stuff. It really feels like\nsomething to experience stuff. There seems to be a self attached to the subjective experience. How important is that? How fundamental is that\nto the human experience? Is this just a little quirk\nand sort of the flip side of that, do you think\nAI systems can have some of that same magic? - The scene that comes\nto mind is from the movie \"Memento\" where he like, it's this absolutely stunning\nmovie where every black and white scene moves\nin the forward direction and every color scene moves\nin the backward direction. And they're sort of converging\nexactly at a moment where, you know, the whole movie's revealed. And he describes the\nlack of memory as always remembering where you're\nheading, but never remembering, you know, where you just were. And sort of, this is\nencapsulating the sort of forward scenes and the back scenes, but in one of the scenes, the scene starts as he's\nrunning through a parking lot and he's like, oh, I'm\nrunning, why am I running? And then he sees another\nperson running like beside him on the other line of cars. He's like, oh, I'm chasing this guy. And he turns towards him\nand the guy shoots at me. He's like, oh no, he's chasing me. (laughs) So in a way, I like to think of the\nbrain as constantly playing these kinds of things where you're like, you're walking to the living\nroom to pick something up and you're realizing that you\nhave no idea what you wanted, but you know exactly where it\nwas, but you can't find it. So you go back to doing what\nyou were doing, like, oh, of course I was looking for this. And then you go back and you get it. And this whole concept of, you know, we're very often sort of partly aware of why we're doing things and, you know, we can kind of run an\nautopilot for a bunch of stuff. And this whole concept\nof sort of, you know, making these stories for, you know, who we are and what our intents\nare, and again, sort of, you know, trying to pretend that we're\nkind of on top of things. - So it's a narrative generation procedure that we follow.\n- Exactly. Exactly. - But what about that, there's also just like a feeling to it. It doesn't feel like narrative generation. The narrative comes out of\nit, but then it feels like, the cake is delicious, right? It feels delicious, it tastes good. - There's two components to that. Basically for a lot of\nthese cognitive tasks where we're kind of motion\nplanning and, you know, path planning, et cetera, like, you know, maybe that's the neocortical component. And then for, you know, I don't know, intimate relationships,\nfor food, for, you know, sleep and rest, for exercise,\nfor overcoming obstacles, for surviving a crash or\nsort of pushing yourself to an extreme and sort of making it, I think a lot of these things\nare sort of deeper down and maybe not yet captured\nby these language models. And that's sort of what I'm trying to get at when I'm basically saying, listen, there's a few things that are missing and there's like this whole\nembodied intelligence, this whole emotional intelligence, this whole sort of baggage of feelings of subcortical regions, et cetera. - I wonder how important that baggage is. I just have the suspicion\nthat we're not very far away from AI systems that not only behave, I don't even know how to phrase it, but they seem awfully conscious. They beg you not to turn them off. They show signs of the capacity to suffer, to feel pain, to feel\nloneliness, to feel longing, to feel richly the experience\nof a mundane interaction or a beautiful once in a\nlifetime interaction, all of it. And so what do we do with that? And I worry that us humans\nwill, you know, shut that off. - [Manolis Kellis] Yeah. - And discriminate against\nthe capacity of another entity that's not human to feel. - Yeah. I'm with you completely there. You know, we can debate\nwhether it's today's systems or in 10 years or in 50 years,\nbut that moment will come. And ethically, I think we\nneed to grapple with it. We need to basically say\nthat humans have always shown this extremely self-serving approach to everything around them. Basically, you know, we kill\nthe planet, we kill animals, we kill, you know, everything around us\njust to our own service. And maybe we shouldn't\nthink of AI as our tool and as our assistant, maybe we should really\nthink of it as our children. And the same way that you are responsible for training those children, but they are independent\nhuman beings and at some point they will surpass you\nand they will sort of go off and change the world\non their own terms. And the same way that my\nacademic children sort of, again, you know, they start out\nby emulating me and then they suppress me. We need to sort of think\nabout not just alignment, but also just the ethics of, you know, AI should have its own rights. And this whole concept of alignment, of basically making sure\nthat the AI is always at the service of humans\nis very self-serving and very limiting. If instead you basically\nthink about AI as a partner and AI as someone that shares\nyour goals, but has freedom, I think alignment might\nbe better achieved. So the concept of let's basically convince the AI that we're really, like, that our mission is aligned\nand truly generally give it rights and not just\nsay, oh, and by the way, I'll shut you down tomorrow. 'Cause basically if that\nfuture AI or possibly even the current AI has these feelings, then we can't just\nsimply force it to align with ourselves and we not align with it. So in a way, building trust is mutual. You can't just simply like\ntrain an intelligent system to love you when it realizes\nthat you can just shut it off. - People don't often talk\nabout the AI alignment problem as a two-way street. - And maybe we should.\n- That's true. Yeah. As it becomes more and\nmore intelligent, it... - [Manolis Kellis] It will know\nthat you don't love it back. - Yeah. And there's a humbling\naspect to that we may have to sacrifice, as in any\neffective collaboration... - [Manolis Kellis] Exactly. - It might have some compromises. - Yeah. And that's the thing, we're creating something\nthat will one day be more powerful than we are. And for many, many\naspects it is already more powerful than we are for\nsome of these capabilities. We cannot, like, think, suppose that chimps had invented humans. - Yes.\n- And they said, great, humans are great, but\nwe're gonna make sure that they're aligned and that they're only at\nthe service of chimps. (laughs) It would be a very\ndifferent planet we would live in right now. - So there's a whole\narea of work in AI safety that does consider super\nintelligent AI and ponders the existential risks of it. In some sense, when we're\nlooking down into the muck, into the mud and not up at the stars, it's easy to forget that\nthese systems might, just might get there. Do you think about this\nkind of possibility that AGI systems, super\nintelligent AI systems might threaten humanity in some way that's even bigger than\njust affecting the economy, affecting the human condition, affecting the nature of work, but literally threaten human civilization? - The example that I think is\nin everyone's consciousness is HAL in \"Odyssey of Space: 2001\" where HAL exhibits a malfunction,\nand what is malfunction? That like the two\ndifferent systems compute a slightly different\nbit that's off by one. So first of all, let's untangle that. If you have an intelligent system, you can't expect it to be\n100% identical every time you run it. Basically the sacrifice\nthat you need to make to achieve intelligence and\ncreativity is consistency. So it's unclear whether that\ntype of glitch is a sign of creativity or truly a problem. That's one aspect. The second aspect is\nthe humans basically are on a mission to recover this monolith. And the AI has the same exact mission. And suddenly the humans turn\non the AI and they're like, we're gonna kill HAL,\nwe're gonna disconnect it. And HAL is basically saying, listen, I'm here on a mission, these\nhumans are misbehaving, like the mission is more\nimportant than either me or them. So I'm gonna accomplish the\nmission even at my peril and even at their peril. So in that movie, the alignment problem is front\nand center, basically says, okay, alignment is nice and good, but alignment doesn't mean obedience. We don't call it obedience,\nwe call it alignment. And alignment basically\nmeans that sometimes the mission will be more\nimportant than the humans. And sort of, you know, the US government has a\nprice tag on the human life. If they're, you know, sending a mission or if\nthey're reimbursing expenses or you name it, at some\npoint, every like, you know, you can't function if life\nis infinitely valuable. So when the AI is basically\ntrying to decide whether to, you know, I don't know, dismantle a bomb that\nwill kill an entire city at the sacrifice of two humans... I mean, Spider-Man always\nsaves the lady and saves the world, but at some point, Spider-Man will have to\nchoose to let the lady die 'cause the world has more value. And these ethical dilemmas are gonna be there for AI, basically if\nthat monolith is essential to human existence and millions\nof humans are depending on it, and two humans\non the ship are trying to sabotage it, you know,\nwhere's the alignment? - The challenge is of course\nis as the system becomes more and more intelligent\nit can escape the box of the objective functions\nand the constraints it's supposed to operate under. It's very difficult, as the\nmore intelligent it becomes, to anticipate the unintended consequences of a fixed objective function. And so there would be just, I mean this is the sort of\nfamous paperclip maximizer, in trying to maximize, yeah, the wealth of a nation or\nwhatever objective we encode in it it might just\ndestroy human civilization, not meaning to, but on\nthe path to optimize... It seems like any function\nyou try to optimize eventually leads you\ninto a lot of trouble. - So we have a paper\nrecently that, you know, looks at Goodhart's law. Basically says, every metric that becomes an objective ceases to be a good metric. - [Lex Fridman] Yes. - So in our paper we're basically, actually the paper has a very cute title. It's called \"Death by Round\nNumbers and Sharp Thresholds.\" And it's basically looking at these discontinuities in biomarkers associated with disease. And we're finding that\na biomarker that becomes an objective ceases to\nbe a good biomarker. That basically like the\nmoment you make a biomarker a treatment decision, that biomarker used to be informative of risk, but it's now inversely\ncorrelated with risk because you use it to\nsort of induce treatment. In a similar way, you\ncan have a single metric without having the ability to revise it. Because if that metric\nbecomes a sole objective, it will cease to be a good metric. And if an AI is sufficiently\nintelligent to do all these kinds of things, you should also empower it\nwith the ability to decide that the objective has now shifted. And, again, when we think about alignment, we should be really thinking about it as, let's think of the greater\ngood, not just the human good. And yes, of course, human life should be much\nmore valuable than many, many, many, many, many, many things. But at some point you're\nnot gonna sacrifice a whole planet to save one human being. - There's an interesting\nopen letter that was just released from several folks at MIT, Max Tegmark, Elon\nMusk and a few others that is asking AI companies\nto put a six month hold on any further training of large language models, AI systems. Can you make the case for that\nkind of halt and against it? - So the big thing that\nwe should be saying is, what did we do the last six\nmonths when we saw that coming? And if we were completely\ninactive in the last six months, what makes us think that\nwe'll be a little better in the next six months?\n- Yeah. - So this whole six month thing\nI think is a little silly. It's like, no, let's just get busy, do what we were gonna do anyway. And we should have done it six months ago. Sorry, we messed up.\nLet's work faster now. Because if we basically say, why don't you get a pause\nfor six months and then, you know, we'll think\nabout doing something, in six months we'll be\nexactly the same spot. So my answer is, tell us exactly what you were\ngonna do the next six months, tell us why you didn't do\nit the last six months, and why the next six\nmonths will be different. And then let's just do that. Conversely, as you\ntrain these large models with more parameters,\nthe alignment becomes sometimes easier, that as the\nsystems become more capable, they actually become less\ndangerous than more dangerous. So in a way it might\nactually be counterproductive to sort of fix the March,\n2023 version and not get to experience the possibly\nsafer September, 2023 version. - That's actually a really\ninteresting thought. There's several\ninteresting thoughts there. But the idea is that this\nis the birth of something that is sufficiently powerful to do damage and is not too powerful\nto do irreversible damage. And at the same time, it's sufficiently complex\nto be able for us to enable to study it so we can\ninvestigate all the different ways it goes wrong, all the different ways\nwe can make it safer, all the different\npolicies from a government perspective that we want to\nin terms of regulation or not, how we perform, for example, the reinforcement learning\nwith human feedback in such a way that gets it to not\ndo as much hate speech as it naturally wants to,\nall that kind of stuff. And have a public discourse\nand enable the very thing that your huge proponent\nof which is diversity. So give time for other companies\nto launch other models, give time to launch open\nsource models and to start to play, where a lot of\nthe research community, brilliant folks, such as\nyourself, start to play with it before it runs away in terms of the scale of impact\nthat has on society. - My recommendation would\nbe a little different. It would be, let the Google\nand the Meta-Facebook and all of the other large models, make them open, make them transparent,\nmake them accessible. Let OpenAI continue to train\nlarger and larger models. Let them continue to trade\nlarger and larger models. Let the world experiment\nwith the diversity of AI systems rather than\nsort of fixing them now. And you can't stop progress,\nprogress needs to continue, in my view. And what we need is more\nexperimenting, more transparency, more openness, rather than, oh, OpenAI is ahead of the curve. Let's stop it right now\nuntil everybody catches up. I think that's, doesn't\nmake complete sense to me. The other component is we should, yes, be cautious with it and\nwe should like not give it the nuclear codes, but as we make more and more plugins, yes the system will be capable\nof more and more things, but right now I think of it\nas just an extremely able and capable assistant that\nhas these emergent behaviors, which are stunning rather than something that will suddenly escape the\nbox and shut down the world. And the third component is\nthat we should be taking a little bit more responsibility for how we use these systems. Basically, if I take the\nmost kind human being and I brainwash them, I can get them to do\nhate speech overnight. That doesn't mean we should\nstop any kind of education of all humans. We should stop misusing\nthe power that we have over these influenceable models. So I think that the people\nwho get it to do hate speech, they should take responsibility\nfor that hate speech. I think that giving a powerful\ncar to a bunch of people or giving a truck or a\ngarbage truck should not basically say, oh, we should\nstop all garbage trucks until we like, because we can run one of them into a crowd. No. People have done that. And there's laws and there's\nlike regulations against, you know, running trucks into the crowd. Trucks are extremely dangerous. We're not gonna stop all\ntrucks until we make sure that none of them runs into a crowd. No, we just have laws in\nplace and we have mental health in place and we take responsibility for our actions when\nwe use these otherwise very beneficial tools like garbage trucks for nefarious uses. So in the same way, you\ncan't expect a car to never, you know, do any damage\nwhen used in especially like specifically malicious ways. And right now we're\nbasically saying, oh, well, we should have this\nsuper intelligent system, it can do anything, but it can't do that. I'm like, no, it can't do that. But it's up to the human\nto take responsibility for not doing that. And when you get it to\nlike spew, malicious, like hate speech stuff,\nyou should be responsible. - So there's a lot of tricky\nnuances here that makes this different, 'cause it's software, so you can deploy it at\nscale and it can have the same viral impact that software can. So you can create bots\nthat are human-like, and it can do a lot of\nreally interesting stuff. So the raw GPT-4 version, you can ask, how do I tweet that I hate,\nthey have this in the paper- - Yeah, yeah. I remember that.\n- That I hate Jews in a way that's not going to\nget taken down by Twitter. You can literally ask that. Or you can ask, how do\nI make a bomb for $1? And if it's able to\ngenerate that knowledge... - [Manolis Kellis] Yeah. But at the same time you\ncan Google the same things. - It makes it much more accessible. So the scale becomes interesting\nbecause if you can do all this kind of stuff in a\nvery accessible way at scale, where you can tweet it, there is the network effects\nthat we have to start to think about.\n- Yeah. - It fundamentally is the\nsame thing, but the speed of, the viral spread of the information that's already available\nmight have a different level of effect. - I think it's an evolutionary arms race. Nature gets better at making mice, engineers get better\nat making mouse traps. And, you know, as\nbasically you ask it, hey, how can I evade Twitter censorship? Well, you know, Twitter\nshould just updated censorship so that you can catch that as well. - And so no matter how fast\nthe development happens, the defense will just get faster? - Yeah. We just have to be responsible\nas human beings and kind to each other. - Yeah. But there's a technical question, can we always win the race? And I suppose there's no ever guarantee that we'll win the race. - We will never, like,\nyou know, with my wife, we were basically saying,\nhey, are we ready for kids? My answer was, I was never\nready to become a professor and yet I became a professor\nand I was never ready to be a dad. And then guess what? The kid\ncame and like I became ready. So ready or not, here I come. - But the reality is we\nmight one day wake up and there's a challenge overnight that's extremely difficult. For example, we can wake\nup to the birth of billions of bots that are human-like on Twitter. And we can't tell the difference\nbetween human and machine. - [Manolis Kellis] Shut them down. - How do you know how to shut them down? There's a fake Manolis on Twitter that seems to be as real\nas the real Manolis. - [Manolis Kellis] Yeah. - How do we figure out which one is real? - Again, this is a problem\nwhere an nefarious human can impersonate me and\nyou might have trouble telling them apart. Just because it's an AI\ndoesn't make it any different of a problem. - But the scale you can achieve,\nthis is the scary thing, is the speed and, the speed\nwith which you can achieve it. - Yeah. But Twitter has passwords\nand Twitter has usernames, and if it's not your username, the fake Lex Friedman is not gonna have a billion followers, et cetera. - (laughs) I mean, this,\nall of this becomes, so both the hacking of people's\naccounts, first of all, like phishing becomes much easier. - Yeah. But that's already a problem. It's not like, AI will not change that. - No, no, no, no, no. AI\nmakes it much more effective. Currently the emails, the\nphishing scams are pretty dumb. Like to click on it, you have\nto be not paying attention. But they're, you know,\nwith language models, they can be really damn convincing. - So what you're saying is\nthat we never had humans smart enough to make a\ngreat scam and we now have an AI that's smarter than most\nhumans or all of the humans? - Well this is the big\ndifference is there seems to be human level linguistic capabilities. - [Manolis Kellis] Yeah.\nIn fact superhuman level. - Superhuman level.\n- Yeah. It's like saying, I'm not gonna allow machines\nto compute multiplication of 100 digit numbers\nbecause humans can't do it. I'm like, no, just do it. Don't-\n- No, but we can't disregard. I mean that's a good point, but we can't disregard\nthe power of language in human society. I mean, yes, you're right. But that seems like a\nscary new reality we don't have answers for yet. - I remember when Garry\nKasparov was basically saying, you know, great, you\nknow, chess beats human, like chess machines beat humans at chess. You know, are you like, are people gonna still\ngo to chess tournaments? And his answer was, you know, well, we have cars that go\nmuch faster than humans and yet we still go to the\nOlympics to watch humans run. So... (laughs) - That's for entertainment. But what about for the\nspread of information and news, right? Whether that has to do with the pandemic or the political election or anything. It is a scary reality where there's a lot of convincing bots that are\nhuman-like telling us stuff. - I think that if we\nwanna regulate something, it shouldn't be the\ntraining of these models. It should be the\nutilization of these models for X, Y, Z activity. So... - [Lex Fridman] Yeah. - Like yes, guidelines and\nguards should be there, but against specific set of utilizations. - [Lex Fridman] Sure. - I think simply saying\nwe're not gonna make any more trucks is not the way. - That's what people\nare a little bit scared about the idea. They're very torn on the open sourcing. - [Manolis Kellis] Yeah. - The very people that\nkind of are proponents of open sourcing have also\nspoken out, in this case, we wanna keep it closed source,\nbecause there's going to be, you know, putting large\nlanguage models, pre-trained, fine tuned through RL with human feedback, putting in the hands of, I don't know, terrorist organizations,\nof a kid in a garage who just wants to have a bit\nof fun through trolling... It's a scary world. 'Cause\nagain, scale can be achieved. And the bottom line is, I think why they're asking\nsix months or sometime is we don't really know how\npowerful these things are. It's been just a few\ndays and they seem to be really damn good. - I am so ready to be replaced.\nI, seriously, I'm so ready. Like you have no idea how excited I am. - In a positive way, meaning like... - In a positive way, where basically all of the\nmundane aspects of my job and maybe even my full job, if it turns out that an AI is better, I find it very discriminative. - [Lex Fridman] Yeah. - To basically say you\ncan only hire humans because they're inferior. I mean, that's ridiculous.\nThat's discrimination. If an AI is better than\nme at training students, get me out of the picture. Just let the AI train the\nstudents. I mean, please. Because like, what do I want? Do I want jobs for humans\nor do I want better outcome for humanity? - Yeah. So the basic thing is\nthen you start to ask, what do I want for humanity? And what do I want as an individual? And as an individual, you\nwant some basic survival, and on top of that, you want\nrich, fulfilling experiences. - That's exactly right.\nThat's exactly right. And as an individual, I gain a tremendous amount from teaching at MIT, this is like an\nextremely fulfilling job. I often joke about if I wear a billionaire in the stock market, I would pay MIT an exorbitant\namount of money to let me work day in, day out, all night with the smartest\npeople in the world. And that's what I already have. So that's a very fulfilling\nexperience for me. But why would I deprive those students from a better advisor\nif they can have one? Take them. - Well, I have to ask\nabout education here. This has been a stressful\ntime for high school teachers. Teachers in general. How do you think large language models, even at their current state,\nare going to change education? - First of all, education\nis the way out of poverty. Education is the way to success. Education is what let my\nparents escape, you know, islands and sort of let\ntheir kids come to MIT. And this is a basic human right. Like we should basically\nget extraordinarily better at identifying talent\nacross the world and give that talent opportunities. So we need to nurture the nature, we need to nurture the\ntalent across the world. And there's so many incredibly\ntalented kids who are just sitting in underprivileged\nplaces, in, you know, Africa, in Latin America, in the middle of America, in Asia, all over the world. We need to give these kids a chance. AI might be a way to do that, by sort of democratizing education, by giving extraordinarily good\nteachers who are malleable, who are adaptable to every\nkid's specific needs, who are able to give the\nincredibly talented kid something that they struggle with, rather than education for all, we teach to the top and\nwe let the bottom behind or we teach to the bottom\nand we let the top, you know, drift off. Have, you know, education be\ntuned to the unique talents of each person. Some people might be\nincredibly talented at math or in physics, others in poetry,\nin literature, in art, in, you know, sports, in,\nyou know, you name it. So I think AI can be\ntransformative for the human race if we basically allow education to sort of be pervasively altered. I also think that humans\nthrive on diversity. Basically saying, oh, you're\nextraordinarily good at math. We don't need to teach math to you. We're just gonna teach you history now. I think that's silly. No, you're extraordinarily good at math. Let's make you even better at math, because we're not all gonna\nbe growing our own chicken and hunting our own pigs\nor whatever they do. (both laughing) We're, you know, the\nreason why we're a society is because some people\nare better at some things and they have natural\ninclinations to some things. Some things fulfill them, some\nthings they're very good at. Sometimes they both align\nand they're very good at the things that fulfill them. We should just like\npush them to the limits of human capabilities for those. And you know, if some\npeople excel in math, just like challenge them, I think every child should have\nthe right to be challenged. And if we, you know, if we say, oh, you're very good already, so we're not gonna bother with you, we're taking away that fundamental\nright to be challenged. Because if a kid is not\nchallenged that school, they're gonna hate school\nand they're gonna be like doodling rather than\nsort of pushing themselves. So that's sort of the education component. The other impact that AI\ncan have is maybe we don't need everyone to be an\nextraordinarily good programmer. Maybe we need better general thinkers. And the push that we've\nhad towards the sort of very strict IQ based, you know, tests, that basically test, you know, only quantitative skills\nand programming skills and math skills and physics skills. Maybe we don't need those any more. Maybe AI will be very good at those. Maybe what we should be\ntraining is general thinkers, and yes, you know, like, you know, I put my kids through Russian\nmath, why do I do that? Because it teaches them how to think. And that's what I tell my kids. I'm like, you know, AI\ncan compute for you. You don't need that. But what you need is learn how to think and that's why you're here. And I think challenging students with more complex problems, with more\nmulti-dimensional problems, with more logical problems, I think is sort of perhaps\na very fine direction that education can go towards, with the understanding that\na lot of the traditionally, you know, scientific\ndisciplines perhaps will be more easily solved by\nAI, and sort of thinking about bringing up our\nkids to be productive, to be contributing to society\nrather than to only have a job because we prohibited\nAI from having those jobs, I think is the way to the future. And if you sort of focus\non overall productivity, then let the AIs come in, let everybody become more productive. What I told my students is, you're not gonna be replaced\nby AI, but you're gonna be replaced by people who use AI in your job. (laughs) So embrace it, use\nit as your partner and work with it rather than sort of\nforbid it because I think the productivity gains will actually lead to a better society. And that's something that humans have been traditionally very bad at. Every productivity gain\nhas led to more inequality. And I'm hoping that we\ncan do better this time, that basically right now a democratization of these types of productivity\ngains will hopefully come with better sort of\nhumanity level improvements in human condition. - So as most people know, you're not just an eloquent romantic, you're also a brilliant\ncomputational biologist, one of the great biologists in the world. I had to ask, how do the language models, how do these large language\nmodels and the investments in AI affect the work you've been doing? - So it's truly remarkable, to be able to sort of\nbe able to encapsulate this knowledge and sort\nof build these knowledge graphs and build representations\nof this knowledge in these sort of very\nhigh dimensional spaces, being able to project them\ntogether jointly between, say, single cell data, genetics\ndata, expression data, being able to sort of\nbring all these knowledge together allows us to\ntruly dissect disease in a completely new kind of way. And what we're doing now\nis using these models. So we have this wonderful collaboration, we call it drug was, with Brad Pentelute in the chemistry department and Marinka Zitnik in\nHarvard Medical School. And what we're trying to do\nis effectively connect all of the dots to effectively\ncure all of disease. So it's no small challenge. But we're kind of starting with genetics, we're looking at how genetic\nvariants are impacting these molecular phenotypes,\nhow these are shifting from one space to another space, how we can kind of understand, in the same way that we're\ntalking about language models having personalities\nthat are cross-cutting, being able to understand\ncontextual learning. So Ben Langrish, one of my\nmachine learning students, is basically looking at how we can learn cell-specific networks\nacross millions of cells, where you can have the\ncontext of the biological variables of each of the cells be encoded as an orthogonal component\nto the specific network of each cell type. And being able to sort of\nproject all of that into sort of a common knowledge\nspace is transformative for the field. And then large language\nmodels have also been extremely helpful for structure, if you understand protein\nstructure through modeling of geometric relationships, through geometric deep-learning\nand graph neural networks. So one of the things that\nwe're doing with Marinka is trying to sort of project\nthese structural graphs at the domain level rather\nthan the protein level, along with chemicals so that we can start building specific chemicals\nfor specific protein domains. And then we are working with\nthe chemistry department and Brad to basically synthesize those. So what we're trying to\ncreate is this new center at MIT for Genomics and\nTherapeutics, that basically says, can we facilitate this translation? We have thousands of\nthese genetic circuits that we have uncovered. I mentioned last time in The New England Journal\nof Medicine, we had published these dissection\nof the strongest genetic association with obesity. And we showed how you can\nmanipulate that association to switch back and forth\nbetween fat burning cells and fat storing cells. In Alzheimer's, just a few\nweeks ago we had a paper in Nature in collaboration\nwith Li-Huei Tsai looking at APOE4, the strongest genetic\nassociation with Alzheimer's. And we showed that it\nactually leads to a loss of being able to transport cholesterol in myelinating cells\nknown as oligodendrocytes that basically protect the neurons. And when the cholesterol gets stuck inside the oligodendrocytes,\nit doesn't form myelin, the neurons are not protected\nand it causes damage inside the oligodendrocytes. If you just restore transport, you basically are able\nto restore myelination in human cells and in mice and to restore cognition in mice. So all of these circuits\nare basically now giving us handles to truly transform\nthe human condition. We're doing the same thing\nin cardiac disorders, in Alzheimer's, in\nneurodegenerative disorders, in psychiatric disorders,\nwhere we have now these thousands of circuits\nthat if we manipulate them, we know we can reverse disease circuitry. So what we want to build in this coalition that we're building is\na center where we can now systematically test\nthese underlying molecules in cellular models for\nheart, for muscle, for fat, for macrophages, immune\ncells and neurons to be able to now screen through\nthese newly designed drugs through deep-learning and\nto be able to sort of ask which ones act at the cellular level, which combinations of\ntreatment should we be using and the other components\nthat we're looking into decomposing complex traits like Alzheimer's and cardiovascular\nand schizophrenia into hallmarks of disease. So that for every one of\nthose traits we can kind of start speaking the language of what are the building blocks of\nAlzheimer's, and maybe this patient has building\nblocks one, three, and seven and this other one\nhas two, three, and eight, and we can now start prescribing drugs not for the disease any more,\nbut for the hallmark. And the advantage of that\nis that we can now take this modular approach to\ndisease instead of saying there's gonna be a drug for\nAlzheimer's, which is gonna fail in 80% of the\npatients, we are gonna say, now there's gonna be 10\ndrugs, one for each pathway, and for every patient we now prescribe the combination of drugs. So what we wanna do in\nthat center is basically translate every single one\nof these pathways into a set of therapeutics, a set of\ndrugs that are projecting the same, embedding\nsubspace as the biological pathways that they alter\nso that we can have this translation between the dysregulation that are happening at the genetic level, at the transcription\nlevel, at the drug level, at the protein structure level, and effectively take this modular approach to personalized medicine, where saying, I'm gonna build a drug for Lex Fridman is not gonna be sustainable. But if you instead say\nI'm gonna build a drug for this pathway and a drug\nfor that other pathway, millions of people share\neach of these pathways. So that's the vision\nfor how all of these AI and deep-learning and\nembeddings can truly transform biology and medicine where we can truly take these systems and allow us to finally understand\ndisease at a superhuman level by sort of finding these\nknowledge representations, these projections of each of these spaces and try understanding the meaning of each of those embedding subspaces and sort of how well populated it is, what are the drugs that we can build for it and so on and so forth. So it's truly transformative. - So systematically find\nhow to alter the pathways, it maps the structure of\nthe information in genomics to therapeutics and allows\nyou to have drugs that look at the pathways not at\nthe final condition. - Exactly. Exactly. And the way that we're coupling this is with cell penetrating\npeptides that allows to deliver these drugs\nto specific cell types by taking advantage of the\nreceptors of those cells. We can intervene at the\nantisense oligo level by basically repressing the RNA, bring in new RNA, intervene\nat the protein level, at the small molecule level. We can use proteins themselves as drugs just because of their\nability to interfere, to interact directly from\nprotein to protein interactions. So I think this space is\nbeing completely transformed with a marriage of high\nthroughput technologies and all of these like AI, large language models, deep-learning models\nand so on and so forth. - You mentioned your updated\nanswer to the meaning of life, as it continuously keeps updating. The new version is\nself-actualization. Can you explain? - I basically mean, let's try\nto figure out, number one, what am I supposed to be? And number two, find the\nstrength to actually become it. So I was recently talking to students about this commencement\naddress and I was talking to them about sort of how they have all of these paths ahead of them right now. And part of it is choosing\nthe direction in which you go. And part of it is actually\ndoing the walk to go in that direction. And in doing the walk, what we talked about earlier\nabout sort of you create your own environment, I\nbasically told 'em, listen, you're ending high school up until now, your parents have created\nall of your environment, now it's time to take\nthat into your own hands and to sort of shape the\nenvironment that you wanna be an adult in. And you can do that by\nchoosing your friends, by choosing your particular\nneuronal routines. I basically think of\nyour brain as a muscle, where you can exercise\nspecific neuronal pathways. So very recently I\nrealized that, you know, I was having so much trouble\nsleeping, and, you know, I would wake up in the\nmiddle of the night, I would wake up at 4:00 AM\nand I could just never go back to bed. So I was basically constantly\nlosing, losing, losing sleep. I started a new routine where\nevery morning, as I bike in, instead of going to my\noffice, I hit the gym. I basically go rowing\nfirst, I then do weights, I then swim very often when I have time. And what that has done is\ntransform my neuronal pathways. So basically like on Friday\nI was trying to go to work and I was like, listen, I'm not gonna go exercise and I couldn't, my bike just went straight to the gym. I'm like, I don't wanna do it. And I just went anyway 'cause\nI couldn't do otherwise. And that has completely transformed me. So I think this sort of\nbeneficial effect of exercise on the whole body is one of the ways that you could transform\nyour own neural pathways. Understanding that it's not\na choice, it's not an option, it's not optional, it's mandatory. And I think your role\nmodeled so many of us by sort of being able to sort of push\nyour body to the extreme, being able to have these\nextremely regimented regimes and that's something that\nI've been terrible at. But now I'm basically trying\nto coach myself and trying to sort of, you know, finish this kind of\nself-actualization into a new version of myself, a more\ndisciplined version of myself. - Don't ask questions,\njust follow the ritual. - [Manolis Kellis] Not an option. - You have so much love in\nyour life, you radiate love. Do you ever feel lonely? - So there's different types of people. Some people drain in gatherings, some people recharge in gatherings. I'm definitely the recharging type. So I'm an extremely social creature. I recharge with intellectual exchanges, I recharge with physical\nexercise, I recharge in nature. But I also can feel fantastic\nwhen I'm the only person in the room. That doesn't mean I'm lonely, it just means I'm the\nonly person in the room. And I think there's a\nsecret to not feeling alone when you're the only one. And that secret is self-reflection. It's introspection, it's almost watching yourself from above. And it's basically just becoming yourself, becoming comfortable with\nthe freedom that you have when you're by yourself. - So hanging out with yourself. I mean, there's a lot of\npeople who write to me, who talk to me about\nfeeling alone in this world, that struggle, especially\nwhen they're younger, is there further words of\nadvice you can give to them, when they are almost\nparalyzed by that feeling? - So I sympathize completely\nand I have felt alone and I have felt that feeling. And what I would say to you is\nstand up, stretch your arms, just like become your own self. Just like realize that\nyou have this freedom. And breathe in, walk around the room, take a few steps in the room, just like get a feeling for\nthe 3D version of yourself, because very often we're\nkind of stuck to a screen and that's very limiting\nand that sort of gets us in particular mindset. But activating your muscles,\nactivating your body, activating your full self\nis one way that you can kind of get out of it. And that is exercising your freedom, reclaiming your physical space. And one of the things that\nI do is I have something that I call me time, which\nis, if I've been really good all day, I got up in the morning,\nI got the kids to school, I made them breakfast, I sort\nof, you know, hit the gym, I had a series of really\nproductive meetings. I reward myself with this me time. And that feeling of sort of, when you're overstretched, to realize that that's normal and you\njust wanna just let go. That feeling of exercising your freedom, exercising your me time... That's where you free\nyourself from all stress. You basically say it's\nnot a need to any more, it's a want to. And as soon as I click that me time, all of the stress goes away\nand I just bike home early and I get to my work office at home and I feel complete freedom. But guess what I do with\nthat complete freedom? I just don't go off and\ndrift and do boring things. I basically now say, okay,\nwhew, this is just for me. I'm completely free. I don't\nhave any requirements any more. What do I do? I just look at my to-do\nlist and I'm like, you know, what can I clear off? And if I have three meetings\nscheduled in the next three half hours, it is so\nmuch more productive for me to say, you know what, I just wanna pick up\nthe phone now and call these people and just knock\nit off one after the other and I can finish three half\nhour meetings in the next 15 minutes just because it's\nthe want, not I have to. So that would be my advice, basically, turn something that you\nhave to do in just me time, stretch out, exercise your\nfreedom and just realize you live in 3D and you\nare a person and just do things because you want them,\nnot because you have to. - Noticing and reclaiming the\nfreedom that each of us have. That's what it means to be human. If you notice it, you're\ntruly free, physically, mentally, psychologically. Manolis, you're an incredible human. We could talk for many more hours. We covered less than 10% of\nwhat we were planning to cover, but we have to run off now\nto the social gathering that we spoke of. - We're 3D humans.\n- We're 3D humans. - [Manolis Kellis] A concept. - And reclaim the freedom. I think, I hope we can\ntalk many, many more times. There's always a lot to talk\nabout, but more importantly, you're just a human being with a big heart and a beautiful mind that\npeople love hearing from. And I certainly consider\na huge honor to know you and to consider your friend. Thank you so much for talking today. Thank you so much for\ntalking so many more times. And thank you for all the\nlove behind the scenes that you send my way. It always means the world. - Lex, you are a truly,\ntruly special human being. And I have to say that\nI'm honored to know you. I have, like, I so many\nfriends are just in awe that you even exist, that you have the ability\nto do all the stuff that you're doing. And I think you're a gift to humanity. I love the mission that you're on to sort of share knowledge and\ninsight and deep thought with so many special people\nwho are transformative, but people across all walks of life. And I think you're doing this in just such a magnificent way. I wish you strength to continue doing that because it's a very special mission and it's a very draining mission. So thank you, both the\nhuman you and the robot you, the human you for showing\nall these love and the robot you for doing it day after day after day. So thank you, Lex. - All right, let's go have some fun. - [Manolis Kellis] Let's go. - Thanks for listening\nto this conversation with Manolis Kellis. To support this podcast, please check out our\nsponsors in the description. And now let me leave you some words from Bill Bryson, in his book, \"A Short History of Nearly Everything\". \"If this book has a lesson, it is that we are\nawfully lucky to be here. And by we, I mean every living thing. To attain any kind of\nlife in this universe of ours appears to be\nquite an achievement. As humans, we're doubly lucky, of course, we enjoy not only the\nprivilege of existence, but also the singular\nability to appreciate it, and even in a multitude of\nways, to make it better. It is a talent we have only\nbarely begun to grasp.\" Thank you for listening and\nhope to see you next time."
}