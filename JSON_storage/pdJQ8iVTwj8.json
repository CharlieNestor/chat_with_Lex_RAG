{
  "video_id": "pdJQ8iVTwj8",
  "title": "Chris Lattner: Future of Programming and AI | Lex Fridman Podcast #381",
  "date": "2023-06-02",
  "transcript": [
    {
      "timestamp": "0:00",
      "section": "Full Transcript",
      "text": "- On one access, you have\nmore hardware coming in. On the other hand, you have an explosion of innovation in AI. And so what happened with\nboth TensorFlow and PyTorch is that the explosion of\ninnovation in AI has led to, it's not just about matrix\nimplication and convolution. These things have now, like,\n2,000 different operators. And on the other hand, you have, I don't know how many pieces of hardware out there\nare there, it's a lot. Part of my thesis, part of my belief of where computing goes, if you look out 10 years from now, is it's not gonna get simpler. Physics isn't going back\nto where we came from. It's only gonna get weirder\nfrom here on out, right? And so to me, the exciting part about\nwhat we're building is it's about building\nthat universal platform, which the world can continue\nto get weird 'cause, again, I don't think it's\navoidable, it's physics, but we can help lift people,\nscale, do things with it, and they don't have to rewrite their code every time a new device comes out. And I think that's pretty cool. - The following is a\nconversation with Chris Lattner, his third time on this podcast. As I've said many times before, he's one of the most brilliant engineers in modern computing, having created LLVM Compiler\nInfrastructure project, the Clang compiler, the\nSwift programming language, a lot of key contributions to TensorFlow and TPUs as part of Google. He's served as Vice President of Autopilot Software at Tesla, was a software innovator\nand leader at Apple. And now he co-created a new\nfull stack AI infrastructure for distributed training, inference, and deployment on all kinds\nof hardware called Modular, and a new programming\nlanguage called Mojo. That is a superset of Python, giving you all the usability of Python, but with the performance of C, C++. In many cases, Mojo code has demonstrated over 30,000x speed up over Python. If you love machine\nlearning, if you love Python, you should definitely give Mojo a try. This programming language, this new AI framework and infrastructure and this conversation with\nChris is mind-blowing. I love it. It gets pretty technical at times, so I hope you hang on for the ride. This is the Lex Fridman podcast. To support it, please check out our\nsponsors in the description. And now, dear friends,\nhere's Chris Lattner. It's been, I think two\nyears since we last talked, and then in that time, you somehow went and co-created a new programming language called Mojo. So it's optimized for AI.\nIt's a superset of Python. Let's look at the big picture.\nWhat is the vision for Mojo? - For Mojo? Well, so I mean,\nI think you have to zoom out. So I've been working on a lot of related technologies\nfor many, many years. So I've worked on LLVM and a lot of things and mobile and servers\nand things like this, but the world's changing. And what's happened with AI is we have new GPUs and new machine\nlearning accelerators and other ASICs and things like that, that make AI go real fast. At Google, I worked on TPUs. That's one of the biggest, largest scale deployed\nsystems that exist for AI. And really what you see is, if you look across all of\nthe things that are happening in the industry, there's this\nnew compute platform coming. And it's not just about\nCPUs, or GPUs, or TPUs, or NPUs, or IPUs, or whatever, all the PUs, (chuckles) right? It's about, how do we\nprogram these things, right? And so for software folks like us, right, it doesn't do us any good if there's this amazing\nhardware that we can't use. And one of the things you\nfind out really quick is that having the theoretical capability of programming something and then having the world's\npower and the innovation of all the smart people in the world get unleashed on something\ncan be quite different. And so really where Mojo came from was, starting from a problem of, we need to be able to\ntake machine learning, take the infrastructure underneath it and make it way more\naccessible, way more usable, way more understandable by\nnormal people and researchers and other folks that are not themselves like experts in GPUs and things like this. And then through that\njourney, we realized, \"Hey, we need syntax for this. We need to do a programming language.\" - So one of the main\nfeatures of the language, I say so, fully in jest, is that it allows you to\nhave the file extension to be an emoji or the fire emoji, which is one of the first emojis used as a file extension\nI've ever seen in my life. And then you ask yourself the question, why in the 21st century, we're not using Unicode\nfor file extensions? This, I mean, it's an epic decision. I think, clearly, the most important\ndecision you made the most, but you could also just use\nM-O-J-O as the file extension. - Well, so, okay. So take a step back. I mean, come on, Lex. You think that the world's ready for this? This is a big moment in the world, right? - We're releasing this onto the world. (chuckles)\n- This is innovation. - I mean, it really is kinda brilliant. Emojis are such a big\npart of our daily lives, why isn't it not in programming? - Well, and like you take a step back and look at what file\nextensions are, right, they're basically metadata, right? And so why are we spending\nall the screen space on them and all this stuff? Also, you know, you have them stacked up next to text files and PDF\nfiles and whatever else. Like, if you're gonna do something cool, you want it to stand out, right? And emojis are colorful. They're visual. They're beautiful, right?\n- Yeah. What's been the response so far from... Is there a support on like Windows on operating system-\n- Yeah. - In displaying like File Explorer? - Yeah, yeah. The one problem I've seen is the git doesn't escape it, right? And so it thinks that the\nfire emoji is unprintable. And so it like prints out weird hex things if you use the command line git tool, but everything else, as far\nas I'm aware, works fine. And I have faith that Git can be improved. So I'm not worried.\n- And so GitHub is fine. - GitHub is fine, yep. GitHub is fine. Visual Studio Code, Windows,\nlike all this stuff, totally ready because people\nhave internationalization in their normal-\n- Yeah. - Part of their paths. So let's just like take\nthe next step, right? Somewhere between, \"Oh,\nwow, that makes sense. Cool, I like new things,\" to \"Oh my god, you're killing my baby. Like, what are you talking\nabout? This can never be. Like, I can never handle this.\nHow am I gonna type this? (imitates bees buzzing)\nlike, all these things. And so this is something where I think that the world will get there. We don't have to bet\nthe whole farm on this. I think we can provide both paths, but I think it'll be great. - When can we have emojis as\npart of the code? I wonder. - Yeah. So, I mean, lots\nof languages provide that. So I think that we have\npartial support for that. It's probably not fully done yet, but yeah, you can do that. For example, in Swift,\nyou can do that for sure. So an example we gave at Apple was the dog cow.\n- Yeah. - So that's a classical\nMac heritage thing. And so you use the dog and\nthe cow emoji together, and that could be your\nvariable name, but of course, the internet went and made pile of poop for everything.\n- Yeah. - So, you know, if you wanna name your\nfunction pile of poop, then you can totally go to town and see how that gets through code review. (Lex chuckling) - Okay. So let me just ask\na bunch of random questions. So is Mojo primarily designed for AI or is it a general purpose programming? - Yeah, good question. So it's AI first. And so AI is driving a\nlot of the requirements. And so Modular is building and designing and driving Mojo forward. And it's not because it's\nan interesting project, theoretically, to build. It's because we need it. And so at Modular, we're really tackling the\nAI infrastructure landscape and the big problems in AI and the reasons that is so\ndifficult to use and scale and adopt and deploy and like\nall these big problems in AI. And so we're coming at\nit from that perspective. Now, when you do that, when you start tackling these problems, you realize that the\nsolution to these problems isn't actually an AI-specific solution. And so while we're doing\nthis we're building Mojo to be a fully general\nprogramming language. And that means that you\ncan obviously tackle GPUs, and CPUs and, like, these AI things, but it's also a really\ngreat way to build NumPy and other things like that, or, you know, just if you look at what many\nPython libraries are today, often they're a layer\nof Python for the API, and they end up being C and\nC++ code underneath them. That's very true in AI. That's true in lots of\nother demands as well. And so anytime you see this pattern, that's an opportunity for Mojo\nto help simplify the world and help people have one thing. - So optimize through\nsimplification by having one thing. So you mentioned Modular. Mojo\nis the programming language. Modular is the whole software stack. - So just over a year ago, we started this company called Modular. - [Lex] Yeah. - Okay, what Modular's about is, it's about taking AI and up-leveling it into the next generation, right? And so if you take a step back, what's gone on in the last\nfive, six, seven, eight years is that we've had things like\nTensorFlow and PyTorch and these other systems come in. You've used them. You know this. And what's happened is these things have grown like crazy, and\nthey get tons of users. It's in production deployment scenarios. It's being used to power so many systems. I mean, AI's all around us now. It used to be controversial\nyears ago, but now it's a thing. But the challenge with these\nsystems is that they haven't always been thought out with\ncurrent demands in mind. And so you think about it. Where were LLMs eight\nyears ago? (chuckles) Well, they didn't exist, right? AI has changed so much, and a lot of what people are doing today are very different than when\nthese systems were built. And meanwhile, the hardware side of this has gone into a huge mess. There's tons of new\nchips and accelerators, and every big company's announcing a new chip every day, it feels like. And so between that, you have like moving system on one side, moving system on the other side, and it just turns into this gigantic mess, which makes it very difficult\nfor people to actually use AI, particularly in production\ndeployment scenarios. And so what Modular's doing is we're helping build\nout that software stack to help solve some of those problems so then people can be more productive and get more AI research into production. Now, what Mojo does is it's a really, really, really important piece of that. And so that is, you know, part of that engine and\npart of the technology that allows us to solve these problems. - So Mojo is a programming\nlanguage that allows you to do the higher level programming,\nthe low-level programming, like do all kinds of\nprogramming in that spectrum that gets you closer and\ncloser to the hardware. - So take a step back. So Lex, what do you love about Python? - Oh, boy. Where do I begin? What is love? What do I love about Python? - [Chris] You're a guy who\nknows love. I know this. - Yes. How intuitive it is, how it feels like I'm writing\nnatural language English. - [Chris] Yeah. - How, when I can not just write, but read other people's codes, somehow I can understand it faster. It's more condensed than other languages, like ones I'm really familiar\nwith, like C++ and C, there's a bunch of sexy little features. - [Chris] Yeah. - We'll probably talk about some of them, but list comprehensions\nand stuff like this. - Well, so Py... And don't forget the entire\necosystem of all the packages. - [Lex] Oh, yeah. There's probably huge- - 'Cause there's always something. If you wanna do anything,\nthere's always a package. - Yeah, so it's not just the\necosystem of the packages and the ecosystem of\nthe humans that do it. That's an interesting dynamic because I think-\n- That's good. Yeah. - Something about the usability and the ecosystem makes\nthe thing viral, it grows, and then it's a virtuous cycle, I think. - Well, and there's many\nthings that went into that. Like, so I think that ML\nwas very good for Python. And so I think that TensorFlow\nand PyTorch and these systems embracing Python really\ntook and helped Python grow, but I think that the major\nthing underlying it is that Python's like the\nuniversal connector, right? It really helps bring together\nlots of different systems so you can compose them and\nbuild out larger systems without having to understand how it works. But then, what is the problem\nwith Python? (chuckles) - Well, I guess you\ncould say several things, but probably that it's slow. - I think that's usually what\npeople complain about, right? And so, slow. I mean, other people would complain about tabs and spaces versus\ncurly braces or whatever, but I mean, those people are just wrong 'cause it is-\n- Yeah. - Actually just better to use indentation. - Wow, strong words.\n(Chris laughing) So actually, I just\nwent on a small tangent. Let's actually take that. Let's\ntake all kinds of tangents. - Oh, come on, Lex. You can push me on it. I can take it. - Design, designed. Listen, I've recently\nleft Emacs for VS Code. - [Chris] Okay. - And the kinda hate\nmail I had to receive, because on the way to\ndoing that, I also said, I've considered Vim. - [Chris] Yep. - And chose not to and\nwent with VS Code and just- - You're touching on\ndeep religions, right? - Anyway, tabs is an\ninteresting design decision. And so you've really written a new programming language here. Yes, it is a superset of Python, but you can make a bunch\nof different interesting decisions here.\n- Totally, yeah. - And you chose actually\nto stick with Python in terms of some of the syntax. - Well, so let me explain why, right? So I mean, you can explain\nthis in many rational ways. I think that the annotation is beautiful, but that's not a rational\nexplanation, right, but I can defend it rationally, right? So first of all, Python 1\nhas millions of programmers. It's huge. It's everywhere.\n- Yeah. It owns machine learning, right? And so, factually, it is the thing, right? Second of all, if you look at it, C code, SQL Plus code,\nJava, whatever, Swift, curly brace languages also run through formatting tools and get indented. And so if they're not indented correctly, first of all, will twist\nyour brain around. (chuckles) It can lead to bugs. There's notorious bugs that\nhave happened across time where the annotation\nwas wrong or misleading and it wasn't formatted right, and so it turned into an issue, right? And so what ends up happening in modern large-scale code bases is people run automatic formatters. So now what you end up with is\nindentation and curly braces. Well, if you're gonna have, you know, the notion of grouping, why not have one thing, right, and get rid of all the clutter and have a more beautiful thing, right? Also, you look at many of these\nlanguages, it's like, okay, well, you can have curly braces, or you can omit them if\nthere's one statement, or you just like enter this entire world of complicated design\nspace that, objectively, you don't need if you have\nPython-style indentation, so. - Yeah, I would love to\nactually see statistics on errors made because of indentation. Like, how many errors are\nmade in Python versus in C++ that have to do with basic formatting, all that kinda stuff? I would love to see. - I think it's probably pretty\nminor because once you get, like you use VS Code, I do too. So if you get VS Code set up, it does the annotation for you, generally, right?\n- Yep. - And so you don't, you know, it's actually really nice\nto not have to fight it. And then what you can see\nis the editors telling you how your code will work by indenting it, which I think is pretty cool. - I honestly don't think I've ever... I don't remember having an\nerror in Python because I indented stuff wrong.\n- Yeah. So I mean, I think that there's, again, this is a religious thing. And so I can joke about it and\nI love to kind of, you know, I realize that this is\nsuch a polarizing thing and everybody wants to argue about it. And so I like poking at the\nbear a little bit, right? But frankly, right, come\nback to the first point, Python 1, like, it's huge.\n- Yeah. - It's in AI. It's the right thing. For us, like, we see Mojo\nas being an incredible part of the Python ecosystem. We're not looking to\nbreak Python or change it, or, quote, unquote, \"fix it.\" We love Python for what it is. Our view is that Python\nis just not done yet. And so if you look at, you know, you mentioned Python being slow. Well, there's a couple of different things that go into that, which we\ncan talk about if you want. But one of them is that it just\ndoesn't have those features that you would use to\ndo C-like programming. And so if you say, okay, well, I'm forced out of Python into\nC, for certain use cases, well, then what we're\ndoing is we're saying, \"Okay, well, why is that? Can we just add those features that are missing from\nPython back up to Mojo?\" And then you can have everything\nthat's great about Python, all the things that you're\ntalking about that you love plus not be forced out of it when you do something a little bit more computationally intense,\nor weird, or hardware-y, or whatever it is that you're doing. - Well, a million questions I wanna ask, but high level again-\n- Yeah. - Is it compiled or is it\nan interpreted language? So Python is just-in-time\ncompilation. What's Mojo? - So Mojo, a complicated\nanswer, does all the things. So it's interpreted, it's JIT compiled, and it's statically compiled. (chuckles) And so this is for a variety of reasons. So one of the things that\nmakes Python beautiful is that it's very dynamic. And because it's dynamic, one of the things they\nadded is that it has this powerful metaprogramming feature. And so if you look at something\nlike PyTorch or TensorFlow or, I mean, even a simple use case, like you define a class that\nhas the plus method, right, you can overload the dunder methods, like dunder add, for example, and then the plus method\nworks on your class. And so it has very nice\nand very expressive dynamic metaprogramming features. In Mojo, we want all\nthose features come in. Like, we don't wanna break\nPython, we want it all to work. But the problem is, is you can't run those\nsuper dynamic features on an embedded processor\nor on a GPU, right? Or if you could, you probably don't want to just\nbecause of the performance. And so we entered this\nquestion of saying, okay, how do you get the power of\nthis dynamic metaprogramming into a language that has to be super efficient in specific cases? And so what we did was we said, okay, well, take that interpreter. Python has an interpreter in it, right? Take that interpreter and allow\nit to run at compile time. And so now what you get is you get compiled time metaprogramming. And so this is super\ninteresting, super powerful, because one of the big advantages you get is you get Python-style expressive APIs, you get the ability to\nhave overloaded operators. And if you look at what happens inside of, like PyTorch, for example, with automatic differentiation and eager mode and like all these things, they're using these really dynamic and powerful features at runtime, but we can take those\nfeatures and lift them so that they run at compile time. - 'Cause C++ has\nmetaprogramming with templates. - [Chris] Yep. - But it's really messy. - It's super messy. It was accidentally, I mean, different people have\ndifferent interpretations. My interpretation is that it\nwas made accidentally powerful. It was not designed to be\nTuring-complete, for example, but that was discovered kind\nof along the way, accidentally. And so there have been a number\nof languages in the space. And so they usually have\ntemplates or code instantiation, code-copying features of various sorts. Some more modern languages or some newer languages, let's say, like, you know, they're fairly unknown. Like Zig, for example, says, okay, well, let's take all of\nthose types you can run it, all those things you can do at runtime and allow them to happen at compile time. And so one of the\nproblems with C++, I mean, which is one of the problems with C++ is-\n- There we go. Strong words. We're gonna offend everybody today. - Oh, that's okay. I mean, everybody hates me\nfor a variety of reasons anyways, I'm sure, right? (chuckles) I've written up- - That's the way they show love is to hurt you.\n- I have written enough C++ code to earn a little\nbit of grumpiness with C++, but one of the problems with\nit is that the metaprogramming system templates is just a\ncompletely different universe from the normal runtime programming world. And so if you do\nmetaprogramming and programming, it's just like a different universe, different syntax, different concepts, different stuff going on. And so, again, one of\nour goals with Mojo is to make things really easy\nto use, easy to learn, and so there's a natural stepping stone. And so as you do this, you say, okay, well, I have to do programming at runtime, I have to do programming at compile time. Why are these different things? - How hard is that to pull it off? 'Cause that sounds, to me, as a fan of metaprogramming and C++ even, how hard is it to pull that off? That sounds really, really exciting 'Cause you can do the\nsame style programming at compile time and at runtime. That's really, really exciting.\n- Yep, yep, and so, I mean, in terms of the compiler\nimplementation details, it's hard. I won't be shy about\nthat. It's super hard. It requires, I mean, what Mojo has underneath the covers is a completely new approach to the design of the compiler itself. And so this builds on these technologies like MiR that you mentioned. That also includes other, like caching and other interpreters and JIT compilers and\nother stuff like that- - [Lex] So you have like\nan interpreter inside the- - Within the compiler, yes. - [Lex] Oh, man. - And so it really takes the standard model of\nprogramming languages and kind of twists it and unifies\nit with the runtime model, which I think is really cool.\n- Right. - And to me, the value\nof that is that, again, many of these languages have\nmetaprogramming features. Like, they grow macros\nor something, right? List, right? - Yes. - I know your roots, right?\n(Lex chuckling) You know, and this is a\npowerful thing, right? And so, you know, if you go back to list, one of the most powerful\nthings about it is that it said that the metaprogramming and the programming are the same, right? And so that made it way\nsimpler, way more consistent, way easier to understand, reason about, and it made it more composable. So if you build a library, you can use it both at\nruntime and compile time, which is pretty cool.\n- Yeah. And for machine learning,\nI think metaprogramming, I think we could generally\nsay, is extremely useful. And so you get features,\nI mean, I'll jump around, but the feature of auto-tuning and adaptive compilation\njust blows my mind. - Yeah, well, so, okay. So\nlet's come back to that. - [Lex] All right. - So what is machine learning, like, what, or what is a machine learning model? Like, you take a PyTorch model off the internet, right?\n- Yeah. - It's really interesting to me because what PyTorch and what TensorFlow and all these frameworks\nare kinda pushing compute into is they're pushing into, like, this abstract specification\nof a compute problem, which then gets mapped in a whole bunch of different ways, right? And so this is why it became\na metaprogramming problem, is that you wanna be able to say, cool, I have this neural net. Now, run it with batch\nsize a thousand, right? Do a mapping across batch. Or, okay, I wanna take this problem. Now, run it across a\nthousand CPUs or GPUs, right? And so, like, this problem of,\nlike, describe the compute, and then map it and do things\nand transform it, or, like, actually it's very profound\nand that's one of the things that makes machine learning\nsystems really special. - Maybe can you describe auto-tuning and how do you pull off, I mean, I guess adaptive compilation is what we're talking about\nis metaprogramming. How do you pull off-\n- Yes. - auto-tuning? I mean, is that as\nprofound as I think it is? It just seems like a really, like, you know, we'll mention\nlist comprehensions. To me, from a quick glance\nof Mojo, which by the way, I have to absolutely, like, dive in, as I realize how amazing this is, I absolutely must dive in it, that looks like just an incredible feature for machine learning people. - Yeah. Well, so what is auto-tuning? So take a step back. Auto-tuning\nis a feature in Mojo. So very little of what we're\ndoing is actually research, like many of these ideas have existed in other systems and other places. And so what we're doing\nis we're pulling together good ideas, remixing them,\nand making them into a, hopefully, a beautiful system, right? And so auto-tuning, the\nobservation is that, turns out, hardware systems' algorithms\nare really complicated. Turns out maybe you don't actually want to know how the hardware\nworks, (chuckles) right? A lot of people don't, right? And so there are lots of\nreally smart hardware people, I know a lot of them, where they know everything about, \"Okay, the cache size is this and the\nnumber of registers is that. And if you use this what length of vector, it's gonna be super efficient\nbecause it maps directly onto what it can do\" and,\nlike, all this kinda stuff, or, \"the GPU has SMs and\nit has a warp size of,\" whatever, right, all this stuff that\ngoes into these things, or \"The tile size of a TPU is 128,\" like, these factoids, right? My belief is that most normal people, and I love hardware people, also I'm not trying to offend literally everybody on the internet, but most programmers actually don't wanna know this stuff, right? And so if you come at\nit from perspective of, how do we allow people to\nbuild both more abstracted but also more portable\ncode because, you know, it could be that the vector length changes or the cache size changes, or it could be that the tile\nsize of your matrix changes, or, the number, you know, an A100 versus an H100 versus\na Volta versus a, whatever, GPU have different characteristics, right? A lot of the algorithms that you run are actually the same, but the parameters, these magic numbers you have to fill in end up being really fiddly numbers that an expert has to go figure out. And so what auto-tuning does is says, okay, well, guess what? There's a lot of compute out there, right? So instead of having humans go randomly try all the things\nor do a grid, search, or go search some complicated\nmulti-dimensional space, how about we have\ncomputers do that, right? And so what auto-tuning\ndoes is you can say, Hey, here's my algorithm. If it's a matrix operation\nor something like that, you can say, okay, I'm gonna\ncarve it up into blocks, I'm gonna do those blocks in parallel and I wanna this, with 128\nthings that I'm running on, I wanna cut it this way\nor that way or whatever. And you can say, hey, go see which one's actually empirically better on the system. - And then the result of that\nyou cache for that system. You save it.\n- Yep. And so come back to twisting\nyour compiler brain, right? So not only does the\ncompiler have an interpreter that's used to do metaprogramming, that compiler, that interpreter, that metaprogramming now has\nto actually take your code and go run it on a target\nmachine, (chuckles) see which one it likes the best, and then stitch it in and\nthen keep going, right? - So part of the compilation\nis machine-specific. - Yeah. Well, so I mean, this\nis an optional feature, right? So you don't have to use it\nfor everything, but yeah. So one of the things\nthat we're in the quest of is ultimate performance, right?\n- Yes. - Ultimate performance is important for a couple of reasons, right? So if you're an enterprise, you're looking to save costs and compute and things like this. Ultimate performance translates to, you know, fewer servers. Like, if you care about\nthe environment, hey, better performance leads\nto more efficiency, right? I mean, you could joke\nand say like, you know, Python's bad for the\nenvironment, (chuckles) right? And so if you move to Mojo, it's like, at least 10x\nbetter just outta the box, and then keep going, right?\n- Yeah. - But performance is also\ninteresting 'cause it leads to better products.\n- Yeah. - And so in the space of\nmachine learning, right, if you reduce the latency of\na model so that it runs faster so every time you query the server running the model it takes less time, well, then the product team can go and make the model bigger. Well, that's actually makes it so you have a better\nexperience as a customer. And so a lot of people care about that. - So for auto-tuning, for like tile size, you mentioned 120f for TPU. You would specify like a\nbunch of options to try, just in the code-\n- Yeah. Yep. - Just simple statement, and then you could just-\n- Yep. - Set and forget and know,\ndepending wherever it compiles, it'll actually be the fastest. - And yeah, exactly. And the beauty of this\nis that it helps you in a whole bunch of different ways, right? So if you're building... So often what'll happen is that, you know, you've written a bunch of\nsoftware yourself, right, you wake up one day, you say, \"I have an idea. I'm gonna\ngo code up some code.\" I get to work, I forget about\nit, I move on with life. I come back six months, or a year, or two years, or three years\nlater, you dust it off, and you go use it again\nin a new environment. And maybe your GPU is different. Maybe you're running on a\nserver instead of a laptop, maybe you're, whatever, right? And so the problem now is\nyou say, okay, well, I mean, again, not everybody\ncares about performance, but if you do, you say, okay, well, I wanna take advantage\nof all these new features. I don't wanna break the\nold thing though, right? And so the typical way of handling this kinda stuff before is, you know, if you're talking about C++ templates or you're talking about C with macros, you end up with #ifdefs. You get like all these weird\nthings that get layered in, make the code super complicated, and then how do you test it, right? Becomes this crazy complexity, multidimensional space that\nyou have to worry about. And, you know, that just\ndoesn't scale very well. - Actually, lemme just jump around, before I go to some specific features, like the increase in performance\nhere that we're talking about can be just insane.\n- Yeah. - You write that Mojo can provide a 35,000x speed up over Python. How does it do that? - Yeah, so I can even do\nmore, but we'll get to that. So first of all, when we say that, we're talking about what's called CPython, it's the default Python\nthat everybody uses. When you type Python 3, that's like typically\nthe one you use, right? CPython is an interpreter. And so interpreters, they\nhave an extra layer of, like bike codes and things like this, that they have to go\nread, parse, interpret, and it makes them kind of\nslow from that perspective. And so one of the first things we do is we moved to a compiler. And so just moving to a compiler, getting the interpreter out of the loop is 2 to 5 to 10x speed\nup, depending on the code. So just out of the gate, it's using more modern techniques right? Now, if you do that, one of the things you\ncan do is you can start to look at how CPython\nstarted to lay out data. And so one of the things that CPython did, and this isn't part of the\nPython spec necessarily, but this is just sets of decisions, is that, if you take\nan integer for example, it'll put it in an object 'cause in Python, everything's an object. And so they do the very logical thing of keeping the memory representation of all objects the same. So all objects have a header,\nthey have like payload data. And what this means is that every time you pass around an object, you're passing around\na pointer to the data. Well, this has overhead, right? Turns out that modern computers don't like chasing pointers\nvery much and things like this. It means that you have\nto allocate the data. It means you have to reference count it, which is another way that Python uses to keep track of memory. And so this has a lot of overhead. And so if you say, okay, let's try to get that out\nof the heap, out of a box, out of an indirection\nand into the registers, that's another 10x, more.\n- So it adds up if you're reference counting every single-\n- Absolutely. - every single thing you\ncreate, that adds up. - Yep, and if you look at, you know, people complain about the Python GIL, this is one of the things\nthat hurts parallelism. That's because the\nreference counting, right? And so the GIL and reference counting are very tightly intertwined in Python. It's not the only thing, but\nit's very tightly intertwined. And so then you lean into\nthis and you say, okay, cool. Well, modern computers, they can do more than\none operation at a time. And so they have vectors.\nWhat is a vector? Well, a vector allows you to, instead of taking one piece of data, doing an add or multiply and\nthen pick up the next one, you can now do a 4, 8, or\n16 or 32 at a time, right? Well, Python doesn't expose\nthat because of reasons. And so now you can say, okay,\nwell, you can adopt that. Now you have threads. Now you have like additional things, like you can control memory hierarchy. And so what Mojo allows\nyou to do is it allows you to start taking advantage\nof all these powerful things that have been built into\nthe hardware over time. The library gives very nice features. So you can say, just parallelize this. Do this in parallel, right? So it's very, very powerful\nweapons against slowness, which is why people have\nbeen, I think having fun, like just taking code and making go fast because it's just kind\nof an adrenaline rush to see like how fast you can get things. - Before I talk about some\nof the interesting stuff with parallelization and all that, let's first talk about, like, the basics. We talked the indentation, right? So this thing looks like Python. It's sexy and beautiful\nlike Python as I mentioned. - [Chris] Yep. - Is it a typed language?\nSo what's the role of types? - Yeah, good question.\nSo Python has types. It has strings, it has integers, it has dictionaries and\nlike all that stuff, but they all live at runtime, right? And so because all those types\nlive at runtime in Python, you never or you don't have\nto spell them. (chuckles) Python also has like\nthis whole typing thing going on now and a lot of people use it. - [Lex] Yeah. - I'm not talking about that. That's kind of a different thing. We can go back to that if\nyou want, but typically the, you know, you just say, I have a def and my def\ntakes two parameters. I'm gonna call them A and B and I don't have to write or type okay? So that is great, but what that does is that forces what's called a consistent representation. So these things have to be a pointer to an object with the object header and they all have to look the same. And then when you dispatch a method, you go through all the\nsame different paths no matter what the receiver,\nwhatever that type is. So what Mojo does is it allows you to have more than one kind of type. And so what it does is allows\nyou to say, okay, cool. I have an object and objects\nbehave like Python does. And so it's fully dynamic\nand that's all great. And for many things, classes, like, that's all very powerful\nand very important. But if you wanna say, hey, it's\nan integer and it's 32 bits, or it's 64 bits or whatever it is, or it's a floating point\nvalue and it's 64 bits, well, then the compiler can take that, and it can use that to do\nway better optimization. And it turns out, again, getting rid of the\nindirections, that's huge. Means you can get better code completion 'cause compiler knows what the type is and so it knows what\noperations work on it. And so that's actually pretty huge. And so what Mojo does is allows you to progressively adopt\ntypes into your program. And so you can start, again,\nit's compatible with Python, and so then you can add\nhowever many types you want, wherever you want them. And if you don't wanna deal with it, you don't have to deal with it, right? And so one of, you know, our\nopinions on this, (chuckles) it's that it's not that\ntypes are the right thing or the wrong thing, it's\nthat they're a useful thing. - So it's kind of optional,\nit's not strict typing, like, you don't have to specify type. - [Chris] Exactly. - Okay, so it's starting from the thing that Python's kinda\nreaching towards right now with trying to inject types into it, what it's doing.\n- Yeah, with a very different approach, but yes, yeah.\n- So what's the different approach? I'm actually one of the people (sighs) that have not been using\ntypes very much in Python. So I haven't-\n- That's okay. Why did you sigh? - It just, well, because\nI know the importance. It's like adults use strict typing. And so I refuse to grow up in that sense. It's a kind of rebellion, but I just know that it probably reduces the amount of errors, even just for, forget about performance improvements, it probably reduces errors\nof when you do strict typing. - Yeah, so I mean, I think it's interesting\nif you look at that, right? And the reason I'm giving\nyou a hard time then is that-\n- Yes. - there's this cultural\nnorm, this pressure, this, like, there has to be\na right way to do things. Like, you know-\n- Yes. - grownups only do it one way. And if you don't do that-\n- Yes. - you should feel bad, right?\n- Yes. - Like, some people feel like\nPython's a guilty pleasure or something, and that's like, when it gets serious, I need\nto go rewrite it, right? Well, I mean, cool.\n- Exactly. - I understand history and I understand kinda\nwhere this comes from, but I don't think it has\nto be a guilty pleasure, (chuckles) right?\n- Yeah. - So if you look at that, you say, why do you have to rewrite it? Well, you have to rewrite it to deploy. Well, why do you wanna deploy? Well, you care about performance, or you care about predictability,\nor you want, you know, a tiny thing on the server\nthat has no dependencies, or, you know, you have objectives\nthat you're trying to attain. So what if Python can\nachieve those objectives? So if you want types, well, maybe you want types\nbecause you wanna make sure you're passing the right thing. Sure, you can add a type. If you don't care, you're\nprototyping some stuff, you're hacking some things out, you're, like, pulling some\nrandom code off the internet, it should just work, (chuckles) right? And you shouldn't be, like, pressured. You shouldn't feel bad\nabout doing the right thing or the thing that feels good. Now, if you're in a team, right, you're working at some\nmassive internet company and you have 400 million\nlines of Python code, well, they may have a house\nrule that you use types, right?\n- Yeah. - Because it makes it easier for different humans to talk to each other and understand what's going\non and bugs at scale, right? And so there are lots of good reasons why you might wanna use types, but that doesn't mean that everybody should use 'em all the time, right? So what Mojo does is it says, cool. Well, allow people to use\ntypes and if you use types, you get nice things out of it, right? You get better performance\nand things like this, right? But Mojo is a full, compatible\nsuperset of Python, right? And so that means it has to\nwork without types. (chuckles) It has to support all the dynamic things. It has to support all the packages. It has to support for comprehension, list comprehensions and\nthings like this, right? And so that starting point\nI think is really important. And I think that, again, you can look at why I\ncare so much about this. And there's many\ndifferent aspects of that, one of which is the world went\nthrough a very challenging migration from Python\n2 to Python 3, right? - [Lex] Yes. - This migration took many years and it was very painful for many teams, right?\n- Yeah. - And there's of a lot of\nthings that went on in that. I'm not an expert in all the details and I honestly don't wanna be. I don't want the world to\nhave to go through that, (chuckles) right?\n- Yeah. - And, you know, people can ignore Mojo. And if it's not their thing, that's cool. But if they wanna use Mojo, I don't want them to have\nto rewrite all their code. - Yeah, I mean, this, okay, the superset part is just, I mean, there's so much brilliant stuff here. That definitely is incredible. We'll talk about that.\n- Yeah. - First of all, how's the typing implemented\ndifferently in Python versus Mojo?\n- Yeah. - So this heterogeneous flexibility you said is definitely implemented. - Yeah, so I'm not a full expert (chuckles) in the whole\nbackstory on types in Python. So I'll give you that. I can\ngive you my understanding. My understanding is, basically,\nlike many dynamic languages, the ecosystem went through a phase where people went from writing scripts to writing large scale,\nhuge code bases in Python. And at scale, kinda helps have types.\n- Yeah. - People wanna be able to\nreason about interfaces, do you expect string, or an int, or, like, these basic things, right? And so what the Python\ncommunity started doing is it started saying, okay,\nlet's have tools on the side, checker tools, right, that go and, like, enforce a variance, check for\nbugs, try to identify things. These are called static\nanalysis tools generally. And so these tools run over your code and try to look for bugs. What ended up happening is\nthere's so many of these things, so many different weird patterns\nand different approaches on specifying the types and\ndifferent things going on, that the Python community\nrealized and recognized, \"Hey, hey, hey, there's\nthe thing here.\" (chuckles) And so what they started\nto do is they started to standardize the syntax\nfor adding types to Python. Now, one of the challenges\nthat they had is that they're coming from\nkinda this fragmented world where there's lots of different tools, they have different\ntrade-offs and interpretations and the types mean different things. And so if you look at types in Python, according to the Python spec,\nthe types are ignored, right? So according to the Python spec, you can write pretty\nmuch anything (chuckles) in a type position, okay? Technically, you can write\nany expression, okay? Now, that's beautiful\nbecause you can extend it. You can do cool things, you can\nwrite, build your own tools, you can build your own house, linter or something like that, right? But it's also a problem because\nany existing Python program may be using different tools and they have different interpretations. And so if you adopt somebody's\npackage into your ecosystem, try to run the tool you prefer, it may throw out tons of\nweird errors and warnings and problems just\nbecause it's incompatible with how these things work. Also because they're added late and they're not checked\nby the Python interpret, it's always kinda more of a\nhint that it is a requirement. Also, the CPython implementation can't use 'em for performance. And so it's really- - I mean, that's a big one, right? So you can't utilize for the compilation, for the just-in-time compilation, okay.\n- Yep, yep, exactly. And this all comes back to\nthe design principle of, they're kinda hints, they're kind of, the definition's a little bit murky. It's unclear exactly the\ninterpretation in a bunch of cases. And so because of that,\nyou can't actually, even if you want to, it's really difficult to use them to say, like, it is going to be an int, and if it's not, it's a problem, right? A lot of code would break\nif you did that, so. So in Mojo, right, so you can still use those kind of type annotations, it's fine. But in Mojo, if you declare\na type and you use it, then it means it is going to be that type. And the compiler helps you check that, and enforce it and it's safe and it's not a, like,\nbest-effort hint kind of a thing. - So if you try to shove a\nstring type thing into a integer- - [Chris] You get an\nerror from the compiler. - From the compiler\ncompile time. Nice, okay. What kinda basic types are there? - Yeah. So Mojo is pretty hardcore in\nterms of what it tries to do in the language, which is the\nphilosophy there is that we, again, if you look at Python, right, Python's a beautiful language because it's so extensible, right? And so all of the\ndifferent things in Python, like for loops and plus\nand like all these things can be accessed through these\nunderbar armbar methods, okay? So you have to say, okay, if I make something that is super fast, I can go all the way down to the metal. Why do I need to have integers built into the language, right? And so what Mojo does is it says, okay, well, we can have this notion of structs. So you have classes in Python. Now you can have structs. Classes are dynamic, structs are static. Cool. We can get high performance. We can write C++ kind of code\nwith structs if you want. These things mix and work\nbeautifully together, but what that means is that you can go and implement strings and\nints and floats and arrays and all that kinda stuff\nin the language, right? And so that's really\ncool because, you know, to me as a idealizing compiler\nlanguage type of person, what I wanna do is I wanna get magic out of the compiler and\nput in the libraries. Because if somebody can, you know, if we can build an\ninteger that's beautiful and it has an amazing API\nand it does all the things you'd expect an integer\nto do, we don't like it, maybe you want a big integer, maybe you want, like, sideways\ninteger, I don't know, like what all the space of integers are, then you can do that, and it's\nnot a second class citizen. And so if you look at\ncertain other languages, like C++, one I also love and use a lot, int is hardcoded in the language, but complex is not. And so isn't it kinda\nweird that, you know, you have this STD complex\nclass, but you have int, and complex tries to look\nlike a natural numeric type and things like this. But integers and floating\npoint have these, like, special promotion rules\nand other things like that, that are magic and they're\nhacked into the compiler. And because of that, you can't actually make something that works like the built-in types. - Is there something provided\nas a standard because, you know, because it's AI first, you know, numerical types are so important here. So is there something, like a nice standard\nimplementation of indigent flows? - Yeah, so we're still\nbuilding all that stuff out. So we provide integers and\nfloats and all that kinda stuff. We also provide like buffers and tensors and things like that you'd\nexpect in an ML context. Honestly, we need to keep\ndesigning and redesigning and working with the community to build that out and make that better. That's not our strength right now. Give us six months or a year and I think it'll be way better, but the power of putting\nin the library means that we can have teams of experts that aren't compiler engineers\nthat can help us design and refine and drive this forward. - So one of the exciting\nthings we should mention here is that this is new and fresh. This cake is unbaked. It's almost baked. You can tell it's delicious, but it's not fully ready to be consumed. - Yep. That's very fair. It is very useful, but it's very useful if you're a super low-level programmer right now. And what we're doing is we're\nworking our way up the stack. And so the way I would look at Mojo today in May and 2023 is that it's like a 0.1. So I think that, you\nknow, a year from now, it's gonna be way more interesting\nto a variety of people. But what we're doing is we\ndecide to release it early so that people can get access\nto it and play with it. We can build it with the community. We have a big roadmap, fully published, being transparent about this and a lot of people are\ninvolved in this stuff. And so what we're doing\nis we're really optimizing for building this thing the right way. And building it the right\nway is kind of interesting, working with the community, because everybody wants it yesterday. And so sometimes it's kind of, you know, there's some dynamics there, but I think-\n- Yeah. - it's the right thing. - So there's a Discord also. So the dynamics is pretty interesting. - [Chris] Yeah. - Sometimes the community probably can be very chaotic and\nintroduce a lot of stress. Guido famously quit over the\nstress of the Walrus operator. I mean, it's, you know-\n- Yeah, yeah. - It broke... - [Chris] Straw that\nbroke the camel's back. - Exactly. And so, like, it could be\nvery stressful to develop, but can you just add a\ntangent upon a tangent? Is it stressful to work through the design\nof various features here, given that the community\nis recently involved? - Well, so I've been\ndoing open development and community stuff for\ndecades now. (chuckles) Somehow this has happened to me. So I've learned some tricks, but the thing that always gets me is I wanna make people happy, right? And so maybe not all people\nall happy all the time, but generally,\n- Yeah. - I want people to be happy, right? And so the challenge is that again, we're tapping into some long, some deep seated long\ntensions and pressures both in the Python world,\nbut also in the AI world, in the hardware world\nand things like this. And so people just want\nus to move faster, right? And so again, our decision\nwas, \"Let's release this early. Let's get people used to it or access to it and play with it. And like, let's build in the open,\" which we could have, you know, had the language monk sitting in the cloister up on the hilltop, like beavering away\ntrying to build something. But in my experience, you get something that's way better if you work with the community, right? And so, yes, it can be frustrating, can be challenging for\nlots of people involved. And, you know, if you, I mean,\nyou mentioned our Discord. We have over 10,000 people on the Discord, 11,000 people or something. Keep in mind we released\nMojo like two weeks ago. (chuckles)\nYeah. So-\n- It's very active. - So it's very cool, but what that means is that, you know, 10, 11,000 people all will want\nsomething different, right? And so what we've done\nis we've tried to say, Okay, cool. Here's our roadmap. And the roadmap isn't\ncompletely arbitrary. It's based on here's the logical order in which to build these features or add these capabilities\nand things like that. And what we've done is we've spun really fast on like bug fixes. And so we actually have very\nfew bugs, which is cool, I mean, actually for a\nproject in this state, but then what we're doing is we're dropping in features\nvery deliberately. - I mean, this is fun to watch 'cause you got the two\ngigantic communities of, like, hardware,\nlike systems engineers, and then you have the machine\nlearning Python people that are like higher level. - [Chris] Yeah. - And it's just two, like, army, like- - They've both, they've been at war, yeah. (Lex chuckling)\nThey've been at war, right? And so here's- - [Lex] It's a Tolkien\nnovel or something. Okay. - Well, so here's a test. And again, like, it's super funny for something that's only\nbeen out for two weeks, right? People are so impatient, right? But, okay, cool, let's\nfast forward a year. Like, in a year's time, Mojo will be actually quite amazing and solve tons of\nproblems and be very good. People still have these problems, right? And so you look at this and you say, and the way I look at this\nat least is to say, okay, well, we're solving big,\nlong-standing problems. To me, again, working on\nmany different problems, I wanna make sure we do it right, right? There's like a responsibility you feel because if you mess it\nup, (chuckles) right, there's very few opportunities\nto do projects like this and have them really\nhave impact on the world. If we do it right, then maybe we can take\nthose feuding armies and actually heal some of those wounds, right?\n- Yeah. - This feels like a speech\nby George Washington or Abraham Lincoln or something. - And you look at this and it's like, okay, well, how different are we? - [Lex] Yeah. - We all want beautiful things. We all want something that's nice. We all wanna be able to work together. We all want our stuff to be used, right? And so if we can help heal that, now I'm not optimistic that all people will use Mojo and they'll stop using C++, like, that's not my\ngoal, (chuckles) right, but if we can heal some of that, I think that'd be pretty cool. That'd be nice.\n- Yeah, and we start by putting the people who like braces into the\nGulag, no. (chuckles) - So there are proposals\nfor adding braces to Mojo and we just we tell them no.\n- Oh, interesting. - Oh, okay, (laughs)\n(Chris laughing) politely, yeah, anyway. So there's a lot of amazing\nfeatures on the roadmap and those already\nimplemented, it'd be awesome if I could just ask you a few things. So-\n- Yeah, go for it. - So the other performance improvement comes from immutability. So what's this var and this\nlet thing that we got going on? And what's immutability?\n- Well, so... - Yeah, so one of the\nthings that is useful, and it's not always\nrequired, but it's useful, is knowing whether something can change out from underneath you, right? And so in Python, you have a\npointer to an array, right? And so you pass that pointer\nto an array around to things. If you pass into a function, they may take that and scroll away in some other data structure. And so you get your array\nback and you go to use it. And now somebody else is like\nputting stuff in your array. How do you reason about that?\n- Yeah. - It gets to be very complicated and leads to lots of bugs, right? And so one of the things\nthat, you know, again, this is not something Mojo forces on you, but something that Mojo enables is this thing called value semantics. And what value semantics do\nis they take collections, like array, like dictionaries, also tensors and strings and\nthings like this that are much higher level and make\nthem behave like proper values. And so it makes it look like, if you pass these things around, you get a logical copy of all the data. And so if I pass you an\narray, it's your array. You can go do what you want to it, you're not gonna hurt my array. Now that is an interesting and very powerful design principle. It defines away a ton of bugs. You have to be careful to\nimplement it in an efficient way. - Yeah, is there a performance\nhit that's significant? - Generally not if you\nimplement it the right way, but it requires a lot of very low-level\ngetting-the-language-right bits. - I assume that'd be\na huge performance hit 'cause the benefit is really nice 'cause you don't get into these- - Absolutely. Well, the\ntrick is you can't do copies. So you have to provide the behavior of copying without doing the copy. - [Lex] Yeah. How do you do that?\n(Chris laughing) How do you do that?\n- It's not magic. It's just-\n- Okay. - It's actually pretty cool. Well, so first, before we\ntalk about how that works, let's talk about how it\nworks in Python, right? So in Python you define a person class, or maybe a person class is a bad idea. You define a database class, right? And a database class\nhas an array of records, something like that, right? And so the problem is, is that if you pass in a record or a class instance into the database, it'll take a hold of that object and then it assumes it has it. And if you're passing an object in, you have to know that that\ndatabase is gonna take it, and therefore you shouldn't change it after you put it in the database, right? This is-\n- You just kinda have to know that. - You just have to kinda know that, right? And so you roll out version\none of the database. You just kinda have to know that. Of course, Lex uses his\nown database, right? - [Lex] Yeah. - Right, 'cause you built it, you understand how this works, right? Somebody else joins the\nteam, they don't know this, right?\n- Yes. - And so now they suddenly get bugs, you're having to maintain the database, you shake your fist, you argue. The 10th time this\nhappens, you're like, okay, we have to do something different, right? And so what you do is you\ngo change your Python code and you change your database class to copy the record every time you add it. And so what ends up\nhappening is you say, okay, I will do what's called a defensive copy inside the database. And then that way if\nsomebody passes something in, I will have my own copy of it\nand they can go do whatever and they're not gonna break\nmy thing, (chuckles) okay? This is usually the two design patterns. If you look in PyTorch, for example, this is cloning a tensor. Like, there's a specific thing and you have to know where to call it. And if you don't call\nit in the right place, you get these bugs and this\nis state-of-the-art, right? So a different approach, so\nit's used in many languages, so I've worked with it in\nSwift, is you say, okay, well, let's provide value semantics. And so we wanna provide\nthe view that you get a logically independent copy,\nbut we wanna do that lazily. And so what what we do is we say, okay, if you pass something into a function, it doesn't actually make a copy. What it actually does is it just increments a reference to it. And if you pass it around,\nyou stick in your database, it can go into the database, you own it. And then you come back outta the stack, nobody's copied anything, you\ncome back outta the stack, and then the caller let's go of it. Well, then you've just handed\nit off to the database, you've transferred it and\nthere's no copies made. Now, on the other hand, if, you know, your coworker goes and hands you a record and you pass it in, you\nstick it in the database, and then you go to town\nand you start modifying it, what happens is you get\na copy lazily on demand. And so what this does, this gives you copies\nonly when you need them. So it defines the way the bugs, but it also generally reduces the number of copies in practice. And so it's-\n- But the implementation details are tricky here, I assume.\n- Yes, yes. - Something with reference counting, but to make it performant across a number of\ndifferent kinds of objects? - Yeah. Well, so you\nneed a couple of things. So this concept has existed\nin many different worlds. And so it's, again, it's not\nnovel research at all, right? The magic is getting the design right so that you can do this in\na reasonable way, right? And so there's a number of\ncomponents that go into this. One is when you're passing around, so we're talking about\nPython and reference counting and the expense of doing that. When you're passing values around, you don't wanna do\nextra reference counting for no good reason. And so you have to make\nsure that you're efficient and you transfer ownership\ninstead of duplicating references and things like that, which\nis a very low-level problem. You also have to adopt this, and you have to build\nthese data structures. And so if you say, you know, Mojo has to be compatible with Python, so of course the default list\nis a reference semantic list that works the way you'd expect in Python, but then you have to design\na value semantic list. And so you just have to implement that, and then you implement the logic within. And so the role of the language here is to provide all the low-level hooks that allow the author of the type to be able to get and\nexpress this behavior without forcing it into all cases or hard coding this into\nthe language itself. - But there's ownership? So you're constantly transferring, you're tracking who owns the thing. - Yes. And so there's a whole\nsystem called ownership. And so this is related to work\ndone in the Rust community. Also, the Swift community\nhas done a bunch of work and there's a bunch of\ndifferent other languages that have all kind of... C++ actually has copy constructors and destructors and things like that. And so, and I mean, C++ has everything. So it has move constructors and has like this whole world of things. And so this is a body of work\nthat's kind of been developing for many, many years now. And so Mojo takes some of the best ideas out of all these systems and\nthen remixes in a nice way so that you get the power of something like the Rust programming language, but you don't have to deal\nwith it when you don't want to, which is a major thing in\nterms of teaching and learning and being able to use\nand scale these systems. - How does that play with\nargument conventions? What are they? Why are they important? How does the value semantics, how does the transfer ownership work with the arguments when they're passing definitions?\n- Yeah. So if you go deep into\nsystems programming land, so this isn't, again, this is\nnot something for everybody, but if you go deep into\nsystems programming land, what you encounter is you encounter these types that get weird. (chuckles) So if you're used to Python,\nyou think about everything. I can just copy it around. I can go change it and mutate it and do these things and it's all cool. If you get into systems programming land, you get into these things, like, I have an atomic number,\nor I have a mutex, or I have a uniquely\nowned database handle, things like this, right? So these types, you\ncan't necessarily copy. Sometimes you can't\nnecessarily even move them to a different address. And so what Mojo allows\nyou to do is it allows you to express, hey, I don't wanna\nget a copy of this thing. I wanna actually just\nget a reference to it. And by doing that, what you\ncan say is, you can say, okay, if I'm defining something\nweird like a, you know, atomic number or something,\nit's like, it has to be... So an atomic number is an area in memory that multiple threads can access at a time without synchronous, without locks, right? And so, like the definition\nof atomic numbers, multiple different things\nhave to be poking at that, therefore they have to\nagree on where it is, (chuckles) right? So you can't just like move\nit out from underneath one because it kinda breaks what it means. And so that's an example of a type that you can't copy, you can't move it. Like, once you create, it has\nto be where it was, right? Now, if you look at many other examples, like a database handle, right, so, okay, well, what happens? How do you copy a database handle? Do you copy the whole database? That's not something you\nnecessarily wanna do. There's a lot of types like\nthat where you wanna be able to say that they are uniquely owned. So there's always one of this thing, or if I create a thing, I don't copy it. And so what Mojo allows you\nto do is it allows you to say, Hey, I wanna pass around\nin reference to this thing without copying it, and so\nit has borrowed conventions. So you can say, you can use it, but you don't get to change it. You can pass it by mutable reference. And so if you do that, then\nyou get a reference to it, but you can change it. And so it manages all that kinda stuff. - So it's just a really nice\nimplementation of, like, C++ has-\n- Yeah. - you know, different kinds of pointers.\n- Reference, yeah, has pointers. - Smart, smart, different kinds of\nimplementations of smart pointers that you can-\n- Yeah. - explicitly define, this allows you, but you're saying that's\nmore like the weird case versus the common case? - Well, it depends on where, I mean, I don't think I'm a normal person, so.\n- Yes. - I mean, I'm not one to\ncall other people weird. - [Lex] Yeah.\n(Chris chuckling) But, you know, if you talk to a typical\nPython programmer, you're typically not\nthinking about this, right? This is a lower level of abstraction. Now, certainly if you\ntalk to a C++ programmer, certainly if you talk to\na Rust programmer, again, they're not weird, they're delightful. Like, these are all good people, right? Those folks will think\nabout all the time, right? And so I look at this as, there's a spectrum between\nvery deep, low-level systems, I'm gonna go poke the bits and care about how they're\nlaid out in memory, all the way up to application and scripting and other things like this. And so it's not that\nanybody's right or wrong, it's about how do we build\none system that scales? - By the way, the idea of an atomic\nnumber has been something that always brought me deep happiness, because the flip side of that, the idea that threads\ncan just modify stuff asynchronously, just the whole idea of\nconcurrent programming is a source of infinite distrust for me. - Well, so this is where you\njump into, you know, again, you zoom out and get out of\nprogram languages or compilers and you just look at what\nthe industry has done, my mind is constantly\nblown by this, right? And you look at what,\nyou know, Moore's law, Moore's Law is this idea that, like computers, for a long time, single thread performance just got faster and faster and faster and faster for free. But then physics and\nother things intervened, and power consumption, like\nother things started to matter. And so what ended up happening is we went from single core\ncomputers to multi-core, then we went to accelerators, right? And this trend towards specialization of hardware is only gonna continue. And so for years, us programming language nerds and compiler people\nhave been saying, okay, well, how do we tackle multi-core, right? For a while it was like,\n\"Multi-core is the future. We have to get on top of this thing.\" And then it was multi-core is the default. \"What are we doing with this thing?\" And then it's like, there's chips with\nhundreds of cores in them. (chuckles) What will happen, right?\n- Yeah. - And so I'm super inspired\nby the fact that, you know, in the face of this, you know, those machine learning people invented this idea of a tensor,\nright? And what's a tensor? A tensor is like an arithmetic\nand algebraic concept. It's like an abstraction around a gigantic\nparallelizable dataset, right? And because of that and because of things like TensorFlow and PyTorch,\nwe're able to say, okay, we'll express the math of the system. This enables you to do\nautomatic differentiations, enables you to do like\nall these cool things. And it's an abstracted representation. Well, because you have that\nabstract representation, you can now map it onto\nthese parallel machines without having to control,\nokay, put that bite here, put that bite there, put that bite there. And this has enabled an\nexplosion in terms of AI, compute, accelerators, like all the stuff. And so that's super, super exciting. - What about the deployment and the execution across\nmultiple machines? - [Chris] Yeah. - So you write that the\nModular compute platform dynamically partitions models\nwith billions of parameters and distributes their execution\nacross multiple machines, enabling unparalleled efficiency. By the way, the use of\nunparalleled in that sentence... Anyway.\n(Chris chuckling) Enabling unparalleled efficiency, scale, and the reliability for\nthe largest workloads. So how do you do this abstraction of distributed\nDeployment of large models? - Yeah, so one of the\nreally interesting tensions, so there's a whole bunch of\nstuff that goes into that. I'll pick a random walk through it. If you go back and replay the history of machine learning, right, I mean, the brief, most recent\nhistory of machine learning, 'cause this is, as you know, very deep. - [Lex] Yeah. - I knew Lex when he had an AI podcast. - [Lex] Yes.\n(Chris chuckling) - [Chris] Right? - Yeah, (chuckles) yeah. - So if you look at just\nTensorFlow and PyTorch, which is pretty recent history\nin the big picture, right, but TensorFlow is all about graphs. PyTorch, I think pretty\nunarguably ended up winning. And why did it win? Mostly\nbecause the usability, right? And the usability of\nPyTorch is I think, huge. And I think, again, that's a huge testament to\nthe power of taking abstract, theoretical technical concepts and bringing it to the masses, right? Now the challenge with what the TensorFlow versus the PyTorch design points was that TensorFlow's kinda\ndifficult to use for researchers, but it was actually pretty\ngood for deployment. PyTorch is really good for researchers. It kind of not super great\nfor deployment, right? And so I think that we as an\nindustry have been struggling. And if you look at what deploying\na machine learning model today means is that you'll\nhave researchers who are, I mean, wicked smart, of course, but they're wicked smart\nat model architecture and data and calculus\nand (chuckles) like all, like, they're wicked\nsmart in various domains. They don't wanna know anything\nabout the hardware deployment or C++ or things like this, right? And so what's happened is you get people who train the model, they throw it over the fence, and then you have people\nthat try to deploy the model. Well, every time you have a Team A does x, they throw it over the fence, Team B does y, like you have a problem, because of course it never\nworks the first time. And so you throw over the\nfence, they figure out, okay, it's too slow, won't fit,\ndoesn't use the right operator, the tool crashes, whatever the problem is, then they have to throw\nit back over the fence. And every time you throw\na thing over a fence, it takes three weeks of project managers and meetings and things like this. And so what we've seen\ntoday is that getting models in production can take weeks or months. Like, it's not atypical, right? I talk to lots of people\nand you talk about, like VP of software at\nsome internet company trying to deploy a\nmodel, and they're like, why do I need a team of\n45 people? (chuckles) Like, it's so easy trying to model. Why can't I deploy it, right? And if you dig into this,\nevery layer is problematic. So if you look at the language piece, I mean, this is tip of the iceberg. It's a very exciting tip\nof the iceberg for folks, but you've got Python on one side and C++ on the other side. Python doesn't really deploy. I mean, can theoretically,\ntechnically in some cases, but often a lot of production teams will wanna get things out of Python because they get better performance and control and whatever else. So Mojo can help with that. If you look at serving, so you\ntalk about gigantic models, well, a gigantic model won't\nfit on one machine, right? And so now you have this\nmodel, it's written Python, it has to be rewritten in C++. Now it also has to be carved up so that half of it runs on one machine, half of it runs on another machine, or maybe it runs on 10 machines. Well, so now, suddenly, the\ncomplexity is exploding, right? And the reason for this is that if you look into TensorFlow\nor PyTorch, these systems, they weren't really designed\nfor this world, right? They were designed for, you know, back in the day when we were\nstarting and doing things where it was a different,\nmuch simpler world, like you wanted to run ResNet-50 or some ancient model\narchitecture like this. It was a completely different world than- - Trained on one GPU. - [Chris] Exactly. AlexNet.\n- Doing it on one GPU. (chuckles)\n- Yeah, AlexNet, right, the major breakthrough, and the world has changed, right? And so now the challenge is, is that TensorFlow,\nPyTorch, these systems, they weren't actually designed for LLMs, like, that was not a thing. And so where TensorFlow\nactually has amazing power in terms of scale and\ndeployment and things like that, and I think Google is, I\nmean, maybe not unmatched, but they're, like, incredible, in terms of their capabilities\nand gigantic scale, many researchers using PyTorch, right? And so PyTorch doesn't have\nthose same capabilities. And so what Modular can do\nis it can help with that. Now, if you take a step\nback and you say like, what is Modular doing, right? So Modular has like a bitter enemy that we're fighting\nagainst in the industry. And it's one of these things\nwhere everybody knows it, but nobody is usually\nwilling to talk about it. - The bitter enemy. - The bitter thing that we have to destroy that we're all struggling\nwith and it's like all around, it's like fish can't see\nwater, it's complexity. - Sure, yes. It's complexity. - [Chris] Right? - That was very philosophical, (Chris chuckling)\nVery well said. - [Chris] And so if you look at it, yes, it is on the hardware side. - Yes. - All these accelerators, all these software stack\nthat go with the accelerator, all these, like, there's\nmassive complexity over there. You look at what's happening\non the modeling side, massive amount of complexity. Like, things are changing all the time. People are inventing. Turns out the research is\nnot done, (chuckles) right? And so people wanna be able to move fast. Transformers are amazing, but there's a ton of diversity\neven within transformers, and what's the next transformer, right? And you look into serving. Also,\nhuge amounts of complexity. It turns out that all the\ncloud providers, right, have all their very weird\nbut very cool hardware for networking and all this kinda stuff. And it's all very complicated.\nPeople aren't using that. You look at classical serving, right, there's this whole world of\npeople who know how to write high-performance servers\nwith zero-copy networking and, like, all this\nfancy asynchronous I/O, and, like, all these fancy\nthings in the serving community, very little that has pervaded into the machine learning world, right? And why is that? Well, it's because, again, these systems have been\nbuilt up over many years. They haven't been rethought, there hasn't been a first\nprinciples approach to this. And so what Modular's doing\nis we're saying, \"Okay, we've built many of these things, right?\" So I've worked on TensorFlow and TPUs and things like that. Other folks on our team have,\nlike, worked on PyTorch Core. We've worked on ONNX one time. We've worked on many\nof these other systems. And so built systems like\nthe Apple accelerators and all that kinda stuff, like\nour team is quite amazing. And so one of the things\nthat roughly everybody at Modular's grumpy about\nis that when you're working on one of these projects,\nyou have a first order goal: Get the hardware to work. Get the system to enable one more model. Get this product out the door. Enable the specific workload, or make it solve this problem\nfor this product team, right? And nobody's been given a chance to actually do that step back. And so we, as an industry, we\ndidn't take two steps forward. We took like 18 steps forward in terms of all this\nreally cool technology across compilers and systems and runtimes and heterogeneous computing,\nlike all this kinda stuff. And like, all this technology\nhas been, you know, I wouldn't say beautifully designed, but it's been proven\nin different quadrants. Like, you know, you look\nat Google with TPUs, massive, huge exif flops of\ncompute strapped together into machines that researchers are programming in Python in a notebook. That's huge. That's amazing.\n- That's amazing. That's incredible.\n- Right, it's incredible. And so you look at the\ntechnology that goes into that, and the algorithms are\nactually quite general. And so lots of other hardware out there and lots of other teams\nout there don't have the sophistication or the,\nmaybe the years working on it, or the budget, or whatever\nthat Google does, right? And so they should be getting\naccess to the same algorithms, but they just don't have that, right? And so what Modular's doing,\nso we're saying, \"Cool, this is not research anymore.\" Like, we've built\nauto-tuning in many systems. We've built programming languages, right? And so like, have implemented\nC++, have implemented Swift, have implemented many of these things. And so, you know, it's\nhard, but it's not research. And you look at accelerators. Well, we know there's\na bunch of different, weird kind of accelerators, but they actually cluster together, right? And you look at GPUs. Well, there's a couple\nof major vendors of GPUs and they maybe don't always get along, but their architectures are very similar. You look at CPUs. CPUs are still super important for the deployment side of things. And you see new architectures coming out from all the cloud providers\nand things like this, and they're all super\nimportant to the world, right, but they don't have the\n30 years of development that the entrenched people do, right? And so what Modular\ncan do is we're saying, \"Okay, all this complexity,\nlike, it's not bad complexity, it's actually innovation,\n(chuckles) right?\" And so it's innovation that's happening and it's for good reasons, but I have sympathy for the\npoor software people, right? I mean, again, I'm a\ngenerally software person too. I love hardware, but software people wanna\nbuild applications and products and solutions that scale over many years. They don't wanna build a\nsolution for one generation of hardware with one\nvendor's tools, right? And because of this, they need something that scales with them. They need something that works\non cloud and mobile, right, because, you know, their\nproduct manager said, Hey, I want it to have lower latency and it's better for personalization, or whatever they decide, right? Products evolve. And so the challenge with the\nmachine learning technology and the infrastructure we\nhave today in the industry is that it's all these point solutions. And because there are all\nthese point solutions, it means that as your product evolves, you have to like switch\ndifferent technology stacks or switch to a different vendor. And what that does is\nthat slows down progress. - So basically a lot of\nthe things we've developed in those little silos for\nmachine learning tasks, you want to make that\nthe first class citizen of a general purpose programming language that can then be compiled across all these kinds of hardware. - Well, so it's not really\nabout a programming language. I mean, the programming language is a component of the mission, right? And the mission is, or not literal, but our joking mission is \"to save the world from\nterrible AI software.\" - [Lex] Excellent. I love it.\n- Okay? (chuckles) - So, you know, if you\nlook at this mission, you need a syntax. So yeah, you need\nprogramming language, right? And like, we wouldn't have to\nbuild the programming language if one existed, right? So if Python was already good enough, then cool, we would've\njust used it, right? We're not just doing very large scale, expensive engineering\nprojects for the sake of it, like, it's to solve a problem, right? It's also about accelerators. It's also about exotic\nnumerics and bfloat16 and matrix multiplication and convolutions and like, this kinda stuff. Within the stack, there are\nthings like kernel fusion. That's a esoteric but\nreally important thing that leads to much better performance and much more general research\nhackability together, right? - And that's enabled by the ASICs. That's enabled by certain hardware. So it's like-\n- Well. - Where's the dance between, I mean, there's several questions here. Like, how do you add-\n- Yep. - a piece of hardware to the stack if a new piece of-\n- Yeah. - like if I have this genius invention of a specialized accelerator-\n- Yeah. - how do I add that to\nthe Modular framework? And also how does Modular as a standard start to define the kinds of hardware that should be developed? - Yeah, so let me take a step back and talk about status quo, okay?\n- Yes. - And so if you go back to\nTensorFlow 1, PyTorch 1, this kinda timeframe, and these have all evolved\nand gone way more complicated. So let's go back to the\nglorious simple days, right? These things basically were CPUs and CUDA. And so what you do is you\nsay, go do a dense layer. And a dense layer has a matrix\nmultiplication in it, right? And so when you say that, you say, go do this big operation,\na matrix multiplication, and if it's on a GPU,\nkick off a CUDA kernel. If it's on a CPU, go do\nlike an Intel algorithm, or something like that\nwith an Intel MKL, okay? Now that's really cool if you're either Nvidia or Intel, right? But then more hardware comes in, right? And on one access, you have\nmore hardware coming in. On the other hand, you have an explosion of innovation in AI. And so what happened with\nboth TensorFlow and PyTorch is that the explosion of\ninnovation in AI has led to, it's not just about matrix\nmultiplication and convolution. These things have now like\n2,000 different operators. And on the other hand, you have, I don't know how many pieces of hardware there are out there. It's a lot, (chuckles) okay?\nIt's not even hundreds. It's probably thousands, okay? And across all of edge and across like, all the different things- - That are used at scale. - [Chris] Yeah, exactly. I mean-\n- Also it's not just like a handful.\n- AI's everywhere. Yeah.\n- It's not a handful of TPU alternatives. It's-\n- Correct. It's every phone, often\nwith many different chips inside of it-\n- Right. - from different vendors from...\n- Right. - Like, AI is everywhere.\nIt's a thing, right? - Why are they all making their own chips? Like, why is everybody\nmaking their own thing? - [Chris] Well, so- - Is that a good thing, first of all? - So Chris's philosophy on hardware, right?\n- Yeah. - So my philosophy is that there isn't one\nright solution, right? And so I think that, again, we're at the end of Moore's\nlaw, specialization happens. - [Lex] Yeah. - If you're building, if\nyou're training GPT-5, you want some crazy super\ncomputer data center thingy. If you're making a smart\ncamera that runs on batteries, you want something that\nlooks very different. If you're building a phone, you want something that\nlooks very different. If you have something like a laptop, you want something that\nlooks maybe similar but a different scale, right? And so AI ends up\ntouching all of our lives. Robotics, right? And, like,\nlots of different things. And so as you look into this, these have different power envelopes. There's different trade-offs\nin terms of the algorithms. There's new innovations and sparsity and other data formats\nand things like that. And so hardware innovation, I think, is a really good thing, right? And what I'm interested in\nis unlocking that innovation. There's also like analog\nand quantum and like all the really weird stuff, right?\n- Yeah. - And so if somebody\ncan come up with a chip that uses analog computing and it's 100x more power efficient, think what that would mean\nin terms of the daily impact on the products we use, that'd be huge. Now, if you're building\nan analog computer, you may not be a compiler\nspecialist, right? These are different skill sets, right? And so you can hire some compiler people if you're running a big company, maybe, but it turns out these are really like exotic new generation\nof compilers. (chuckles) Like, this is a different thing, right? So if you take a step back out and come back to what is the status quo, the status quo is that if\nyou're Intel or you're Nvidia, you keep up with the industry\nand you chase and, okay, there's 1,900 now, there's\n2-000 now, there's 2,100. And you have a huge team of\npeople that are like trying to keep up and tune and optimize. And even when one of\nthe big guys comes out with a new generation of their chip, they have to go back and\nrewrite all these things, right? So really it's only powered\nby having hundreds of people that are all, like,\nfrantically trying to keep up. And what that does is that\nkeeps out the little guys, and sometimes they're not so little guys, the big guys that are also just not in those dominant positions. And so what has been happening, and so you talk about\nthe rise of new exotic, crazy accelerators is people\nhave been trying to turn this from a let's go write lots\nof special kernels problem into a compiler problem. And so we, and I contributed\nto this as well, (chuckles) we as an industry went into a like, let's go make this compiler\nproblem phase, let's call it. And much of the industry is\nstill in this phase, by the way. So I wouldn't say this phase is over. And so the idea is to say, look, okay, what a compiler does is it\nprovides a much more general, extensible hackable interface for dealing with the general case, right? And so within machine learning\nalgorithms, for example, people figured out that, hey, if I do a matrix multiplication\nand I do a ReLU, right, the classic activation function, it is way faster to do\none passover the data and then do the ReLU on the output where I'm writing out the data, 'cause ReLU is just a maximum\noperation, right, max at zero. And so it's an amazing\noptimization. Take MathML, ReLU. Squished together in one\noperation, now I have MathML ReLU. Well, wait a second. If I do that, now, I\njust went from having, you know, two operators to three. But now I figure out, okay, well, there's a lot of\nactivation functions. What about a leaky value? What about... Like, a million things\nthat are out there, right? And so as I start fusing these in, now I get permutations of\nall these algorithms, right? And so what the compiler people said is they said, \"Hey, well, cool. Well, I will go enumerate\nall the algorithms and I will enumerate all the pairs and I will actually\ngenerate a kernel for you.\" And I think that this has been very, very useful for the industry. This is one of the things\nthat powers Google TPUs. PyTorch 2's, like, rolling\nout really cool compiler stuff with Triton, this other\ntechnology, and things like this. And so the compiler\npeople are kind of coming into their fore and saying like, Awesome, this is a compiler\nproblem. We'll compiler it. Here's the problem. (chuckles) Not everybody's a compiler person. I love compiler people, trust me, right, but not everybody can or\nshould be a compiler person. It turns out that they're people that know analog computers really well, or they know some GPU internal architecture thing really well, or they know some crazy sparse numeric interesting algorithm that\nis the cusp of research, but they're not compiler people. And so one of the challenges\nwith this new wave of technology trying to turn\neverything into a compiler, 'cause again, it has\nexcluded a ton of people. And so you look at what does Mojo do, what does the Modular stack do is brings programmability\nback into this world. Like, it enables, I\nwouldn't say normal people, but like a new, you know, a different kind of delightful nerd that cares about numerics,\nor cares about hardware, or cares about things like this, to be able to express that in\nthe stack and extend the stack without having to actually\ngo hack the compiler itself. - So extend the stack\non the algorithm side. - [Chris] Yeah. - And then on the hardware side. - Yeah, so again, go back to, like, the simplest example of int, right? And so what both Swift\nand Mojo and other things like this did is we said, okay, pull magic out of the compiler and put it in the standard library, right? And so what Modular's doing with the engine that we're providing and like, this very deep\ntechnology stack, right, which goes into heterogeneous runtimes and like a whole bunch of\nreally cool, really cool things, this whole stack allows that\nstack to be extended and hacked and changed by researchers\nand by hardware innovators and by people who know\nthings that we don't know, (chuckles) 'cause, you know,\nModular has some smart people, but we don't have all the smart\npeople it turns out, right? - What are heterogeneous runtimes? - Yeah. So what is heterogeneous, right? So heterogeneous just means many different kinds of things together. And so the simplest example you might come up with is a CPU and a GPU. And so it's a simple\nheterogeneous computer to say, I'll run my data loading\nand pre-processing and other algorithms on the CPU. And then once I get it\ninto the right shape, I shove it into the GPU. I do a lot of matrix multiplication and convolutions and things like this. And then I get it back out and I do some reductions and summaries and they shove it across the wire, to across the network to\nanother machine, right? And so you've got now what\nare effectively two computers, a CPU and a GPU talking to each other, working together in a\nheterogeneous system. But that was 10 years\nago, (chuckles) okay? You look at a modern cell phone. Modern cell phone, you've got CPUs, and they're not just CPUs, there's like big.LITTLE CPUs and there's multiple different kinds of CPUs that are kind-\n- Yep. - of working together, they're multi-core. You've got GPUs, you've got\nneural network accelerators, you've got dedicated\nhardware blocks for media, so for video decode and jpeg\ndecode and things like this. And so you've got this\nmassively complicated system, and this isn't just cell phones. Every laptop these days\nis doing the same thing. And all these blocks\ncan run at the same time and need to be choreographed, right? And so again, one of the cool\nthings about machine learning is it's moving things\nto like data flow graphs and higher level of\nabstractions and tensors and these things that it doesn't specify, here's how to do the algorithm. It gives the system a lot more flexibility in terms of how to translate or map it or compile it onto the\nsystem that you have. And so what you need, you know, the bottom-est part of the layer there is a way for all these devices\nto talk to each other. And so this is one thing that, you know, I'm very passionate about. I mean, you know, I'm a nerd, but all these machines\nand all these systems are effectively parallel computers running at the same time,\nsending messages to each other. And so they're all fully asynchronous. Well, this is actually a small\nversion of the same problem you have in a data center, right? In a data center, you now have\nmultiple different machines, sometimes very specialized, sometimes with GPUs or TPUs in one node and sometimes with disks in other nodes. And so you get a much larger\nscale heterogenous computer. And so what ends up happening\nis you have this, like, multi-layer abstraction of\nhierarchical parallelism, hierarchical, asynchronous\ncommunication and making that, again, my enemy, is complexity. By getting that away from being different specialized systems\nat every different part of the stack and having more\nconsistency and uniformity, I think we can help lift the world and make it much simpler\nand actually get used. - Well, how do you leverage, like, the strengths of the\ndifferent specialized systems? So looking inside the smartphone, like there's what, like-\n- Yeah. - I don't know, five,\nsix computers essentially inside the smartphone?\n- Yeah. - How do you, without trying\nto minimize the explicit, making it explicit, which computer is supposed to\nbe used for which operation? - Yeah, so there's a pretty\nwell-known algorithm, and what you're doing is\nyou're looking at two factors. You're looking at the\nfactor of sending data from one thing to another, right, 'cause it takes time to get\nit from that side of the chip to that side of the chip\nand things like this. And then you're looking at\nwhat is the time it takes to do an operation on a particular block. So take CPUs. CPUs are fully general.\nThey can do anything, right? But then you have a neural net accelerator that's really good at\nmatrix multiplication, okay? And so you say, okay, well, if my workload is\nall matrix multiplication, I start up, I send the data\nover the neural net thing, it goes and does matrix multiplication. When it's done, it sends\nme back the result. All is good, right? And so the simplest thing is just saying, do matrix operations over there, right? But then you realize you get\na little bit more complicated because you can do matrix\nmultiplication on a GPU, you can do it on a neural net accelerator, you can do it on CPU, and they'll have different\ntrade-offs and costs. And it's not just matrix multiplication. And so what you actually\nlook at is you look at, I have generally a graph of compute. I wanna do a partitioning. I wanna look at the communication,\nthe bisection bandwidth, and like the overhead-\n- Overheads. - and the sending of all\nthese different things and build a model for this\nand then decide, okay, it's an optimization problem of where do I wanna place this compute? - So it's the old school\ntheoretical computer science problem of scheduling.\n- Yep. - And then, presumably it's possible to, somehow, magically include auto-tune into this. - Absolutely, so I mean, in my\nopinion, this is an opinion, not everybody would agree\nwith this, but in my opinion, the world benefits from\nsimple and predictable systems at the bottom you can control. But then once you have a\npredictable execution layer, you can build lots of different policies on top of it, right? And so one policy can be that\nthe human programmer says, do that here, do that here,\ndo that here, do that here, and like, fully manually\ncontrols everything and the systems should just do it, right? But then you quickly\nget in the mode of like, I don't wanna have to tell it to do it. (chuckles)\n- Yeah. - And so the next logical step\nthat people typically take is they write some terrible heuristic. \"Oh, if it's a information\nlocation, do it over there. or if it's floating\npoint, do it on the GPU. If it's integer, do it on the CPU,\" like, something like that, right? And then you then get\ninto this mode of like, people care more and more\nand more, and you say, okay, well, let's actually, like,\nmake the heuristic better. Let's get into auto-tuning. Let's actually do a search\nof the space to decide, well, what is actually better, right? Well, then you get into this problem where you realize this\nis not a small space. This is a many-dimensional\nhyperdimensional space that you cannot exhaustively search. So do you know of any\nalgorithms that are good at searching very\ncomplicated spaces for... - Don't tell me you're gonna turn this into a machine learning problem. - So then you turn into a\nmachine learning problem, and then you have a space\nof genetic algorithms and reinforcement learning and, like, all these concerns.\n- Can you include that into the stack,\ninto the Modular stack? - Yeah, yeah. And so- - Where does it sit? Where does it live? Is it separate thing or is\nit part of the compilation? - So you start from simple\nand predictable models. And so you can have full control and you can have coarse grain knobs that, like, nudge systems so\nyou don't have to do this. But if you really care\nabout getting the best, you know, the last ounce out of a problem, then you can use additional tools. The cool thing is you don't wanna do this every time you run a model. You wanna figure out the right\nanswer and then cache it. (chuckles) And once you do\nthat, you can say, okay, cool. Well, I can get up and\nrunning very quickly. I can get good execution out of my system, I can decide if something's important, and if it's important, I can go throw a bunch of\nmachines at it and do a big, expensive search over the space using whatever technique I feel like, it's really up to the problem. And then when I get\nthe right answer, cool, I can just start using it, right? And so you can get out of this, this trade-off between, okay, am I gonna like spend\nforever doing a thing or do I get up and running quickly? And as a quality result, like, these are actually not in\ncontention with each other if the system's designed to scale. - You started and did a little\nbit of a whirlwind overview of how you get the 35,000x\nspeed up or more over Python. Jeremy Howard did a\nreally great presentation about sort of the basic,\nlike, looking at the code, here's how you get the speed up. Like you said, that's something\nprobably developers can do for their own code to see how you can get these gigantic speed ups. But can you maybe speak to the machine learning task in general? How do you make some of this\ncode fast, and specifics. Like, what would you say\nis the main bottleneck for machine learning tasks? So are we talking about\nMathML matrix multiplication? How do you make that fast? - So I mean, if you just look\nat the Python problem, right? You can say, how do I make Python faster? And there's been a lot of people that have been working on the, okay, how do I make Python 2x\nfaster, or 10x faster, or something like that, right? And there've been a ton of\nprojects in that vein, right? Mojo started from the,\nwhat can the hardware do? Like, what is the limit of physics? What is the speed of light?\n- Yeah. What is the-\n- Yeah, yeah. - Like, how fast can this thing go? And then how do I express that, right?\n- Yeah. - And so it wasn't anchored relatively on make Python a little bit faster. It's saying, cool, I know\nwhat the hard work can do. Let's unlock that, right? Now when you-\n(Lex chuckling) - Yeah, just say how gutsy\nthat is to be in the meeting and as opposed to trying to see, how do we get the improvement? It's like, what can the physics do? - I mean, maybe I'm a special kinda nerd, but you look at that, what\nis the limit of physics? How fast can these things go, right? When you start looking at that, typically it ends up being\na memory problem, right? And so today, particularly with these\nspecialized accelerators, the problem is that you can\ndo a lot of math within them, but you get bottleneck sending data back and forth to memory, whether it be local\nmemory, or distant memory, or disk, or whatever it is. And that bottleneck, particularly as the\ntraining sizes get large as you start doing tons of\ninferences all over the place, like, that becomes a huge\nbottleneck for people, right? So again, what happened\nis we went through a phase of many years where people\ntook the special case and hand-tuned it and tweaked\nit and tricked it out. And they knew exactly\nhow the hardware worked and they knew the model\nand they made it fast, didn't generalize. (chuckles) And so you can make, you know, ResNet-50, or AlexNet, or\nsomething, Inception v1, like, you can do that, right? Because the models are small,\nthey fit in your head, right? But as the models get\nbigger, more complicated, as the machines get more complicated, it stops working, right? And so this is where things\nlike kernel fusion come in. So what is kernel fusion? This is this idea of saying, let's avoid going to memory\nand let's do that by building a new hybrid kernel and\na numerical algorithm that actually keeps\nthings in the accelerator instead of having to write it all the way out to memory, right? What's happened with\nthese accelerators now is you get multiple levels of memory. Like, in a GPU for example, you'll have global\nmemory and local memory, and, like, all these things. If you zoom way into how hardware works, the register file is\nactually a memory. (chuckles) So the registers are like an L0 cache. And so a lot of taking\nadvantage of the hardware ends up being fully\nutilizing the full power in all of its capability. And this has a number of problems, right? One of which is again, the\ncomplexity of disaster, right? There's too much hardware. Even if you just say let's look at the chips from one line of vendor, like Apple, or Intel, or whatever it is, each version of the chip\ncomes out with new features and they change things so that it takes more time or less to do different things. And you can't rewrite all the software whenever a new chip comes out, right? And so this is where you need\na much more scalable approach. And this is what Mojo and what\nthe Modular stack provides is it provides this\ninfrastructure and the system for factoring all this complexity and then allowing people\nto express algorithms, you talk about auto-tuning, for example, express algorithms in a more portable way so that when a new chip comes out, you don't have to rewrite it all. So to me, like, you know, I kinda joke, like, what is a compiler? Well, there's many ways to explain that. You convert thing A into thing B and you convert source\ncode to machine code. Like, you can talk about many, many things that compilers do, but to me it's about a bag of tricks. It's about a system and a framework that you can hang complexity. It's a system that can then generalize and it can work on\nproblems that are bigger than fit in one human's\nhead, (chuckles) right? And so what that means, what a good stack and what\nthe Modular stack provides is the ability to walk up\nto it with a new problem and it'll generally work quite well. And that's something that\na lot of machine learning infrastructure and tools\nand technologies don't have. Typical state-of-the-art\ntoday is you walk up, particularly if you're deploying, if you walk up with a new model, you try to push it through the converter and the converter crashes, that's crazy. The state of ML tooling\ntoday is not anything that a C programmer\nwould ever accept, right? And it's always been this\nkind of flaky set of tooling that's never been integrated well, and it's never worked together because it's not designed together. It's built by different teams, it's built by different hardware vendors, it's built by different systems, it's built by different\ninternet companies. They're trying to solve\ntheir problems, right? And so that means that\nwe get this fragmented, terrible mess of complexity. - So I mean, the specifics of, and Jeremy showed this-\n- Yeah. - there's the vectorized function, which I guess is built into Mojo? - [Chris] Vectorized, as he showed, is built into the library. - Into the library, it's\ndone on the library. - [Chris] Yep. - Vectorize, parallelize. - [Chris] Yep. - Which vectorize is more low-level, parallelize is higher level. There's the tiling thing, which is how he demonstrated\nthe auto-tune, I think. - So think about this in, like, levels, hierarchical levels of abstraction, right? If you zoom all the way\ninto a compute problem, you have one floating point number, right? And so then you say, okay, I can do things one at a\ntime in an interpreter. (chuckles) It's pretty slow, right? So I can get to doing one\nat a time in a compiler, like in C. I can get to doing 4, or 8\nor 16 at a time with vectors. That's called vectorization. Then you can say, hey, I have\na whole bunch of different... You know, what a multi-core computer is, is it's basically a bunch\nof computers, right? So they're all independent computers that they can talk to each\nother and they share memory. And so now what parallelize\ndoes, it says, okay, run multiple instances\non different computers. And now, they can all work\ntogether on Chrome, right? And so what you're doing is you're saying, keep going out to the next level out. And as you do that, how do\nI take advantage of this? So tiling is a memory optimization, right? It says, okay, let's make sure\nthat we're keeping the data close to the compute part of the problem instead of sending it all back and forth through memory every time I load a block. - And the size of the block, size is, that's how you get to the\nauto-tune to make sure it's optimized.\n- Right, yeah. Well, so all of these, the details matter so much\nto get good performance. This is another funny thing\nabout machine learning and high-performance computing that is very different than C compilers we all grew up with where, you know, if you get a new version of\nGCC, or a new version of Clang, or something like that, you know, maybe something will go 1% faster, right? And so compiler engine\nwill work really, really, really hard to get half a percent out of your C code, something like that. But when you're talking\nabout an accelerator, or an AI application, or you're talking about\nthese kinds of algorithms, now these are things people\nused to write in Fortran, for example, right? If you get it wrong, it's not 5% or 1%, it could be 2x or 10x, (chuckles) right? If you think about it, you really want to make\nuse of the full memory you have, the cache, for example. But if you use too much space,\nit doesn't fit in the cache, now you're gonna be thrashing all the way back out to main memory. And these can be 2x, 10x\nmajor performance differences. And so this is where\ngetting these magic numbers and these things right is\nreally actually quite important. - So you mentioned that Mojo\nis a superset of Python. Can you run Python code\nas if it's Mojo code? - Yes, yes,\n(Lex chuckling) and this has two sides of it. So Mojo's not done yet. So\nI'll give you a disclaimer. Mojo's not done yet, but already we see people\nthat take small pieces of Python code, move it\nover, they don't change it, and you can get 12x speed ups. Like, somebody was just\ntweeting about that yesterday, which is pretty cool, right? And again, interpreters, compilers, right? And so without changing\nany code, without... Also, this is not JIT compiling\nor doing anything fancy. This is just basic stuff,\nmove it straight over. Now Mojo will continue to\ngrow out and as it grows out, it will have more and\nmore and more features and our North Star's to be\na full superset of Python. And so you can bring over, basically, arbitrary Python code\nand have it just work. It may not always be 12x faster, but it should be at least\nas fast and way faster in many cases, is the goal, right? Now, it'll take time to do that. And Python is a complicated language. There's not just the obvious things, but there's also non-obvious\nthings that are complicated. Like, we have to be able to\ntalk to CPython packages, to talk to the CPI, and there's\na bunch of pieces to this. - So you have to, I mean, just to make explicit the obvious that may not be so obvious\nuntil you think about it. So, you know, to run Python code, that means you have to run all the Python packages and libraries. - [Chris] Yeah, yeah. - So that means what? What's the relationship\nbetween Mojo and CPython, the interpreter that's-\n- Yep. - presumably would be tasked with getting those packages to work? - Yep, so in the fullness of time, Mojo will solve for all the problems and you'll be able to move Python packages over and run them in Mojo. - [Lex] Without the CPython. - Without Cpython, someday, right, not today, but someday.\n- Yeah. And that'll be a beautiful day because then you'll get a\nwhole bunch of advantages and you'll get massive\nspeedups and things like this. - But you can do that\none at a time, right? You can move packages one at a time.\n- Exactly, but we're not willing to\nwait for that. (chuckles) Python is too important.\nThe ecosystem is too broad. We wanna both be able to build Mojo out, we also wanna do it the\nright way without time, like, without intense time pressure. We're obviously moving fast, but. And so what we do is we say, okay, well, let's make it so you can import an arbitrary existing package, arbitrary, including, like, you write your own on your local disk (chuckles) or whatever. It's not like a standard,\nlike an arbitrary package, and import that using CPython because CPython already runs\nall the packages, right? And so what we do is we\nbuilt an integration layer where we can actually use Cpython, again, I'm practical, and to actually just load and use all the existing\npackages as they are. The downside of that is you don't get the benefits of Mojo for\nthose packages, right? And so they'll run as fast, as they do in the traditional CPython way, but what that does is that gives you an\nincremental migration path. And so if you say, hey,\ncool, well, here's a, you know, the Python ecosystem is vast. I want all of it to just work, but there's certain things\nthat are really important. And so if I'm doing weather\nforecasting or something, (chuckles) well, I wanna be\nable to load all the data, I wanna be able to work with it, and then I have my own crazy\nalgorithm inside of it. Well, normally I'd write that in C++. If I can write in Mojo and\nhave one system that scales, well, that's way easier to work with. - Is it hard to do that, to have that layer that's running CPython? Because is there some\ncommunication back and forth? - Yes, it's complicated. I\nmean, this is what we do. So, I mean, we make it look\neasy, but it is complicated. But what we do is we use the\nCPython existing interpreter. So it's running its own bike codes, and that's how it provides\nfull compatibility. And then it gives us CPython objects, and we use those objects as is. And so that way we're fully compatible with all the CPython objects\nand all the, you know, it's not just the Python part,\nit's also the C packages, the C libraries underneath them, because they're often hybrid. And so we can fully run and we're fully compatible with all that. And the way we do that is that we have to play by their rules, right? And so we keep objects\nin that representation when they're coming from that world. - What's the representation\nthat's being used? - In memory. We'd have to know a lot about how the CPython interpreter works. It has, for example, reference counting, but also different rules on\nhow to pass pointers around, and things like this,\nsuper low-level fiddly. And it's not like Python. It's like how the interpreter works, okay? And so that gets all exposed out, and then you have to define wrappers around the low-level C code, right? And so what this means is\nyou have to know not only C, which is a different role\nfrom Python, obviously, not only Python- - [Lex] But the wrappers. - but the interpreter and the wrappers and the implementation\ndetails and the conventions. And it's just this reall complicated mess. And when you do that, now suddenly you have a\ndebugger that debugs Python, they can't step into C code, right? So you have this two-world problem, right? And so by pulling this all into Mojo, what you get is you get one world. You get the ability to say, cool, I have un-typed, very dynamic,\nbeautiful, simple code. Okay, I care about performance,\nfor whatever reason, right? There's lots of reasons you might care. And so then you add types,\nyou can parallelize things. You can vectorize things,\nyou can use these techniques, which are general techniques\nto solve a problem. And then you can do that\nby staying in the system. And if you have that one Python package that's really important to\nyou, you can move it to Mojo. You get massive performance benefits on that and other advantages. You know, if you like static types, it's nice if they're enforced. Some people like that, right,\nrather than being hints. So there's other advantages too. And then you can do that\nincrementally as you go. - So one different perspective\non this would be why Mojo instead of making CPython\nfaster, redesigning CPython. - Yeah, well, I mean, you could argue Mojo\nis redesigning CPython, but why not make CPython faster and better and other things like that, there's lots of people working on that. So actually there's a team at Microsoft that is really improving... I think CPython 3.11 came out in October or something like that,\nand it was, you know, 15% faster, 20% faster across the board, which is pretty huge\ngiven how mature Python is and things like this. And so that's awesome. I love it. Doesn't run on GPU. (chuckles)\nIt doesn't do AI stuff. Like, it doesn't do\nvectors, doesn't do things. 20 percent's good. 35,000\ntimes is better, right? So like, they're definitely... I'm a huge fan of that work, by the way, and it composes well\nwith what we're doing. It's not like we're fighting\nor anything like that. It's actually just, it's\ngoodness for the world, but it's just a different path, right? And again, we're not working forwards from making Python a little bit better. We're working backwards from\nwhat is the limit of physics? - What's the process of\nimporting Python code to Mojo? Is there... What's involved in that process?\n- Yeah. - Is there tooling for that? - Not yet. So we're missing some\nbasic features right now. And so we're continuing\nto drop out new features, like, on a weekly basis, but, you know, at the fullness of time, give us a year and a\nhalf, maybe two years. - Is it an automatable process? - So when we're ready, it'll\nbe very automatable, yes. - Is it automatable? Like, is it possible to automate, in the general case of Python-\n- Yeah. - to Mojo conversion, and\nyou're saying it's possible. - Well, so, and this is why, I mean, among other reasons why we use tabs, (chuckles) right?\n- Yes. - [Chris] So first of\nall, by being a superset- - Yep. - it's like C versus C++. Can you move C code to C++? Yeah, right?\n- Yes. - And you can move C code to C++, and then you can adopt classes,\nyou can add adopt templates, you can adopt other references or whatever C++ features you want. After you move C code to C++, like, you can't use templates in C, right? And so if you leave it at C, fine. You can't use the cool features,\nbut it still works, right? And C and C++ good work together. And so that's the analogy, right? Now here, right, there's not a Python is\nbad and Mojo is good, (chuckles) right? Mojo just gives you superpowers, right? And so if you wanna stay\nwith Python, that's cool, but the tooling should be\nactually very beautiful and simple because we're doing the hard\nwork of defining a superset. - Right. So you're right. So there's several things to say there, but also the conversion tooling\nshould probably give you hints as to, like, how\nyou can improve the code? - Yeah, exactly. Once you're in the new world, then you can build all kinds\nof cool tools to say like, hey, should you adopt this feature? And we haven't built those tools yet, but I fully expect those tools will exist. And then you can like, you know, quote, unquote, \"modernize your code,\" or however you wanna look at it, right? So I mean one of the things that I think is really\ninteresting about Mojo is that there have been a lot of projects to improve Python over the years. Everything from, you know, getting Python run on\nthe Java virtual machine, PyPy, which is a JIT compiler. There's tons of these projects\nout there that have been working on improving\nPython in various ways. They fall into one or two camps. So PyPy is a great example of a camp that is trying to be\ncompatible with Python. Even there, not really. Doesn't work with all the C\npackages and stuff like that, but they're trying to be\ncompatible with Python. There's also another\ncategory of these things where they're saying, well,\nPython is too complicated and, you know, I'm gonna cheat on\nthe edges and at, you know, like integers in Python can\nbe an arbitrary size integer. Like if you care about it fitting in a, going fast in a register in a computer, that's really annoying, right? And so you can choose\ntwo pass on that, right? You can say, well, people don't really use\nbig integers that often, therefore I'm gonna just\nnot do it and it'll be fine, not a Python superset.\n- Yeah. - (chuckles) Or you can do the hard thing and say, okay, this is Python, and you can't be a superset of Python without being a superset of Python. And that's a really hard\ntechnical problem, but it's, in my opinion, worth it, right? And it's worth it because it's\nnot about any one package. It's about this ecosystem. It's about what Python\nmeans for the world. And it also means we don't wanna repeat the Python 2 to Python 3 transition. Like we want people to be able\nto adopt this stuff quickly. And so by doing that work,\nwe can help lift people. - Yeah, the challenge, it's\nreally interesting, technical, philosophical challenge of\nreally making a language a superset of another language. It's breaking my brain a little bit. - Well, it paints you into corners. So again, I'm very happy\nwith Python, right? So all joking aside, I think that the indentation thing is not the actual important\npart of the problem. - [Lex] Yes.\n(Chris chuckling) - Right? But the fact that Python has amazing dynamic metaprogramming\nfeatures and they translate to beautiful static\nmetaprogramming features, I think is profound I\nthink that's huge, right? And so Python, I've talked with\nGuido about this, it's like, it was not designed to\ndo what we're doing. That was not the reason\nthey built it this way, but because they really cared\nand they were very thoughtful about how they designed the language, it scales very elegantly in this space. But if you look at other languages, for example, C and C++, right, if you're building a superset, you get stuck with the design decisions of the subset, right? And so, you know, C++\nis way more complicated because of C in the legacy\nthan it would've been if they would've theoretically designed a from scratch thing. And there's lots of people right now that are trying to make C++\nbetter and recent syntax C++, it's gonna be great, we'll\njust change all the syntax. But if you do that, now\nsuddenly you have zero packages, you don't have compatibility. - If you could just linger on that, what are the biggest challenges of keeping that superset status? What are the things\nyou're struggling with? Does it all boiled down\nto having a big integer? - No, I mean, it's- - What are the other things like? - Usually it's the long tail weird things. So let me give you a war story. - [Lex] Okay. - So war story in the\nspace is you go away... Back in time, project I\nworked on is called Clang. Clang, what it is a C++ parser, right? And when I started working on Clang, it must have been like\n2006 or something, less, or 2007 something, 2006 when I first started\nworking on it, right? It's funny how time flies. - [Lex] Yeah, yeah. - I started that project\nand I'm like, okay, well, I wanna build a C\nparser, C++ parser for LVM? It's gonna be the... GCC is yucky. You know,\nthis is me in earlier times. It's yucky, it's unprincipled, it has all these weird features, like all these bugs, like it's yucky. So I'm gonna build a standard\ncompliant C and C++ parser. It's gonna be beautiful, it'll\nbe amazing, well-engineered, all the cool things an\nengineer wants to do. And so I started implementing\nand building it out and building it out and building it out. And then I got to include standard io.h, and all of the headers in the\nworld use all the GCC stuff, (chuckles) okay?\n- Yeah. - And so, again, come back away from theory\nback to reality, right? I was at a fork on the road. I could have built an amazingly\nbeautiful academic thing that nobody would ever use or I could say, well, it's\nyucky in various ways. All these design mistakes,\naccents of history, the legacy. At that point, GCC was\nlike over 20 years old, which, by the way-\n- Yeah. - now, LLVM's over 20\nyears old, (laughs) right? And so it's funny how-\n- Yep. - time catches up to you, right? And so you say, okay, well,\nwhat is easier, right? I mean, as an engineer, it's actually much easier\nfor me to go implement long tail compatibility weird features, even if they're distasteful\nand just do the hard work and like figure it out,\nreverse engineer it, understand what it is,\nwrite a bunch of test cases, like, try to understand the behavior. It's way easier to do all\nthat work as an engineer than it is to go talk to all C programmers and argue with them and try to get them to rewrite their code, right?\n- Yeah. - And- - [Lex] 'Cause that\nbreaks a lot more things. - Yeah. The reality is like nobody\nactually even understands how the code works 'cause it was written by the person who quit 10\nyears ago, (chuckles) right? And so this software is kind\nof frustrating that way, but it's, that's how the world works, right?\n- Yeah. Unfortunately, it can never be this perfect, beautiful thing. - Well, there are occasions in which you get to build, like, you know, you invent a new data structure\nor something like that, or there's this beautiful\nalgorithm that's just like, makes you super happy,\nand I love that moment. But when you're working with people-\n- Yeah. - and you're working with\ncode and dusty deck code bases and things like this, right, it's not about what's\ntheoretically beautiful, it's about what's practical, what's real, what people will actually use. And I don't meet a lot of people that say, I wanna rewrite all my code\njust for the sake of it. - By the way, there could\nbe interesting possibilities and we'll probably talk about it where AI can help rewrite some code. That might be farther out feature, but it's a really interesting one, how that could create more-\n- Yeah, yeah. - be a tool in the battle against\nthis monster of complexity that you mentioned.\n- Yeah. - You mentioned Guido, the benevolent dictator\nfor life of Python. What does he think about Mojo? Have you talked to him much about it? - I have talked with him about it. He found it very interesting. We actually talked with\nbefore it launched, and so he was aware of\nit before it went public. I have a ton of respect for Guido for a bunch of different reasons. You talk about walrus operator and, like, Guido's pretty amazing\nin terms of steering such a huge and diverse community and, like, driving it forward. And I think Python is what\nit is thanks to him, right? And so to me it was really important starting to work on\nMojo to get his feedback and get his input and get\nhis eyes on this, right? Now a lot of what Guido was and is I think concerned about is, how do we not fragment the community? - [Lex] Yeah. - We don't want a Python\n2 to Python 3 thing. Like, that was really painful\nfor everybody involved. And so we spent quite a bit\nof time talking about that. And some of the tricks I\nlearned from Swift, for example, so in the migration from Swift, we managed to, like, not\njust convert Objective-C into a slightly prettier\nObjective-C, which we did, we then converted, not entirely, but almost an entire community to a completely different language, right? And so there's a bunch\nof tricks that you learn along the way that are directly\nrelevant to what we do. And so this is where, for example, you leverage CPython while\nbringing up the new thing. Like, that approach is, I think, proven and comes from experience. And so Guido's very interested\nin like, okay, cool. Like, I think that Python is really his legacy, it's his baby. I have tons of respect for that. Incidentally, I see Mojo as a\nmember of the Python family. I'm not trying to take Python from Guido and from the Python community. And so to me it's really important that we're a good member\nof that community. I think that, again, you\nwould have to ask Guido this, but I think that he was very\ninterested in this notion of like, cool Python gets\nbeaten up for being slow. Maybe there's a path out of that, right? And that, you know, if the\nfuture is Python, right, I mean, look at the far\noutside case on this, right? And I'm not saying this\nis Guido's perspective, but, you know, there's this\npath of saying like, okay, well, suddenly Python can\nsuddenly go all the places it's never been able to go before, right? And that means that\nPython can go even further and can have even more\nimpact on the world. - So in some sense, Mojo\ncould be seen as Python 4.0. - I would not say that. I think that would drive a\nlot of people really crazy. - Because of the PTSD of the 3.0, 2.0. - I'm willing to annoy\npeople about Emacs versus Vim or-\n- Not that one. - [Chris] Versus spaces. - Not that one. - I don't know. That might be\na little bit far even for me. Like, my skin may not be that thick. - But the point is the\nstep to being a superset and allowing all of these capabilities, I think is the evolution of a language. It feels like an evolution of a language. So he's interested by the\nideas that you're playing with, but also concerned\nabout the fragmentation. So what are the ideas you've learned? What are you thinking about? How do we avoid fragmenting the community where the Pythonistas and the, I don't know what to call the Mojo people. - [Chris] Mojicians. - The mojicians, I like it. - [Chris] There you go. - Can coexist happily and share code and basically just have\nthese big code bases that are using Cpython and more and more moving towards Mojo.\n- Yeah. Yeah. Well, so again, these are\nlessons I learned from Swift. And here, we face very\nsimilar problems, right? In Swift, you have\nObjective-C, super dynamic. They're very different\nsyntax, (chuckles) right? But you're talking to people who have large scale code bases. I mean, Apple's got the biggest, largest scale code base of\nObjective-C code, right? And so, you know, none of the companies, none of the other iOS developers, none of the other developers want to rewrite everything all at once. And so you wanna be able to\nadopt things piece at a time. And so a thing that I\nfound that worked very well in the Swift community\nwas saying, okay, cool, and this is when Swift was\nvery young, and you say, okay, you have a million line\nof code Objective-C app. Don't rewrite it all, but when\nyou implement a new feature, go implement that new\nclass using Swift, right? And so now this turns out is a very wonderful thing\nfor an app developer, but it's a huge challenge\nfor the compiler team and the systems people that\nare implementing this, right? And this comes back to what is\nthis trade-off between doing the hard thing that enables scale versus doing the theoretically\npure and ideal thing, right? And so Swift had adopted and built a lot of different machinery\nto deeply integrate with the Objective-C runtime. And we're doing the same\nthing with Python right now. What happened in the case of\nSwift is that Swift's language got more and more and more\nmature over time, right? And incidentally, Mojo is a much simpler language\nthan Swift in many ways. And so I think that Mojo\nwill develop way faster than Swift for a variety of reasons. But as the language gets more\nmature and parallel with that, you have new people starting\nnew projects, right? And so if when the language is mature and somebody's starting a new project, that's when they say, okay, cool, I'm not dealing with a\nmillion lines of code. I'll just start and use the\nnew thing for my whole stack. Now the problem is, again, you come back to we're communities and we're people that work together. You build new subsystem or a new feature or a new thing in Swift, or\nyou build a new thing in Mojo, then you want it to be end up being used on the other side, (chuckles) right? And so then you need\nto work on integration back the other way. And so it's not just\nMojo talking to Python, it's also Python talking to Mojo, right? And so what I would love to see, I don't wanna see this next month, right, but what I wanna see over the\ncourse of time is I would love to see people that are\nbuilding these packages, like, you know, NumPy or, you know,\nTensorFlow or what, you know, these packages that are\nhalf Python, half C++. And if you say, okay, cool, I want to get out of this Python C++ world into a unified world and\nso I can move to Mojo, but I can't give up all my Python clients 'cause they're like, these levers get used by everybody and they're not all gonna\nswitch every, all, you know, all at once and maybe never, right? Well, so the way we should do that is we should vend Python\ninterfaces to the Mojo types. And that's what we did in\nSwift and worked great. I mean, it was a huge\nimplementation challenge for the compiler people, right? But there's only a dozen\nof those compiler people and there are millions of users. And so it's a very\nexpensive, capital-intensive, like, skillset intensive problem. But once you solve that problem, it really helps adoption and\nit really helps the community progressively adopt technologies. And so I think that this\napproach will work quite well with the Python and the Mojo world. - So for a package, port it to Mojo, and then create a Python interface. - [Chris] Yep. - So when you're on these packages, NumPy, PyTorch, TensorFlow. - Yeah. - How do they play nicely together? So is Mojo supposed to be... Let's talk about the\nmachine learning ones. Is Mojo kind of visioned\nto replace PyTorch, TensorFlow to incorporate it? What's the relationship in this? - All right, so take a step back. So I wear many hats. (chuckles) So you're angling it on the Mojo side. Mojo's a programming language.\n- Yes. - And so it can help solve the C, C++ Python feud that's happening. - The fire emoji got me. I'm sorry. We should be talking Modular. Yes, yes. - Yes, okay. So the fire emoji is amazing. I love it. It's a big deal. The other side of this\nis the fire emoji is in service of solving\nsome big AI problems, right?\n- Yes. - And so the big AI problems are, again, this fragmentation,\nthis hardware nightmare, this explosion of new potential, but it's not getting felt\nby the industry, right? And so when you look at, how does the Modular engine\nhelp tens and PyTorch, right, it's not replacing them, right? In fact, when I talk to people, again, they don't like to rewrite all their code. You have people that are\nusing a bunch of PyTorch, a bunch of TensorFlow. They have models that\nthey've been building over the course of many years, right? And when I talk to them,\nthere's a few exceptions, but generally they don't wanna rewrite all their code, right? And so what we're doing is\nwe're saying, \"Okay, well, you don't have to rewrite all your code.\" What happens is the Modular\nengine goes in there and goes underneath\nTensorFlow and PyTorch. It's fully compatible and it just provides better performance, better\npredictability, better tooling. It's a better experience\nthat helps lift TensorFlow and PyTorch and make them even better. I love Python, I love TensorFlow,\nI love PyTorch, right? This is about making the world better because we need AI to go further. - But if I have a process\nthat trains a model and I have a process that\nperforms inference on that model and I have the model itself, what should I do with that\nin the long arc of history in terms of if I use PyTorch to train it. Should I rewrite stuff in Mojo\nif I care about performance? - Oh, so I mean, again, it depends. So if you care about performance, then writing it in Mojo is gonna be way better than writing in Python. But if you look at LLM\ncompanies, for example, so you look at Open AI, rumored, and you look at many of the other folks that are working on many of these LLMs and other like innovative\nmachine learning models, on the one hand they're\ninnovating in the data collection and the model, billions of parameters, and the model architecture\nand the RLHF and the, like all the cool things that\npeople are talking about. But on the other hand, they're spending a lot of time\nwriting CUDA curls, right? And so you say, wait a second, how much faster could all this\nprogress go if they were not having to hand write all\nthese CUDA curls, right? And so there are a few\ntechnologies that are out there, and people have been working\non this problem for a while and they're trying to solve\nsubsets of the problem, again, kinda fragmenting the space. And so what Mojo provides for\nthese kinds of companies is the ability to say, cool, I can have a unifying theory, right? And again, the better\ntogether, the unifying theory, the two-world problem, or\nthe three-world problem, or the N-world problem, like, this is the thing\nthat is slowing people down. And so as we help solve this problem, I think it'll be very helpful for making this whole cycle go faster. - So obviously we've\ntalked about the transition from Objective-C to Swift. You've designed this programming language, and you've also talked quite\na bit about the use of Swift for machine learning context. Why have you decided to move away from maybe an intense focus on Swift for the machine learning\ncontext versus sort of designing a new programming language\nthat happens to be a superset? - You're saying this is an irrational set of life choices I make or what? (chuckles)\n(Lex laughing) - Did you go to the desert\nand did you meditate on it? Okay, all right. No, it was bold. It was bold and needed\nand I think, I mean, it's just bold and sometimes\nto take those leaps, it's a difficult leap to take. - Yeah. Well, so, okay. I mean, I think there's a\ncouple of different things. So actually I left to Apple back in 2017, like January, 2017. So it's been a number of\nyears that I left Apple. And the reason I left\nApple was to do AI, okay? So, and again, I won't\ncomment on Apple and AI, but at the time, right, I wanted to get into and\nunderstand the technology, understand the\napplications, the workloads. And so I was like, okay, I'm gonna go dive deep\ninto Applied and AI, and then the technology\nunderneath it, right? I found myself at Google. - And that was like when TPUs were waking up.\n- Yep, exactly. - And so I found myself\nat Google and Jeff Dean, who's a rockstar as you know, right? And in 2017, TensorFlow's, like, really taking off and\ndoing incredible things. And I was attracted to Google to help them with the TPUs, right? And TPUs are an innovative\nhardware accelerator platform, have now I mean I think\nproven massive scale and like done incredible things, right? And so one of the things\nthat this led into is a bunch of different projects,\nwhich I'll skip over, right? One of which was this Swift\nfor TensorFlow project, right? And so that project\nwas a research project. And so the idea of that is say, okay, well, let's look at innovative\nnew programming models where we can get a fast\nprogramming language, we can get automatic\ndifferentiation into the language. Let's push the boundaries of these things in a\nresearch setting, right? Now, that project I think\nlasted two, three years. There's some really cool outcomes of that. So one of the things that's\nreally interesting is I published a talk at an\nLLVM conference in 2018, again, this seems like so long ago, about graph program abstraction, which is basically the\nthing that's in PyTorch 2. And so PyTorch 2 with\nall this DynamoRIO thing, it's all about this graph\nprogram abstraction thing from Python bike codes. And so a lot of the research\nthat was done ended up pursuing and going out through the\nindustry and influencing things. And I think it's super exciting\nand awesome to see that, but the Swift for\nTensorFlow project itself did not work out super well. And so there's a couple of\ndifferent problems with that. One of which is that,\nyou may have noticed, Swift is not Python. (chuckles) There's a few\npeople that write Python code. - [Lex] Yes. - And so it turns out that all of ML is pretty happy with Python. - It's actually a problem that other programming\nlanguages have as well, that they're not Python. We'll probably maybe\nbriefly talk about Julia, was a very interesting,\nbeautiful programming language, but it's not Python. - Exactly. And so like if you're saying, I'm gonna solve a machine learning problem where all the programmers\nare Python programmers. - [Lex] Yeah. - And you say the first\nthing you have to do is switch to a different language, well, your new thing may\nbe good or bad or whatever, but if it's a new thing, the adoption barrier is massive less. - It's still possible. - Still possible, yeah, absolutely. The world changes and evolves and there's definitely room\nfor new and good ideas, but it just makes it\nso much harder, right? And so lesson learned,\nSwift is not Python, and people are not always\nin search of, like, learning a new thing for the\nsake of learning a new thing. And if you wanna be compatible\nwith all the world's code, turns out meet the world\nwhere it is, right? Second thing is that, you know, a lesson learned is that Swift is a very fast and efficient\nlanguage, kind of like Mojo, but a different take on it still, really worked well with eager mode. And so eager mode is\nsomething that PyTorch does, and it proved out really well, and it enables really\nexpressive and dynamic and easy to debug programming. TensorFlow at the time was not\nset up for that, let's say. That was not... - [Lex] The timing is also\nimportant in this world. - Yeah, yeah. And TensorFlow is a good thing and it has many, many strengths, but you could say Swift for\nTensorFlow is a good idea, except for the Swift and\nexcept for the TensorFlow part. (chuckles) - Swift because it's not Python\nand TensorFlow because it- - [Chris] It wasn't set up for\neager mode at the time, yeah. - It was 1.0. - Exactly. And so one of the things about that is that in the context of it\nbeing a research project, I'm very happy with the fact that we built a lot of\nreally cool technology. We learned a lot of things. I think the ideas went\non to have influence in other systems, like PyTorch. A few people use that I hear, right? And so I think that's super cool. And for me personally, I\nlearned so much from it, right? And I think a lot of the\nengineers that worked on it also learned a tremendous amount. And so, you know, I think that that's just\nreally exciting to see. And, you know, I'm sorry that\nthe project didn't work out. I wish it did, of course, right, but, you know, it's a research project. And so you're there to learn from it. - Well, it's interesting to think about the evolution of programming as we come up with these\nwhole new set of algorithms in machine learning, in\nartificial intelligence. And what's going to win out is it could be a new programming language. It could be-\n- Yeah. - I mean, I just mentioned Julia. I think there's a lot of ideas behind Julia that Mojo shares. What are your thoughts\nabout Julia in general? - So I will have to say\nthat when we launched Mojo, one of the biggest things I didn't predict was the response from the Julia community. And so I was not, I mean, I've, okay, lemme take a step back. I've known the Julia folks\nfor a really long time. They're an adopter of\nLLVM a long time ago. They've been pushing state-of-the-art in a bunch of different ways. Julia's a really cool system. I had always thought of Julia as being mostly a scientific computing\nfocused environment, right? And I thought that was its focus. I neglected to understand\nthat one of their missions is to, like, help make Python\nwork end-to-end. (chuckles) And so I think that was my error\nfor not understanding that. And so I could have been\nmaybe more sensitive to that, but there's major differences between what Mojo's doing\nand what Julia's doing. So as you say, Julia is not Python, right? And so one of the things that a lot of the Julia people came\nout and said is like, \"Okay, well, if we put a ton of more energy into, ton more money or in engineering\nor whatever into Julia, maybe that would be better\nthan starting Mojo, right?\" Well, I mean, maybe that's true, but it still wouldn't make\nJulia into Python. (chuckles) So if you worked backwards\nfrom the goal of, let's build something\nfor Python programmers without requiring them to relearn syntax, then Julia just isn't there, right? I mean, that's a different thing, right? And so if you anchor on, I love Julia, and I want Julia to go further, then you can look at it\nfrom a different lens, but the lens we were coming at was, Hey, everybody is using Python.\nThe syntax isn't broken. Let's take what's great about Python and make it even better. And so it was just a\ndifferent starting point. So I think Julie's a great language. The community's a lovely community. They're doing really cool stuff,\nbut it's just a different, it's slightly different angle. - But it does seem that\nPython is quite sticky. Is there some philosophical, almost thing you could\nsay about why Python, by many measures, seems to be the most popular programming language in the world? - Well, I can tell you\nthings I love about it. Maybe that's one way to\nanswer the question, right? So huge package ecosystem, super lightweight and easy to integrate. It has very low startup time, right? - [Lex] So what's startup time? You mean like learning curve or what? - Yeah, so if you look at\ncertain other languages, you say like, go, and it just takes a, like Java, for example, it takes a long time to\nJIT compile all the things and then the VM starts up and the garbage (indistinct) kicks in and then it revs its engines and then it can plow through\na lot of internet stuff or whatever, right? Python is like scripting. Like it just goes, right?\n- Yeah. - Python has a very low compile time. Like, so you're not sitting there waiting. Python integrates in a\nnotebooks in a very elegant way that makes exploration super interactive and it's awesome, right? Python is also, it's like almost the glue of computing. Because it has such a simple\nobject representation, a lot of things plug into it. That dynamic metaprogramming\nthing we were talking about, also enables really expressive\nand beautiful APIs, right? So there's lots of reasons\nthat you can look at, technical things the Python\nhas done and say, like, okay, wow, this is actually\na pretty amazing thing. And any one of those you can neglect, people will all just\ntalk about indentation (chuckles) and ignore like\nthe fundamental things. But then you also look at\nthe community side, right? So Python owns machine learning. Machine learning's pretty big. - Yeah, and it's growing. - And it's growing, right? And it's growing in importance, right? And so-\n- And there's a reputation of prestige to machine learning to where like if you're a new programmer, you're thinking about, like, which program and language do I use? Well, I should probably\ncare about machine learning, therefore let me try Python, and kinda builds and builds and builds. - And even go back before that. Like, my kids learned Python, right, not because I'm telling\n'em to learn Python, but because-\n- Were they rebelling against you or what? - Oh, no, no. Well, they they also learn Scratch, right, and things like this too, but it's because Python is\ntaught everywhere, right? Because it's easy to learn, right? And because it's pervasive, right? And there's-\n- Back in my day, we learned Java and C++. - [Chris] Yeah, well. - Well, uphill both directions, but yes. I guess Python-\n- Yeah. - is the main language of teaching software\nengineering schools now. - Yeah, well, and if you look at this, there's these growth cycles, right? If you look at what causes\nthings to become popular and then gain in popularity, there's reinforcing feedback\nloops and things like this. And I think Python has done, again, the whole community has done\na really good job of building those growth loops and\nhelp propel the ecosystem. And I think that, again, you look at what you can get done with just a few lines\nof code, it's amazing. - So this kinda self-building\nloop is interesting to understand because\nwhen you look at Mojo, what it stands for some of the features, it seems sort of clear that\nthis is a good direction for programming languages to evolve in the machine\nlearning community, but it's still not obvious\nthat it will because of this, whatever the engine of\npopularity of virality. Is there something you\ncould speak to, like, how do you get people to switch? - Yeah, well, I mean, I think that the viral growth loop is to switch people to Unicode. - [Lex] Yes. - I think the Unicode file extensions are what I'm betting on. I think that's gonna be the thing. - Yeah.\n(Chris chuckling) - Tell the kids that you\ncould use the fire emoji and they'd be like, what?\n- Exactly, exactly. (Lex chuckling) Well, in all seriousness, like, I mean, I think there's really, I'll\ngive you two opposite answers. One is, I hope if it's\nuseful, if it solves problems, and if people care about\nthose problems being solved, they'll adopt the tech, right? That's kinda the simple answer. And when you're looking\nto get tech adopted, the question is, is it solving an important\nproblem people need solved, and is the adoption cost low\nenough that they're willing to make the switch and cut\nover and do the pain upfront so that they can actually do it, right? And so hopefully Mojo will be\nthat for a bunch of people. And, you know, people building these hybrid\npackages are suffering. It is really painful. And so I think that we have a\ngood shot of helping people, but the other side is like, it's okay if people don't use Mojo. Like, it's not my job to say\nlike, everybody should do this. Like, I'm not saying Python is bad. Like, I hope Python, CPython, like, all these implementations 'cause Python ecosystems,\nnot just CPython, it's also a bunch of\ndifferent implementations with different trade-offs. And this ecosystem is\nreally powerful and exciting as are other programming languages. It's not like type script or something is gonna go away, right? And so there's not a\nwinner-take-all thing. And so I hope that Mojo's\nexciting and useful to people, but if it's not, that's also fine. - But I also wonder what the use case for why you should try Mojo would be. So practically speaking- - [Chris] Yeah. - it seems like, so there's entertainment. There's the dopamine hit of saying, holy, this is 10 times faster. This little piece of code\nis 10 times faster in Mojo. - [Chris] Outta the box\nbefore you get to 35,000. - Exactly, I mean, just even that, I mean, that's the dopamine hit\nthat every programmer sorta dreams of is the optimization. It's also the drug that can pull you in and have you waste way\ntoo much of your life optimizing and over optimizing, right? But so what do you see\nwould be, like, common? It's very hard to predict,\nof course, but, you know, if you look 10 years from now\nand Mojo's super successful. - [Chris] Yeah. - What do you think would be the thing where people like try\nand then use it regularly and it kinda grows and\ngrows and grows and grows? - Well, so you talked about dopamine hit. And so one, again,\nhumans are not one thing. And some people love rewriting their code and learning new things\nand throwing themselves in the deep end and\ntrying out a new thing. In my experience, most\npeople, they're too busy. They have other things going on. By number, most people\ndon't want like this. I wanna rewrite all my code. But (chuckles) even those\npeople, the two busy people, the people that don't actually\ncare about the language, that just care about getting stuff done, those people do like\nlearning new things, right? - [Lex] Yeah. - And so you talk about the\ndopamine rush of 10x faster, Wow, that's cool. I wanna do that again. Well, it's also like, here's the thing I've heard\nabout in a different domain, and I don't have to rewrite on my code. I can learn a new trick, right? Well, that's called growth,\n(chuckles) you know? And so, one thing that I\nthink is cool about Mojo, and again, those will\ntake a little bit of time, for example, the blog posts\nand the books and, like, all that kinda stuff to develop and the language needs\nto get further along. But what we're doing,\nyou talk about types, like you can say, look, you can start with the\nworld you already know and you can progressively learn new things and adopt them where it makes sense. And if you never do that, that's cool. You're not a bad person. (chuckles) If you get really excited about\nit and wanna go all the way in the deep end and rewrite\neverything and, like, whatever, that's cool, right? But I think the middle path is\nactually the more likely one where it's, you know, you come out with a a new\nidea and you discover, wow, that makes my code way simpler, way more beautiful way,\nfaster way, whatever. And I think that's what people like. Now if you fast forward and you said, like, 10 years out, right, I can give you a very different\nanswer on that, which is, I mean, if you go back and look at what computers looked\nlike 20 years ago, every 18 months, they got\nfaster for free, right, 2x faster every 18 months. It was like clockwork. It was free, right? You go back 10 years ago\nand we entered in this world where suddenly we had\nmulti-core CPUs and we had, and if you squint and turn your head, what a GPUs is just a many-core, very simple CPU thing kind of, right? And 10 years ago it was\nCPUs and GPUs and graphics. Today, we have CPU, GPUs, graphics. And AI, because it's so important, because the compute is so demanding because of the smart\ncameras and the watches and all the different places that AI needs to work in our lives, it's caused this explosion of hardware. And so part of my thesis, part of my belief of where computing goes, if you look out 10 years from now, is it's not gonna get simpler. Physics isn't going back\nto where we came from. It's only gonna get weirder\nfrom here on out, right? And so to me, the exciting part about\nwhat we're building is it's about building\nthat universal platform, which the world can continue to get weird. 'Cause again, I don't think\nit's avoidable, it's physics, but we can help lift people,\nscale, do things with it, and they don't have to rewrite their code every time a new device comes out. And I think that's pretty cool. And so if Mojo can help with that problem, then I think that it will be\nhopefully quite interesting and quite useful to a wide range of people because there's so much potential. And like there's so much, you know, maybe analog computers will become a thing or something, right? And we need to be able to get into a mode where we can move this\nprogramming model forward, but do so in a way where\nwe're lifting people and growing them instead of forcing them to rewrite all their\ncode and exploding them. - Do you think there'll be a few major libraries that go Mojo first? - Well, so I mean, the Modular\nengines on Mojo. (chuckles) So again, come back to, like, we're not building Mojo because it's fun. We're building Mojo because we had to solve these accelerators. - That's the origin story, but I mean, ones that are currently in Python. - Yeah, so I think that a\nnumber of these projects will. And so one of the things, and again, this is just my best guess. Like, each of the package\nmaintainers also has... I'm sure plenty of other things going on. People really don't like rewriting code just for the sake of rewriting code. But sometimes like people are\nexcited about like adopting a new idea.\n- Yeah. - And turns out that while rewriting code is generally not people's first thing, turns out that redesigning\nsomething while you rewrite it and using a rewrite as\nan excuse to redesign can lead to the 2.0 of your thing that's way better than the 1.0, right? And so I have no idea,\nI can't predict that, but there's a lot of\nthese places where, again, if you have a package that is\nhalf C and half Python, right, you just solve the pain, make it easier to move things faster, make it easier to bug and\nevolve your tech adopting Mojo kinda makes sense to start with. And then it gives you this opportunity to rethink these things. - So the two big gains are\nthat there's a performance gain and then there's the portability to all kinds of different devices. - And there's safety, right?\nSo you talk about real types. I mean, not saying this is for everybody, but that's actually a\npretty big thing, right? - [Lex] Yeah, types are. - And so there's a bunch of\ndifferent aspects of what, you know, what value Mojo provides. And so, I mean, it's funny for me, like, I've been working on these\nkinds of technologies and tools for too many years now, but you look at Swift, right, and we talked about Swift for TensorFlow, but Swift as a programming\nlanguage, right? Swift's now 13 years old from when I started it? - [Lex] Yeah. - 'Cause I started in 2010, if I remember. And so that project, and I was involved with it for\n12 years or something, right, that project has gone through its own really interesting story arc, right? And it's a mature, successful, used by millions of people system, right? Certainly not dead yet, right? But also going through that story arc, I learned a tremendous amount\nabout building languages, about building compilers, about working with the\ncommunity and things like this. And so that experience, like I'm helping channel\nand bring directly into Mojo and, you know, other systems, same thing. Like, apparently I like building, and iterating and evolving things. And so you look at this LLVM thing that I worked on 20 years ago,\nand you look at MLIR, right? And so a lot of the lessons learned in LLVM got fed into MLIR, and I think that MLIR is a way\nbetter system than LLVM was. And, you know, Swift is a really good\nsystem and it's amazing, but I hope that Mojo will take the next step forward in terms of design. - In terms of running Mojo\nand people can play with it, what's Mojo Playground?\n- Yeah. - And from the interface perspective and from the hardware perspective, what's this incredible thing running on? - Yeah, so right now, so here we are, two weeks after launch. - Yes. - We decided that, okay, we have this incredible set of technology that we think might be good, but we have not given it\nto lots of people yet. And so we were very conservative and said, \"Let's put it in a workbook\nso that if it crashes, we can do something about it. We can monitor and track that, right?\" And so, again, things\nare still super early, but we're having like one\nperson a minute sign up with over 70,000 people (chuckles) two weeks in is kinda crazy. - And you can sign up to Mojo Playground and you can use it in the cloud. - [Chris] Yeah. - In your browser. - [Chris] And so what\nthat's running on, right? - Notebook. - Yeah, What that's running on is that's running on cloud VMs. And so you share a machine\nwith a bunch of other people, but turns out there's a bunch of them now because there's a lot of people. And so what you're doing is\nyou're getting free compute and you're getting to play with this thing in kind of a limited controlled way so that we can make sure that it doesn't totally crash and be embarrassing, right?\n- Yeah. - So now a lot of the\nfeedback we've gotten is people wanna download it around locally. So we're working on that right now. And so-\n- So that's the goal, to be able to download locally to it. - Yeah, that's what everybody expects. And so we're working on that right now. And so we just wanna make\nsure that we do it right. I think this is one of the lessons I learned from Swift also, by the way, is when we launched Swift, gosh, it feels like\nforever ago, it was 2014, and we, I mean, it was super exciting. I, and we, the team had worked on Swift for a number of years in secrecy, okay? And (chuckles) four years\ninto this development, roughly, of working on this thing, at that point, about 250\npeople at Apple knew about it. - [Lex] Yeah. - Okay? So it was secret. Apple's good at secrecy and\nit was a secret project. And so we launched this at WWC, a bunch of hoopla and excitement and said developers are\ngonna be able to develop and submit apps in the App\nStore in three months, okay? Well, several interesting\nthings happened, right? So first of all, we learned\nthat it had a lot of bugs. It was not actually production quality, and it was extremely stressful\nin terms of like trying to get it working for a bunch of people. And so what happened was we\nwent from zero to, you know, I don't know how many developers\nApple had at the time, but a lot of developers overnight. And they ran to a lot of bugs\nand it was really embarrassing and it was very stressful for\neverybody involved, right? It was also very exciting 'cause everybody was excited about that. The other thing I learned\nis that when that happened, roughly every software engineer who did not know about\nthe project at Apple, their head exploded when it was launched 'cause they didn't know it was coming. And so they're like, \"Wait, what is this? I signed up to work for Apple\nbecause I love Objective-C. Why is there a new thing?,\" right?\n- Yeah. - And so now what that\nmeant practically is that the push from launch\nto first of all the fall, but then to 2.0 and 3.0 and\nlike all the way forward was super painful for the\nengineering team and myself. It was very stressful. The developer community\nwas very grumpy about it because they're like,\n\"Okay, well, wait a second. You're changing and breaking my code, and like, we have to fix the bugs.\" And it was just like a lot of tension and friction on all sides. There's a lot of technical\ndebt in the compiler because we have to run really fast and you have to go implement the thing and unblock the use case and do the thing. And you know it's not right, but you never have time to\ngo back and do it right. And I'm very proud of the Swift team because they've come, I mean, we, but they came so far and\nmade so much progress over this time since launch,\nit's pretty incredible. And Swift is a very, very good thing, but I just don't wanna\ndo that again, right? And so-\n- So iterate more through the development process. - And so what we're doing\nis we're not launching it when it's hopefully 0.9 with no testers. We're launching it and\nsaying it's 0.1, right? And so we're setting expectations\nof saying like, okay, well, don't use this\nfor production, right? If you're interested in what we're doing, we'll do it in an open way\nand we can do it together, but don't use it in production yet. Like, we'll get there, but\nlet's do it the right way. And I'm also saying we're not in a race. The thing that I wanna do is\nbuild the world's best thing. - [Lex] Yeah. - Right, because if you do it right and it lifts the industry, it doesn't matter if it takes an extra two months.\n- Yeah. - Like two months is worth waiting. And so doing it right and not being overwhelmed\nwith technical debt and things like this is like, again, war wounds, lessons learned, whatever you wanna say, I think is absolutely\nthe right thing to do. Even though right now people are very frustrated that, you know, you can't download it\nor that it doesn't have feature X or something like this. And so-\n- What have you learned in a little bit of time since it's been released into the wild that people have been complaining\nabout feature X or Y or Z? What have they been complaining about? Whether they have been excited about like, almost like detailed\nthings versus a big thing. I think everyone's would be very excited about the big vision. - Yeah, yeah. Well, so I\nmean, I've been very pleased. I mean, in fact, I mean, we've been massively\noverwhelmed with response, which is a good problem to have. It's kinda like a success disaster, in a sense, right?\n- Yeah. - And so, I mean, if you go back in time\nwhen we started Modular, which is just not yet\na year and a half ago, so it's still a pretty\nnew company, new team, small but very good team of people, like we started with extreme conviction that there's a set of problems\nthat we need to solve. And if we solve it, then people will be interested\nin what we're doing, right? But again, you're building\nin basically secret, right? You're trying to figure it out. The creation's a messy process. You're having to go\nthrough different paths and understand what you wanna\ndo and how to explain it. Often when you're doing disruptive\nand new kinds of things, just knowing how to explain\nit is super difficult, right? And so when we launched, we\nhope people would be excited, but, you know, I'm an\noptimist, but I'm also like, don't wanna get ahead of myself. And so when people found out about Mojo, I think their heads exploded\na little bit, right? And, you know, here's a, I think a pretty credible\nteam that has built some languages and some tools before. And so they have some lessons learned and are tackling some of the deep problems in the Python ecosystem and giving it the love and attention\nthat it should be getting. And I think people got\nvery excited about that. And so if you look at that, I mean, I think people are excited about ownership and taking a step beyond Rust, right? And there's people that\nare very excited about that and there's people that are\nexcited about, you know, just like I made Game of Life\ngo 400 times faster, right, and things like that,\nand that's really cool. There are people that are\nreally excited about the, okay, I really hate writing\nstuff in C++, save me. - Like systems in your, they're like stepping up, like, oh yes. - And so that's me by the way, also. - [Lex] Yeah. - I really wanna stop\nwriting C++, but the- - I get third person\nexcitement when people tweet, Here, I made this code, Game\nof Life or whatever, faster. And you're like, yeah. - Yeah, and also like, well, I would also say that, let me cast blame out to\npeople who deserve it. - [Lex] Sure. - These terrible people who\nconvinced me to do some of this. Jeremy Howard, that guy.\n- Yes, yes. Well, he's been pushing\nfor this kinda thing. He's been pushing-\n- He's wanted this for years. - Yeah, he's wanted this\nfor a long, long time. - [Chris] He's wanted\nthis for years. And so- - For people who don't know Jeremy Howard, he is like one of the most legit people in the machine learning community. He's a grassroots, he really teaches, he's an incredible educator,\nhe is an incredible teacher, but also legit in terms of\na machine learning engineer himself.\n- Yes. - And he's been running\nthe fast.AI and looking, I think for exactly what you've done with Mojo.\n- Exactly. And so, I mean, the first time, so I met Jeremy pretty early on, but the first time I sat up and I'm like, this guy is ridiculous, is when I was at Google and\nwe were bringing up TPUs and we had a whole team of people and there was this\ncompetition called Don Bench of who can train ImageNet fastest, right?\n- Yeah. Yes. - And Jeremy and one of his\nresearchers crushed Google (chuckles) by not through sheer force of the amazing amount of compute and the number of TPUs\nand stuff like that, that he just decided that\nprogressive imagery sizing was the right way to train the model in. You were epoch faster and make the whole thing go vroom, right?\n- Yep. - And I'm like, \"This guy is incredible.\" So you can say,\n- Right. anyways, come back to, you know, where's Mojo coming from? Chris finally listened to Jeremy. (Lex laughing) It's all his fault. - Well, there's a kinda very refreshing, pragmatic view that he has about machine learning\nthat I don't know if it, it's like this mix of a\ndesire for efficiency, but ultimately grounded and\ndesired to make machine learning more accessible to a lot of people. I don't know what that is.\n- Yeah. - I guess that's coupled with\nefficiency and performance, but it's not just obsessed\nabout performance. - Well, so a lot of AI and AI research ends up being that it has to go fast\nenough to get scale. So a lot of people don't\nactually care about performance, particularly on the research side until it allows 'em to have\nmore a bigger dataset, right? And so suddenly now you care\nabout distributed compute and like, all these exotic HPC, like, you don't actually\nwanna know about that. You just want to be able to\ndo more experiments faster and do so with bigger datasets, right? And so Jeremy has been\nreally pushing the limits. And one of the things\nI'll say about Jeremy, and there's many things\nI could say about Jeremy, 'cause I'm a fanboy of his,\nbut it fits in his head, and Jeremy actually takes the\ntime where many people don't to really dive deep into\nwhy is the beta parameter of the atom optimizer equal to this, right?\n- Yeah. - And he'll go survey and understand what are all the activation\nfunctions in the trade-offs, and why is it that everybody\nthat does, you know, this model, pick that thing. - So the why, not just\ntrying different values, like, really what is going on here? - Right, and so as a consequence\nof that, like he's always, he, again, he makes time, but he spends time to understand things at a depth that a lot of people don't. And as you say, he then brings it and teaches people- - [Lex] Teaches it. - And his mission is\nto help lift, you know, his website says \"making AI uncool again,\" like it's about, like,\nforget about the hype. It's actually practical and useful. Let's teach people how to do this, right? Now the problem Jeremy struggled with is that he's pushing the envelope, right? Research isn't about doing the thing that is staying on the happy path or the well-paved road, right? And so a lot of the systems today have been these really\nfragile, fragmented things, are special case in this happy path. And if you fall off the happy path, you get eaten by an alligator. (chuckles) - (chuckles) So what about... So Python has this giant\necosystem of packages and there's a package repository. Do you have ideas of how\nto do that well for Mojo, how to do a repository of packages well? - So that's another\nreally interesting problem that I knew about but I didn't understand how big of a problem it\nwas: Python packaging. A lot of people have very big pain points and a lot of scars with Python packaging. - Oh, you mean, so there's\nseveral things to say. - [Chris] Building and distributing and managing dependencies\n- Yes. - [Chris] and versioning\nand all this stuff. - So from the perspective of, if you want to create your own package, and then\n- Yes, yeah. - or you wanna build on top of a bunch of other people's packages and then they get updated\nand things like this. Now, I'm not an expert in this,\nso I don't know the answer. I think this is one the reasons why it's great that we work as a team and there's other really good\nand smart people involved, but one of the things I've\nheard from smart people who've done a lot of this is\nthat the packaging becomes a huge disaster when you get\nthe Python and C together. And so if you have this problem where you have code split\nbetween Python and C, now not only do you have\nto package the C code, you have to build the C code. C doesn't have a package manager, right? C doesn't have a dependency versioning management system, right? And so I'm not experiencing\nthe state-of-the-art and all the different\nPython package managers, but my understanding is that's a massive part of the problem. And I think Mojo solves that part of the problem\ndirectly heads on. Now, one of the things I think\nwe'll do with the community, and this isn't, again, we're not solving all the\nworld's problems at once, we have to be kinda focused, start with, is that I think that we\nwill have an opportunity to reevaluate packaging, right? And so I think that we can\ncome back and say, okay, well, given the new tools and technologies and the cool things we\nhave that we've built up, because we have not just syntax we have an entirely new compiler stack that works in a new way, maybe there's other innovations\nwe can bring together and maybe we can help solve that problem. - So almost a tangent to that question from the user perspective of packages. It was always surprising to\nme that it was not easier to sort of explore and find packages, you know, with, with PIP install. It's an incredible ecosystem. It's huge. It's just interesting that it wasn't made. It's still, I think, not made easier to discover\npackages to do, yeah. like search and discovery\nas YouTube calls it. - Well, I mean, it is kinda funny because this is one of the challenges of these like intentionally\ndecentralized communities. And so-\n- Yeah. - I don't know what the\nright answer is for Python. I mean, there are many people that I don't even know\nthe right answer for Mojo. Like, so there are many\npeople that would have much more informed opinions than I do, but it's interesting, if\nyou look at this, right? Open source communities,\nyou know, there's Git. Git is a fully de decentralized and anybody can do it any way they want, but then there's GitHub, right? And GitHub centralized\ncommercial in that case, right? Thing really helped pull together and help solve some of\nthe discovery problems and help build a more\nconsistent community. And so maybe there's opportunities for- - There's something like a GitHub for-\n- Yeah. - Although even GitHub,\nI might be wrong on this, but the search and discovery\nfor GitHub is not that great. Like, I still use Google search. - Yeah, well, I mean, maybe that's because GitHub doesn't wanna replace Google search, right? I think there is room\nfor specialized solutions to specific problems,\nbut sure, I don't know. I don't know the right\nanswer for GitHub either. They can go figure that out. - But the point is to have\nan interface that's usable, that's successful to people\nof all different skill levels and-\n- So, well, and again, like what are the benefit\nof standards, right? Standards allow you to build\nthese next level-up ecosystem and next level-up infrastructure\nand next level-up things. And so, again, come back\nto, I hate complexity, C+ Python is complicated. It makes everything more\ndifficult to deal with. It makes it difficult to\nport, move code around, work with all these things\nget more complicated. And so, I mean, I'm not an expert, but maybe Mojo can help a little bit by helping reduce the amount\nof C in this ecosystem and make it therefore scale better. - So any kinda packages\nthat are hybrid in nature would be a natural fit\nto move to Mojo, which- - Which is a lot of them, by the way. - Yeah. - So a lot of them, especially that are doing\nsome interesting stuff computation wise.\n- Yeah, yeah. Let me ask you about some features. - Yeah. - So we talked about\nobviously indentation, that it's a typed language\nor optionally typed. Is that the right way to say it? - It's either optional\nor progressively or- - Progressively, okay. - I think the... So people have very strong opinions on the right word to use.\n- Yeah. - [Chris] I don't know. - I look forward to your letters. So there's the var versus\nlet, but let is for constance. - Yeah. - Var is an optional. - Yeah, var makes it\nmutable. So you can reassign. - Okay. Then there's function overloading. - Oh okay, yeah. - I mean, there's a lot of\nsource of happiness for me, but function overloading, that's, I guess, is that for performance or is that... Why does Python not have\nfunction overloading? - So I can speculate. So\nPython is a dynamic language. The way it works is that\nPython and Objective-C are actually very similar\nworlds if you ignore syntax. And so Objective-C is straight\nline derived from Smalltalk, a really venerable interesting language that much of the world\nhas forgotten about, but the people that remember\nit love it generally. And the way that Smalltalk works is that every object has a dictionary in it. And the dictionary maps\nfrom the name of a function or the name of a value within an object to its implementation. And so the way you call a method\nand Objective-C is you say, go look up, the way I call\nfoo is I go look up foo, I get a pointer to the function\nback, and then I call it, okay, that's how Python works, right? And so now the problem with that is that the dictionary\nwithin a Python object, all the keys are strings\nand it's a dictionary. Yeah. So you can only have one\nentry per name. You think. - It's as simple as that. - I think it's as simple as that. And so now why do they never fix this? Like, why do they not change it to not be a dictionary anymore, they not change, like do other things? - Well, you don't really have to in Python because it's dynamic. And so you can say, I get\ninto the function now, if I got past an integer,\ndo some dynamic test for it, if it's a string, go do another thing. There's another additional challenge, which is even if you did support\noverloading, you're saying, okay, well, here's a version\nof a function for integers and a function for strings. Well, even if you could\nput it in that dictionary, you'd have to have the\ncollar do the dispatch. And so every time you call the function, you'd have to say like, is it\nan integer or is it a string? And so you'd have to figure\nout where to do that test. And so in a dynamic language, overloading is something you, general, you don't have to have. But now you get into a type\nlanguage and, you know, in Python, if you\nsubscript with an integer, then you get typically one\nelement out of a collection. If you subscript with a range, you get a different thing out, right? And so often in type languages, you'll wanna be able to\nexpress the fact that, cool, I have different behavior, depending on what I actually\npass into this thing. And if you can model that, it can make it safer and\nmore predictable and faster, and, like, all these things. - It somehow feels safer, yes, but also feels empowering,\nlike in terms of clarity. Like you don't have to design\nwhole different functions. - Yeah, well, and this is\nalso one of the challenges with the existing Python typing\nsystems is that in practice, like you take subscript, like in practice, a lot of these functions, they don't have one signature, right? They actually have different\nbehavior in different cases. And so this is why it's difficult to like retrofit this\ninto existing Python code and make it play well, with typing. You kinda have to design for that. - Okay, so there's a interesting\ndistinction that people that program Python might be\ninterested in is def versus fn. So it's two different\nways to define a function. - Yep. - And fn is a stricter version of def. What's the coolness that\ncomes from the strictness? - So here you get into, what is the trade-off with the superset? - Yes. - Okay, so superset, you have to, or you really want to be compatible. Like, if you're doing a superset, you've decided compatibility\nwith existing code is the important thing, even if some of the decisions they made were maybe not what you'd choose. - Yeah, okay. - So that means you put a lot\nof time into compatibility and it means that you get locked\ninto decisions of the past, even if they may not have\nbeen a good thing, right? Now, systems programmers\ntypically like to control things, right, and they wanna\nmake sure that, you know, not all cases of course, and even systems programmers\nare not one thing, right, but often you want predictability. And so one of the things that Python has, for example, as you know, is\nthat if you define a variable, you just say, X equals four,\nI have a variable name to X. Now I say some long method,\nsome long name equals 17, print out some long name,\noops, but I typoed it, right? Well, the compiler, the Python compiler\ndoesn't know in all cases what you're defining\nand what you're using, and did you typo the use of\nit or the definition, right? And so for people coming\nfrom type languages, again, I'm not saying they're right or wrong, but that drives 'em crazy\nbecause they want the compiler to tell them, you type out\nthe name of this thing, right? And so what fn does is\nit turns on, as you say, it's a strict mode and so it says, okay, well, you have to actually declare, intentionally declare your\nvariables before you use them. That gives you more predictability, more error checking and things like this, but you don't have to use it. And this is a way that\nMojo is both compatible 'cause defs work the same way that defs have already always worked, but it provides a new alternative that gives you more control. And it allows certain kinds of people that have a different philosophy to be able to express that and get that. - But usually if you're\nwriting Mojo code from scratch, you'll be using fn. - It depends, again, it depends\non your mentality, right? It's not that def is\nPython and fn is Mojo. Mojo has both, and it loves both, right? It really depends on that is\njust strict. Yeah, exactly. Are you playing around and\nscripting something out? Is it a one-off throwaway script? Cool. Like, Python is great at that. - I'll still be using fn, but yeah. - Well, so I love strictness. Okay. - Well, so control, power. You\nalso like suffering, right? Yes, go hand in hand. - How many pull-ups? - I've lost count at this. Yeah, exactly. At this point. - So, and that's cool. I love you for that. Yeah. And I love other people who\nlike strict things, right, but I don't want to say\nthat that's the right thing because python's also very beautiful for hacking around and\ndoing stuff in research and these other cases where\nyou may not want that. - You see, I just feel like\nmaybe I'm wrong in that, but it feels like strictness\nleads to faster debugging. So in terms of going from, even on a small project from\nzero to completion, it just, I guess it depends how many\nbugs you generate usually. Yeah. - Well, so I mean, if it's again, lessons learned in\nlooking at the ecosystem, it's really, I mean, I think it's, if you study some of\nthese languages over time, like the Ruby community for example, now Ruby is a pretty well, developed, pretty established community, but along their path they\nreally invested in unit testing. Like, so I think that\nthe Ruby community is really pushed forward the\nstate-of-the-art of testing because they didn't have a type system that caught a lot of bugs\nat compile time, right? And so you can have the\nbest of both worlds. You can have good testing\nand good types, right, and things like this, but I thought that it\nwas really interesting to see how certain challenges get solved. And in Python, for example, the interactive notebook\nkind of experiences and stuff like this are really amazing. And if you typo something,\nit doesn't matter. It just tells you it's fine, right? And so I think that the\ntrades are very different if you're building a, you know, large scale production system versus you're building\nan exploring a notebook. - And speaking of control, the hilarious thing, if you look at code, I write just for myself, for fun, it's like littered with\nasserts everywhere, okay? - It's a kinda, well, then. - Yeah, you would like text. - It's basically saying\nin a dictatorial way, this should be true now,\notherwise everything stops. - Well, and that is the sign. And I love you, man, but that is a sign of\nsomebody who likes control. And so, yes.\n- Yeah. - I think that you'll like\nf i this turning into a, I think I like Mojo. - Therapy session. Yes. I definitely will. Speaking of asserts\nexceptions are called errors. Why is it called errors? - So we, I mean, we use the same, we're the same as Python, right, but we implement it a\nvery different way, right? And so if you look at other languages, like we'll pick on C++\nour favorite, right? C++ has a thing called zero-cost\nexception handling, okay? So, and this is in my opinion, something to learn lessons from. - It's a nice polite way of saying it. - And so, zero-cost exception handling, the way it works is that\nit's called zero-cost because if you don't throw an exception, there's supposed to be no\noverhead for the non-error code. And so it takes the error\npath out of the common path. It does this by making throwing an error extremely expensive. And so if you actually throw an error with a C++ compiler using exceptions has to go look up in tables on the side and do all this stuff. And so throwing an error\ncan be like 10,000 times more expensive than referring\nfrom a function, right? Also, it's called zero-cost exceptions, but it's not zero-cost by any\nstretch of the imagination because it massively blows\nout your code, your binary, it also adds a whole\nbunch of different paths because of disrupts and other things like that that exist in C++ plus, and it reduces the number\nof optimizations it has, like all these effects. And so this thing that was\ncalled zero-cost exceptions, it really ain't, okay. Now if you fast forward to newer languages and this includes Swift and\nRust and Go and now Mojo, well, and Python's a little bit different because it's interpreted and so like, it's got a little bit of a\ndifferent thing going on. But if you look at it, if you\nlook at compiled languages, many newer languages say, okay, well, let's not do that zero-cost\nexception handling thing. Let's actually treat and throwing an error the same as returning a variant returning either the\nnormal result or an error. Now programmers generally\ndon't want to deal with all the typing machinery and like\npushing around a variant. And so you use all the\nsyntax that Python gives us, for example, try and\ncatch and it, you know, functions that raise and things like this. You can put a raises decorator on your functions, stuff like this. And if you wanna control that, and then the language can\nprovide syntax for it. But under the hood, the way\nthe computer executes it, throwing an error is basically as fast as returning something. - Oh, interesting. So it's exactly the same way\nfrom a compile perspective. - And so this is actually, I mean, it's a fairly nerdy thing,\nright, which is why I love it, but this has a huge impact on the way you design your APIs, right? So in C++ huge communities\nturn off exceptions because the cost is just so high, right? And so the zero-cost\ncost is so high, right? And so that means you can't\nactually use exceptions in many libraries, right? Interesting. Yeah. And even for the people\nthat do use it, well, okay, how and when do you wanna pay the cost? If I try to open a file,\nshould I throw an error? Well, what if I'm probing around, looking for something, right, and I'm looking it up\nin many different paths? Well, if it's really slow to do that, maybe I'll add another function\nthat doesn't throw an error or turns in error code instead. And now I have two different\nversions of the same thing. And so it causes you to fork your APIs. And so, you know, one of the things I learned\nfrom Apple and I so love is the art of API design is\nactually really profound. I think this is something\nthat Python's also done a pretty good job at in\nterms of building out this large scale package ecosystem. It's about having standards\nand things like this. And so, you know, we wouldn't wanna enter\na mode where, you know, there's this theoretical\nfeature that exists in language, but people don't use it in practice. Now I'll also say one of\nthe other really cool things about this implementation approach is that it can run on GPUs and it can run on accelerators\nand things like this. And that standard\nzero-cost exception thing would never work on an accelerator. And so this is also part of how Mojo can scale all the way down to\nlike little embedded systems and to running on GPUs\nand things like that. - Can you actually say about the... Maybe is there some high-level\nway to describe the challenge of exceptions and how they work\nin code during compilation? So it's just this idea of\npercolating up a thing and error. - Yeah, yeah. So the way to think about it is, think about a function that\ndoesn't return anything, just as a simple case, right? And so you have function\none calls function two, calls function three, calls function four, along that call stack that\nare tribe blocks, right? And so if you have function\none calls function two, function two has a tribe block, and then within it it calls\nfunction three, right? Well, what happens if\nfunction three throws? Well, actually start simpler.\nWhat happens if it returns? Well, if it returns, it's supposed to go back\nout and continue executing and then fall off the\nbottom of the tribe block and keep going and it all's good. If the function throws, you're supposed to exit\nthe current function and then get into the\naccept clause, right, and then do whatever codes there and then keep falling on and going on. And so the way that a\ncompiler like Mojo works is that the call to that function, which happens in the accept\nblock calls the function, which happens in the accept\nblock calls the function, and then instead of returning\nnothing, it actually returns, you know, an a variant\nbetween nothing and an error. And so if you return,\nnormally fall off the bottom, or do return, you return nothing. And if you throw, throw an error, you return the variant. That is, I'm an error, right? So when you get to the call, you say, okay, cool, I called a function. Hey, I know locally I'm\nin a tribe block, right? And so I call the function and then I check to see what it returns. Aha. Is that error thing\njump to the accept block. - And that's all done for\nyou behind the scenes. - Exactly. And so the competitor\ndoes all this for you. And I mean, one of the things, if you dig into how this\nstuff works in Python, it gets a little bit more complicated because you have finally blocks, which you need to go into do some stuff, and then those can also throw and return. - Wait, What? Nested? - Yeah, and like the stuff\nmatters for compatibility. Like, there's really- - Can nest them. - there's with clauses,\nand so with clauses, are kinda like finely blocks with some special stuff going on. And so there's nesting. - In general, nesting of anything, nesting of functions should be illegal. Well, it just feels like it\nadds a level of complexity. - Lex, I'm merely an implementer. And so this is again, one last question. One of the trade-offs you\nget when you decide to build a superset is you get to\nimplement a full fidelity implementation of the thing\nthat you decided is good. And so, yeah, I mean, we can complain about\nthe reality of the world and shake our fist, but- - It always feels like you\nshouldn't be allowed to do that. Like, to declare functions\nin certain functions inside functions, that seems-\n- Oh, wait, wait, wait. What happened to Lex, the Lisp guy? - No, I understand that, but Lisp is what I used to do in college. - So now you've grown up. - You know, we've all done things in college we're not proud of. No, wait a sec, wait a sec.\nI love Lis, I love Lis. - Okay. Yeah, I was gonna say, you're afraid of me\nirritating the whole internet. - Like yeah, no, I love Lisp. It worked as a joke in my\nhead and come out, right? - So nested functions are, joking aside, actually really great and\nfor certain things, right? And so these are also called closures. Closures are pretty cool\nand you can pass callbacks. There's a lot of good patterns. And so- - So speaking of which, I don't think you have nested function implemented yet in Mojo. - We don't have Lambda\nsyntax, but we do have Nest. - Lambda syntax nested. - Functions. Yeah. - There's a few things on\nthe roadmap that you have that it'd be cool to\nsort of just fly through, 'cause it's interesting to see, you know, how many features there are\nin a language small and big. Yep. They have to implement. Yeah. So first of all there's Tuple support, and that has to do with some\nof their specific aspect of it, like the parentheses or\nnot parenthesis that Yeah. - This is just a totally\na syntactic thing. - A syntactic thing, okay. There's, but it is cool. It's still so keyword\narguments and functions. - Yeah, so this is where in Python, you can say call function X equals four and X is the name-\n- Yeah. - of the argument. That's a nice sort of documenting\nsalt documenting feature. Yep. - Yeah, I mean, and again, this isn't rocket science\nto implement this, just the laundry list. - It's just on the list. The bigger features\nare things like traits. So traits are when you\nwanna define abstract. So when you get into typed languages, you need the ability to write generics. And so you wanna say, I wanna write this function\nand now I want to work on all things that are arithmetic. Like, well, what does\narithmetic like, mean? Well, arithmetic like is a categorization of a bunch of types. Again, you can define many different ways, and I'm not gonna go into ring\ntheory or something, but the, you know, you can say it's arithmetic. Like if you can add, subtract, multiply, divide it for example, right? And so what you're saying is\nyou're saying there's a set of traits that apply to\na broad variety of types. And so they're all these types arithmetic, like, all these tensors\nand floating point integer and, like, there's this\ncategory of of types. And then I can define on an\northogonal access algorithms that then work against types\nthat have those properties. It's been implemented in Swift\nand Rust in many languages. So it's not Haskell,\nwhich is where everybody learns their tricks from, but we need to implement that, and that'll enable a new\nlevel of expressivity. - So classes. - Yeah, classes are a big deal. - It's a big deal still to be implemented. Like you said, Lambda syntax, and there's,, like, detailed stuff, like whole module import support for top-level code and file scope. And then global variables also. So being able to have\nvariables outside of a top level.\n- Well, and so this comes back to\nthe where Mojo came from, and the fact that this is your 0.1, right? So Modular's building an AI stack, right? And an AI stack has a\nbunch of problems working with hardware and writing\nhigh-performance kernels and doing this kernel fusion\nthing I was talking about, and getting the most out of the hardware. And so we've really\nprioritized and built Mojo to solve Modular's problem. Right now our North Star is built out to support all the things. And so we're making incredible progress. By the way, Mojo's only,\nlike, seven months old. So that's another interesting thing. - Well, I mean part of the\nreason I wanted to mention some of these things is\nlike, there's a lot to do and it's pretty cool how you just kinda, sometimes you take for granted how much there is in a\nprogramming language, how many cool features you kinda rely on. And this is kinda a nice reminder when you lay it as its do list. - Yeah and so, I mean,\nbut also you look into, it's amazing how much is\nalso there and you take it for granted that a\nvalue, if you define it, it will get destroyed automatically. Like, that little feature itself is actually really complicated given the way the ownership\nsystem has to work. And the way that works within\nMojo is a huge step forward from what Rust and Swift have done. - Wait, can you say that again? When value-\n- Yeah. When you define it gets\ndestroyed automatically. - Yeah. So like, like say\nyou have a string, right? So you define a string on the stack. Okay. Or on whatever that means, like in your local function, right? And so you say like whether it be in a def and so you just say X\nequals hello world, right? Well, if your strength type\nrequires you to allocate memory, then when it's destroyed,\nyou have to deallocate it. So in Python and in Mojo, you define that with a Dell method, right? Where does that get run? Well, it gets run sometime\nbetween the last use of the value and the end of the program. Like in this, you now get\ninto garbage collection, you get into, like,\nall these long debated, you talk about religions and trade-offs and things like this. This is a hugely hotly contested world. If you look at C++, the way this works is that\nif you define a variable or a set of variables within a function, they get destroyed in a\nlast in, first out order. So it's like nesting, okay. This has a huge problem\nbecause if you have a big scope and you define a whole\nbunch of values at the top and then you use 'em and then you do a whole bunch of code\nthat doesn't use them, they don't get destroyed until the very end of that scope, right? And so this also destroys tail calls. So good functional programming, right? This has a bunch of different\nimpacts on, you know, you talk about reference counting optimizations and things like this. A bunch of very low-level things. And so what Mojo does is\nit has a different approach on that from any language\nI'm familiar with, where it destroys them\nas soon as possible. And by doing that you\nget better memory use, you get better predictability,\nyou get tail calls that work, you get a bunch of other things, you get better ownership tracking. There's a bunch of\nthese very simple things that are very fundamental that are already built in there in Mojo today that are the things that\nnobody talks about generally, but when they don't work right, you find out and you\nhave to complain about. - Is it trivial to know\nwhat's the soonest possible to delete a thing that it's\nnot gonna be used again? - Yeah. Well, I mean,\nit's generally trivial. It's after the last use of it. So if you just find X as a string and then you have some use\nof X somewhere in your code- - Within that scope, you mean, within the scope that is accessible? - It's, yeah, exactly. So you can only use something\nwithin its scope. Yeah. And so then it doesn't wait\nuntil the end of the scope to delete it, it destroys\nit after the last use. - So there's kinda some very eager machine that's just sitting\nthere and deleting. Yeah. - And it's all in the compiler. So it's not at runtime,\nwhich is also cool. And so interesting. Yeah. And this is actually non-trivial because you have control flow, right? And so it gets complicated pretty quickly. And so like angst, right? Was not, not. - Well, so you have to insert delete, like in a lot of places. - Potentially. Yeah, exactly. So the compiler has to reason about this. And this is where again, it's experience building languages and not getting this right. So again, you get another chance to do it and you get basic things like this, right? But it's extremely powerful\nwhen you do that, right? And so there's a bunch\nof things like that, that kinda combine together. And this comes back to the, you get a chance to do it the right way, do it the right way, and make sure that every brick\nyou put down is really good. So that when you put\nmore bricks on top of it, they stack up to something\nthat's beautiful. - Well, there's also, like, how many design discussions\ndo there have to be about particular details\nlike implementation of particular small features? Because the features that seem small, I bet some of them might\nbe like really require really big design decisions. - Yeah. Well, so I mean, lemme give\nyou another example of this. Python has a feature called async/await. So it's a new feature. I mean, in the long arc of Python history, it's a relatively new feature, right, that allows way more expressive,\nasynchronous programming. Okay? Again, this is a\nPython's a beautiful thing. And they did things that are great for Mojo for completely different reasons. The reason that async/await\ngot added to Python, as far as I know, is because Python doesn't\nsupport threads, okay? And so Python doesn't support threads, but you wanna work with networking and other things, like, that can block. I mean, Python does support threads, it's just not its strength. And so they added this\nfeature called async/await. It's also seen in other\nlanguages like Swift and JavaScript and many\nother places as well. Async/await and Mojo is amazing 'cause we have a high-performance, heterogeneous compute\nruntime underneath the covers that then allows non-blocking I/O so you get full use of your accelerator. That's huge. Turns out it's actually\nreally an important part of fully utilizing the machine. You talk about design discussions, that took a lot of discussions, right? And it probably will\nrequire more iteration. And so my philosophy with\nMojo is that, you know, we have a small team of really good people that are pushing forward\nand they're very good at the extremely deep knowing how the compiler and runtime and, like, all the low-level\nstuff works together, but they're not perfect. It's the same thing as\nthe Swift team, right? And this is where one of\nthe reasons we released Mojo much earlier is so we can get feedback and we've already like renamed a keyword data community\nfeedback, which one? We use an ampersand now it's named in out. We're not renaming existing Python keyword 'cause that breaks compatibility, right? We're renaming things. We're adding and making sure\nthat they are designed well. We get usage experience, we iterate and work with the community. Because again, if you\nscale something really fast and everybody writes all their code and they start using it in production, then it's impossible to change. And so you wanna learn from people. You wanna iterate and\nwork on that early on. And this is where design discussions, it's actually quite important to do. - Could you incorporate an emoji, like into the language,\ninto the main language? Like a good... Like do you have a favorite one? - Well, I really, like in terms of humor, like rofl, whatever, rolling\non the floor laughing. So that could be like a, what would that be the use case for that? Like an except throw an\nexception of some sort. I don't- - You should totally\nfile a feature request. - Or maybe a heart one.\nIt has to be a heart one. - People have told me that\nI'm insane. I'm liking this. - I'm gonna use the viral\nnature of the internet to get this passed. - I mean, it's funny you come back to the flame emoji file extension, right? You know, we have the option\nto use the flame emoji, which just even that\nconcept, 'cause for example, the people at GitHub say,\nnow I've seen everything. You know, like. - Yeah, and there's something, it kinda, it's reinvigorating. It's like, oh, that's possible. That's really cool that for some reason that makes everything else,\nlike, seem really excited. - I think the world is\nready for this stuff, right? And so, you know, when we\nhave a package manager, we'll clearly have to innovate by having the compiled package thing be the little box with the bow on it, right? I mean, it has to be done. - It has to be done. Is there some stuff on the roadmap that you're particularly stressed about, or excited about that\nyou're thinking about? - A lot, I mean, as of today's snapshot, which will be obsolete tomorrow, the lifetime stuff is really exciting. And so lifetimes give you safe references to memory without dangling pointers. And so this has been done in\nlanguages like Rust before. And so we have a new approach,\nwhich is really cool. I'm very excited about that. That'll be out to the community very soon. The traits feature is really a big deal. And so that's blocking\na lot of API design. And so there's that. I think\nthat's really exciting. A lot of it is these kinda\ntable stakes features. One of the things that is again, also lessons learned with\nSwift is that programmers in general like to add syntactic sugar. And so it's like, oh\nwell, this annoying thing, like in Python, you have to\nspell Underbar armbar ad. Why can't I just use plus def plus? Come on. Why can't I just do that, right? And so trivial bit of syntactic sugar. It makes sense, it's\nbeautiful, it's obvious. We're trying not to do that. And so for two different\nreasons, one of which is that, again, lesson learned with Swift. Swift has a lot of syntactic sugar, which may may be a good thing,\nmaybe not, I don't know. But because it's such an easy\nand addictive thing to do, sugar, like make sure\nblood get crazy, right? Like, the community will really dig into that and wanna do a lot of that. And I think it's very distracting from building the core abstractions. The second is we wanna be a good member of the Python community, right? And so we wanna work with\nthe broader Python community and yeah, we're pushing forward a bunch of systems programming features and we need to build them\nout to understand them. But once we get a long ways forward, I wanna make sure that we go\nback to the Python community and say, okay, let's\ndo some design reviews. Let's actually talk about this stuff. Let's figure out how we want this stuff all to work together. And syntactic sugar just makes\nall that more complicated. So. - And yeah, list comprehension. Is that yet to be implemented? Yeah. And my favorite d I mean, I dictionaries. - Yeah, there's some basic 0.1. - 0.1, yeah. - But nonetheless, it's actually still quite\ninteresting and useful. - As you've mentioned,\nModular is very new. Mojo is very new. It's\na relatively small team. Yeah. It's building up this. - Yeah, we're just gigantic stack. Yeah. This incredible stack that's\ngoing to perhaps define the future of development\nof our AI overlords. - We just hope it will be useful. - As do all of us. So what have you learned from this process of building up a team? Maybe one question is how do you hire-\n- Yeah. - great programmers, great people that operate\nin this compiler hardware, machine learning, software\ninterface design space? And maybe are-\nYeah. - a little bit fluid in what they can do. - So, okay, so language design too. - So building a company\nis just as interesting in different ways is building a language, like different skill\nsets, different things, but super interesting. And I've built a lot of teams,\na lot of different places. If you zoom in from the big\nproblem into recruiting, well, so here's our problem, okay. I'll be very straightforward about this. We started Modular with\na lot of conviction about we understand the problems, we understand the customer pain points. We need to work backwards from the suffering in the industry. And if we solve those problems, we think it'll be useful for people. But the problem is that the people we need to hire, as you say, are all these super specialized people that have jobs at big tech,\nbig tech worlds, right? And, you know, I don't think we have product market fit in the way that a normal startup does, or we don't have product\nmarket fit challenges because right now everybody's using AI and so many of them are\nsuffering and they want help. And so again, we started\nwith strong conviction. Now again, you have to\nhire and recruit the best and the best all have jobs. And so what we've done\nis we've said, okay, well, let's build an amazing culture. Start with that. That's usually not something\na company starts with. Usually you hire a bunch of people and then people start fighting and it turns into gigantic mess. And then you try to figure out how to improve your culture later. My co-founder, Tim in particular, is super passionate about\nmaking sure that that's right. And we've spent a lot of time, early on, to make sure that we can scale. - Can you comment... Sorry, before we get to the second, what makes for a good culture?\n- Yeah, so, I mean, there's many different\ncultures and I have learned many things from many different people, several very unique, almost\nfamously unique cultures. And some of them I learned what to do and some of them I learned\nwhat not to do. Yep. Okay. And so we want an inclusive culture. I believe in like amazing\npeople working together. And so I've seen cultures\nwhere you have amazing people and they're fighting each other. I see amazing people and\nthey're told what to do, like doubt. Shout line\nup and do what I say, it doesn't matter if it's\nthe right thing, do it right. And neither of these is the... and I've seen people\nthat have no direction. They're just kinda floating\nin different places and they wanna be amazing,\nthey just don't know how. And so a lot of it starts with\nhave a clear vision, right? And so we have a clear\nvision of what we're doing. And so I kind of grew up at Apple in my engineering life, right? And so a lot of the Apple\nDNA rubbed off on me. My co-founder Tim also is\nlike a strong product guy. And so what we learned is, you know, I saw at Apple that you don't work from building cool technology. You don't work from, like, come up with cool product\nand think about the features you'll have in the big check\nboxes and stuff like this. 'Cause if you go talk to customers, they don't actually\ncare about your product, they don't care about your technology. What they care about is\ntheir problems, right? And if your product can\nhelp solve their problems, well, hey, they might be\ninterested in that, right? And so if you speak to\nthem about their problems, if you understand you have compassion, you understand what\npeople are working with, then you can work backwards to\nbuilding an amazing product. - So the vision's done\nby defining the problem. - And then you can work\nbackwards in solving technology. Got it. And at Apple, like it's, I think pretty famously said\nthat, you know, for every, you know, there's a\nhundred nos for every yes. I would refine that to say that there's a hundred\nnot yets for every yes. Yeah. But famously, if you\ngo back to the iPhone, for example, right? iPhone 1, every, I mean, many people laughed at it\nbecause it didn't have 3G, it didn't have copy and paste, right? And then a year later,\nokay, finally it has 3G, but it still doesn't have\ncopy and paste, it's a joke. \"Nobody will ever use this product,\" blah, blah, blah, blah,\nblah, blah, blah, right? Well, year three, had copy and paste, and people stopped\ntalking about it, right? And so, being laser focused\nand having conviction and understanding what the core problems are and giving the team\nthe space to be able to build the right tech\nis really important. Also, I mean, you come back to recruiting, you have to pay well, right? So we have to pay\nindustry leading salaries and have good benefits\nand things like this. That's a big piece. We're a remote-first\ncompany. And so we have to... So remote-first has a very\nstrong set of pros and cons. On the one hand, you can hire\npeople from wherever they are, and you can attract amazing talent even if they live in strange\nplaces or unusual places. On the other hand, you have time zones. On the other hand, you have, like, everybody on the internet will fight if they don't understand each other. And so we've had to learn\nhow to like have a system where we actually fly people in and we get the whole company\ntogether periodically, and then we get work groups together and we plan and execute together. - And there's like an intimacy to the in-person brainstorming. Yeah, I guess you lose,\nbut maybe you don't. Maybe if you get to know each other well, and you trust each other,\nmaybe you can do that. Yeah. - Well, so when the\npandemic first hit, I mean, I'm curious about your experience too. The first thing I missed\nwas having whiteboards, right?\n- Yeah. - Those design discussions\nwhere you're like, I can high, high intensity\nwork through things, get things done, work through\nthe problem of the day, understand where you're on, figure out and solve the\nproblem and move forward. But we've figured out ways-\n- Yeah. - to work around that now with, you know, all these screen sharing and other things like that that we do. The thing I miss now is sitting down at a lunch table with the team. Yeah. The spontaneous things\nlike the coffee bar things and the bumping into each other\nand getting to know people outside of the transactional\nsolve a problem over Zoom. - And I think there's just a lot of stuff that I'm not an expert at this. I don't know who is,\nhopefully there's some people, but there's stuff that\nsomehow is missing on Zoom. Even with the Y board,\nif you look at that, if you have a room with one\nperson at the whiteboard, and then there's like three\nother people at a table, there's a, first of all, there's a social aspect to\nthat where you're just shooting the a little bit, almost like. - Yeah, as people are just\nkinda coming in and Yeah. - That, but also while the, like it's a breakout discussion that happens for like seconds at a time, maybe an inside joke or like\nthis interesting dynamic that happens that's Zoom. - And you're bonding. Yeah. - You're bonding, you're bonding. But through that bonding,\nyou get the excitement. There's certain ideas are like complete. And you'll see that in the faces of others that you won't see necessarily\non Zoom and like something, it feels like that should be possible to do without being in-person. - Well, I mean, being in person\nis a very different thing. Yeah. It's worth it, but\nyou can't always do it. And so again, we're still learning. Yeah. And we're also learning as like humanity with\nthis new reality, right? But what we found is that\ngetting people together, whether it be a team or the whole company or whatever is worth the expense because people work together\nand are happier after that. Like, it just, like, there's a massive period\nof time where you're like, go out and things, start getting frayed, pull people together, and then yeah, you realize that we're\nall working together, we see things the same way. We work through the disagreement\nor the misunderstanding. We're talking across each other and then you work much better together. And so things like that I think\nare really quite important. - What about people that\nare kinda specialized in very different aspects of\nthe stack working together? What are some interesting\nchallenges there? - Yeah, well, so I mean, I mean, there's lots of interesting\npeople, as you can tell, I'm, you know, hard to deal with too, but- - You're one of the most lovable people. - So there's different philosophies in building teams for me. And so some people say\nhire 10x programmers, and that's the only thing,\nwhatever that means, right? What I believe in is\nbuilding well-balanced teams, teams that have people\nthat are different in them. Like if you have all\ngenerals and no troops or all troops and no generals, or you have all people\nthat think in one way and not the other way, what you get is you get\na very biased and skewed and weird situation where\npeople end up being unhappy. And so what I like to do is I\nlike to build teams of people where they're not all the same. You know, we do have\nteams and they're focused on like runtime, or compiler GP, or accelerator, or\nwhatever the specialty is, but people bring a different take and have a different perspective. And I look for people that\ncompliment each other. And particularly if you look at leadership teams and things like this, you don't want everybody\nthinking the same way. You want people bringing different perspectives and experiences. And so I think that's really important. - That's team. But what about building a\ncompany as ambitious as Modular? So what are some\ninteresting questions there? - Oh, I mean, so many. Like, so one of the things I love about... Okay, so Modular's the first\ncompany I built from scratch. One of the first things\nthat was profound was I'm not cleaning up\nsomebody else's mess, right? And so if you look at, and. - That's liberating to some degree. - It's super liberating. And also many of the projects\nI've built in the past have not been core to the\nproject of the company. Swift is not Apple's product, right? MLIR is not Google's revenue\nmachine or whatever, right? It's important, but it's like working on\nthe accounting software for, you know, the retail\ngiant or something, right? It's like enabling\ninfrastructure and technology. And so at Modular, the tech we're building is here\nto solve people's problems. Like, it is directly the thing\nthat we're giving to people. And so this is a really big difference. And what it means for me as a leader, but also for many of our engineers, is they're working on\nthe thing that matters. And that's actually pretty, I mean, again, for compiler people and things like that, that's usually not the case, right? And so that's also pretty\nexciting and quite nice, but one of the ways that this manifests is it makes it easier to make decisions. And so one of the challenges I've had in other worlds is it's like, okay, well, community matters somehow for the goodness of the world, or open source matters theoretically, but I don't wanna pay for a t-shirt. Yeah. right, or some swag, like, well, t-shirts cost 10 bucks each. You can have 100 t-shirts\nfor $1,000 to a Megacorp, but $1,000 is unaccountably\ncan't count that low. Yes. Right. But justifying it and getting\na t-shirt, by the way, if you'd like a t-shirt,\nI can give you a t-shirt. - Well, I would 100% like a t-shirt. Are you joking? - You can have a fire\nemoji t-shirt. Is that- - I will treasure this.\nIs that a good thing? I will pass it down to my grandchildren. - And so, you know, it's very liberating to be able to decide. I think that Lex should\nhave a T-shirt, right? And it becomes very\nsimple because I like Lex. - This is awesome. So I have to ask you about\none of the interesting developments with large language models is that they're able to\ngenerate code recently. Really? Well, yes. To a degree that maybe, I\ndon't know if you understand, but I struggle to understand\nbecause it forces me to ask questions about\nthe nature of programming, of the nature of thought\nbecause the language models are able to predict the kinda code I was about to write so well. Yep. That it makes me wonder\nlike how unique my brain is and where the valuable\nideas actually come from. Like, how much do I contribute\nin terms of ingenuity, innovation to code I write or\ndesign and that kinda stuff. When you stand on the shoulders of giants, are you really doing anything? And what LLMs are helping\nyou do is they help you stand on the shoulders of\ngiants in your program. There's mistakes. They're interesting that\nyou learn from, but I just, it would love to get your\nopinion first high level. Yeah. Of what you think about this\nimpact of large language models when they do program synthesis,\nwhen they generate code. - Yeah. Well, so I don't\nknow where it all goes. Yeah. I'm an optimist and I'm\na human optimist, right? I think that things I've seen\nare that a lot of the LLMs are really good at\ncrushing leak code projects and they can reverse the\nlink list like crazy. Well, it turns out\nthere's a lot of instances of that on the internet, and\nit's a pretty stock thing. And so if you want to see\nstandard questions answered, LMS can memorize all the answers,\nthen that can be amazing. And also they do generalize out from that. And so there's good work on that, but I think that if you, in my\nexperience, building things, building something like\nyou talk about Mojo, where you talk about these things, where you talk about\nbuilding an applied solution to a problem, it's also about\nworking with people, right? It's about understanding the problem. What is the product that you wanna build? What are the use case?\nWhat are the customers? You can't just go survey all the customers because they'll tell you that\nthey want a faster horse. Maybe they need a car, right? And so a lot of it comes into, you know, I don't feel like we have\nto compete with LLMs. I think they'll help automate a ton of the mechanical stuff out of the way. And just like, you know, I think we all try to\nscale through delegation and things like this, delegating rote things\nto an LLVM I think is an extremely valuable and approach that will help us all scale\nand be more productive. - But I think it's a\nfascinating companion, but. - I'd say I don't think that that means that we're gonna be done with coding. - Sure. But there's power in\nit as a companion and- - Yeah, absolutely. - So from there, I would love to zoom in\nonto Mojo a little bit. Do you think about that? Do you think about LMS\ngenerating Mojo code and helping sort of like, yeah. When you design new programming language, it almost seems like,\nman, it would be nice to, this sort of almost as a way to learn how I'm supposed to\nuse this thing for them to be trained on some of the Mojo code. - Yeah. So I do lead an AI company. So maybe there'll be a\nMojo LLM at some point. But if your question is like, how do we make a language to be suitable for LLMs?\n- Yeah. - I think the cool thing about LLMs is you don't have to, right? And so if you look at what is English or any of these other terrible languages that we as humans deal\nwith on a continuous basis, they're never designed for machines and yet they're the\nintermediate representation. They're the exchange\nformat that we humans use to get stuff done, right? And so these programming languages, they're an intermediate representation between the human and the computer or the human and the\ncompiler, roughly, right? And so I think the LMS\nwill have no problem learning whatever keyword we pick. - Maybe the fire emoji is gonna, oh. - Maybe that's gonna break\nit. It doesn't tokenize. - No, the reverse of that.\nIt will actually enable it. Because one of the issues I\ncould see with being a superset of Python is there will be\nconfusion about the gray area. So it'll be mixing stuff, but. - Well, I'm a human optimist.\nI'm also an LM optimist. I think that we'll solve that problem. But you look at that and you say, okay, well, reducing the rote thing, right? Turns out compilers are very particular and they really want the\nindentation to be right. They really want the colon\nto be there on your Els or else it'll complain, right? I mean, compilers can do better at this, but LMS can totally\nhelp solve that problem. And so I'm very happy about\nthe new predictive coding and co-pilot type features\nand things like this, because I think it'll all\njust make us more productive. - It's still messy and fuzzy\nand uncertain. Unpredictable. So, but is there a future you see, given how big of a leap\nGPT-4 was where you start to see something like LMS\ninside a compiler or no? - I mean, you could do\nthat. Yeah, absolutely. I mean, I think that would be interesting. - Is that wise? - Well, well, I mean, it\nwould be very expensive. So compilers run fast and\nthey're very efficient and LMS are currently very expensive. There's on-device LLMs and\nthere's other things going on. And so maybe there's an answer there. I think that one of the things that I haven't seen enough of is that, so LLMs to me are amazing when you tap into the creative potential\nof the hallucinations, right? And so if you're doing\ncreative brainstorming or creative writing or things like that, the hallucinations work in your favor. If you're writing code\nthat has to be correct 'cause you're gonna ship it in production, then maybe that's not actually a feature. And so I think that\nthere has been research and there has been work on building algebraic reasoning systems\nand kind of like figuring out more things that feel like proofs. And so I think that there\ncould be interesting work in terms of building more\nreliable at scale systems, and that could be interesting. But if you've chased\nthat rabbit hole down, the question then becomes, how do you express your\nintent to the machine? And so maybe you want\nLLLM to provide the spec, but you have a different kind of net that then actually\nimplements the code, right? So it's to use the\ndocumentation and inspiration versus the actual implementation. - Yeah.\n- Potentially. Since if successful Modular\nwill be the thing that runs, I say so jokingly, our AI overlords, but AI systems that are used across, I know it's a cliche term,\nbut internet of things. So across. - So I'll joke and say like,\nAGI should be written in Mojo. - Yeah. AGI should be written in Mojo. You're joking, but it's also possible\nthat it's not a joke that a lot of the ideas behind Mojo seems like the natural set of\nideas that would enable at scale training and\ninferences of AI systems. So it's just, I have to ask you about the\nbig philosophical question about human civilization. So folks like Eli Kowski\nare really concerned about the threat of AI. - Yeah. - Do you think about the good\nand the bad that can happen at scale deployment of AI systems? - Well, so I've thought a lot about it, and there's a lot of different\nparts to this problem, everything from job\ndisplacement to Skynet, things like this.\n- Yeah. - And so you can zoom into\nsub parts of this problem. I'm not super optimistic about\nAGI being solved next year. I don't think that's\ngonna happen personally. - So you have a kinda\nzen-like calm about... There's a nervousness because the leap of GPT-4 seems so big. - Sure, it's huge. - It's like there's some\nkinda transitionary period. You're thinking- - Well so I mean, there's a\ncouple of things going on there. One is I'm sure GPT-5 and 7 and 19 will be also huge leaps. They're also getting much\nmore expensive to run. And so there may be a limiting function in terms of just expense. On the one hand, train, like, that could be a limiter\nthat slows things down, but I think the bigger limiter outside of, like, Skynet takes over. And I don't spend any\ntime thinking about that, because if Skynet takes\nover and kills us all, then I'll be dead. So I don't worry about that. So, you know, I mean, that's just, okay. Other things worry about,\nI'll just focus on. I'll focus and not worry about that one. But I think that the other thing I'd say is that AI moves quickly, but humans move slowly\nand we adapt slowly. And so what I expect to happen is just like any technology diffusion, like the promise and then the application takes time to roll out. And so I think that I'm\nnot even too worried about autonomous cars defining\naway all the taxi drivers. Remember autonomy was\nsupposed to be solved by 2020. Yeah. - Boy, do I remember. - And so like, I think that on the one hand\nwe can see amazing progress, but on the other hand, we\ncan see that, you know, the reality is a little\nbit more complicated and it may take longer to roll\nout than you might expect. - Well, that's in the physical space. I do think in the digital spaces, the stuff that's built on top\nof LLMs that runs, you know, the millions of apps that\ncould be built on top of them, and that could be run\non millions of devices, millions of types of devices. - Yeah. - I just think that the rapid effect it has on human civilization could be truly transformative to it. - Yeah.\n- We don't even know. - Well, and so the predict well, and there I think it depends on, are you an optimist or a pessimist? Or a masochist?\n- Yeah. Just to clarify optimist\nabout human civilization. - Me too. And so I look at that as saying, okay, cool, well, AI do, right? And so some people say, \"Oh my god. Is it gonna destroy us all?\nHow do we prevent that?\" I kinda look at it from a, is\nit gonna unlock us all right? You talk about coding, is it gonna make so I don't have to do all the repetitive stuff? Well, suddenly that's a very\noptimistic way to look at it. And you look at what a lot of\nthese technologies have done to improve our lives, and\nI want that to go faster. - So what do you think the\nfuture of programming looks like in the next 10, 20, 30, 50 years? That alums, LLMs and\nwith Mojo, with Modular, like your vision for devices, the hardware to compilers to this, to the different stacks of software. - Yeah. Yeah. Well, so what I want, I mean, coming back to my arch nemesis, right? It's complexity, right? So again, me being the optimist, if we drive down complexity, we can make these tools,\nthese technologies, these cool hardware widgets accessible to way more people, right? And so what I'd love to see is more personalized\nexperiences, more things, the research getting into production instead of being lost\nin (indistinct) right? And so, and like these things that impact people's lives\nby entering products. And so one of the things that\nI'm a little bit concerned about is right now the big\ncompanies are investing huge amounts of money and\nare driving the top line of AI capability forward really quickly. But if it means that you\nhave to have $100 million to train a model or more\n$100 billion, right, well, that's gonna make\nit very concentrated with very few people in the world that can actually do this stuff. I would much rather see lots\nof people across the industry be able to participate\nand use this, right? And you look at this, you know, I mean, a lot of great research has\nbeen done in the health world and looking at like detecting pathologies and doing radiology with AI and\nlike doing all these things. Well, the problem today is that to deploy and build these systems, you have to be an expert in\nradiology and an expert in AI. And if we can break down the barriers so that more people can use AI techniques, and it's more like programming Python, which roughly everybody can\ndo if they want to, right, then I think that we'll get a\nlot more practical application of these techniques and a lot more nicher cool but narrower demands. And I think that's gonna be really cool. - Do you think we'll have\nmore or less programmers in the world than now? - Well, so I think we'll\nhave more programmers, but they may not consider\nthemselves to be programmers. - That'd be a different\nname for it, right? I mean, do you consider\nsomebody that uses, you know, I think that arguably the most popular programming language is Excel. - Yeah. - Right? Yep. And so do they consider\nthemselves to be programmers? Maybe not. I mean, some of them make crazy\nmacros and stuff like that, but what you mentioned Steve Job is, it's the bicycle for the mind that allows you to go faster, right? And so I think that as\nwe look forward, right? What is AI? I look at it as hopefully\na new programming paradigm. It's like object-oriented\nprogramming, right? If you wanna write a cat\ndetector, you don't use for loops. Turns out that's not the\nright tool for the job, right? And so right now, unfortunately, because I mean, it's not unfortunate, but it's just kinda where things are, AI is this weird different\nthing that's not integrated into programming languages\nand normal tool chains and all the technology is really weird and doesn't work, right? And you have to babysit it and every time you switch\nhardware, it's different. It shouldn't be that way. When you change that, when\nyou fix that, suddenly, again, the tools and technologies\ncan be way easier to use. You can start using them\nfor many more things . And so that's what I\nwould be excited about. - What kinda advice could you give to somebody in high school right now or maybe early college who's\ncurious about programming and feeling like the world is\nchanging really quickly here? - Yeah. - Well, what kinda stuff to learn, what kinda stuff to work on? Should they finish college? Should they go work at a company?\nShould they build a thing? What do you think?\n- Yeah. Well, so I mean, one of the things I'd say is\nthat you'll be most successful if you work on something\nyou're excited by. And so don't get the book and\nread the book cover to cover and study and memorize and\nrecite and flashcard and... Go build something.\nLike, go solve a problem. Go build the thing that\nyou wanted to exist. Go build an app. Go build, train a model. Like, go build something\nand actually use it, and set a goal for yourself. And if you do that, then you'll, you know, there's a success, there's\nthe adrenaline rush, there's the achievement. There's the unlock that I\nthink is where, you know, if you keep setting goals and you keep doing things\nand building things, learning by building is really powerful. In terms of career advice, I\nmean, everybody's different. It's very hard to give generalized advice. I'll speak as you know, a compiler nerd. If everybody's going left, sometimes it's pretty cool to go, right? - Yeah. - And so just because\neverybody's doing a thing, it doesn't mean you have to do the same thing and follow the herd. In fact, I think that sometimes\nthe most exciting paths through life lead to\nbeing curious about things that nobody else actually\nfocuses on, right? And turns out that understanding\ndeeply parts of the problem that people want to take for granted makes you extremely\nvaluable and specialized in ways that the herd is not. And so, again, I mean, there's lots of rooms for specialization, lots of rooms for generalists. There's lots of room for\ndifferent kinds and parts of the problem, but I\nthink that it's, you know, just because everything\neverybody's doing one thing doesn't mean you should necessarily do it. - And now the herd is using Python. So if you wanna be a rebel, go check out Mojo and help\nChris and the rest of the world fight the arch nemesis of complexity 'cause simple is beautiful. - There we go. Yeah. - Chris, you're an incredible person. You've been so kind to\nme ever since we met. You've been extremely supportive. I'm forever grateful for that. Thank you for being who you are, for being legit, for being kind, for fighting this really\ninteresting problem of how to make AI accessible\nto a huge number of people, huge number of devices. - Yeah, well, so Lex, you're a pretty special person too, right? And so I think that, you know, one of the funny things about you is that besides being curious\nand pretty damn smart, you're actually willing to push on things and you're, I think that\nyou've got an agenda to like, make the world think, which I\nthink is a pretty good agenda. It's a pretty good one. - Thank you so much for\ntalking to me, Chris. - Yeah. Thanks Lex. - Thanks for listening\nto this conversation with Chris Lattner. To support this podcast, please check out our\nsponsors in the description. And now let me leave you\nsome words from Isaac Zimov. \"I do not fear computers.\nI fear the lack of them.\" Thank you for listening and\nhope to see you next time."
    }
  ],
  "full_text": "- On one access, you have\nmore hardware coming in. On the other hand, you have an explosion of innovation in AI. And so what happened with\nboth TensorFlow and PyTorch is that the explosion of\ninnovation in AI has led to, it's not just about matrix\nimplication and convolution. These things have now, like,\n2,000 different operators. And on the other hand, you have, I don't know how many pieces of hardware out there\nare there, it's a lot. Part of my thesis, part of my belief of where computing goes, if you look out 10 years from now, is it's not gonna get simpler. Physics isn't going back\nto where we came from. It's only gonna get weirder\nfrom here on out, right? And so to me, the exciting part about\nwhat we're building is it's about building\nthat universal platform, which the world can continue\nto get weird 'cause, again, I don't think it's\navoidable, it's physics, but we can help lift people,\nscale, do things with it, and they don't have to rewrite their code every time a new device comes out. And I think that's pretty cool. - The following is a\nconversation with Chris Lattner, his third time on this podcast. As I've said many times before, he's one of the most brilliant engineers in modern computing, having created LLVM Compiler\nInfrastructure project, the Clang compiler, the\nSwift programming language, a lot of key contributions to TensorFlow and TPUs as part of Google. He's served as Vice President of Autopilot Software at Tesla, was a software innovator\nand leader at Apple. And now he co-created a new\nfull stack AI infrastructure for distributed training, inference, and deployment on all kinds\nof hardware called Modular, and a new programming\nlanguage called Mojo. That is a superset of Python, giving you all the usability of Python, but with the performance of C, C++. In many cases, Mojo code has demonstrated over 30,000x speed up over Python. If you love machine\nlearning, if you love Python, you should definitely give Mojo a try. This programming language, this new AI framework and infrastructure and this conversation with\nChris is mind-blowing. I love it. It gets pretty technical at times, so I hope you hang on for the ride. This is the Lex Fridman podcast. To support it, please check out our\nsponsors in the description. And now, dear friends,\nhere's Chris Lattner. It's been, I think two\nyears since we last talked, and then in that time, you somehow went and co-created a new programming language called Mojo. So it's optimized for AI.\nIt's a superset of Python. Let's look at the big picture.\nWhat is the vision for Mojo? - For Mojo? Well, so I mean,\nI think you have to zoom out. So I've been working on a lot of related technologies\nfor many, many years. So I've worked on LLVM and a lot of things and mobile and servers\nand things like this, but the world's changing. And what's happened with AI is we have new GPUs and new machine\nlearning accelerators and other ASICs and things like that, that make AI go real fast. At Google, I worked on TPUs. That's one of the biggest, largest scale deployed\nsystems that exist for AI. And really what you see is, if you look across all of\nthe things that are happening in the industry, there's this\nnew compute platform coming. And it's not just about\nCPUs, or GPUs, or TPUs, or NPUs, or IPUs, or whatever, all the PUs, (chuckles) right? It's about, how do we\nprogram these things, right? And so for software folks like us, right, it doesn't do us any good if there's this amazing\nhardware that we can't use. And one of the things you\nfind out really quick is that having the theoretical capability of programming something and then having the world's\npower and the innovation of all the smart people in the world get unleashed on something\ncan be quite different. And so really where Mojo came from was, starting from a problem of, we need to be able to\ntake machine learning, take the infrastructure underneath it and make it way more\naccessible, way more usable, way more understandable by\nnormal people and researchers and other folks that are not themselves like experts in GPUs and things like this. And then through that\njourney, we realized, \"Hey, we need syntax for this. We need to do a programming language.\" - So one of the main\nfeatures of the language, I say so, fully in jest, is that it allows you to\nhave the file extension to be an emoji or the fire emoji, which is one of the first emojis used as a file extension\nI've ever seen in my life. And then you ask yourself the question, why in the 21st century, we're not using Unicode\nfor file extensions? This, I mean, it's an epic decision. I think, clearly, the most important\ndecision you made the most, but you could also just use\nM-O-J-O as the file extension. - Well, so, okay. So take a step back. I mean, come on, Lex. You think that the world's ready for this? This is a big moment in the world, right? - We're releasing this onto the world. (chuckles)\n- This is innovation. - I mean, it really is kinda brilliant. Emojis are such a big\npart of our daily lives, why isn't it not in programming? - Well, and like you take a step back and look at what file\nextensions are, right, they're basically metadata, right? And so why are we spending\nall the screen space on them and all this stuff? Also, you know, you have them stacked up next to text files and PDF\nfiles and whatever else. Like, if you're gonna do something cool, you want it to stand out, right? And emojis are colorful. They're visual. They're beautiful, right?\n- Yeah. What's been the response so far from... Is there a support on like Windows on operating system-\n- Yeah. - In displaying like File Explorer? - Yeah, yeah. The one problem I've seen is the git doesn't escape it, right? And so it thinks that the\nfire emoji is unprintable. And so it like prints out weird hex things if you use the command line git tool, but everything else, as far\nas I'm aware, works fine. And I have faith that Git can be improved. So I'm not worried.\n- And so GitHub is fine. - GitHub is fine, yep. GitHub is fine. Visual Studio Code, Windows,\nlike all this stuff, totally ready because people\nhave internationalization in their normal-\n- Yeah. - Part of their paths. So let's just like take\nthe next step, right? Somewhere between, \"Oh,\nwow, that makes sense. Cool, I like new things,\" to \"Oh my god, you're killing my baby. Like, what are you talking\nabout? This can never be. Like, I can never handle this.\nHow am I gonna type this? (imitates bees buzzing)\nlike, all these things. And so this is something where I think that the world will get there. We don't have to bet\nthe whole farm on this. I think we can provide both paths, but I think it'll be great. - When can we have emojis as\npart of the code? I wonder. - Yeah. So, I mean, lots\nof languages provide that. So I think that we have\npartial support for that. It's probably not fully done yet, but yeah, you can do that. For example, in Swift,\nyou can do that for sure. So an example we gave at Apple was the dog cow.\n- Yeah. - So that's a classical\nMac heritage thing. And so you use the dog and\nthe cow emoji together, and that could be your\nvariable name, but of course, the internet went and made pile of poop for everything.\n- Yeah. - So, you know, if you wanna name your\nfunction pile of poop, then you can totally go to town and see how that gets through code review. (Lex chuckling) - Okay. So let me just ask\na bunch of random questions. So is Mojo primarily designed for AI or is it a general purpose programming? - Yeah, good question. So it's AI first. And so AI is driving a\nlot of the requirements. And so Modular is building and designing and driving Mojo forward. And it's not because it's\nan interesting project, theoretically, to build. It's because we need it. And so at Modular, we're really tackling the\nAI infrastructure landscape and the big problems in AI and the reasons that is so\ndifficult to use and scale and adopt and deploy and like\nall these big problems in AI. And so we're coming at\nit from that perspective. Now, when you do that, when you start tackling these problems, you realize that the\nsolution to these problems isn't actually an AI-specific solution. And so while we're doing\nthis we're building Mojo to be a fully general\nprogramming language. And that means that you\ncan obviously tackle GPUs, and CPUs and, like, these AI things, but it's also a really\ngreat way to build NumPy and other things like that, or, you know, just if you look at what many\nPython libraries are today, often they're a layer\nof Python for the API, and they end up being C and\nC++ code underneath them. That's very true in AI. That's true in lots of\nother demands as well. And so anytime you see this pattern, that's an opportunity for Mojo\nto help simplify the world and help people have one thing. - So optimize through\nsimplification by having one thing. So you mentioned Modular. Mojo\nis the programming language. Modular is the whole software stack. - So just over a year ago, we started this company called Modular. - [Lex] Yeah. - Okay, what Modular's about is, it's about taking AI and up-leveling it into the next generation, right? And so if you take a step back, what's gone on in the last\nfive, six, seven, eight years is that we've had things like\nTensorFlow and PyTorch and these other systems come in. You've used them. You know this. And what's happened is these things have grown like crazy, and\nthey get tons of users. It's in production deployment scenarios. It's being used to power so many systems. I mean, AI's all around us now. It used to be controversial\nyears ago, but now it's a thing. But the challenge with these\nsystems is that they haven't always been thought out with\ncurrent demands in mind. And so you think about it. Where were LLMs eight\nyears ago? (chuckles) Well, they didn't exist, right? AI has changed so much, and a lot of what people are doing today are very different than when\nthese systems were built. And meanwhile, the hardware side of this has gone into a huge mess. There's tons of new\nchips and accelerators, and every big company's announcing a new chip every day, it feels like. And so between that, you have like moving system on one side, moving system on the other side, and it just turns into this gigantic mess, which makes it very difficult\nfor people to actually use AI, particularly in production\ndeployment scenarios. And so what Modular's doing is we're helping build\nout that software stack to help solve some of those problems so then people can be more productive and get more AI research into production. Now, what Mojo does is it's a really, really, really important piece of that. And so that is, you know, part of that engine and\npart of the technology that allows us to solve these problems. - So Mojo is a programming\nlanguage that allows you to do the higher level programming,\nthe low-level programming, like do all kinds of\nprogramming in that spectrum that gets you closer and\ncloser to the hardware. - So take a step back. So Lex, what do you love about Python? - Oh, boy. Where do I begin? What is love? What do I love about Python? - [Chris] You're a guy who\nknows love. I know this. - Yes. How intuitive it is, how it feels like I'm writing\nnatural language English. - [Chris] Yeah. - How, when I can not just write, but read other people's codes, somehow I can understand it faster. It's more condensed than other languages, like ones I'm really familiar\nwith, like C++ and C, there's a bunch of sexy little features. - [Chris] Yeah. - We'll probably talk about some of them, but list comprehensions\nand stuff like this. - Well, so Py... And don't forget the entire\necosystem of all the packages. - [Lex] Oh, yeah. There's probably huge- - 'Cause there's always something. If you wanna do anything,\nthere's always a package. - Yeah, so it's not just the\necosystem of the packages and the ecosystem of\nthe humans that do it. That's an interesting dynamic because I think-\n- That's good. Yeah. - Something about the usability and the ecosystem makes\nthe thing viral, it grows, and then it's a virtuous cycle, I think. - Well, and there's many\nthings that went into that. Like, so I think that ML\nwas very good for Python. And so I think that TensorFlow\nand PyTorch and these systems embracing Python really\ntook and helped Python grow, but I think that the major\nthing underlying it is that Python's like the\nuniversal connector, right? It really helps bring together\nlots of different systems so you can compose them and\nbuild out larger systems without having to understand how it works. But then, what is the problem\nwith Python? (chuckles) - Well, I guess you\ncould say several things, but probably that it's slow. - I think that's usually what\npeople complain about, right? And so, slow. I mean, other people would complain about tabs and spaces versus\ncurly braces or whatever, but I mean, those people are just wrong 'cause it is-\n- Yeah. - Actually just better to use indentation. - Wow, strong words.\n(Chris laughing) So actually, I just\nwent on a small tangent. Let's actually take that. Let's\ntake all kinds of tangents. - Oh, come on, Lex. You can push me on it. I can take it. - Design, designed. Listen, I've recently\nleft Emacs for VS Code. - [Chris] Okay. - And the kinda hate\nmail I had to receive, because on the way to\ndoing that, I also said, I've considered Vim. - [Chris] Yep. - And chose not to and\nwent with VS Code and just- - You're touching on\ndeep religions, right? - Anyway, tabs is an\ninteresting design decision. And so you've really written a new programming language here. Yes, it is a superset of Python, but you can make a bunch\nof different interesting decisions here.\n- Totally, yeah. - And you chose actually\nto stick with Python in terms of some of the syntax. - Well, so let me explain why, right? So I mean, you can explain\nthis in many rational ways. I think that the annotation is beautiful, but that's not a rational\nexplanation, right, but I can defend it rationally, right? So first of all, Python 1\nhas millions of programmers. It's huge. It's everywhere.\n- Yeah. It owns machine learning, right? And so, factually, it is the thing, right? Second of all, if you look at it, C code, SQL Plus code,\nJava, whatever, Swift, curly brace languages also run through formatting tools and get indented. And so if they're not indented correctly, first of all, will twist\nyour brain around. (chuckles) It can lead to bugs. There's notorious bugs that\nhave happened across time where the annotation\nwas wrong or misleading and it wasn't formatted right, and so it turned into an issue, right? And so what ends up happening in modern large-scale code bases is people run automatic formatters. So now what you end up with is\nindentation and curly braces. Well, if you're gonna have, you know, the notion of grouping, why not have one thing, right, and get rid of all the clutter and have a more beautiful thing, right? Also, you look at many of these\nlanguages, it's like, okay, well, you can have curly braces, or you can omit them if\nthere's one statement, or you just like enter this entire world of complicated design\nspace that, objectively, you don't need if you have\nPython-style indentation, so. - Yeah, I would love to\nactually see statistics on errors made because of indentation. Like, how many errors are\nmade in Python versus in C++ that have to do with basic formatting, all that kinda stuff? I would love to see. - I think it's probably pretty\nminor because once you get, like you use VS Code, I do too. So if you get VS Code set up, it does the annotation for you, generally, right?\n- Yep. - And so you don't, you know, it's actually really nice\nto not have to fight it. And then what you can see\nis the editors telling you how your code will work by indenting it, which I think is pretty cool. - I honestly don't think I've ever... I don't remember having an\nerror in Python because I indented stuff wrong.\n- Yeah. So I mean, I think that there's, again, this is a religious thing. And so I can joke about it and\nI love to kind of, you know, I realize that this is\nsuch a polarizing thing and everybody wants to argue about it. And so I like poking at the\nbear a little bit, right? But frankly, right, come\nback to the first point, Python 1, like, it's huge.\n- Yeah. - It's in AI. It's the right thing. For us, like, we see Mojo\nas being an incredible part of the Python ecosystem. We're not looking to\nbreak Python or change it, or, quote, unquote, \"fix it.\" We love Python for what it is. Our view is that Python\nis just not done yet. And so if you look at, you know, you mentioned Python being slow. Well, there's a couple of different things that go into that, which we\ncan talk about if you want. But one of them is that it just\ndoesn't have those features that you would use to\ndo C-like programming. And so if you say, okay, well, I'm forced out of Python into\nC, for certain use cases, well, then what we're\ndoing is we're saying, \"Okay, well, why is that? Can we just add those features that are missing from\nPython back up to Mojo?\" And then you can have everything\nthat's great about Python, all the things that you're\ntalking about that you love plus not be forced out of it when you do something a little bit more computationally intense,\nor weird, or hardware-y, or whatever it is that you're doing. - Well, a million questions I wanna ask, but high level again-\n- Yeah. - Is it compiled or is it\nan interpreted language? So Python is just-in-time\ncompilation. What's Mojo? - So Mojo, a complicated\nanswer, does all the things. So it's interpreted, it's JIT compiled, and it's statically compiled. (chuckles) And so this is for a variety of reasons. So one of the things that\nmakes Python beautiful is that it's very dynamic. And because it's dynamic, one of the things they\nadded is that it has this powerful metaprogramming feature. And so if you look at something\nlike PyTorch or TensorFlow or, I mean, even a simple use case, like you define a class that\nhas the plus method, right, you can overload the dunder methods, like dunder add, for example, and then the plus method\nworks on your class. And so it has very nice\nand very expressive dynamic metaprogramming features. In Mojo, we want all\nthose features come in. Like, we don't wanna break\nPython, we want it all to work. But the problem is, is you can't run those\nsuper dynamic features on an embedded processor\nor on a GPU, right? Or if you could, you probably don't want to just\nbecause of the performance. And so we entered this\nquestion of saying, okay, how do you get the power of\nthis dynamic metaprogramming into a language that has to be super efficient in specific cases? And so what we did was we said, okay, well, take that interpreter. Python has an interpreter in it, right? Take that interpreter and allow\nit to run at compile time. And so now what you get is you get compiled time metaprogramming. And so this is super\ninteresting, super powerful, because one of the big advantages you get is you get Python-style expressive APIs, you get the ability to\nhave overloaded operators. And if you look at what happens inside of, like PyTorch, for example, with automatic differentiation and eager mode and like all these things, they're using these really dynamic and powerful features at runtime, but we can take those\nfeatures and lift them so that they run at compile time. - 'Cause C++ has\nmetaprogramming with templates. - [Chris] Yep. - But it's really messy. - It's super messy. It was accidentally, I mean, different people have\ndifferent interpretations. My interpretation is that it\nwas made accidentally powerful. It was not designed to be\nTuring-complete, for example, but that was discovered kind\nof along the way, accidentally. And so there have been a number\nof languages in the space. And so they usually have\ntemplates or code instantiation, code-copying features of various sorts. Some more modern languages or some newer languages, let's say, like, you know, they're fairly unknown. Like Zig, for example, says, okay, well, let's take all of\nthose types you can run it, all those things you can do at runtime and allow them to happen at compile time. And so one of the\nproblems with C++, I mean, which is one of the problems with C++ is-\n- There we go. Strong words. We're gonna offend everybody today. - Oh, that's okay. I mean, everybody hates me\nfor a variety of reasons anyways, I'm sure, right? (chuckles) I've written up- - That's the way they show love is to hurt you.\n- I have written enough C++ code to earn a little\nbit of grumpiness with C++, but one of the problems with\nit is that the metaprogramming system templates is just a\ncompletely different universe from the normal runtime programming world. And so if you do\nmetaprogramming and programming, it's just like a different universe, different syntax, different concepts, different stuff going on. And so, again, one of\nour goals with Mojo is to make things really easy\nto use, easy to learn, and so there's a natural stepping stone. And so as you do this, you say, okay, well, I have to do programming at runtime, I have to do programming at compile time. Why are these different things? - How hard is that to pull it off? 'Cause that sounds, to me, as a fan of metaprogramming and C++ even, how hard is it to pull that off? That sounds really, really exciting 'Cause you can do the\nsame style programming at compile time and at runtime. That's really, really exciting.\n- Yep, yep, and so, I mean, in terms of the compiler\nimplementation details, it's hard. I won't be shy about\nthat. It's super hard. It requires, I mean, what Mojo has underneath the covers is a completely new approach to the design of the compiler itself. And so this builds on these technologies like MiR that you mentioned. That also includes other, like caching and other interpreters and JIT compilers and\nother stuff like that- - [Lex] So you have like\nan interpreter inside the- - Within the compiler, yes. - [Lex] Oh, man. - And so it really takes the standard model of\nprogramming languages and kind of twists it and unifies\nit with the runtime model, which I think is really cool.\n- Right. - And to me, the value\nof that is that, again, many of these languages have\nmetaprogramming features. Like, they grow macros\nor something, right? List, right? - Yes. - I know your roots, right?\n(Lex chuckling) You know, and this is a\npowerful thing, right? And so, you know, if you go back to list, one of the most powerful\nthings about it is that it said that the metaprogramming and the programming are the same, right? And so that made it way\nsimpler, way more consistent, way easier to understand, reason about, and it made it more composable. So if you build a library, you can use it both at\nruntime and compile time, which is pretty cool.\n- Yeah. And for machine learning,\nI think metaprogramming, I think we could generally\nsay, is extremely useful. And so you get features,\nI mean, I'll jump around, but the feature of auto-tuning and adaptive compilation\njust blows my mind. - Yeah, well, so, okay. So\nlet's come back to that. - [Lex] All right. - So what is machine learning, like, what, or what is a machine learning model? Like, you take a PyTorch model off the internet, right?\n- Yeah. - It's really interesting to me because what PyTorch and what TensorFlow and all these frameworks\nare kinda pushing compute into is they're pushing into, like, this abstract specification\nof a compute problem, which then gets mapped in a whole bunch of different ways, right? And so this is why it became\na metaprogramming problem, is that you wanna be able to say, cool, I have this neural net. Now, run it with batch\nsize a thousand, right? Do a mapping across batch. Or, okay, I wanna take this problem. Now, run it across a\nthousand CPUs or GPUs, right? And so, like, this problem of,\nlike, describe the compute, and then map it and do things\nand transform it, or, like, actually it's very profound\nand that's one of the things that makes machine learning\nsystems really special. - Maybe can you describe auto-tuning and how do you pull off, I mean, I guess adaptive compilation is what we're talking about\nis metaprogramming. How do you pull off-\n- Yes. - auto-tuning? I mean, is that as\nprofound as I think it is? It just seems like a really, like, you know, we'll mention\nlist comprehensions. To me, from a quick glance\nof Mojo, which by the way, I have to absolutely, like, dive in, as I realize how amazing this is, I absolutely must dive in it, that looks like just an incredible feature for machine learning people. - Yeah. Well, so what is auto-tuning? So take a step back. Auto-tuning\nis a feature in Mojo. So very little of what we're\ndoing is actually research, like many of these ideas have existed in other systems and other places. And so what we're doing\nis we're pulling together good ideas, remixing them,\nand making them into a, hopefully, a beautiful system, right? And so auto-tuning, the\nobservation is that, turns out, hardware systems' algorithms\nare really complicated. Turns out maybe you don't actually want to know how the hardware\nworks, (chuckles) right? A lot of people don't, right? And so there are lots of\nreally smart hardware people, I know a lot of them, where they know everything about, \"Okay, the cache size is this and the\nnumber of registers is that. And if you use this what length of vector, it's gonna be super efficient\nbecause it maps directly onto what it can do\" and,\nlike, all this kinda stuff, or, \"the GPU has SMs and\nit has a warp size of,\" whatever, right, all this stuff that\ngoes into these things, or \"The tile size of a TPU is 128,\" like, these factoids, right? My belief is that most normal people, and I love hardware people, also I'm not trying to offend literally everybody on the internet, but most programmers actually don't wanna know this stuff, right? And so if you come at\nit from perspective of, how do we allow people to\nbuild both more abstracted but also more portable\ncode because, you know, it could be that the vector length changes or the cache size changes, or it could be that the tile\nsize of your matrix changes, or, the number, you know, an A100 versus an H100 versus\na Volta versus a, whatever, GPU have different characteristics, right? A lot of the algorithms that you run are actually the same, but the parameters, these magic numbers you have to fill in end up being really fiddly numbers that an expert has to go figure out. And so what auto-tuning does is says, okay, well, guess what? There's a lot of compute out there, right? So instead of having humans go randomly try all the things\nor do a grid, search, or go search some complicated\nmulti-dimensional space, how about we have\ncomputers do that, right? And so what auto-tuning\ndoes is you can say, Hey, here's my algorithm. If it's a matrix operation\nor something like that, you can say, okay, I'm gonna\ncarve it up into blocks, I'm gonna do those blocks in parallel and I wanna this, with 128\nthings that I'm running on, I wanna cut it this way\nor that way or whatever. And you can say, hey, go see which one's actually empirically better on the system. - And then the result of that\nyou cache for that system. You save it.\n- Yep. And so come back to twisting\nyour compiler brain, right? So not only does the\ncompiler have an interpreter that's used to do metaprogramming, that compiler, that interpreter, that metaprogramming now has\nto actually take your code and go run it on a target\nmachine, (chuckles) see which one it likes the best, and then stitch it in and\nthen keep going, right? - So part of the compilation\nis machine-specific. - Yeah. Well, so I mean, this\nis an optional feature, right? So you don't have to use it\nfor everything, but yeah. So one of the things\nthat we're in the quest of is ultimate performance, right?\n- Yes. - Ultimate performance is important for a couple of reasons, right? So if you're an enterprise, you're looking to save costs and compute and things like this. Ultimate performance translates to, you know, fewer servers. Like, if you care about\nthe environment, hey, better performance leads\nto more efficiency, right? I mean, you could joke\nand say like, you know, Python's bad for the\nenvironment, (chuckles) right? And so if you move to Mojo, it's like, at least 10x\nbetter just outta the box, and then keep going, right?\n- Yeah. - But performance is also\ninteresting 'cause it leads to better products.\n- Yeah. - And so in the space of\nmachine learning, right, if you reduce the latency of\na model so that it runs faster so every time you query the server running the model it takes less time, well, then the product team can go and make the model bigger. Well, that's actually makes it so you have a better\nexperience as a customer. And so a lot of people care about that. - So for auto-tuning, for like tile size, you mentioned 120f for TPU. You would specify like a\nbunch of options to try, just in the code-\n- Yeah. Yep. - Just simple statement, and then you could just-\n- Yep. - Set and forget and know,\ndepending wherever it compiles, it'll actually be the fastest. - And yeah, exactly. And the beauty of this\nis that it helps you in a whole bunch of different ways, right? So if you're building... So often what'll happen is that, you know, you've written a bunch of\nsoftware yourself, right, you wake up one day, you say, \"I have an idea. I'm gonna\ngo code up some code.\" I get to work, I forget about\nit, I move on with life. I come back six months, or a year, or two years, or three years\nlater, you dust it off, and you go use it again\nin a new environment. And maybe your GPU is different. Maybe you're running on a\nserver instead of a laptop, maybe you're, whatever, right? And so the problem now is\nyou say, okay, well, I mean, again, not everybody\ncares about performance, but if you do, you say, okay, well, I wanna take advantage\nof all these new features. I don't wanna break the\nold thing though, right? And so the typical way of handling this kinda stuff before is, you know, if you're talking about C++ templates or you're talking about C with macros, you end up with #ifdefs. You get like all these weird\nthings that get layered in, make the code super complicated, and then how do you test it, right? Becomes this crazy complexity, multidimensional space that\nyou have to worry about. And, you know, that just\ndoesn't scale very well. - Actually, lemme just jump around, before I go to some specific features, like the increase in performance\nhere that we're talking about can be just insane.\n- Yeah. - You write that Mojo can provide a 35,000x speed up over Python. How does it do that? - Yeah, so I can even do\nmore, but we'll get to that. So first of all, when we say that, we're talking about what's called CPython, it's the default Python\nthat everybody uses. When you type Python 3, that's like typically\nthe one you use, right? CPython is an interpreter. And so interpreters, they\nhave an extra layer of, like bike codes and things like this, that they have to go\nread, parse, interpret, and it makes them kind of\nslow from that perspective. And so one of the first things we do is we moved to a compiler. And so just moving to a compiler, getting the interpreter out of the loop is 2 to 5 to 10x speed\nup, depending on the code. So just out of the gate, it's using more modern techniques right? Now, if you do that, one of the things you\ncan do is you can start to look at how CPython\nstarted to lay out data. And so one of the things that CPython did, and this isn't part of the\nPython spec necessarily, but this is just sets of decisions, is that, if you take\nan integer for example, it'll put it in an object 'cause in Python, everything's an object. And so they do the very logical thing of keeping the memory representation of all objects the same. So all objects have a header,\nthey have like payload data. And what this means is that every time you pass around an object, you're passing around\na pointer to the data. Well, this has overhead, right? Turns out that modern computers don't like chasing pointers\nvery much and things like this. It means that you have\nto allocate the data. It means you have to reference count it, which is another way that Python uses to keep track of memory. And so this has a lot of overhead. And so if you say, okay, let's try to get that out\nof the heap, out of a box, out of an indirection\nand into the registers, that's another 10x, more.\n- So it adds up if you're reference counting every single-\n- Absolutely. - every single thing you\ncreate, that adds up. - Yep, and if you look at, you know, people complain about the Python GIL, this is one of the things\nthat hurts parallelism. That's because the\nreference counting, right? And so the GIL and reference counting are very tightly intertwined in Python. It's not the only thing, but\nit's very tightly intertwined. And so then you lean into\nthis and you say, okay, cool. Well, modern computers, they can do more than\none operation at a time. And so they have vectors.\nWhat is a vector? Well, a vector allows you to, instead of taking one piece of data, doing an add or multiply and\nthen pick up the next one, you can now do a 4, 8, or\n16 or 32 at a time, right? Well, Python doesn't expose\nthat because of reasons. And so now you can say, okay,\nwell, you can adopt that. Now you have threads. Now you have like additional things, like you can control memory hierarchy. And so what Mojo allows\nyou to do is it allows you to start taking advantage\nof all these powerful things that have been built into\nthe hardware over time. The library gives very nice features. So you can say, just parallelize this. Do this in parallel, right? So it's very, very powerful\nweapons against slowness, which is why people have\nbeen, I think having fun, like just taking code and making go fast because it's just kind\nof an adrenaline rush to see like how fast you can get things. - Before I talk about some\nof the interesting stuff with parallelization and all that, let's first talk about, like, the basics. We talked the indentation, right? So this thing looks like Python. It's sexy and beautiful\nlike Python as I mentioned. - [Chris] Yep. - Is it a typed language?\nSo what's the role of types? - Yeah, good question.\nSo Python has types. It has strings, it has integers, it has dictionaries and\nlike all that stuff, but they all live at runtime, right? And so because all those types\nlive at runtime in Python, you never or you don't have\nto spell them. (chuckles) Python also has like\nthis whole typing thing going on now and a lot of people use it. - [Lex] Yeah. - I'm not talking about that. That's kind of a different thing. We can go back to that if\nyou want, but typically the, you know, you just say, I have a def and my def\ntakes two parameters. I'm gonna call them A and B and I don't have to write or type okay? So that is great, but what that does is that forces what's called a consistent representation. So these things have to be a pointer to an object with the object header and they all have to look the same. And then when you dispatch a method, you go through all the\nsame different paths no matter what the receiver,\nwhatever that type is. So what Mojo does is it allows you to have more than one kind of type. And so what it does is allows\nyou to say, okay, cool. I have an object and objects\nbehave like Python does. And so it's fully dynamic\nand that's all great. And for many things, classes, like, that's all very powerful\nand very important. But if you wanna say, hey, it's\nan integer and it's 32 bits, or it's 64 bits or whatever it is, or it's a floating point\nvalue and it's 64 bits, well, then the compiler can take that, and it can use that to do\nway better optimization. And it turns out, again, getting rid of the\nindirections, that's huge. Means you can get better code completion 'cause compiler knows what the type is and so it knows what\noperations work on it. And so that's actually pretty huge. And so what Mojo does is allows you to progressively adopt\ntypes into your program. And so you can start, again,\nit's compatible with Python, and so then you can add\nhowever many types you want, wherever you want them. And if you don't wanna deal with it, you don't have to deal with it, right? And so one of, you know, our\nopinions on this, (chuckles) it's that it's not that\ntypes are the right thing or the wrong thing, it's\nthat they're a useful thing. - So it's kind of optional,\nit's not strict typing, like, you don't have to specify type. - [Chris] Exactly. - Okay, so it's starting from the thing that Python's kinda\nreaching towards right now with trying to inject types into it, what it's doing.\n- Yeah, with a very different approach, but yes, yeah.\n- So what's the different approach? I'm actually one of the people (sighs) that have not been using\ntypes very much in Python. So I haven't-\n- That's okay. Why did you sigh? - It just, well, because\nI know the importance. It's like adults use strict typing. And so I refuse to grow up in that sense. It's a kind of rebellion, but I just know that it probably reduces the amount of errors, even just for, forget about performance improvements, it probably reduces errors\nof when you do strict typing. - Yeah, so I mean, I think it's interesting\nif you look at that, right? And the reason I'm giving\nyou a hard time then is that-\n- Yes. - there's this cultural\nnorm, this pressure, this, like, there has to be\na right way to do things. Like, you know-\n- Yes. - grownups only do it one way. And if you don't do that-\n- Yes. - you should feel bad, right?\n- Yes. - Like, some people feel like\nPython's a guilty pleasure or something, and that's like, when it gets serious, I need\nto go rewrite it, right? Well, I mean, cool.\n- Exactly. - I understand history and I understand kinda\nwhere this comes from, but I don't think it has\nto be a guilty pleasure, (chuckles) right?\n- Yeah. - So if you look at that, you say, why do you have to rewrite it? Well, you have to rewrite it to deploy. Well, why do you wanna deploy? Well, you care about performance, or you care about predictability,\nor you want, you know, a tiny thing on the server\nthat has no dependencies, or, you know, you have objectives\nthat you're trying to attain. So what if Python can\nachieve those objectives? So if you want types, well, maybe you want types\nbecause you wanna make sure you're passing the right thing. Sure, you can add a type. If you don't care, you're\nprototyping some stuff, you're hacking some things out, you're, like, pulling some\nrandom code off the internet, it should just work, (chuckles) right? And you shouldn't be, like, pressured. You shouldn't feel bad\nabout doing the right thing or the thing that feels good. Now, if you're in a team, right, you're working at some\nmassive internet company and you have 400 million\nlines of Python code, well, they may have a house\nrule that you use types, right?\n- Yeah. - Because it makes it easier for different humans to talk to each other and understand what's going\non and bugs at scale, right? And so there are lots of good reasons why you might wanna use types, but that doesn't mean that everybody should use 'em all the time, right? So what Mojo does is it says, cool. Well, allow people to use\ntypes and if you use types, you get nice things out of it, right? You get better performance\nand things like this, right? But Mojo is a full, compatible\nsuperset of Python, right? And so that means it has to\nwork without types. (chuckles) It has to support all the dynamic things. It has to support all the packages. It has to support for comprehension, list comprehensions and\nthings like this, right? And so that starting point\nI think is really important. And I think that, again, you can look at why I\ncare so much about this. And there's many\ndifferent aspects of that, one of which is the world went\nthrough a very challenging migration from Python\n2 to Python 3, right? - [Lex] Yes. - This migration took many years and it was very painful for many teams, right?\n- Yeah. - And there's of a lot of\nthings that went on in that. I'm not an expert in all the details and I honestly don't wanna be. I don't want the world to\nhave to go through that, (chuckles) right?\n- Yeah. - And, you know, people can ignore Mojo. And if it's not their thing, that's cool. But if they wanna use Mojo, I don't want them to have\nto rewrite all their code. - Yeah, I mean, this, okay, the superset part is just, I mean, there's so much brilliant stuff here. That definitely is incredible. We'll talk about that.\n- Yeah. - First of all, how's the typing implemented\ndifferently in Python versus Mojo?\n- Yeah. - So this heterogeneous flexibility you said is definitely implemented. - Yeah, so I'm not a full expert (chuckles) in the whole\nbackstory on types in Python. So I'll give you that. I can\ngive you my understanding. My understanding is, basically,\nlike many dynamic languages, the ecosystem went through a phase where people went from writing scripts to writing large scale,\nhuge code bases in Python. And at scale, kinda helps have types.\n- Yeah. - People wanna be able to\nreason about interfaces, do you expect string, or an int, or, like, these basic things, right? And so what the Python\ncommunity started doing is it started saying, okay,\nlet's have tools on the side, checker tools, right, that go and, like, enforce a variance, check for\nbugs, try to identify things. These are called static\nanalysis tools generally. And so these tools run over your code and try to look for bugs. What ended up happening is\nthere's so many of these things, so many different weird patterns\nand different approaches on specifying the types and\ndifferent things going on, that the Python community\nrealized and recognized, \"Hey, hey, hey, there's\nthe thing here.\" (chuckles) And so what they started\nto do is they started to standardize the syntax\nfor adding types to Python. Now, one of the challenges\nthat they had is that they're coming from\nkinda this fragmented world where there's lots of different tools, they have different\ntrade-offs and interpretations and the types mean different things. And so if you look at types in Python, according to the Python spec,\nthe types are ignored, right? So according to the Python spec, you can write pretty\nmuch anything (chuckles) in a type position, okay? Technically, you can write\nany expression, okay? Now, that's beautiful\nbecause you can extend it. You can do cool things, you can\nwrite, build your own tools, you can build your own house, linter or something like that, right? But it's also a problem because\nany existing Python program may be using different tools and they have different interpretations. And so if you adopt somebody's\npackage into your ecosystem, try to run the tool you prefer, it may throw out tons of\nweird errors and warnings and problems just\nbecause it's incompatible with how these things work. Also because they're added late and they're not checked\nby the Python interpret, it's always kinda more of a\nhint that it is a requirement. Also, the CPython implementation can't use 'em for performance. And so it's really- - I mean, that's a big one, right? So you can't utilize for the compilation, for the just-in-time compilation, okay.\n- Yep, yep, exactly. And this all comes back to\nthe design principle of, they're kinda hints, they're kind of, the definition's a little bit murky. It's unclear exactly the\ninterpretation in a bunch of cases. And so because of that,\nyou can't actually, even if you want to, it's really difficult to use them to say, like, it is going to be an int, and if it's not, it's a problem, right? A lot of code would break\nif you did that, so. So in Mojo, right, so you can still use those kind of type annotations, it's fine. But in Mojo, if you declare\na type and you use it, then it means it is going to be that type. And the compiler helps you check that, and enforce it and it's safe and it's not a, like,\nbest-effort hint kind of a thing. - So if you try to shove a\nstring type thing into a integer- - [Chris] You get an\nerror from the compiler. - From the compiler\ncompile time. Nice, okay. What kinda basic types are there? - Yeah. So Mojo is pretty hardcore in\nterms of what it tries to do in the language, which is the\nphilosophy there is that we, again, if you look at Python, right, Python's a beautiful language because it's so extensible, right? And so all of the\ndifferent things in Python, like for loops and plus\nand like all these things can be accessed through these\nunderbar armbar methods, okay? So you have to say, okay, if I make something that is super fast, I can go all the way down to the metal. Why do I need to have integers built into the language, right? And so what Mojo does is it says, okay, well, we can have this notion of structs. So you have classes in Python. Now you can have structs. Classes are dynamic, structs are static. Cool. We can get high performance. We can write C++ kind of code\nwith structs if you want. These things mix and work\nbeautifully together, but what that means is that you can go and implement strings and\nints and floats and arrays and all that kinda stuff\nin the language, right? And so that's really\ncool because, you know, to me as a idealizing compiler\nlanguage type of person, what I wanna do is I wanna get magic out of the compiler and\nput in the libraries. Because if somebody can, you know, if we can build an\ninteger that's beautiful and it has an amazing API\nand it does all the things you'd expect an integer\nto do, we don't like it, maybe you want a big integer, maybe you want, like, sideways\ninteger, I don't know, like what all the space of integers are, then you can do that, and it's\nnot a second class citizen. And so if you look at\ncertain other languages, like C++, one I also love and use a lot, int is hardcoded in the language, but complex is not. And so isn't it kinda\nweird that, you know, you have this STD complex\nclass, but you have int, and complex tries to look\nlike a natural numeric type and things like this. But integers and floating\npoint have these, like, special promotion rules\nand other things like that, that are magic and they're\nhacked into the compiler. And because of that, you can't actually make something that works like the built-in types. - Is there something provided\nas a standard because, you know, because it's AI first, you know, numerical types are so important here. So is there something, like a nice standard\nimplementation of indigent flows? - Yeah, so we're still\nbuilding all that stuff out. So we provide integers and\nfloats and all that kinda stuff. We also provide like buffers and tensors and things like that you'd\nexpect in an ML context. Honestly, we need to keep\ndesigning and redesigning and working with the community to build that out and make that better. That's not our strength right now. Give us six months or a year and I think it'll be way better, but the power of putting\nin the library means that we can have teams of experts that aren't compiler engineers\nthat can help us design and refine and drive this forward. - So one of the exciting\nthings we should mention here is that this is new and fresh. This cake is unbaked. It's almost baked. You can tell it's delicious, but it's not fully ready to be consumed. - Yep. That's very fair. It is very useful, but it's very useful if you're a super low-level programmer right now. And what we're doing is we're\nworking our way up the stack. And so the way I would look at Mojo today in May and 2023 is that it's like a 0.1. So I think that, you\nknow, a year from now, it's gonna be way more interesting\nto a variety of people. But what we're doing is we\ndecide to release it early so that people can get access\nto it and play with it. We can build it with the community. We have a big roadmap, fully published, being transparent about this and a lot of people are\ninvolved in this stuff. And so what we're doing\nis we're really optimizing for building this thing the right way. And building it the right\nway is kind of interesting, working with the community, because everybody wants it yesterday. And so sometimes it's kind of, you know, there's some dynamics there, but I think-\n- Yeah. - it's the right thing. - So there's a Discord also. So the dynamics is pretty interesting. - [Chris] Yeah. - Sometimes the community probably can be very chaotic and\nintroduce a lot of stress. Guido famously quit over the\nstress of the Walrus operator. I mean, it's, you know-\n- Yeah, yeah. - It broke... - [Chris] Straw that\nbroke the camel's back. - Exactly. And so, like, it could be\nvery stressful to develop, but can you just add a\ntangent upon a tangent? Is it stressful to work through the design\nof various features here, given that the community\nis recently involved? - Well, so I've been\ndoing open development and community stuff for\ndecades now. (chuckles) Somehow this has happened to me. So I've learned some tricks, but the thing that always gets me is I wanna make people happy, right? And so maybe not all people\nall happy all the time, but generally,\n- Yeah. - I want people to be happy, right? And so the challenge is that again, we're tapping into some long, some deep seated long\ntensions and pressures both in the Python world,\nbut also in the AI world, in the hardware world\nand things like this. And so people just want\nus to move faster, right? And so again, our decision\nwas, \"Let's release this early. Let's get people used to it or access to it and play with it. And like, let's build in the open,\" which we could have, you know, had the language monk sitting in the cloister up on the hilltop, like beavering away\ntrying to build something. But in my experience, you get something that's way better if you work with the community, right? And so, yes, it can be frustrating, can be challenging for\nlots of people involved. And, you know, if you, I mean,\nyou mentioned our Discord. We have over 10,000 people on the Discord, 11,000 people or something. Keep in mind we released\nMojo like two weeks ago. (chuckles)\nYeah. So-\n- It's very active. - So it's very cool, but what that means is that, you know, 10, 11,000 people all will want\nsomething different, right? And so what we've done\nis we've tried to say, Okay, cool. Here's our roadmap. And the roadmap isn't\ncompletely arbitrary. It's based on here's the logical order in which to build these features or add these capabilities\nand things like that. And what we've done is we've spun really fast on like bug fixes. And so we actually have very\nfew bugs, which is cool, I mean, actually for a\nproject in this state, but then what we're doing is we're dropping in features\nvery deliberately. - I mean, this is fun to watch 'cause you got the two\ngigantic communities of, like, hardware,\nlike systems engineers, and then you have the machine\nlearning Python people that are like higher level. - [Chris] Yeah. - And it's just two, like, army, like- - They've both, they've been at war, yeah. (Lex chuckling)\nThey've been at war, right? And so here's- - [Lex] It's a Tolkien\nnovel or something. Okay. - Well, so here's a test. And again, like, it's super funny for something that's only\nbeen out for two weeks, right? People are so impatient, right? But, okay, cool, let's\nfast forward a year. Like, in a year's time, Mojo will be actually quite amazing and solve tons of\nproblems and be very good. People still have these problems, right? And so you look at this and you say, and the way I look at this\nat least is to say, okay, well, we're solving big,\nlong-standing problems. To me, again, working on\nmany different problems, I wanna make sure we do it right, right? There's like a responsibility you feel because if you mess it\nup, (chuckles) right, there's very few opportunities\nto do projects like this and have them really\nhave impact on the world. If we do it right, then maybe we can take\nthose feuding armies and actually heal some of those wounds, right?\n- Yeah. - This feels like a speech\nby George Washington or Abraham Lincoln or something. - And you look at this and it's like, okay, well, how different are we? - [Lex] Yeah. - We all want beautiful things. We all want something that's nice. We all wanna be able to work together. We all want our stuff to be used, right? And so if we can help heal that, now I'm not optimistic that all people will use Mojo and they'll stop using C++, like, that's not my\ngoal, (chuckles) right, but if we can heal some of that, I think that'd be pretty cool. That'd be nice.\n- Yeah, and we start by putting the people who like braces into the\nGulag, no. (chuckles) - So there are proposals\nfor adding braces to Mojo and we just we tell them no.\n- Oh, interesting. - Oh, okay, (laughs)\n(Chris laughing) politely, yeah, anyway. So there's a lot of amazing\nfeatures on the roadmap and those already\nimplemented, it'd be awesome if I could just ask you a few things. So-\n- Yeah, go for it. - So the other performance improvement comes from immutability. So what's this var and this\nlet thing that we got going on? And what's immutability?\n- Well, so... - Yeah, so one of the\nthings that is useful, and it's not always\nrequired, but it's useful, is knowing whether something can change out from underneath you, right? And so in Python, you have a\npointer to an array, right? And so you pass that pointer\nto an array around to things. If you pass into a function, they may take that and scroll away in some other data structure. And so you get your array\nback and you go to use it. And now somebody else is like\nputting stuff in your array. How do you reason about that?\n- Yeah. - It gets to be very complicated and leads to lots of bugs, right? And so one of the things\nthat, you know, again, this is not something Mojo forces on you, but something that Mojo enables is this thing called value semantics. And what value semantics do\nis they take collections, like array, like dictionaries, also tensors and strings and\nthings like this that are much higher level and make\nthem behave like proper values. And so it makes it look like, if you pass these things around, you get a logical copy of all the data. And so if I pass you an\narray, it's your array. You can go do what you want to it, you're not gonna hurt my array. Now that is an interesting and very powerful design principle. It defines away a ton of bugs. You have to be careful to\nimplement it in an efficient way. - Yeah, is there a performance\nhit that's significant? - Generally not if you\nimplement it the right way, but it requires a lot of very low-level\ngetting-the-language-right bits. - I assume that'd be\na huge performance hit 'cause the benefit is really nice 'cause you don't get into these- - Absolutely. Well, the\ntrick is you can't do copies. So you have to provide the behavior of copying without doing the copy. - [Lex] Yeah. How do you do that?\n(Chris laughing) How do you do that?\n- It's not magic. It's just-\n- Okay. - It's actually pretty cool. Well, so first, before we\ntalk about how that works, let's talk about how it\nworks in Python, right? So in Python you define a person class, or maybe a person class is a bad idea. You define a database class, right? And a database class\nhas an array of records, something like that, right? And so the problem is, is that if you pass in a record or a class instance into the database, it'll take a hold of that object and then it assumes it has it. And if you're passing an object in, you have to know that that\ndatabase is gonna take it, and therefore you shouldn't change it after you put it in the database, right? This is-\n- You just kinda have to know that. - You just have to kinda know that, right? And so you roll out version\none of the database. You just kinda have to know that. Of course, Lex uses his\nown database, right? - [Lex] Yeah. - Right, 'cause you built it, you understand how this works, right? Somebody else joins the\nteam, they don't know this, right?\n- Yes. - And so now they suddenly get bugs, you're having to maintain the database, you shake your fist, you argue. The 10th time this\nhappens, you're like, okay, we have to do something different, right? And so what you do is you\ngo change your Python code and you change your database class to copy the record every time you add it. And so what ends up\nhappening is you say, okay, I will do what's called a defensive copy inside the database. And then that way if\nsomebody passes something in, I will have my own copy of it\nand they can go do whatever and they're not gonna break\nmy thing, (chuckles) okay? This is usually the two design patterns. If you look in PyTorch, for example, this is cloning a tensor. Like, there's a specific thing and you have to know where to call it. And if you don't call\nit in the right place, you get these bugs and this\nis state-of-the-art, right? So a different approach, so\nit's used in many languages, so I've worked with it in\nSwift, is you say, okay, well, let's provide value semantics. And so we wanna provide\nthe view that you get a logically independent copy,\nbut we wanna do that lazily. And so what what we do is we say, okay, if you pass something into a function, it doesn't actually make a copy. What it actually does is it just increments a reference to it. And if you pass it around,\nyou stick in your database, it can go into the database, you own it. And then you come back outta the stack, nobody's copied anything, you\ncome back outta the stack, and then the caller let's go of it. Well, then you've just handed\nit off to the database, you've transferred it and\nthere's no copies made. Now, on the other hand, if, you know, your coworker goes and hands you a record and you pass it in, you\nstick it in the database, and then you go to town\nand you start modifying it, what happens is you get\na copy lazily on demand. And so what this does, this gives you copies\nonly when you need them. So it defines the way the bugs, but it also generally reduces the number of copies in practice. And so it's-\n- But the implementation details are tricky here, I assume.\n- Yes, yes. - Something with reference counting, but to make it performant across a number of\ndifferent kinds of objects? - Yeah. Well, so you\nneed a couple of things. So this concept has existed\nin many different worlds. And so it's, again, it's not\nnovel research at all, right? The magic is getting the design right so that you can do this in\na reasonable way, right? And so there's a number of\ncomponents that go into this. One is when you're passing around, so we're talking about\nPython and reference counting and the expense of doing that. When you're passing values around, you don't wanna do\nextra reference counting for no good reason. And so you have to make\nsure that you're efficient and you transfer ownership\ninstead of duplicating references and things like that, which\nis a very low-level problem. You also have to adopt this, and you have to build\nthese data structures. And so if you say, you know, Mojo has to be compatible with Python, so of course the default list\nis a reference semantic list that works the way you'd expect in Python, but then you have to design\na value semantic list. And so you just have to implement that, and then you implement the logic within. And so the role of the language here is to provide all the low-level hooks that allow the author of the type to be able to get and\nexpress this behavior without forcing it into all cases or hard coding this into\nthe language itself. - But there's ownership? So you're constantly transferring, you're tracking who owns the thing. - Yes. And so there's a whole\nsystem called ownership. And so this is related to work\ndone in the Rust community. Also, the Swift community\nhas done a bunch of work and there's a bunch of\ndifferent other languages that have all kind of... C++ actually has copy constructors and destructors and things like that. And so, and I mean, C++ has everything. So it has move constructors and has like this whole world of things. And so this is a body of work\nthat's kind of been developing for many, many years now. And so Mojo takes some of the best ideas out of all these systems and\nthen remixes in a nice way so that you get the power of something like the Rust programming language, but you don't have to deal\nwith it when you don't want to, which is a major thing in\nterms of teaching and learning and being able to use\nand scale these systems. - How does that play with\nargument conventions? What are they? Why are they important? How does the value semantics, how does the transfer ownership work with the arguments when they're passing definitions?\n- Yeah. So if you go deep into\nsystems programming land, so this isn't, again, this is\nnot something for everybody, but if you go deep into\nsystems programming land, what you encounter is you encounter these types that get weird. (chuckles) So if you're used to Python,\nyou think about everything. I can just copy it around. I can go change it and mutate it and do these things and it's all cool. If you get into systems programming land, you get into these things, like, I have an atomic number,\nor I have a mutex, or I have a uniquely\nowned database handle, things like this, right? So these types, you\ncan't necessarily copy. Sometimes you can't\nnecessarily even move them to a different address. And so what Mojo allows\nyou to do is it allows you to express, hey, I don't wanna\nget a copy of this thing. I wanna actually just\nget a reference to it. And by doing that, what you\ncan say is, you can say, okay, if I'm defining something\nweird like a, you know, atomic number or something,\nit's like, it has to be... So an atomic number is an area in memory that multiple threads can access at a time without synchronous, without locks, right? And so, like the definition\nof atomic numbers, multiple different things\nhave to be poking at that, therefore they have to\nagree on where it is, (chuckles) right? So you can't just like move\nit out from underneath one because it kinda breaks what it means. And so that's an example of a type that you can't copy, you can't move it. Like, once you create, it has\nto be where it was, right? Now, if you look at many other examples, like a database handle, right, so, okay, well, what happens? How do you copy a database handle? Do you copy the whole database? That's not something you\nnecessarily wanna do. There's a lot of types like\nthat where you wanna be able to say that they are uniquely owned. So there's always one of this thing, or if I create a thing, I don't copy it. And so what Mojo allows you\nto do is it allows you to say, Hey, I wanna pass around\nin reference to this thing without copying it, and so\nit has borrowed conventions. So you can say, you can use it, but you don't get to change it. You can pass it by mutable reference. And so if you do that, then\nyou get a reference to it, but you can change it. And so it manages all that kinda stuff. - So it's just a really nice\nimplementation of, like, C++ has-\n- Yeah. - you know, different kinds of pointers.\n- Reference, yeah, has pointers. - Smart, smart, different kinds of\nimplementations of smart pointers that you can-\n- Yeah. - explicitly define, this allows you, but you're saying that's\nmore like the weird case versus the common case? - Well, it depends on where, I mean, I don't think I'm a normal person, so.\n- Yes. - I mean, I'm not one to\ncall other people weird. - [Lex] Yeah.\n(Chris chuckling) But, you know, if you talk to a typical\nPython programmer, you're typically not\nthinking about this, right? This is a lower level of abstraction. Now, certainly if you\ntalk to a C++ programmer, certainly if you talk to\na Rust programmer, again, they're not weird, they're delightful. Like, these are all good people, right? Those folks will think\nabout all the time, right? And so I look at this as, there's a spectrum between\nvery deep, low-level systems, I'm gonna go poke the bits and care about how they're\nlaid out in memory, all the way up to application and scripting and other things like this. And so it's not that\nanybody's right or wrong, it's about how do we build\none system that scales? - By the way, the idea of an atomic\nnumber has been something that always brought me deep happiness, because the flip side of that, the idea that threads\ncan just modify stuff asynchronously, just the whole idea of\nconcurrent programming is a source of infinite distrust for me. - Well, so this is where you\njump into, you know, again, you zoom out and get out of\nprogram languages or compilers and you just look at what\nthe industry has done, my mind is constantly\nblown by this, right? And you look at what,\nyou know, Moore's law, Moore's Law is this idea that, like computers, for a long time, single thread performance just got faster and faster and faster and faster for free. But then physics and\nother things intervened, and power consumption, like\nother things started to matter. And so what ended up happening is we went from single core\ncomputers to multi-core, then we went to accelerators, right? And this trend towards specialization of hardware is only gonna continue. And so for years, us programming language nerds and compiler people\nhave been saying, okay, well, how do we tackle multi-core, right? For a while it was like,\n\"Multi-core is the future. We have to get on top of this thing.\" And then it was multi-core is the default. \"What are we doing with this thing?\" And then it's like, there's chips with\nhundreds of cores in them. (chuckles) What will happen, right?\n- Yeah. - And so I'm super inspired\nby the fact that, you know, in the face of this, you know, those machine learning people invented this idea of a tensor,\nright? And what's a tensor? A tensor is like an arithmetic\nand algebraic concept. It's like an abstraction around a gigantic\nparallelizable dataset, right? And because of that and because of things like TensorFlow and PyTorch,\nwe're able to say, okay, we'll express the math of the system. This enables you to do\nautomatic differentiations, enables you to do like\nall these cool things. And it's an abstracted representation. Well, because you have that\nabstract representation, you can now map it onto\nthese parallel machines without having to control,\nokay, put that bite here, put that bite there, put that bite there. And this has enabled an\nexplosion in terms of AI, compute, accelerators, like all the stuff. And so that's super, super exciting. - What about the deployment and the execution across\nmultiple machines? - [Chris] Yeah. - So you write that the\nModular compute platform dynamically partitions models\nwith billions of parameters and distributes their execution\nacross multiple machines, enabling unparalleled efficiency. By the way, the use of\nunparalleled in that sentence... Anyway.\n(Chris chuckling) Enabling unparalleled efficiency, scale, and the reliability for\nthe largest workloads. So how do you do this abstraction of distributed\nDeployment of large models? - Yeah, so one of the\nreally interesting tensions, so there's a whole bunch of\nstuff that goes into that. I'll pick a random walk through it. If you go back and replay the history of machine learning, right, I mean, the brief, most recent\nhistory of machine learning, 'cause this is, as you know, very deep. - [Lex] Yeah. - I knew Lex when he had an AI podcast. - [Lex] Yes.\n(Chris chuckling) - [Chris] Right? - Yeah, (chuckles) yeah. - So if you look at just\nTensorFlow and PyTorch, which is pretty recent history\nin the big picture, right, but TensorFlow is all about graphs. PyTorch, I think pretty\nunarguably ended up winning. And why did it win? Mostly\nbecause the usability, right? And the usability of\nPyTorch is I think, huge. And I think, again, that's a huge testament to\nthe power of taking abstract, theoretical technical concepts and bringing it to the masses, right? Now the challenge with what the TensorFlow versus the PyTorch design points was that TensorFlow's kinda\ndifficult to use for researchers, but it was actually pretty\ngood for deployment. PyTorch is really good for researchers. It kind of not super great\nfor deployment, right? And so I think that we as an\nindustry have been struggling. And if you look at what deploying\na machine learning model today means is that you'll\nhave researchers who are, I mean, wicked smart, of course, but they're wicked smart\nat model architecture and data and calculus\nand (chuckles) like all, like, they're wicked\nsmart in various domains. They don't wanna know anything\nabout the hardware deployment or C++ or things like this, right? And so what's happened is you get people who train the model, they throw it over the fence, and then you have people\nthat try to deploy the model. Well, every time you have a Team A does x, they throw it over the fence, Team B does y, like you have a problem, because of course it never\nworks the first time. And so you throw over the\nfence, they figure out, okay, it's too slow, won't fit,\ndoesn't use the right operator, the tool crashes, whatever the problem is, then they have to throw\nit back over the fence. And every time you throw\na thing over a fence, it takes three weeks of project managers and meetings and things like this. And so what we've seen\ntoday is that getting models in production can take weeks or months. Like, it's not atypical, right? I talk to lots of people\nand you talk about, like VP of software at\nsome internet company trying to deploy a\nmodel, and they're like, why do I need a team of\n45 people? (chuckles) Like, it's so easy trying to model. Why can't I deploy it, right? And if you dig into this,\nevery layer is problematic. So if you look at the language piece, I mean, this is tip of the iceberg. It's a very exciting tip\nof the iceberg for folks, but you've got Python on one side and C++ on the other side. Python doesn't really deploy. I mean, can theoretically,\ntechnically in some cases, but often a lot of production teams will wanna get things out of Python because they get better performance and control and whatever else. So Mojo can help with that. If you look at serving, so you\ntalk about gigantic models, well, a gigantic model won't\nfit on one machine, right? And so now you have this\nmodel, it's written Python, it has to be rewritten in C++. Now it also has to be carved up so that half of it runs on one machine, half of it runs on another machine, or maybe it runs on 10 machines. Well, so now, suddenly, the\ncomplexity is exploding, right? And the reason for this is that if you look into TensorFlow\nor PyTorch, these systems, they weren't really designed\nfor this world, right? They were designed for, you know, back in the day when we were\nstarting and doing things where it was a different,\nmuch simpler world, like you wanted to run ResNet-50 or some ancient model\narchitecture like this. It was a completely different world than- - Trained on one GPU. - [Chris] Exactly. AlexNet.\n- Doing it on one GPU. (chuckles)\n- Yeah, AlexNet, right, the major breakthrough, and the world has changed, right? And so now the challenge is, is that TensorFlow,\nPyTorch, these systems, they weren't actually designed for LLMs, like, that was not a thing. And so where TensorFlow\nactually has amazing power in terms of scale and\ndeployment and things like that, and I think Google is, I\nmean, maybe not unmatched, but they're, like, incredible, in terms of their capabilities\nand gigantic scale, many researchers using PyTorch, right? And so PyTorch doesn't have\nthose same capabilities. And so what Modular can do\nis it can help with that. Now, if you take a step\nback and you say like, what is Modular doing, right? So Modular has like a bitter enemy that we're fighting\nagainst in the industry. And it's one of these things\nwhere everybody knows it, but nobody is usually\nwilling to talk about it. - The bitter enemy. - The bitter thing that we have to destroy that we're all struggling\nwith and it's like all around, it's like fish can't see\nwater, it's complexity. - Sure, yes. It's complexity. - [Chris] Right? - That was very philosophical, (Chris chuckling)\nVery well said. - [Chris] And so if you look at it, yes, it is on the hardware side. - Yes. - All these accelerators, all these software stack\nthat go with the accelerator, all these, like, there's\nmassive complexity over there. You look at what's happening\non the modeling side, massive amount of complexity. Like, things are changing all the time. People are inventing. Turns out the research is\nnot done, (chuckles) right? And so people wanna be able to move fast. Transformers are amazing, but there's a ton of diversity\neven within transformers, and what's the next transformer, right? And you look into serving. Also,\nhuge amounts of complexity. It turns out that all the\ncloud providers, right, have all their very weird\nbut very cool hardware for networking and all this kinda stuff. And it's all very complicated.\nPeople aren't using that. You look at classical serving, right, there's this whole world of\npeople who know how to write high-performance servers\nwith zero-copy networking and, like, all this\nfancy asynchronous I/O, and, like, all these fancy\nthings in the serving community, very little that has pervaded into the machine learning world, right? And why is that? Well, it's because, again, these systems have been\nbuilt up over many years. They haven't been rethought, there hasn't been a first\nprinciples approach to this. And so what Modular's doing\nis we're saying, \"Okay, we've built many of these things, right?\" So I've worked on TensorFlow and TPUs and things like that. Other folks on our team have,\nlike, worked on PyTorch Core. We've worked on ONNX one time. We've worked on many\nof these other systems. And so built systems like\nthe Apple accelerators and all that kinda stuff, like\nour team is quite amazing. And so one of the things\nthat roughly everybody at Modular's grumpy about\nis that when you're working on one of these projects,\nyou have a first order goal: Get the hardware to work. Get the system to enable one more model. Get this product out the door. Enable the specific workload, or make it solve this problem\nfor this product team, right? And nobody's been given a chance to actually do that step back. And so we, as an industry, we\ndidn't take two steps forward. We took like 18 steps forward in terms of all this\nreally cool technology across compilers and systems and runtimes and heterogeneous computing,\nlike all this kinda stuff. And like, all this technology\nhas been, you know, I wouldn't say beautifully designed, but it's been proven\nin different quadrants. Like, you know, you look\nat Google with TPUs, massive, huge exif flops of\ncompute strapped together into machines that researchers are programming in Python in a notebook. That's huge. That's amazing.\n- That's amazing. That's incredible.\n- Right, it's incredible. And so you look at the\ntechnology that goes into that, and the algorithms are\nactually quite general. And so lots of other hardware out there and lots of other teams\nout there don't have the sophistication or the,\nmaybe the years working on it, or the budget, or whatever\nthat Google does, right? And so they should be getting\naccess to the same algorithms, but they just don't have that, right? And so what Modular's doing,\nso we're saying, \"Cool, this is not research anymore.\" Like, we've built\nauto-tuning in many systems. We've built programming languages, right? And so like, have implemented\nC++, have implemented Swift, have implemented many of these things. And so, you know, it's\nhard, but it's not research. And you look at accelerators. Well, we know there's\na bunch of different, weird kind of accelerators, but they actually cluster together, right? And you look at GPUs. Well, there's a couple\nof major vendors of GPUs and they maybe don't always get along, but their architectures are very similar. You look at CPUs. CPUs are still super important for the deployment side of things. And you see new architectures coming out from all the cloud providers\nand things like this, and they're all super\nimportant to the world, right, but they don't have the\n30 years of development that the entrenched people do, right? And so what Modular\ncan do is we're saying, \"Okay, all this complexity,\nlike, it's not bad complexity, it's actually innovation,\n(chuckles) right?\" And so it's innovation that's happening and it's for good reasons, but I have sympathy for the\npoor software people, right? I mean, again, I'm a\ngenerally software person too. I love hardware, but software people wanna\nbuild applications and products and solutions that scale over many years. They don't wanna build a\nsolution for one generation of hardware with one\nvendor's tools, right? And because of this, they need something that scales with them. They need something that works\non cloud and mobile, right, because, you know, their\nproduct manager said, Hey, I want it to have lower latency and it's better for personalization, or whatever they decide, right? Products evolve. And so the challenge with the\nmachine learning technology and the infrastructure we\nhave today in the industry is that it's all these point solutions. And because there are all\nthese point solutions, it means that as your product evolves, you have to like switch\ndifferent technology stacks or switch to a different vendor. And what that does is\nthat slows down progress. - So basically a lot of\nthe things we've developed in those little silos for\nmachine learning tasks, you want to make that\nthe first class citizen of a general purpose programming language that can then be compiled across all these kinds of hardware. - Well, so it's not really\nabout a programming language. I mean, the programming language is a component of the mission, right? And the mission is, or not literal, but our joking mission is \"to save the world from\nterrible AI software.\" - [Lex] Excellent. I love it.\n- Okay? (chuckles) - So, you know, if you\nlook at this mission, you need a syntax. So yeah, you need\nprogramming language, right? And like, we wouldn't have to\nbuild the programming language if one existed, right? So if Python was already good enough, then cool, we would've\njust used it, right? We're not just doing very large scale, expensive engineering\nprojects for the sake of it, like, it's to solve a problem, right? It's also about accelerators. It's also about exotic\nnumerics and bfloat16 and matrix multiplication and convolutions and like, this kinda stuff. Within the stack, there are\nthings like kernel fusion. That's a esoteric but\nreally important thing that leads to much better performance and much more general research\nhackability together, right? - And that's enabled by the ASICs. That's enabled by certain hardware. So it's like-\n- Well. - Where's the dance between, I mean, there's several questions here. Like, how do you add-\n- Yep. - a piece of hardware to the stack if a new piece of-\n- Yeah. - like if I have this genius invention of a specialized accelerator-\n- Yeah. - how do I add that to\nthe Modular framework? And also how does Modular as a standard start to define the kinds of hardware that should be developed? - Yeah, so let me take a step back and talk about status quo, okay?\n- Yes. - And so if you go back to\nTensorFlow 1, PyTorch 1, this kinda timeframe, and these have all evolved\nand gone way more complicated. So let's go back to the\nglorious simple days, right? These things basically were CPUs and CUDA. And so what you do is you\nsay, go do a dense layer. And a dense layer has a matrix\nmultiplication in it, right? And so when you say that, you say, go do this big operation,\na matrix multiplication, and if it's on a GPU,\nkick off a CUDA kernel. If it's on a CPU, go do\nlike an Intel algorithm, or something like that\nwith an Intel MKL, okay? Now that's really cool if you're either Nvidia or Intel, right? But then more hardware comes in, right? And on one access, you have\nmore hardware coming in. On the other hand, you have an explosion of innovation in AI. And so what happened with\nboth TensorFlow and PyTorch is that the explosion of\ninnovation in AI has led to, it's not just about matrix\nmultiplication and convolution. These things have now like\n2,000 different operators. And on the other hand, you have, I don't know how many pieces of hardware there are out there. It's a lot, (chuckles) okay?\nIt's not even hundreds. It's probably thousands, okay? And across all of edge and across like, all the different things- - That are used at scale. - [Chris] Yeah, exactly. I mean-\n- Also it's not just like a handful.\n- AI's everywhere. Yeah.\n- It's not a handful of TPU alternatives. It's-\n- Correct. It's every phone, often\nwith many different chips inside of it-\n- Right. - from different vendors from...\n- Right. - Like, AI is everywhere.\nIt's a thing, right? - Why are they all making their own chips? Like, why is everybody\nmaking their own thing? - [Chris] Well, so- - Is that a good thing, first of all? - So Chris's philosophy on hardware, right?\n- Yeah. - So my philosophy is that there isn't one\nright solution, right? And so I think that, again, we're at the end of Moore's\nlaw, specialization happens. - [Lex] Yeah. - If you're building, if\nyou're training GPT-5, you want some crazy super\ncomputer data center thingy. If you're making a smart\ncamera that runs on batteries, you want something that\nlooks very different. If you're building a phone, you want something that\nlooks very different. If you have something like a laptop, you want something that\nlooks maybe similar but a different scale, right? And so AI ends up\ntouching all of our lives. Robotics, right? And, like,\nlots of different things. And so as you look into this, these have different power envelopes. There's different trade-offs\nin terms of the algorithms. There's new innovations and sparsity and other data formats\nand things like that. And so hardware innovation, I think, is a really good thing, right? And what I'm interested in\nis unlocking that innovation. There's also like analog\nand quantum and like all the really weird stuff, right?\n- Yeah. - And so if somebody\ncan come up with a chip that uses analog computing and it's 100x more power efficient, think what that would mean\nin terms of the daily impact on the products we use, that'd be huge. Now, if you're building\nan analog computer, you may not be a compiler\nspecialist, right? These are different skill sets, right? And so you can hire some compiler people if you're running a big company, maybe, but it turns out these are really like exotic new generation\nof compilers. (chuckles) Like, this is a different thing, right? So if you take a step back out and come back to what is the status quo, the status quo is that if\nyou're Intel or you're Nvidia, you keep up with the industry\nand you chase and, okay, there's 1,900 now, there's\n2-000 now, there's 2,100. And you have a huge team of\npeople that are like trying to keep up and tune and optimize. And even when one of\nthe big guys comes out with a new generation of their chip, they have to go back and\nrewrite all these things, right? So really it's only powered\nby having hundreds of people that are all, like,\nfrantically trying to keep up. And what that does is that\nkeeps out the little guys, and sometimes they're not so little guys, the big guys that are also just not in those dominant positions. And so what has been happening, and so you talk about\nthe rise of new exotic, crazy accelerators is people\nhave been trying to turn this from a let's go write lots\nof special kernels problem into a compiler problem. And so we, and I contributed\nto this as well, (chuckles) we as an industry went into a like, let's go make this compiler\nproblem phase, let's call it. And much of the industry is\nstill in this phase, by the way. So I wouldn't say this phase is over. And so the idea is to say, look, okay, what a compiler does is it\nprovides a much more general, extensible hackable interface for dealing with the general case, right? And so within machine learning\nalgorithms, for example, people figured out that, hey, if I do a matrix multiplication\nand I do a ReLU, right, the classic activation function, it is way faster to do\none passover the data and then do the ReLU on the output where I'm writing out the data, 'cause ReLU is just a maximum\noperation, right, max at zero. And so it's an amazing\noptimization. Take MathML, ReLU. Squished together in one\noperation, now I have MathML ReLU. Well, wait a second. If I do that, now, I\njust went from having, you know, two operators to three. But now I figure out, okay, well, there's a lot of\nactivation functions. What about a leaky value? What about... Like, a million things\nthat are out there, right? And so as I start fusing these in, now I get permutations of\nall these algorithms, right? And so what the compiler people said is they said, \"Hey, well, cool. Well, I will go enumerate\nall the algorithms and I will enumerate all the pairs and I will actually\ngenerate a kernel for you.\" And I think that this has been very, very useful for the industry. This is one of the things\nthat powers Google TPUs. PyTorch 2's, like, rolling\nout really cool compiler stuff with Triton, this other\ntechnology, and things like this. And so the compiler\npeople are kind of coming into their fore and saying like, Awesome, this is a compiler\nproblem. We'll compiler it. Here's the problem. (chuckles) Not everybody's a compiler person. I love compiler people, trust me, right, but not everybody can or\nshould be a compiler person. It turns out that they're people that know analog computers really well, or they know some GPU internal architecture thing really well, or they know some crazy sparse numeric interesting algorithm that\nis the cusp of research, but they're not compiler people. And so one of the challenges\nwith this new wave of technology trying to turn\neverything into a compiler, 'cause again, it has\nexcluded a ton of people. And so you look at what does Mojo do, what does the Modular stack do is brings programmability\nback into this world. Like, it enables, I\nwouldn't say normal people, but like a new, you know, a different kind of delightful nerd that cares about numerics,\nor cares about hardware, or cares about things like this, to be able to express that in\nthe stack and extend the stack without having to actually\ngo hack the compiler itself. - So extend the stack\non the algorithm side. - [Chris] Yeah. - And then on the hardware side. - Yeah, so again, go back to, like, the simplest example of int, right? And so what both Swift\nand Mojo and other things like this did is we said, okay, pull magic out of the compiler and put it in the standard library, right? And so what Modular's doing with the engine that we're providing and like, this very deep\ntechnology stack, right, which goes into heterogeneous runtimes and like a whole bunch of\nreally cool, really cool things, this whole stack allows that\nstack to be extended and hacked and changed by researchers\nand by hardware innovators and by people who know\nthings that we don't know, (chuckles) 'cause, you know,\nModular has some smart people, but we don't have all the smart\npeople it turns out, right? - What are heterogeneous runtimes? - Yeah. So what is heterogeneous, right? So heterogeneous just means many different kinds of things together. And so the simplest example you might come up with is a CPU and a GPU. And so it's a simple\nheterogeneous computer to say, I'll run my data loading\nand pre-processing and other algorithms on the CPU. And then once I get it\ninto the right shape, I shove it into the GPU. I do a lot of matrix multiplication and convolutions and things like this. And then I get it back out and I do some reductions and summaries and they shove it across the wire, to across the network to\nanother machine, right? And so you've got now what\nare effectively two computers, a CPU and a GPU talking to each other, working together in a\nheterogeneous system. But that was 10 years\nago, (chuckles) okay? You look at a modern cell phone. Modern cell phone, you've got CPUs, and they're not just CPUs, there's like big.LITTLE CPUs and there's multiple different kinds of CPUs that are kind-\n- Yep. - of working together, they're multi-core. You've got GPUs, you've got\nneural network accelerators, you've got dedicated\nhardware blocks for media, so for video decode and jpeg\ndecode and things like this. And so you've got this\nmassively complicated system, and this isn't just cell phones. Every laptop these days\nis doing the same thing. And all these blocks\ncan run at the same time and need to be choreographed, right? And so again, one of the cool\nthings about machine learning is it's moving things\nto like data flow graphs and higher level of\nabstractions and tensors and these things that it doesn't specify, here's how to do the algorithm. It gives the system a lot more flexibility in terms of how to translate or map it or compile it onto the\nsystem that you have. And so what you need, you know, the bottom-est part of the layer there is a way for all these devices\nto talk to each other. And so this is one thing that, you know, I'm very passionate about. I mean, you know, I'm a nerd, but all these machines\nand all these systems are effectively parallel computers running at the same time,\nsending messages to each other. And so they're all fully asynchronous. Well, this is actually a small\nversion of the same problem you have in a data center, right? In a data center, you now have\nmultiple different machines, sometimes very specialized, sometimes with GPUs or TPUs in one node and sometimes with disks in other nodes. And so you get a much larger\nscale heterogenous computer. And so what ends up happening\nis you have this, like, multi-layer abstraction of\nhierarchical parallelism, hierarchical, asynchronous\ncommunication and making that, again, my enemy, is complexity. By getting that away from being different specialized systems\nat every different part of the stack and having more\nconsistency and uniformity, I think we can help lift the world and make it much simpler\nand actually get used. - Well, how do you leverage, like, the strengths of the\ndifferent specialized systems? So looking inside the smartphone, like there's what, like-\n- Yeah. - I don't know, five,\nsix computers essentially inside the smartphone?\n- Yeah. - How do you, without trying\nto minimize the explicit, making it explicit, which computer is supposed to\nbe used for which operation? - Yeah, so there's a pretty\nwell-known algorithm, and what you're doing is\nyou're looking at two factors. You're looking at the\nfactor of sending data from one thing to another, right, 'cause it takes time to get\nit from that side of the chip to that side of the chip\nand things like this. And then you're looking at\nwhat is the time it takes to do an operation on a particular block. So take CPUs. CPUs are fully general.\nThey can do anything, right? But then you have a neural net accelerator that's really good at\nmatrix multiplication, okay? And so you say, okay, well, if my workload is\nall matrix multiplication, I start up, I send the data\nover the neural net thing, it goes and does matrix multiplication. When it's done, it sends\nme back the result. All is good, right? And so the simplest thing is just saying, do matrix operations over there, right? But then you realize you get\na little bit more complicated because you can do matrix\nmultiplication on a GPU, you can do it on a neural net accelerator, you can do it on CPU, and they'll have different\ntrade-offs and costs. And it's not just matrix multiplication. And so what you actually\nlook at is you look at, I have generally a graph of compute. I wanna do a partitioning. I wanna look at the communication,\nthe bisection bandwidth, and like the overhead-\n- Overheads. - and the sending of all\nthese different things and build a model for this\nand then decide, okay, it's an optimization problem of where do I wanna place this compute? - So it's the old school\ntheoretical computer science problem of scheduling.\n- Yep. - And then, presumably it's possible to, somehow, magically include auto-tune into this. - Absolutely, so I mean, in my\nopinion, this is an opinion, not everybody would agree\nwith this, but in my opinion, the world benefits from\nsimple and predictable systems at the bottom you can control. But then once you have a\npredictable execution layer, you can build lots of different policies on top of it, right? And so one policy can be that\nthe human programmer says, do that here, do that here,\ndo that here, do that here, and like, fully manually\ncontrols everything and the systems should just do it, right? But then you quickly\nget in the mode of like, I don't wanna have to tell it to do it. (chuckles)\n- Yeah. - And so the next logical step\nthat people typically take is they write some terrible heuristic. \"Oh, if it's a information\nlocation, do it over there. or if it's floating\npoint, do it on the GPU. If it's integer, do it on the CPU,\" like, something like that, right? And then you then get\ninto this mode of like, people care more and more\nand more, and you say, okay, well, let's actually, like,\nmake the heuristic better. Let's get into auto-tuning. Let's actually do a search\nof the space to decide, well, what is actually better, right? Well, then you get into this problem where you realize this\nis not a small space. This is a many-dimensional\nhyperdimensional space that you cannot exhaustively search. So do you know of any\nalgorithms that are good at searching very\ncomplicated spaces for... - Don't tell me you're gonna turn this into a machine learning problem. - So then you turn into a\nmachine learning problem, and then you have a space\nof genetic algorithms and reinforcement learning and, like, all these concerns.\n- Can you include that into the stack,\ninto the Modular stack? - Yeah, yeah. And so- - Where does it sit? Where does it live? Is it separate thing or is\nit part of the compilation? - So you start from simple\nand predictable models. And so you can have full control and you can have coarse grain knobs that, like, nudge systems so\nyou don't have to do this. But if you really care\nabout getting the best, you know, the last ounce out of a problem, then you can use additional tools. The cool thing is you don't wanna do this every time you run a model. You wanna figure out the right\nanswer and then cache it. (chuckles) And once you do\nthat, you can say, okay, cool. Well, I can get up and\nrunning very quickly. I can get good execution out of my system, I can decide if something's important, and if it's important, I can go throw a bunch of\nmachines at it and do a big, expensive search over the space using whatever technique I feel like, it's really up to the problem. And then when I get\nthe right answer, cool, I can just start using it, right? And so you can get out of this, this trade-off between, okay, am I gonna like spend\nforever doing a thing or do I get up and running quickly? And as a quality result, like, these are actually not in\ncontention with each other if the system's designed to scale. - You started and did a little\nbit of a whirlwind overview of how you get the 35,000x\nspeed up or more over Python. Jeremy Howard did a\nreally great presentation about sort of the basic,\nlike, looking at the code, here's how you get the speed up. Like you said, that's something\nprobably developers can do for their own code to see how you can get these gigantic speed ups. But can you maybe speak to the machine learning task in general? How do you make some of this\ncode fast, and specifics. Like, what would you say\nis the main bottleneck for machine learning tasks? So are we talking about\nMathML matrix multiplication? How do you make that fast? - So I mean, if you just look\nat the Python problem, right? You can say, how do I make Python faster? And there's been a lot of people that have been working on the, okay, how do I make Python 2x\nfaster, or 10x faster, or something like that, right? And there've been a ton of\nprojects in that vein, right? Mojo started from the,\nwhat can the hardware do? Like, what is the limit of physics? What is the speed of light?\n- Yeah. What is the-\n- Yeah, yeah. - Like, how fast can this thing go? And then how do I express that, right?\n- Yeah. - And so it wasn't anchored relatively on make Python a little bit faster. It's saying, cool, I know\nwhat the hard work can do. Let's unlock that, right? Now when you-\n(Lex chuckling) - Yeah, just say how gutsy\nthat is to be in the meeting and as opposed to trying to see, how do we get the improvement? It's like, what can the physics do? - I mean, maybe I'm a special kinda nerd, but you look at that, what\nis the limit of physics? How fast can these things go, right? When you start looking at that, typically it ends up being\na memory problem, right? And so today, particularly with these\nspecialized accelerators, the problem is that you can\ndo a lot of math within them, but you get bottleneck sending data back and forth to memory, whether it be local\nmemory, or distant memory, or disk, or whatever it is. And that bottleneck, particularly as the\ntraining sizes get large as you start doing tons of\ninferences all over the place, like, that becomes a huge\nbottleneck for people, right? So again, what happened\nis we went through a phase of many years where people\ntook the special case and hand-tuned it and tweaked\nit and tricked it out. And they knew exactly\nhow the hardware worked and they knew the model\nand they made it fast, didn't generalize. (chuckles) And so you can make, you know, ResNet-50, or AlexNet, or\nsomething, Inception v1, like, you can do that, right? Because the models are small,\nthey fit in your head, right? But as the models get\nbigger, more complicated, as the machines get more complicated, it stops working, right? And so this is where things\nlike kernel fusion come in. So what is kernel fusion? This is this idea of saying, let's avoid going to memory\nand let's do that by building a new hybrid kernel and\na numerical algorithm that actually keeps\nthings in the accelerator instead of having to write it all the way out to memory, right? What's happened with\nthese accelerators now is you get multiple levels of memory. Like, in a GPU for example, you'll have global\nmemory and local memory, and, like, all these things. If you zoom way into how hardware works, the register file is\nactually a memory. (chuckles) So the registers are like an L0 cache. And so a lot of taking\nadvantage of the hardware ends up being fully\nutilizing the full power in all of its capability. And this has a number of problems, right? One of which is again, the\ncomplexity of disaster, right? There's too much hardware. Even if you just say let's look at the chips from one line of vendor, like Apple, or Intel, or whatever it is, each version of the chip\ncomes out with new features and they change things so that it takes more time or less to do different things. And you can't rewrite all the software whenever a new chip comes out, right? And so this is where you need\na much more scalable approach. And this is what Mojo and what\nthe Modular stack provides is it provides this\ninfrastructure and the system for factoring all this complexity and then allowing people\nto express algorithms, you talk about auto-tuning, for example, express algorithms in a more portable way so that when a new chip comes out, you don't have to rewrite it all. So to me, like, you know, I kinda joke, like, what is a compiler? Well, there's many ways to explain that. You convert thing A into thing B and you convert source\ncode to machine code. Like, you can talk about many, many things that compilers do, but to me it's about a bag of tricks. It's about a system and a framework that you can hang complexity. It's a system that can then generalize and it can work on\nproblems that are bigger than fit in one human's\nhead, (chuckles) right? And so what that means, what a good stack and what\nthe Modular stack provides is the ability to walk up\nto it with a new problem and it'll generally work quite well. And that's something that\na lot of machine learning infrastructure and tools\nand technologies don't have. Typical state-of-the-art\ntoday is you walk up, particularly if you're deploying, if you walk up with a new model, you try to push it through the converter and the converter crashes, that's crazy. The state of ML tooling\ntoday is not anything that a C programmer\nwould ever accept, right? And it's always been this\nkind of flaky set of tooling that's never been integrated well, and it's never worked together because it's not designed together. It's built by different teams, it's built by different hardware vendors, it's built by different systems, it's built by different\ninternet companies. They're trying to solve\ntheir problems, right? And so that means that\nwe get this fragmented, terrible mess of complexity. - So I mean, the specifics of, and Jeremy showed this-\n- Yeah. - there's the vectorized function, which I guess is built into Mojo? - [Chris] Vectorized, as he showed, is built into the library. - Into the library, it's\ndone on the library. - [Chris] Yep. - Vectorize, parallelize. - [Chris] Yep. - Which vectorize is more low-level, parallelize is higher level. There's the tiling thing, which is how he demonstrated\nthe auto-tune, I think. - So think about this in, like, levels, hierarchical levels of abstraction, right? If you zoom all the way\ninto a compute problem, you have one floating point number, right? And so then you say, okay, I can do things one at a\ntime in an interpreter. (chuckles) It's pretty slow, right? So I can get to doing one\nat a time in a compiler, like in C. I can get to doing 4, or 8\nor 16 at a time with vectors. That's called vectorization. Then you can say, hey, I have\na whole bunch of different... You know, what a multi-core computer is, is it's basically a bunch\nof computers, right? So they're all independent computers that they can talk to each\nother and they share memory. And so now what parallelize\ndoes, it says, okay, run multiple instances\non different computers. And now, they can all work\ntogether on Chrome, right? And so what you're doing is you're saying, keep going out to the next level out. And as you do that, how do\nI take advantage of this? So tiling is a memory optimization, right? It says, okay, let's make sure\nthat we're keeping the data close to the compute part of the problem instead of sending it all back and forth through memory every time I load a block. - And the size of the block, size is, that's how you get to the\nauto-tune to make sure it's optimized.\n- Right, yeah. Well, so all of these, the details matter so much\nto get good performance. This is another funny thing\nabout machine learning and high-performance computing that is very different than C compilers we all grew up with where, you know, if you get a new version of\nGCC, or a new version of Clang, or something like that, you know, maybe something will go 1% faster, right? And so compiler engine\nwill work really, really, really hard to get half a percent out of your C code, something like that. But when you're talking\nabout an accelerator, or an AI application, or you're talking about\nthese kinds of algorithms, now these are things people\nused to write in Fortran, for example, right? If you get it wrong, it's not 5% or 1%, it could be 2x or 10x, (chuckles) right? If you think about it, you really want to make\nuse of the full memory you have, the cache, for example. But if you use too much space,\nit doesn't fit in the cache, now you're gonna be thrashing all the way back out to main memory. And these can be 2x, 10x\nmajor performance differences. And so this is where\ngetting these magic numbers and these things right is\nreally actually quite important. - So you mentioned that Mojo\nis a superset of Python. Can you run Python code\nas if it's Mojo code? - Yes, yes,\n(Lex chuckling) and this has two sides of it. So Mojo's not done yet. So\nI'll give you a disclaimer. Mojo's not done yet, but already we see people\nthat take small pieces of Python code, move it\nover, they don't change it, and you can get 12x speed ups. Like, somebody was just\ntweeting about that yesterday, which is pretty cool, right? And again, interpreters, compilers, right? And so without changing\nany code, without... Also, this is not JIT compiling\nor doing anything fancy. This is just basic stuff,\nmove it straight over. Now Mojo will continue to\ngrow out and as it grows out, it will have more and\nmore and more features and our North Star's to be\na full superset of Python. And so you can bring over, basically, arbitrary Python code\nand have it just work. It may not always be 12x faster, but it should be at least\nas fast and way faster in many cases, is the goal, right? Now, it'll take time to do that. And Python is a complicated language. There's not just the obvious things, but there's also non-obvious\nthings that are complicated. Like, we have to be able to\ntalk to CPython packages, to talk to the CPI, and there's\na bunch of pieces to this. - So you have to, I mean, just to make explicit the obvious that may not be so obvious\nuntil you think about it. So, you know, to run Python code, that means you have to run all the Python packages and libraries. - [Chris] Yeah, yeah. - So that means what? What's the relationship\nbetween Mojo and CPython, the interpreter that's-\n- Yep. - presumably would be tasked with getting those packages to work? - Yep, so in the fullness of time, Mojo will solve for all the problems and you'll be able to move Python packages over and run them in Mojo. - [Lex] Without the CPython. - Without Cpython, someday, right, not today, but someday.\n- Yeah. And that'll be a beautiful day because then you'll get a\nwhole bunch of advantages and you'll get massive\nspeedups and things like this. - But you can do that\none at a time, right? You can move packages one at a time.\n- Exactly, but we're not willing to\nwait for that. (chuckles) Python is too important.\nThe ecosystem is too broad. We wanna both be able to build Mojo out, we also wanna do it the\nright way without time, like, without intense time pressure. We're obviously moving fast, but. And so what we do is we say, okay, well, let's make it so you can import an arbitrary existing package, arbitrary, including, like, you write your own on your local disk (chuckles) or whatever. It's not like a standard,\nlike an arbitrary package, and import that using CPython because CPython already runs\nall the packages, right? And so what we do is we\nbuilt an integration layer where we can actually use Cpython, again, I'm practical, and to actually just load and use all the existing\npackages as they are. The downside of that is you don't get the benefits of Mojo for\nthose packages, right? And so they'll run as fast, as they do in the traditional CPython way, but what that does is that gives you an\nincremental migration path. And so if you say, hey,\ncool, well, here's a, you know, the Python ecosystem is vast. I want all of it to just work, but there's certain things\nthat are really important. And so if I'm doing weather\nforecasting or something, (chuckles) well, I wanna be\nable to load all the data, I wanna be able to work with it, and then I have my own crazy\nalgorithm inside of it. Well, normally I'd write that in C++. If I can write in Mojo and\nhave one system that scales, well, that's way easier to work with. - Is it hard to do that, to have that layer that's running CPython? Because is there some\ncommunication back and forth? - Yes, it's complicated. I\nmean, this is what we do. So, I mean, we make it look\neasy, but it is complicated. But what we do is we use the\nCPython existing interpreter. So it's running its own bike codes, and that's how it provides\nfull compatibility. And then it gives us CPython objects, and we use those objects as is. And so that way we're fully compatible with all the CPython objects\nand all the, you know, it's not just the Python part,\nit's also the C packages, the C libraries underneath them, because they're often hybrid. And so we can fully run and we're fully compatible with all that. And the way we do that is that we have to play by their rules, right? And so we keep objects\nin that representation when they're coming from that world. - What's the representation\nthat's being used? - In memory. We'd have to know a lot about how the CPython interpreter works. It has, for example, reference counting, but also different rules on\nhow to pass pointers around, and things like this,\nsuper low-level fiddly. And it's not like Python. It's like how the interpreter works, okay? And so that gets all exposed out, and then you have to define wrappers around the low-level C code, right? And so what this means is\nyou have to know not only C, which is a different role\nfrom Python, obviously, not only Python- - [Lex] But the wrappers. - but the interpreter and the wrappers and the implementation\ndetails and the conventions. And it's just this reall complicated mess. And when you do that, now suddenly you have a\ndebugger that debugs Python, they can't step into C code, right? So you have this two-world problem, right? And so by pulling this all into Mojo, what you get is you get one world. You get the ability to say, cool, I have un-typed, very dynamic,\nbeautiful, simple code. Okay, I care about performance,\nfor whatever reason, right? There's lots of reasons you might care. And so then you add types,\nyou can parallelize things. You can vectorize things,\nyou can use these techniques, which are general techniques\nto solve a problem. And then you can do that\nby staying in the system. And if you have that one Python package that's really important to\nyou, you can move it to Mojo. You get massive performance benefits on that and other advantages. You know, if you like static types, it's nice if they're enforced. Some people like that, right,\nrather than being hints. So there's other advantages too. And then you can do that\nincrementally as you go. - So one different perspective\non this would be why Mojo instead of making CPython\nfaster, redesigning CPython. - Yeah, well, I mean, you could argue Mojo\nis redesigning CPython, but why not make CPython faster and better and other things like that, there's lots of people working on that. So actually there's a team at Microsoft that is really improving... I think CPython 3.11 came out in October or something like that,\nand it was, you know, 15% faster, 20% faster across the board, which is pretty huge\ngiven how mature Python is and things like this. And so that's awesome. I love it. Doesn't run on GPU. (chuckles)\nIt doesn't do AI stuff. Like, it doesn't do\nvectors, doesn't do things. 20 percent's good. 35,000\ntimes is better, right? So like, they're definitely... I'm a huge fan of that work, by the way, and it composes well\nwith what we're doing. It's not like we're fighting\nor anything like that. It's actually just, it's\ngoodness for the world, but it's just a different path, right? And again, we're not working forwards from making Python a little bit better. We're working backwards from\nwhat is the limit of physics? - What's the process of\nimporting Python code to Mojo? Is there... What's involved in that process?\n- Yeah. - Is there tooling for that? - Not yet. So we're missing some\nbasic features right now. And so we're continuing\nto drop out new features, like, on a weekly basis, but, you know, at the fullness of time, give us a year and a\nhalf, maybe two years. - Is it an automatable process? - So when we're ready, it'll\nbe very automatable, yes. - Is it automatable? Like, is it possible to automate, in the general case of Python-\n- Yeah. - to Mojo conversion, and\nyou're saying it's possible. - Well, so, and this is why, I mean, among other reasons why we use tabs, (chuckles) right?\n- Yes. - [Chris] So first of\nall, by being a superset- - Yep. - it's like C versus C++. Can you move C code to C++? Yeah, right?\n- Yes. - And you can move C code to C++, and then you can adopt classes,\nyou can add adopt templates, you can adopt other references or whatever C++ features you want. After you move C code to C++, like, you can't use templates in C, right? And so if you leave it at C, fine. You can't use the cool features,\nbut it still works, right? And C and C++ good work together. And so that's the analogy, right? Now here, right, there's not a Python is\nbad and Mojo is good, (chuckles) right? Mojo just gives you superpowers, right? And so if you wanna stay\nwith Python, that's cool, but the tooling should be\nactually very beautiful and simple because we're doing the hard\nwork of defining a superset. - Right. So you're right. So there's several things to say there, but also the conversion tooling\nshould probably give you hints as to, like, how\nyou can improve the code? - Yeah, exactly. Once you're in the new world, then you can build all kinds\nof cool tools to say like, hey, should you adopt this feature? And we haven't built those tools yet, but I fully expect those tools will exist. And then you can like, you know, quote, unquote, \"modernize your code,\" or however you wanna look at it, right? So I mean one of the things that I think is really\ninteresting about Mojo is that there have been a lot of projects to improve Python over the years. Everything from, you know, getting Python run on\nthe Java virtual machine, PyPy, which is a JIT compiler. There's tons of these projects\nout there that have been working on improving\nPython in various ways. They fall into one or two camps. So PyPy is a great example of a camp that is trying to be\ncompatible with Python. Even there, not really. Doesn't work with all the C\npackages and stuff like that, but they're trying to be\ncompatible with Python. There's also another\ncategory of these things where they're saying, well,\nPython is too complicated and, you know, I'm gonna cheat on\nthe edges and at, you know, like integers in Python can\nbe an arbitrary size integer. Like if you care about it fitting in a, going fast in a register in a computer, that's really annoying, right? And so you can choose\ntwo pass on that, right? You can say, well, people don't really use\nbig integers that often, therefore I'm gonna just\nnot do it and it'll be fine, not a Python superset.\n- Yeah. - (chuckles) Or you can do the hard thing and say, okay, this is Python, and you can't be a superset of Python without being a superset of Python. And that's a really hard\ntechnical problem, but it's, in my opinion, worth it, right? And it's worth it because it's\nnot about any one package. It's about this ecosystem. It's about what Python\nmeans for the world. And it also means we don't wanna repeat the Python 2 to Python 3 transition. Like we want people to be able\nto adopt this stuff quickly. And so by doing that work,\nwe can help lift people. - Yeah, the challenge, it's\nreally interesting, technical, philosophical challenge of\nreally making a language a superset of another language. It's breaking my brain a little bit. - Well, it paints you into corners. So again, I'm very happy\nwith Python, right? So all joking aside, I think that the indentation thing is not the actual important\npart of the problem. - [Lex] Yes.\n(Chris chuckling) - Right? But the fact that Python has amazing dynamic metaprogramming\nfeatures and they translate to beautiful static\nmetaprogramming features, I think is profound I\nthink that's huge, right? And so Python, I've talked with\nGuido about this, it's like, it was not designed to\ndo what we're doing. That was not the reason\nthey built it this way, but because they really cared\nand they were very thoughtful about how they designed the language, it scales very elegantly in this space. But if you look at other languages, for example, C and C++, right, if you're building a superset, you get stuck with the design decisions of the subset, right? And so, you know, C++\nis way more complicated because of C in the legacy\nthan it would've been if they would've theoretically designed a from scratch thing. And there's lots of people right now that are trying to make C++\nbetter and recent syntax C++, it's gonna be great, we'll\njust change all the syntax. But if you do that, now\nsuddenly you have zero packages, you don't have compatibility. - If you could just linger on that, what are the biggest challenges of keeping that superset status? What are the things\nyou're struggling with? Does it all boiled down\nto having a big integer? - No, I mean, it's- - What are the other things like? - Usually it's the long tail weird things. So let me give you a war story. - [Lex] Okay. - So war story in the\nspace is you go away... Back in time, project I\nworked on is called Clang. Clang, what it is a C++ parser, right? And when I started working on Clang, it must have been like\n2006 or something, less, or 2007 something, 2006 when I first started\nworking on it, right? It's funny how time flies. - [Lex] Yeah, yeah. - I started that project\nand I'm like, okay, well, I wanna build a C\nparser, C++ parser for LVM? It's gonna be the... GCC is yucky. You know,\nthis is me in earlier times. It's yucky, it's unprincipled, it has all these weird features, like all these bugs, like it's yucky. So I'm gonna build a standard\ncompliant C and C++ parser. It's gonna be beautiful, it'll\nbe amazing, well-engineered, all the cool things an\nengineer wants to do. And so I started implementing\nand building it out and building it out and building it out. And then I got to include standard io.h, and all of the headers in the\nworld use all the GCC stuff, (chuckles) okay?\n- Yeah. - And so, again, come back away from theory\nback to reality, right? I was at a fork on the road. I could have built an amazingly\nbeautiful academic thing that nobody would ever use or I could say, well, it's\nyucky in various ways. All these design mistakes,\naccents of history, the legacy. At that point, GCC was\nlike over 20 years old, which, by the way-\n- Yeah. - now, LLVM's over 20\nyears old, (laughs) right? And so it's funny how-\n- Yep. - time catches up to you, right? And so you say, okay, well,\nwhat is easier, right? I mean, as an engineer, it's actually much easier\nfor me to go implement long tail compatibility weird features, even if they're distasteful\nand just do the hard work and like figure it out,\nreverse engineer it, understand what it is,\nwrite a bunch of test cases, like, try to understand the behavior. It's way easier to do all\nthat work as an engineer than it is to go talk to all C programmers and argue with them and try to get them to rewrite their code, right?\n- Yeah. - And- - [Lex] 'Cause that\nbreaks a lot more things. - Yeah. The reality is like nobody\nactually even understands how the code works 'cause it was written by the person who quit 10\nyears ago, (chuckles) right? And so this software is kind\nof frustrating that way, but it's, that's how the world works, right?\n- Yeah. Unfortunately, it can never be this perfect, beautiful thing. - Well, there are occasions in which you get to build, like, you know, you invent a new data structure\nor something like that, or there's this beautiful\nalgorithm that's just like, makes you super happy,\nand I love that moment. But when you're working with people-\n- Yeah. - and you're working with\ncode and dusty deck code bases and things like this, right, it's not about what's\ntheoretically beautiful, it's about what's practical, what's real, what people will actually use. And I don't meet a lot of people that say, I wanna rewrite all my code\njust for the sake of it. - By the way, there could\nbe interesting possibilities and we'll probably talk about it where AI can help rewrite some code. That might be farther out feature, but it's a really interesting one, how that could create more-\n- Yeah, yeah. - be a tool in the battle against\nthis monster of complexity that you mentioned.\n- Yeah. - You mentioned Guido, the benevolent dictator\nfor life of Python. What does he think about Mojo? Have you talked to him much about it? - I have talked with him about it. He found it very interesting. We actually talked with\nbefore it launched, and so he was aware of\nit before it went public. I have a ton of respect for Guido for a bunch of different reasons. You talk about walrus operator and, like, Guido's pretty amazing\nin terms of steering such a huge and diverse community and, like, driving it forward. And I think Python is what\nit is thanks to him, right? And so to me it was really important starting to work on\nMojo to get his feedback and get his input and get\nhis eyes on this, right? Now a lot of what Guido was and is I think concerned about is, how do we not fragment the community? - [Lex] Yeah. - We don't want a Python\n2 to Python 3 thing. Like, that was really painful\nfor everybody involved. And so we spent quite a bit\nof time talking about that. And some of the tricks I\nlearned from Swift, for example, so in the migration from Swift, we managed to, like, not\njust convert Objective-C into a slightly prettier\nObjective-C, which we did, we then converted, not entirely, but almost an entire community to a completely different language, right? And so there's a bunch\nof tricks that you learn along the way that are directly\nrelevant to what we do. And so this is where, for example, you leverage CPython while\nbringing up the new thing. Like, that approach is, I think, proven and comes from experience. And so Guido's very interested\nin like, okay, cool. Like, I think that Python is really his legacy, it's his baby. I have tons of respect for that. Incidentally, I see Mojo as a\nmember of the Python family. I'm not trying to take Python from Guido and from the Python community. And so to me it's really important that we're a good member\nof that community. I think that, again, you\nwould have to ask Guido this, but I think that he was very\ninterested in this notion of like, cool Python gets\nbeaten up for being slow. Maybe there's a path out of that, right? And that, you know, if the\nfuture is Python, right, I mean, look at the far\noutside case on this, right? And I'm not saying this\nis Guido's perspective, but, you know, there's this\npath of saying like, okay, well, suddenly Python can\nsuddenly go all the places it's never been able to go before, right? And that means that\nPython can go even further and can have even more\nimpact on the world. - So in some sense, Mojo\ncould be seen as Python 4.0. - I would not say that. I think that would drive a\nlot of people really crazy. - Because of the PTSD of the 3.0, 2.0. - I'm willing to annoy\npeople about Emacs versus Vim or-\n- Not that one. - [Chris] Versus spaces. - Not that one. - I don't know. That might be\na little bit far even for me. Like, my skin may not be that thick. - But the point is the\nstep to being a superset and allowing all of these capabilities, I think is the evolution of a language. It feels like an evolution of a language. So he's interested by the\nideas that you're playing with, but also concerned\nabout the fragmentation. So what are the ideas you've learned? What are you thinking about? How do we avoid fragmenting the community where the Pythonistas and the, I don't know what to call the Mojo people. - [Chris] Mojicians. - The mojicians, I like it. - [Chris] There you go. - Can coexist happily and share code and basically just have\nthese big code bases that are using Cpython and more and more moving towards Mojo.\n- Yeah. Yeah. Well, so again, these are\nlessons I learned from Swift. And here, we face very\nsimilar problems, right? In Swift, you have\nObjective-C, super dynamic. They're very different\nsyntax, (chuckles) right? But you're talking to people who have large scale code bases. I mean, Apple's got the biggest, largest scale code base of\nObjective-C code, right? And so, you know, none of the companies, none of the other iOS developers, none of the other developers want to rewrite everything all at once. And so you wanna be able to\nadopt things piece at a time. And so a thing that I\nfound that worked very well in the Swift community\nwas saying, okay, cool, and this is when Swift was\nvery young, and you say, okay, you have a million line\nof code Objective-C app. Don't rewrite it all, but when\nyou implement a new feature, go implement that new\nclass using Swift, right? And so now this turns out is a very wonderful thing\nfor an app developer, but it's a huge challenge\nfor the compiler team and the systems people that\nare implementing this, right? And this comes back to what is\nthis trade-off between doing the hard thing that enables scale versus doing the theoretically\npure and ideal thing, right? And so Swift had adopted and built a lot of different machinery\nto deeply integrate with the Objective-C runtime. And we're doing the same\nthing with Python right now. What happened in the case of\nSwift is that Swift's language got more and more and more\nmature over time, right? And incidentally, Mojo is a much simpler language\nthan Swift in many ways. And so I think that Mojo\nwill develop way faster than Swift for a variety of reasons. But as the language gets more\nmature and parallel with that, you have new people starting\nnew projects, right? And so if when the language is mature and somebody's starting a new project, that's when they say, okay, cool, I'm not dealing with a\nmillion lines of code. I'll just start and use the\nnew thing for my whole stack. Now the problem is, again, you come back to we're communities and we're people that work together. You build new subsystem or a new feature or a new thing in Swift, or\nyou build a new thing in Mojo, then you want it to be end up being used on the other side, (chuckles) right? And so then you need\nto work on integration back the other way. And so it's not just\nMojo talking to Python, it's also Python talking to Mojo, right? And so what I would love to see, I don't wanna see this next month, right, but what I wanna see over the\ncourse of time is I would love to see people that are\nbuilding these packages, like, you know, NumPy or, you know,\nTensorFlow or what, you know, these packages that are\nhalf Python, half C++. And if you say, okay, cool, I want to get out of this Python C++ world into a unified world and\nso I can move to Mojo, but I can't give up all my Python clients 'cause they're like, these levers get used by everybody and they're not all gonna\nswitch every, all, you know, all at once and maybe never, right? Well, so the way we should do that is we should vend Python\ninterfaces to the Mojo types. And that's what we did in\nSwift and worked great. I mean, it was a huge\nimplementation challenge for the compiler people, right? But there's only a dozen\nof those compiler people and there are millions of users. And so it's a very\nexpensive, capital-intensive, like, skillset intensive problem. But once you solve that problem, it really helps adoption and\nit really helps the community progressively adopt technologies. And so I think that this\napproach will work quite well with the Python and the Mojo world. - So for a package, port it to Mojo, and then create a Python interface. - [Chris] Yep. - So when you're on these packages, NumPy, PyTorch, TensorFlow. - Yeah. - How do they play nicely together? So is Mojo supposed to be... Let's talk about the\nmachine learning ones. Is Mojo kind of visioned\nto replace PyTorch, TensorFlow to incorporate it? What's the relationship in this? - All right, so take a step back. So I wear many hats. (chuckles) So you're angling it on the Mojo side. Mojo's a programming language.\n- Yes. - And so it can help solve the C, C++ Python feud that's happening. - The fire emoji got me. I'm sorry. We should be talking Modular. Yes, yes. - Yes, okay. So the fire emoji is amazing. I love it. It's a big deal. The other side of this\nis the fire emoji is in service of solving\nsome big AI problems, right?\n- Yes. - And so the big AI problems are, again, this fragmentation,\nthis hardware nightmare, this explosion of new potential, but it's not getting felt\nby the industry, right? And so when you look at, how does the Modular engine\nhelp tens and PyTorch, right, it's not replacing them, right? In fact, when I talk to people, again, they don't like to rewrite all their code. You have people that are\nusing a bunch of PyTorch, a bunch of TensorFlow. They have models that\nthey've been building over the course of many years, right? And when I talk to them,\nthere's a few exceptions, but generally they don't wanna rewrite all their code, right? And so what we're doing is\nwe're saying, \"Okay, well, you don't have to rewrite all your code.\" What happens is the Modular\nengine goes in there and goes underneath\nTensorFlow and PyTorch. It's fully compatible and it just provides better performance, better\npredictability, better tooling. It's a better experience\nthat helps lift TensorFlow and PyTorch and make them even better. I love Python, I love TensorFlow,\nI love PyTorch, right? This is about making the world better because we need AI to go further. - But if I have a process\nthat trains a model and I have a process that\nperforms inference on that model and I have the model itself, what should I do with that\nin the long arc of history in terms of if I use PyTorch to train it. Should I rewrite stuff in Mojo\nif I care about performance? - Oh, so I mean, again, it depends. So if you care about performance, then writing it in Mojo is gonna be way better than writing in Python. But if you look at LLM\ncompanies, for example, so you look at Open AI, rumored, and you look at many of the other folks that are working on many of these LLMs and other like innovative\nmachine learning models, on the one hand they're\ninnovating in the data collection and the model, billions of parameters, and the model architecture\nand the RLHF and the, like all the cool things that\npeople are talking about. But on the other hand, they're spending a lot of time\nwriting CUDA curls, right? And so you say, wait a second, how much faster could all this\nprogress go if they were not having to hand write all\nthese CUDA curls, right? And so there are a few\ntechnologies that are out there, and people have been working\non this problem for a while and they're trying to solve\nsubsets of the problem, again, kinda fragmenting the space. And so what Mojo provides for\nthese kinds of companies is the ability to say, cool, I can have a unifying theory, right? And again, the better\ntogether, the unifying theory, the two-world problem, or\nthe three-world problem, or the N-world problem, like, this is the thing\nthat is slowing people down. And so as we help solve this problem, I think it'll be very helpful for making this whole cycle go faster. - So obviously we've\ntalked about the transition from Objective-C to Swift. You've designed this programming language, and you've also talked quite\na bit about the use of Swift for machine learning context. Why have you decided to move away from maybe an intense focus on Swift for the machine learning\ncontext versus sort of designing a new programming language\nthat happens to be a superset? - You're saying this is an irrational set of life choices I make or what? (chuckles)\n(Lex laughing) - Did you go to the desert\nand did you meditate on it? Okay, all right. No, it was bold. It was bold and needed\nand I think, I mean, it's just bold and sometimes\nto take those leaps, it's a difficult leap to take. - Yeah. Well, so, okay. I mean, I think there's a\ncouple of different things. So actually I left to Apple back in 2017, like January, 2017. So it's been a number of\nyears that I left Apple. And the reason I left\nApple was to do AI, okay? So, and again, I won't\ncomment on Apple and AI, but at the time, right, I wanted to get into and\nunderstand the technology, understand the\napplications, the workloads. And so I was like, okay, I'm gonna go dive deep\ninto Applied and AI, and then the technology\nunderneath it, right? I found myself at Google. - And that was like when TPUs were waking up.\n- Yep, exactly. - And so I found myself\nat Google and Jeff Dean, who's a rockstar as you know, right? And in 2017, TensorFlow's, like, really taking off and\ndoing incredible things. And I was attracted to Google to help them with the TPUs, right? And TPUs are an innovative\nhardware accelerator platform, have now I mean I think\nproven massive scale and like done incredible things, right? And so one of the things\nthat this led into is a bunch of different projects,\nwhich I'll skip over, right? One of which was this Swift\nfor TensorFlow project, right? And so that project\nwas a research project. And so the idea of that is say, okay, well, let's look at innovative\nnew programming models where we can get a fast\nprogramming language, we can get automatic\ndifferentiation into the language. Let's push the boundaries of these things in a\nresearch setting, right? Now, that project I think\nlasted two, three years. There's some really cool outcomes of that. So one of the things that's\nreally interesting is I published a talk at an\nLLVM conference in 2018, again, this seems like so long ago, about graph program abstraction, which is basically the\nthing that's in PyTorch 2. And so PyTorch 2 with\nall this DynamoRIO thing, it's all about this graph\nprogram abstraction thing from Python bike codes. And so a lot of the research\nthat was done ended up pursuing and going out through the\nindustry and influencing things. And I think it's super exciting\nand awesome to see that, but the Swift for\nTensorFlow project itself did not work out super well. And so there's a couple of\ndifferent problems with that. One of which is that,\nyou may have noticed, Swift is not Python. (chuckles) There's a few\npeople that write Python code. - [Lex] Yes. - And so it turns out that all of ML is pretty happy with Python. - It's actually a problem that other programming\nlanguages have as well, that they're not Python. We'll probably maybe\nbriefly talk about Julia, was a very interesting,\nbeautiful programming language, but it's not Python. - Exactly. And so like if you're saying, I'm gonna solve a machine learning problem where all the programmers\nare Python programmers. - [Lex] Yeah. - And you say the first\nthing you have to do is switch to a different language, well, your new thing may\nbe good or bad or whatever, but if it's a new thing, the adoption barrier is massive less. - It's still possible. - Still possible, yeah, absolutely. The world changes and evolves and there's definitely room\nfor new and good ideas, but it just makes it\nso much harder, right? And so lesson learned,\nSwift is not Python, and people are not always\nin search of, like, learning a new thing for the\nsake of learning a new thing. And if you wanna be compatible\nwith all the world's code, turns out meet the world\nwhere it is, right? Second thing is that, you know, a lesson learned is that Swift is a very fast and efficient\nlanguage, kind of like Mojo, but a different take on it still, really worked well with eager mode. And so eager mode is\nsomething that PyTorch does, and it proved out really well, and it enables really\nexpressive and dynamic and easy to debug programming. TensorFlow at the time was not\nset up for that, let's say. That was not... - [Lex] The timing is also\nimportant in this world. - Yeah, yeah. And TensorFlow is a good thing and it has many, many strengths, but you could say Swift for\nTensorFlow is a good idea, except for the Swift and\nexcept for the TensorFlow part. (chuckles) - Swift because it's not Python\nand TensorFlow because it- - [Chris] It wasn't set up for\neager mode at the time, yeah. - It was 1.0. - Exactly. And so one of the things about that is that in the context of it\nbeing a research project, I'm very happy with the fact that we built a lot of\nreally cool technology. We learned a lot of things. I think the ideas went\non to have influence in other systems, like PyTorch. A few people use that I hear, right? And so I think that's super cool. And for me personally, I\nlearned so much from it, right? And I think a lot of the\nengineers that worked on it also learned a tremendous amount. And so, you know, I think that that's just\nreally exciting to see. And, you know, I'm sorry that\nthe project didn't work out. I wish it did, of course, right, but, you know, it's a research project. And so you're there to learn from it. - Well, it's interesting to think about the evolution of programming as we come up with these\nwhole new set of algorithms in machine learning, in\nartificial intelligence. And what's going to win out is it could be a new programming language. It could be-\n- Yeah. - I mean, I just mentioned Julia. I think there's a lot of ideas behind Julia that Mojo shares. What are your thoughts\nabout Julia in general? - So I will have to say\nthat when we launched Mojo, one of the biggest things I didn't predict was the response from the Julia community. And so I was not, I mean, I've, okay, lemme take a step back. I've known the Julia folks\nfor a really long time. They're an adopter of\nLLVM a long time ago. They've been pushing state-of-the-art in a bunch of different ways. Julia's a really cool system. I had always thought of Julia as being mostly a scientific computing\nfocused environment, right? And I thought that was its focus. I neglected to understand\nthat one of their missions is to, like, help make Python\nwork end-to-end. (chuckles) And so I think that was my error\nfor not understanding that. And so I could have been\nmaybe more sensitive to that, but there's major differences between what Mojo's doing\nand what Julia's doing. So as you say, Julia is not Python, right? And so one of the things that a lot of the Julia people came\nout and said is like, \"Okay, well, if we put a ton of more energy into, ton more money or in engineering\nor whatever into Julia, maybe that would be better\nthan starting Mojo, right?\" Well, I mean, maybe that's true, but it still wouldn't make\nJulia into Python. (chuckles) So if you worked backwards\nfrom the goal of, let's build something\nfor Python programmers without requiring them to relearn syntax, then Julia just isn't there, right? I mean, that's a different thing, right? And so if you anchor on, I love Julia, and I want Julia to go further, then you can look at it\nfrom a different lens, but the lens we were coming at was, Hey, everybody is using Python.\nThe syntax isn't broken. Let's take what's great about Python and make it even better. And so it was just a\ndifferent starting point. So I think Julie's a great language. The community's a lovely community. They're doing really cool stuff,\nbut it's just a different, it's slightly different angle. - But it does seem that\nPython is quite sticky. Is there some philosophical, almost thing you could\nsay about why Python, by many measures, seems to be the most popular programming language in the world? - Well, I can tell you\nthings I love about it. Maybe that's one way to\nanswer the question, right? So huge package ecosystem, super lightweight and easy to integrate. It has very low startup time, right? - [Lex] So what's startup time? You mean like learning curve or what? - Yeah, so if you look at\ncertain other languages, you say like, go, and it just takes a, like Java, for example, it takes a long time to\nJIT compile all the things and then the VM starts up and the garbage (indistinct) kicks in and then it revs its engines and then it can plow through\na lot of internet stuff or whatever, right? Python is like scripting. Like it just goes, right?\n- Yeah. - Python has a very low compile time. Like, so you're not sitting there waiting. Python integrates in a\nnotebooks in a very elegant way that makes exploration super interactive and it's awesome, right? Python is also, it's like almost the glue of computing. Because it has such a simple\nobject representation, a lot of things plug into it. That dynamic metaprogramming\nthing we were talking about, also enables really expressive\nand beautiful APIs, right? So there's lots of reasons\nthat you can look at, technical things the Python\nhas done and say, like, okay, wow, this is actually\na pretty amazing thing. And any one of those you can neglect, people will all just\ntalk about indentation (chuckles) and ignore like\nthe fundamental things. But then you also look at\nthe community side, right? So Python owns machine learning. Machine learning's pretty big. - Yeah, and it's growing. - And it's growing, right? And it's growing in importance, right? And so-\n- And there's a reputation of prestige to machine learning to where like if you're a new programmer, you're thinking about, like, which program and language do I use? Well, I should probably\ncare about machine learning, therefore let me try Python, and kinda builds and builds and builds. - And even go back before that. Like, my kids learned Python, right, not because I'm telling\n'em to learn Python, but because-\n- Were they rebelling against you or what? - Oh, no, no. Well, they they also learn Scratch, right, and things like this too, but it's because Python is\ntaught everywhere, right? Because it's easy to learn, right? And because it's pervasive, right? And there's-\n- Back in my day, we learned Java and C++. - [Chris] Yeah, well. - Well, uphill both directions, but yes. I guess Python-\n- Yeah. - is the main language of teaching software\nengineering schools now. - Yeah, well, and if you look at this, there's these growth cycles, right? If you look at what causes\nthings to become popular and then gain in popularity, there's reinforcing feedback\nloops and things like this. And I think Python has done, again, the whole community has done\na really good job of building those growth loops and\nhelp propel the ecosystem. And I think that, again, you look at what you can get done with just a few lines\nof code, it's amazing. - So this kinda self-building\nloop is interesting to understand because\nwhen you look at Mojo, what it stands for some of the features, it seems sort of clear that\nthis is a good direction for programming languages to evolve in the machine\nlearning community, but it's still not obvious\nthat it will because of this, whatever the engine of\npopularity of virality. Is there something you\ncould speak to, like, how do you get people to switch? - Yeah, well, I mean, I think that the viral growth loop is to switch people to Unicode. - [Lex] Yes. - I think the Unicode file extensions are what I'm betting on. I think that's gonna be the thing. - Yeah.\n(Chris chuckling) - Tell the kids that you\ncould use the fire emoji and they'd be like, what?\n- Exactly, exactly. (Lex chuckling) Well, in all seriousness, like, I mean, I think there's really, I'll\ngive you two opposite answers. One is, I hope if it's\nuseful, if it solves problems, and if people care about\nthose problems being solved, they'll adopt the tech, right? That's kinda the simple answer. And when you're looking\nto get tech adopted, the question is, is it solving an important\nproblem people need solved, and is the adoption cost low\nenough that they're willing to make the switch and cut\nover and do the pain upfront so that they can actually do it, right? And so hopefully Mojo will be\nthat for a bunch of people. And, you know, people building these hybrid\npackages are suffering. It is really painful. And so I think that we have a\ngood shot of helping people, but the other side is like, it's okay if people don't use Mojo. Like, it's not my job to say\nlike, everybody should do this. Like, I'm not saying Python is bad. Like, I hope Python, CPython, like, all these implementations 'cause Python ecosystems,\nnot just CPython, it's also a bunch of\ndifferent implementations with different trade-offs. And this ecosystem is\nreally powerful and exciting as are other programming languages. It's not like type script or something is gonna go away, right? And so there's not a\nwinner-take-all thing. And so I hope that Mojo's\nexciting and useful to people, but if it's not, that's also fine. - But I also wonder what the use case for why you should try Mojo would be. So practically speaking- - [Chris] Yeah. - it seems like, so there's entertainment. There's the dopamine hit of saying, holy, this is 10 times faster. This little piece of code\nis 10 times faster in Mojo. - [Chris] Outta the box\nbefore you get to 35,000. - Exactly, I mean, just even that, I mean, that's the dopamine hit\nthat every programmer sorta dreams of is the optimization. It's also the drug that can pull you in and have you waste way\ntoo much of your life optimizing and over optimizing, right? But so what do you see\nwould be, like, common? It's very hard to predict,\nof course, but, you know, if you look 10 years from now\nand Mojo's super successful. - [Chris] Yeah. - What do you think would be the thing where people like try\nand then use it regularly and it kinda grows and\ngrows and grows and grows? - Well, so you talked about dopamine hit. And so one, again,\nhumans are not one thing. And some people love rewriting their code and learning new things\nand throwing themselves in the deep end and\ntrying out a new thing. In my experience, most\npeople, they're too busy. They have other things going on. By number, most people\ndon't want like this. I wanna rewrite all my code. But (chuckles) even those\npeople, the two busy people, the people that don't actually\ncare about the language, that just care about getting stuff done, those people do like\nlearning new things, right? - [Lex] Yeah. - And so you talk about the\ndopamine rush of 10x faster, Wow, that's cool. I wanna do that again. Well, it's also like, here's the thing I've heard\nabout in a different domain, and I don't have to rewrite on my code. I can learn a new trick, right? Well, that's called growth,\n(chuckles) you know? And so, one thing that I\nthink is cool about Mojo, and again, those will\ntake a little bit of time, for example, the blog posts\nand the books and, like, all that kinda stuff to develop and the language needs\nto get further along. But what we're doing,\nyou talk about types, like you can say, look, you can start with the\nworld you already know and you can progressively learn new things and adopt them where it makes sense. And if you never do that, that's cool. You're not a bad person. (chuckles) If you get really excited about\nit and wanna go all the way in the deep end and rewrite\neverything and, like, whatever, that's cool, right? But I think the middle path is\nactually the more likely one where it's, you know, you come out with a a new\nidea and you discover, wow, that makes my code way simpler, way more beautiful way,\nfaster way, whatever. And I think that's what people like. Now if you fast forward and you said, like, 10 years out, right, I can give you a very different\nanswer on that, which is, I mean, if you go back and look at what computers looked\nlike 20 years ago, every 18 months, they got\nfaster for free, right, 2x faster every 18 months. It was like clockwork. It was free, right? You go back 10 years ago\nand we entered in this world where suddenly we had\nmulti-core CPUs and we had, and if you squint and turn your head, what a GPUs is just a many-core, very simple CPU thing kind of, right? And 10 years ago it was\nCPUs and GPUs and graphics. Today, we have CPU, GPUs, graphics. And AI, because it's so important, because the compute is so demanding because of the smart\ncameras and the watches and all the different places that AI needs to work in our lives, it's caused this explosion of hardware. And so part of my thesis, part of my belief of where computing goes, if you look out 10 years from now, is it's not gonna get simpler. Physics isn't going back\nto where we came from. It's only gonna get weirder\nfrom here on out, right? And so to me, the exciting part about\nwhat we're building is it's about building\nthat universal platform, which the world can continue to get weird. 'Cause again, I don't think\nit's avoidable, it's physics, but we can help lift people,\nscale, do things with it, and they don't have to rewrite their code every time a new device comes out. And I think that's pretty cool. And so if Mojo can help with that problem, then I think that it will be\nhopefully quite interesting and quite useful to a wide range of people because there's so much potential. And like there's so much, you know, maybe analog computers will become a thing or something, right? And we need to be able to get into a mode where we can move this\nprogramming model forward, but do so in a way where\nwe're lifting people and growing them instead of forcing them to rewrite all their\ncode and exploding them. - Do you think there'll be a few major libraries that go Mojo first? - Well, so I mean, the Modular\nengines on Mojo. (chuckles) So again, come back to, like, we're not building Mojo because it's fun. We're building Mojo because we had to solve these accelerators. - That's the origin story, but I mean, ones that are currently in Python. - Yeah, so I think that a\nnumber of these projects will. And so one of the things, and again, this is just my best guess. Like, each of the package\nmaintainers also has... I'm sure plenty of other things going on. People really don't like rewriting code just for the sake of rewriting code. But sometimes like people are\nexcited about like adopting a new idea.\n- Yeah. - And turns out that while rewriting code is generally not people's first thing, turns out that redesigning\nsomething while you rewrite it and using a rewrite as\nan excuse to redesign can lead to the 2.0 of your thing that's way better than the 1.0, right? And so I have no idea,\nI can't predict that, but there's a lot of\nthese places where, again, if you have a package that is\nhalf C and half Python, right, you just solve the pain, make it easier to move things faster, make it easier to bug and\nevolve your tech adopting Mojo kinda makes sense to start with. And then it gives you this opportunity to rethink these things. - So the two big gains are\nthat there's a performance gain and then there's the portability to all kinds of different devices. - And there's safety, right?\nSo you talk about real types. I mean, not saying this is for everybody, but that's actually a\npretty big thing, right? - [Lex] Yeah, types are. - And so there's a bunch of\ndifferent aspects of what, you know, what value Mojo provides. And so, I mean, it's funny for me, like, I've been working on these\nkinds of technologies and tools for too many years now, but you look at Swift, right, and we talked about Swift for TensorFlow, but Swift as a programming\nlanguage, right? Swift's now 13 years old from when I started it? - [Lex] Yeah. - 'Cause I started in 2010, if I remember. And so that project, and I was involved with it for\n12 years or something, right, that project has gone through its own really interesting story arc, right? And it's a mature, successful, used by millions of people system, right? Certainly not dead yet, right? But also going through that story arc, I learned a tremendous amount\nabout building languages, about building compilers, about working with the\ncommunity and things like this. And so that experience, like I'm helping channel\nand bring directly into Mojo and, you know, other systems, same thing. Like, apparently I like building, and iterating and evolving things. And so you look at this LLVM thing that I worked on 20 years ago,\nand you look at MLIR, right? And so a lot of the lessons learned in LLVM got fed into MLIR, and I think that MLIR is a way\nbetter system than LLVM was. And, you know, Swift is a really good\nsystem and it's amazing, but I hope that Mojo will take the next step forward in terms of design. - In terms of running Mojo\nand people can play with it, what's Mojo Playground?\n- Yeah. - And from the interface perspective and from the hardware perspective, what's this incredible thing running on? - Yeah, so right now, so here we are, two weeks after launch. - Yes. - We decided that, okay, we have this incredible set of technology that we think might be good, but we have not given it\nto lots of people yet. And so we were very conservative and said, \"Let's put it in a workbook\nso that if it crashes, we can do something about it. We can monitor and track that, right?\" And so, again, things\nare still super early, but we're having like one\nperson a minute sign up with over 70,000 people (chuckles) two weeks in is kinda crazy. - And you can sign up to Mojo Playground and you can use it in the cloud. - [Chris] Yeah. - In your browser. - [Chris] And so what\nthat's running on, right? - Notebook. - Yeah, What that's running on is that's running on cloud VMs. And so you share a machine\nwith a bunch of other people, but turns out there's a bunch of them now because there's a lot of people. And so what you're doing is\nyou're getting free compute and you're getting to play with this thing in kind of a limited controlled way so that we can make sure that it doesn't totally crash and be embarrassing, right?\n- Yeah. - So now a lot of the\nfeedback we've gotten is people wanna download it around locally. So we're working on that right now. And so-\n- So that's the goal, to be able to download locally to it. - Yeah, that's what everybody expects. And so we're working on that right now. And so we just wanna make\nsure that we do it right. I think this is one of the lessons I learned from Swift also, by the way, is when we launched Swift, gosh, it feels like\nforever ago, it was 2014, and we, I mean, it was super exciting. I, and we, the team had worked on Swift for a number of years in secrecy, okay? And (chuckles) four years\ninto this development, roughly, of working on this thing, at that point, about 250\npeople at Apple knew about it. - [Lex] Yeah. - Okay? So it was secret. Apple's good at secrecy and\nit was a secret project. And so we launched this at WWC, a bunch of hoopla and excitement and said developers are\ngonna be able to develop and submit apps in the App\nStore in three months, okay? Well, several interesting\nthings happened, right? So first of all, we learned\nthat it had a lot of bugs. It was not actually production quality, and it was extremely stressful\nin terms of like trying to get it working for a bunch of people. And so what happened was we\nwent from zero to, you know, I don't know how many developers\nApple had at the time, but a lot of developers overnight. And they ran to a lot of bugs\nand it was really embarrassing and it was very stressful for\neverybody involved, right? It was also very exciting 'cause everybody was excited about that. The other thing I learned\nis that when that happened, roughly every software engineer who did not know about\nthe project at Apple, their head exploded when it was launched 'cause they didn't know it was coming. And so they're like, \"Wait, what is this? I signed up to work for Apple\nbecause I love Objective-C. Why is there a new thing?,\" right?\n- Yeah. - And so now what that\nmeant practically is that the push from launch\nto first of all the fall, but then to 2.0 and 3.0 and\nlike all the way forward was super painful for the\nengineering team and myself. It was very stressful. The developer community\nwas very grumpy about it because they're like,\n\"Okay, well, wait a second. You're changing and breaking my code, and like, we have to fix the bugs.\" And it was just like a lot of tension and friction on all sides. There's a lot of technical\ndebt in the compiler because we have to run really fast and you have to go implement the thing and unblock the use case and do the thing. And you know it's not right, but you never have time to\ngo back and do it right. And I'm very proud of the Swift team because they've come, I mean, we, but they came so far and\nmade so much progress over this time since launch,\nit's pretty incredible. And Swift is a very, very good thing, but I just don't wanna\ndo that again, right? And so-\n- So iterate more through the development process. - And so what we're doing\nis we're not launching it when it's hopefully 0.9 with no testers. We're launching it and\nsaying it's 0.1, right? And so we're setting expectations\nof saying like, okay, well, don't use this\nfor production, right? If you're interested in what we're doing, we'll do it in an open way\nand we can do it together, but don't use it in production yet. Like, we'll get there, but\nlet's do it the right way. And I'm also saying we're not in a race. The thing that I wanna do is\nbuild the world's best thing. - [Lex] Yeah. - Right, because if you do it right and it lifts the industry, it doesn't matter if it takes an extra two months.\n- Yeah. - Like two months is worth waiting. And so doing it right and not being overwhelmed\nwith technical debt and things like this is like, again, war wounds, lessons learned, whatever you wanna say, I think is absolutely\nthe right thing to do. Even though right now people are very frustrated that, you know, you can't download it\nor that it doesn't have feature X or something like this. And so-\n- What have you learned in a little bit of time since it's been released into the wild that people have been complaining\nabout feature X or Y or Z? What have they been complaining about? Whether they have been excited about like, almost like detailed\nthings versus a big thing. I think everyone's would be very excited about the big vision. - Yeah, yeah. Well, so I\nmean, I've been very pleased. I mean, in fact, I mean, we've been massively\noverwhelmed with response, which is a good problem to have. It's kinda like a success disaster, in a sense, right?\n- Yeah. - And so, I mean, if you go back in time\nwhen we started Modular, which is just not yet\na year and a half ago, so it's still a pretty\nnew company, new team, small but very good team of people, like we started with extreme conviction that there's a set of problems\nthat we need to solve. And if we solve it, then people will be interested\nin what we're doing, right? But again, you're building\nin basically secret, right? You're trying to figure it out. The creation's a messy process. You're having to go\nthrough different paths and understand what you wanna\ndo and how to explain it. Often when you're doing disruptive\nand new kinds of things, just knowing how to explain\nit is super difficult, right? And so when we launched, we\nhope people would be excited, but, you know, I'm an\noptimist, but I'm also like, don't wanna get ahead of myself. And so when people found out about Mojo, I think their heads exploded\na little bit, right? And, you know, here's a, I think a pretty credible\nteam that has built some languages and some tools before. And so they have some lessons learned and are tackling some of the deep problems in the Python ecosystem and giving it the love and attention\nthat it should be getting. And I think people got\nvery excited about that. And so if you look at that, I mean, I think people are excited about ownership and taking a step beyond Rust, right? And there's people that\nare very excited about that and there's people that are\nexcited about, you know, just like I made Game of Life\ngo 400 times faster, right, and things like that,\nand that's really cool. There are people that are\nreally excited about the, okay, I really hate writing\nstuff in C++, save me. - Like systems in your, they're like stepping up, like, oh yes. - And so that's me by the way, also. - [Lex] Yeah. - I really wanna stop\nwriting C++, but the- - I get third person\nexcitement when people tweet, Here, I made this code, Game\nof Life or whatever, faster. And you're like, yeah. - Yeah, and also like, well, I would also say that, let me cast blame out to\npeople who deserve it. - [Lex] Sure. - These terrible people who\nconvinced me to do some of this. Jeremy Howard, that guy.\n- Yes, yes. Well, he's been pushing\nfor this kinda thing. He's been pushing-\n- He's wanted this for years. - Yeah, he's wanted this\nfor a long, long time. - [Chris] He's wanted\nthis for years. And so- - For people who don't know Jeremy Howard, he is like one of the most legit people in the machine learning community. He's a grassroots, he really teaches, he's an incredible educator,\nhe is an incredible teacher, but also legit in terms of\na machine learning engineer himself.\n- Yes. - And he's been running\nthe fast.AI and looking, I think for exactly what you've done with Mojo.\n- Exactly. And so, I mean, the first time, so I met Jeremy pretty early on, but the first time I sat up and I'm like, this guy is ridiculous, is when I was at Google and\nwe were bringing up TPUs and we had a whole team of people and there was this\ncompetition called Don Bench of who can train ImageNet fastest, right?\n- Yeah. Yes. - And Jeremy and one of his\nresearchers crushed Google (chuckles) by not through sheer force of the amazing amount of compute and the number of TPUs\nand stuff like that, that he just decided that\nprogressive imagery sizing was the right way to train the model in. You were epoch faster and make the whole thing go vroom, right?\n- Yep. - And I'm like, \"This guy is incredible.\" So you can say,\n- Right. anyways, come back to, you know, where's Mojo coming from? Chris finally listened to Jeremy. (Lex laughing) It's all his fault. - Well, there's a kinda very refreshing, pragmatic view that he has about machine learning\nthat I don't know if it, it's like this mix of a\ndesire for efficiency, but ultimately grounded and\ndesired to make machine learning more accessible to a lot of people. I don't know what that is.\n- Yeah. - I guess that's coupled with\nefficiency and performance, but it's not just obsessed\nabout performance. - Well, so a lot of AI and AI research ends up being that it has to go fast\nenough to get scale. So a lot of people don't\nactually care about performance, particularly on the research side until it allows 'em to have\nmore a bigger dataset, right? And so suddenly now you care\nabout distributed compute and like, all these exotic HPC, like, you don't actually\nwanna know about that. You just want to be able to\ndo more experiments faster and do so with bigger datasets, right? And so Jeremy has been\nreally pushing the limits. And one of the things\nI'll say about Jeremy, and there's many things\nI could say about Jeremy, 'cause I'm a fanboy of his,\nbut it fits in his head, and Jeremy actually takes the\ntime where many people don't to really dive deep into\nwhy is the beta parameter of the atom optimizer equal to this, right?\n- Yeah. - And he'll go survey and understand what are all the activation\nfunctions in the trade-offs, and why is it that everybody\nthat does, you know, this model, pick that thing. - So the why, not just\ntrying different values, like, really what is going on here? - Right, and so as a consequence\nof that, like he's always, he, again, he makes time, but he spends time to understand things at a depth that a lot of people don't. And as you say, he then brings it and teaches people- - [Lex] Teaches it. - And his mission is\nto help lift, you know, his website says \"making AI uncool again,\" like it's about, like,\nforget about the hype. It's actually practical and useful. Let's teach people how to do this, right? Now the problem Jeremy struggled with is that he's pushing the envelope, right? Research isn't about doing the thing that is staying on the happy path or the well-paved road, right? And so a lot of the systems today have been these really\nfragile, fragmented things, are special case in this happy path. And if you fall off the happy path, you get eaten by an alligator. (chuckles) - (chuckles) So what about... So Python has this giant\necosystem of packages and there's a package repository. Do you have ideas of how\nto do that well for Mojo, how to do a repository of packages well? - So that's another\nreally interesting problem that I knew about but I didn't understand how big of a problem it\nwas: Python packaging. A lot of people have very big pain points and a lot of scars with Python packaging. - Oh, you mean, so there's\nseveral things to say. - [Chris] Building and distributing and managing dependencies\n- Yes. - [Chris] and versioning\nand all this stuff. - So from the perspective of, if you want to create your own package, and then\n- Yes, yeah. - or you wanna build on top of a bunch of other people's packages and then they get updated\nand things like this. Now, I'm not an expert in this,\nso I don't know the answer. I think this is one the reasons why it's great that we work as a team and there's other really good\nand smart people involved, but one of the things I've\nheard from smart people who've done a lot of this is\nthat the packaging becomes a huge disaster when you get\nthe Python and C together. And so if you have this problem where you have code split\nbetween Python and C, now not only do you have\nto package the C code, you have to build the C code. C doesn't have a package manager, right? C doesn't have a dependency versioning management system, right? And so I'm not experiencing\nthe state-of-the-art and all the different\nPython package managers, but my understanding is that's a massive part of the problem. And I think Mojo solves that part of the problem\ndirectly heads on. Now, one of the things I think\nwe'll do with the community, and this isn't, again, we're not solving all the\nworld's problems at once, we have to be kinda focused, start with, is that I think that we\nwill have an opportunity to reevaluate packaging, right? And so I think that we can\ncome back and say, okay, well, given the new tools and technologies and the cool things we\nhave that we've built up, because we have not just syntax we have an entirely new compiler stack that works in a new way, maybe there's other innovations\nwe can bring together and maybe we can help solve that problem. - So almost a tangent to that question from the user perspective of packages. It was always surprising to\nme that it was not easier to sort of explore and find packages, you know, with, with PIP install. It's an incredible ecosystem. It's huge. It's just interesting that it wasn't made. It's still, I think, not made easier to discover\npackages to do, yeah. like search and discovery\nas YouTube calls it. - Well, I mean, it is kinda funny because this is one of the challenges of these like intentionally\ndecentralized communities. And so-\n- Yeah. - I don't know what the\nright answer is for Python. I mean, there are many people that I don't even know\nthe right answer for Mojo. Like, so there are many\npeople that would have much more informed opinions than I do, but it's interesting, if\nyou look at this, right? Open source communities,\nyou know, there's Git. Git is a fully de decentralized and anybody can do it any way they want, but then there's GitHub, right? And GitHub centralized\ncommercial in that case, right? Thing really helped pull together and help solve some of\nthe discovery problems and help build a more\nconsistent community. And so maybe there's opportunities for- - There's something like a GitHub for-\n- Yeah. - Although even GitHub,\nI might be wrong on this, but the search and discovery\nfor GitHub is not that great. Like, I still use Google search. - Yeah, well, I mean, maybe that's because GitHub doesn't wanna replace Google search, right? I think there is room\nfor specialized solutions to specific problems,\nbut sure, I don't know. I don't know the right\nanswer for GitHub either. They can go figure that out. - But the point is to have\nan interface that's usable, that's successful to people\nof all different skill levels and-\n- So, well, and again, like what are the benefit\nof standards, right? Standards allow you to build\nthese next level-up ecosystem and next level-up infrastructure\nand next level-up things. And so, again, come back\nto, I hate complexity, C+ Python is complicated. It makes everything more\ndifficult to deal with. It makes it difficult to\nport, move code around, work with all these things\nget more complicated. And so, I mean, I'm not an expert, but maybe Mojo can help a little bit by helping reduce the amount\nof C in this ecosystem and make it therefore scale better. - So any kinda packages\nthat are hybrid in nature would be a natural fit\nto move to Mojo, which- - Which is a lot of them, by the way. - Yeah. - So a lot of them, especially that are doing\nsome interesting stuff computation wise.\n- Yeah, yeah. Let me ask you about some features. - Yeah. - So we talked about\nobviously indentation, that it's a typed language\nor optionally typed. Is that the right way to say it? - It's either optional\nor progressively or- - Progressively, okay. - I think the... So people have very strong opinions on the right word to use.\n- Yeah. - [Chris] I don't know. - I look forward to your letters. So there's the var versus\nlet, but let is for constance. - Yeah. - Var is an optional. - Yeah, var makes it\nmutable. So you can reassign. - Okay. Then there's function overloading. - Oh okay, yeah. - I mean, there's a lot of\nsource of happiness for me, but function overloading, that's, I guess, is that for performance or is that... Why does Python not have\nfunction overloading? - So I can speculate. So\nPython is a dynamic language. The way it works is that\nPython and Objective-C are actually very similar\nworlds if you ignore syntax. And so Objective-C is straight\nline derived from Smalltalk, a really venerable interesting language that much of the world\nhas forgotten about, but the people that remember\nit love it generally. And the way that Smalltalk works is that every object has a dictionary in it. And the dictionary maps\nfrom the name of a function or the name of a value within an object to its implementation. And so the way you call a method\nand Objective-C is you say, go look up, the way I call\nfoo is I go look up foo, I get a pointer to the function\nback, and then I call it, okay, that's how Python works, right? And so now the problem with that is that the dictionary\nwithin a Python object, all the keys are strings\nand it's a dictionary. Yeah. So you can only have one\nentry per name. You think. - It's as simple as that. - I think it's as simple as that. And so now why do they never fix this? Like, why do they not change it to not be a dictionary anymore, they not change, like do other things? - Well, you don't really have to in Python because it's dynamic. And so you can say, I get\ninto the function now, if I got past an integer,\ndo some dynamic test for it, if it's a string, go do another thing. There's another additional challenge, which is even if you did support\noverloading, you're saying, okay, well, here's a version\nof a function for integers and a function for strings. Well, even if you could\nput it in that dictionary, you'd have to have the\ncollar do the dispatch. And so every time you call the function, you'd have to say like, is it\nan integer or is it a string? And so you'd have to figure\nout where to do that test. And so in a dynamic language, overloading is something you, general, you don't have to have. But now you get into a type\nlanguage and, you know, in Python, if you\nsubscript with an integer, then you get typically one\nelement out of a collection. If you subscript with a range, you get a different thing out, right? And so often in type languages, you'll wanna be able to\nexpress the fact that, cool, I have different behavior, depending on what I actually\npass into this thing. And if you can model that, it can make it safer and\nmore predictable and faster, and, like, all these things. - It somehow feels safer, yes, but also feels empowering,\nlike in terms of clarity. Like you don't have to design\nwhole different functions. - Yeah, well, and this is\nalso one of the challenges with the existing Python typing\nsystems is that in practice, like you take subscript, like in practice, a lot of these functions, they don't have one signature, right? They actually have different\nbehavior in different cases. And so this is why it's difficult to like retrofit this\ninto existing Python code and make it play well, with typing. You kinda have to design for that. - Okay, so there's a interesting\ndistinction that people that program Python might be\ninterested in is def versus fn. So it's two different\nways to define a function. - Yep. - And fn is a stricter version of def. What's the coolness that\ncomes from the strictness? - So here you get into, what is the trade-off with the superset? - Yes. - Okay, so superset, you have to, or you really want to be compatible. Like, if you're doing a superset, you've decided compatibility\nwith existing code is the important thing, even if some of the decisions they made were maybe not what you'd choose. - Yeah, okay. - So that means you put a lot\nof time into compatibility and it means that you get locked\ninto decisions of the past, even if they may not have\nbeen a good thing, right? Now, systems programmers\ntypically like to control things, right, and they wanna\nmake sure that, you know, not all cases of course, and even systems programmers\nare not one thing, right, but often you want predictability. And so one of the things that Python has, for example, as you know, is\nthat if you define a variable, you just say, X equals four,\nI have a variable name to X. Now I say some long method,\nsome long name equals 17, print out some long name,\noops, but I typoed it, right? Well, the compiler, the Python compiler\ndoesn't know in all cases what you're defining\nand what you're using, and did you typo the use of\nit or the definition, right? And so for people coming\nfrom type languages, again, I'm not saying they're right or wrong, but that drives 'em crazy\nbecause they want the compiler to tell them, you type out\nthe name of this thing, right? And so what fn does is\nit turns on, as you say, it's a strict mode and so it says, okay, well, you have to actually declare, intentionally declare your\nvariables before you use them. That gives you more predictability, more error checking and things like this, but you don't have to use it. And this is a way that\nMojo is both compatible 'cause defs work the same way that defs have already always worked, but it provides a new alternative that gives you more control. And it allows certain kinds of people that have a different philosophy to be able to express that and get that. - But usually if you're\nwriting Mojo code from scratch, you'll be using fn. - It depends, again, it depends\non your mentality, right? It's not that def is\nPython and fn is Mojo. Mojo has both, and it loves both, right? It really depends on that is\njust strict. Yeah, exactly. Are you playing around and\nscripting something out? Is it a one-off throwaway script? Cool. Like, Python is great at that. - I'll still be using fn, but yeah. - Well, so I love strictness. Okay. - Well, so control, power. You\nalso like suffering, right? Yes, go hand in hand. - How many pull-ups? - I've lost count at this. Yeah, exactly. At this point. - So, and that's cool. I love you for that. Yeah. And I love other people who\nlike strict things, right, but I don't want to say\nthat that's the right thing because python's also very beautiful for hacking around and\ndoing stuff in research and these other cases where\nyou may not want that. - You see, I just feel like\nmaybe I'm wrong in that, but it feels like strictness\nleads to faster debugging. So in terms of going from, even on a small project from\nzero to completion, it just, I guess it depends how many\nbugs you generate usually. Yeah. - Well, so I mean, if it's again, lessons learned in\nlooking at the ecosystem, it's really, I mean, I think it's, if you study some of\nthese languages over time, like the Ruby community for example, now Ruby is a pretty well, developed, pretty established community, but along their path they\nreally invested in unit testing. Like, so I think that\nthe Ruby community is really pushed forward the\nstate-of-the-art of testing because they didn't have a type system that caught a lot of bugs\nat compile time, right? And so you can have the\nbest of both worlds. You can have good testing\nand good types, right, and things like this, but I thought that it\nwas really interesting to see how certain challenges get solved. And in Python, for example, the interactive notebook\nkind of experiences and stuff like this are really amazing. And if you typo something,\nit doesn't matter. It just tells you it's fine, right? And so I think that the\ntrades are very different if you're building a, you know, large scale production system versus you're building\nan exploring a notebook. - And speaking of control, the hilarious thing, if you look at code, I write just for myself, for fun, it's like littered with\nasserts everywhere, okay? - It's a kinda, well, then. - Yeah, you would like text. - It's basically saying\nin a dictatorial way, this should be true now,\notherwise everything stops. - Well, and that is the sign. And I love you, man, but that is a sign of\nsomebody who likes control. And so, yes.\n- Yeah. - I think that you'll like\nf i this turning into a, I think I like Mojo. - Therapy session. Yes. I definitely will. Speaking of asserts\nexceptions are called errors. Why is it called errors? - So we, I mean, we use the same, we're the same as Python, right, but we implement it a\nvery different way, right? And so if you look at other languages, like we'll pick on C++\nour favorite, right? C++ has a thing called zero-cost\nexception handling, okay? So, and this is in my opinion, something to learn lessons from. - It's a nice polite way of saying it. - And so, zero-cost exception handling, the way it works is that\nit's called zero-cost because if you don't throw an exception, there's supposed to be no\noverhead for the non-error code. And so it takes the error\npath out of the common path. It does this by making throwing an error extremely expensive. And so if you actually throw an error with a C++ compiler using exceptions has to go look up in tables on the side and do all this stuff. And so throwing an error\ncan be like 10,000 times more expensive than referring\nfrom a function, right? Also, it's called zero-cost exceptions, but it's not zero-cost by any\nstretch of the imagination because it massively blows\nout your code, your binary, it also adds a whole\nbunch of different paths because of disrupts and other things like that that exist in C++ plus, and it reduces the number\nof optimizations it has, like all these effects. And so this thing that was\ncalled zero-cost exceptions, it really ain't, okay. Now if you fast forward to newer languages and this includes Swift and\nRust and Go and now Mojo, well, and Python's a little bit different because it's interpreted and so like, it's got a little bit of a\ndifferent thing going on. But if you look at it, if you\nlook at compiled languages, many newer languages say, okay, well, let's not do that zero-cost\nexception handling thing. Let's actually treat and throwing an error the same as returning a variant returning either the\nnormal result or an error. Now programmers generally\ndon't want to deal with all the typing machinery and like\npushing around a variant. And so you use all the\nsyntax that Python gives us, for example, try and\ncatch and it, you know, functions that raise and things like this. You can put a raises decorator on your functions, stuff like this. And if you wanna control that, and then the language can\nprovide syntax for it. But under the hood, the way\nthe computer executes it, throwing an error is basically as fast as returning something. - Oh, interesting. So it's exactly the same way\nfrom a compile perspective. - And so this is actually, I mean, it's a fairly nerdy thing,\nright, which is why I love it, but this has a huge impact on the way you design your APIs, right? So in C++ huge communities\nturn off exceptions because the cost is just so high, right? And so the zero-cost\ncost is so high, right? And so that means you can't\nactually use exceptions in many libraries, right? Interesting. Yeah. And even for the people\nthat do use it, well, okay, how and when do you wanna pay the cost? If I try to open a file,\nshould I throw an error? Well, what if I'm probing around, looking for something, right, and I'm looking it up\nin many different paths? Well, if it's really slow to do that, maybe I'll add another function\nthat doesn't throw an error or turns in error code instead. And now I have two different\nversions of the same thing. And so it causes you to fork your APIs. And so, you know, one of the things I learned\nfrom Apple and I so love is the art of API design is\nactually really profound. I think this is something\nthat Python's also done a pretty good job at in\nterms of building out this large scale package ecosystem. It's about having standards\nand things like this. And so, you know, we wouldn't wanna enter\na mode where, you know, there's this theoretical\nfeature that exists in language, but people don't use it in practice. Now I'll also say one of\nthe other really cool things about this implementation approach is that it can run on GPUs and it can run on accelerators\nand things like this. And that standard\nzero-cost exception thing would never work on an accelerator. And so this is also part of how Mojo can scale all the way down to\nlike little embedded systems and to running on GPUs\nand things like that. - Can you actually say about the... Maybe is there some high-level\nway to describe the challenge of exceptions and how they work\nin code during compilation? So it's just this idea of\npercolating up a thing and error. - Yeah, yeah. So the way to think about it is, think about a function that\ndoesn't return anything, just as a simple case, right? And so you have function\none calls function two, calls function three, calls function four, along that call stack that\nare tribe blocks, right? And so if you have function\none calls function two, function two has a tribe block, and then within it it calls\nfunction three, right? Well, what happens if\nfunction three throws? Well, actually start simpler.\nWhat happens if it returns? Well, if it returns, it's supposed to go back\nout and continue executing and then fall off the\nbottom of the tribe block and keep going and it all's good. If the function throws, you're supposed to exit\nthe current function and then get into the\naccept clause, right, and then do whatever codes there and then keep falling on and going on. And so the way that a\ncompiler like Mojo works is that the call to that function, which happens in the accept\nblock calls the function, which happens in the accept\nblock calls the function, and then instead of returning\nnothing, it actually returns, you know, an a variant\nbetween nothing and an error. And so if you return,\nnormally fall off the bottom, or do return, you return nothing. And if you throw, throw an error, you return the variant. That is, I'm an error, right? So when you get to the call, you say, okay, cool, I called a function. Hey, I know locally I'm\nin a tribe block, right? And so I call the function and then I check to see what it returns. Aha. Is that error thing\njump to the accept block. - And that's all done for\nyou behind the scenes. - Exactly. And so the competitor\ndoes all this for you. And I mean, one of the things, if you dig into how this\nstuff works in Python, it gets a little bit more complicated because you have finally blocks, which you need to go into do some stuff, and then those can also throw and return. - Wait, What? Nested? - Yeah, and like the stuff\nmatters for compatibility. Like, there's really- - Can nest them. - there's with clauses,\nand so with clauses, are kinda like finely blocks with some special stuff going on. And so there's nesting. - In general, nesting of anything, nesting of functions should be illegal. Well, it just feels like it\nadds a level of complexity. - Lex, I'm merely an implementer. And so this is again, one last question. One of the trade-offs you\nget when you decide to build a superset is you get to\nimplement a full fidelity implementation of the thing\nthat you decided is good. And so, yeah, I mean, we can complain about\nthe reality of the world and shake our fist, but- - It always feels like you\nshouldn't be allowed to do that. Like, to declare functions\nin certain functions inside functions, that seems-\n- Oh, wait, wait, wait. What happened to Lex, the Lisp guy? - No, I understand that, but Lisp is what I used to do in college. - So now you've grown up. - You know, we've all done things in college we're not proud of. No, wait a sec, wait a sec.\nI love Lis, I love Lis. - Okay. Yeah, I was gonna say, you're afraid of me\nirritating the whole internet. - Like yeah, no, I love Lisp. It worked as a joke in my\nhead and come out, right? - So nested functions are, joking aside, actually really great and\nfor certain things, right? And so these are also called closures. Closures are pretty cool\nand you can pass callbacks. There's a lot of good patterns. And so- - So speaking of which, I don't think you have nested function implemented yet in Mojo. - We don't have Lambda\nsyntax, but we do have Nest. - Lambda syntax nested. - Functions. Yeah. - There's a few things on\nthe roadmap that you have that it'd be cool to\nsort of just fly through, 'cause it's interesting to see, you know, how many features there are\nin a language small and big. Yep. They have to implement. Yeah. So first of all there's Tuple support, and that has to do with some\nof their specific aspect of it, like the parentheses or\nnot parenthesis that Yeah. - This is just a totally\na syntactic thing. - A syntactic thing, okay. There's, but it is cool. It's still so keyword\narguments and functions. - Yeah, so this is where in Python, you can say call function X equals four and X is the name-\n- Yeah. - of the argument. That's a nice sort of documenting\nsalt documenting feature. Yep. - Yeah, I mean, and again, this isn't rocket science\nto implement this, just the laundry list. - It's just on the list. The bigger features\nare things like traits. So traits are when you\nwanna define abstract. So when you get into typed languages, you need the ability to write generics. And so you wanna say, I wanna write this function\nand now I want to work on all things that are arithmetic. Like, well, what does\narithmetic like, mean? Well, arithmetic like is a categorization of a bunch of types. Again, you can define many different ways, and I'm not gonna go into ring\ntheory or something, but the, you know, you can say it's arithmetic. Like if you can add, subtract, multiply, divide it for example, right? And so what you're saying is\nyou're saying there's a set of traits that apply to\na broad variety of types. And so they're all these types arithmetic, like, all these tensors\nand floating point integer and, like, there's this\ncategory of of types. And then I can define on an\northogonal access algorithms that then work against types\nthat have those properties. It's been implemented in Swift\nand Rust in many languages. So it's not Haskell,\nwhich is where everybody learns their tricks from, but we need to implement that, and that'll enable a new\nlevel of expressivity. - So classes. - Yeah, classes are a big deal. - It's a big deal still to be implemented. Like you said, Lambda syntax, and there's,, like, detailed stuff, like whole module import support for top-level code and file scope. And then global variables also. So being able to have\nvariables outside of a top level.\n- Well, and so this comes back to\nthe where Mojo came from, and the fact that this is your 0.1, right? So Modular's building an AI stack, right? And an AI stack has a\nbunch of problems working with hardware and writing\nhigh-performance kernels and doing this kernel fusion\nthing I was talking about, and getting the most out of the hardware. And so we've really\nprioritized and built Mojo to solve Modular's problem. Right now our North Star is built out to support all the things. And so we're making incredible progress. By the way, Mojo's only,\nlike, seven months old. So that's another interesting thing. - Well, I mean part of the\nreason I wanted to mention some of these things is\nlike, there's a lot to do and it's pretty cool how you just kinda, sometimes you take for granted how much there is in a\nprogramming language, how many cool features you kinda rely on. And this is kinda a nice reminder when you lay it as its do list. - Yeah and so, I mean,\nbut also you look into, it's amazing how much is\nalso there and you take it for granted that a\nvalue, if you define it, it will get destroyed automatically. Like, that little feature itself is actually really complicated given the way the ownership\nsystem has to work. And the way that works within\nMojo is a huge step forward from what Rust and Swift have done. - Wait, can you say that again? When value-\n- Yeah. When you define it gets\ndestroyed automatically. - Yeah. So like, like say\nyou have a string, right? So you define a string on the stack. Okay. Or on whatever that means, like in your local function, right? And so you say like whether it be in a def and so you just say X\nequals hello world, right? Well, if your strength type\nrequires you to allocate memory, then when it's destroyed,\nyou have to deallocate it. So in Python and in Mojo, you define that with a Dell method, right? Where does that get run? Well, it gets run sometime\nbetween the last use of the value and the end of the program. Like in this, you now get\ninto garbage collection, you get into, like,\nall these long debated, you talk about religions and trade-offs and things like this. This is a hugely hotly contested world. If you look at C++, the way this works is that\nif you define a variable or a set of variables within a function, they get destroyed in a\nlast in, first out order. So it's like nesting, okay. This has a huge problem\nbecause if you have a big scope and you define a whole\nbunch of values at the top and then you use 'em and then you do a whole bunch of code\nthat doesn't use them, they don't get destroyed until the very end of that scope, right? And so this also destroys tail calls. So good functional programming, right? This has a bunch of different\nimpacts on, you know, you talk about reference counting optimizations and things like this. A bunch of very low-level things. And so what Mojo does is\nit has a different approach on that from any language\nI'm familiar with, where it destroys them\nas soon as possible. And by doing that you\nget better memory use, you get better predictability,\nyou get tail calls that work, you get a bunch of other things, you get better ownership tracking. There's a bunch of\nthese very simple things that are very fundamental that are already built in there in Mojo today that are the things that\nnobody talks about generally, but when they don't work right, you find out and you\nhave to complain about. - Is it trivial to know\nwhat's the soonest possible to delete a thing that it's\nnot gonna be used again? - Yeah. Well, I mean,\nit's generally trivial. It's after the last use of it. So if you just find X as a string and then you have some use\nof X somewhere in your code- - Within that scope, you mean, within the scope that is accessible? - It's, yeah, exactly. So you can only use something\nwithin its scope. Yeah. And so then it doesn't wait\nuntil the end of the scope to delete it, it destroys\nit after the last use. - So there's kinda some very eager machine that's just sitting\nthere and deleting. Yeah. - And it's all in the compiler. So it's not at runtime,\nwhich is also cool. And so interesting. Yeah. And this is actually non-trivial because you have control flow, right? And so it gets complicated pretty quickly. And so like angst, right? Was not, not. - Well, so you have to insert delete, like in a lot of places. - Potentially. Yeah, exactly. So the compiler has to reason about this. And this is where again, it's experience building languages and not getting this right. So again, you get another chance to do it and you get basic things like this, right? But it's extremely powerful\nwhen you do that, right? And so there's a bunch\nof things like that, that kinda combine together. And this comes back to the, you get a chance to do it the right way, do it the right way, and make sure that every brick\nyou put down is really good. So that when you put\nmore bricks on top of it, they stack up to something\nthat's beautiful. - Well, there's also, like, how many design discussions\ndo there have to be about particular details\nlike implementation of particular small features? Because the features that seem small, I bet some of them might\nbe like really require really big design decisions. - Yeah. Well, so I mean, lemme give\nyou another example of this. Python has a feature called async/await. So it's a new feature. I mean, in the long arc of Python history, it's a relatively new feature, right, that allows way more expressive,\nasynchronous programming. Okay? Again, this is a\nPython's a beautiful thing. And they did things that are great for Mojo for completely different reasons. The reason that async/await\ngot added to Python, as far as I know, is because Python doesn't\nsupport threads, okay? And so Python doesn't support threads, but you wanna work with networking and other things, like, that can block. I mean, Python does support threads, it's just not its strength. And so they added this\nfeature called async/await. It's also seen in other\nlanguages like Swift and JavaScript and many\nother places as well. Async/await and Mojo is amazing 'cause we have a high-performance, heterogeneous compute\nruntime underneath the covers that then allows non-blocking I/O so you get full use of your accelerator. That's huge. Turns out it's actually\nreally an important part of fully utilizing the machine. You talk about design discussions, that took a lot of discussions, right? And it probably will\nrequire more iteration. And so my philosophy with\nMojo is that, you know, we have a small team of really good people that are pushing forward\nand they're very good at the extremely deep knowing how the compiler and runtime and, like, all the low-level\nstuff works together, but they're not perfect. It's the same thing as\nthe Swift team, right? And this is where one of\nthe reasons we released Mojo much earlier is so we can get feedback and we've already like renamed a keyword data community\nfeedback, which one? We use an ampersand now it's named in out. We're not renaming existing Python keyword 'cause that breaks compatibility, right? We're renaming things. We're adding and making sure\nthat they are designed well. We get usage experience, we iterate and work with the community. Because again, if you\nscale something really fast and everybody writes all their code and they start using it in production, then it's impossible to change. And so you wanna learn from people. You wanna iterate and\nwork on that early on. And this is where design discussions, it's actually quite important to do. - Could you incorporate an emoji, like into the language,\ninto the main language? Like a good... Like do you have a favorite one? - Well, I really, like in terms of humor, like rofl, whatever, rolling\non the floor laughing. So that could be like a, what would that be the use case for that? Like an except throw an\nexception of some sort. I don't- - You should totally\nfile a feature request. - Or maybe a heart one.\nIt has to be a heart one. - People have told me that\nI'm insane. I'm liking this. - I'm gonna use the viral\nnature of the internet to get this passed. - I mean, it's funny you come back to the flame emoji file extension, right? You know, we have the option\nto use the flame emoji, which just even that\nconcept, 'cause for example, the people at GitHub say,\nnow I've seen everything. You know, like. - Yeah, and there's something, it kinda, it's reinvigorating. It's like, oh, that's possible. That's really cool that for some reason that makes everything else,\nlike, seem really excited. - I think the world is\nready for this stuff, right? And so, you know, when we\nhave a package manager, we'll clearly have to innovate by having the compiled package thing be the little box with the bow on it, right? I mean, it has to be done. - It has to be done. Is there some stuff on the roadmap that you're particularly stressed about, or excited about that\nyou're thinking about? - A lot, I mean, as of today's snapshot, which will be obsolete tomorrow, the lifetime stuff is really exciting. And so lifetimes give you safe references to memory without dangling pointers. And so this has been done in\nlanguages like Rust before. And so we have a new approach,\nwhich is really cool. I'm very excited about that. That'll be out to the community very soon. The traits feature is really a big deal. And so that's blocking\na lot of API design. And so there's that. I think\nthat's really exciting. A lot of it is these kinda\ntable stakes features. One of the things that is again, also lessons learned with\nSwift is that programmers in general like to add syntactic sugar. And so it's like, oh\nwell, this annoying thing, like in Python, you have to\nspell Underbar armbar ad. Why can't I just use plus def plus? Come on. Why can't I just do that, right? And so trivial bit of syntactic sugar. It makes sense, it's\nbeautiful, it's obvious. We're trying not to do that. And so for two different\nreasons, one of which is that, again, lesson learned with Swift. Swift has a lot of syntactic sugar, which may may be a good thing,\nmaybe not, I don't know. But because it's such an easy\nand addictive thing to do, sugar, like make sure\nblood get crazy, right? Like, the community will really dig into that and wanna do a lot of that. And I think it's very distracting from building the core abstractions. The second is we wanna be a good member of the Python community, right? And so we wanna work with\nthe broader Python community and yeah, we're pushing forward a bunch of systems programming features and we need to build them\nout to understand them. But once we get a long ways forward, I wanna make sure that we go\nback to the Python community and say, okay, let's\ndo some design reviews. Let's actually talk about this stuff. Let's figure out how we want this stuff all to work together. And syntactic sugar just makes\nall that more complicated. So. - And yeah, list comprehension. Is that yet to be implemented? Yeah. And my favorite d I mean, I dictionaries. - Yeah, there's some basic 0.1. - 0.1, yeah. - But nonetheless, it's actually still quite\ninteresting and useful. - As you've mentioned,\nModular is very new. Mojo is very new. It's\na relatively small team. Yeah. It's building up this. - Yeah, we're just gigantic stack. Yeah. This incredible stack that's\ngoing to perhaps define the future of development\nof our AI overlords. - We just hope it will be useful. - As do all of us. So what have you learned from this process of building up a team? Maybe one question is how do you hire-\n- Yeah. - great programmers, great people that operate\nin this compiler hardware, machine learning, software\ninterface design space? And maybe are-\nYeah. - a little bit fluid in what they can do. - So, okay, so language design too. - So building a company\nis just as interesting in different ways is building a language, like different skill\nsets, different things, but super interesting. And I've built a lot of teams,\na lot of different places. If you zoom in from the big\nproblem into recruiting, well, so here's our problem, okay. I'll be very straightforward about this. We started Modular with\na lot of conviction about we understand the problems, we understand the customer pain points. We need to work backwards from the suffering in the industry. And if we solve those problems, we think it'll be useful for people. But the problem is that the people we need to hire, as you say, are all these super specialized people that have jobs at big tech,\nbig tech worlds, right? And, you know, I don't think we have product market fit in the way that a normal startup does, or we don't have product\nmarket fit challenges because right now everybody's using AI and so many of them are\nsuffering and they want help. And so again, we started\nwith strong conviction. Now again, you have to\nhire and recruit the best and the best all have jobs. And so what we've done\nis we've said, okay, well, let's build an amazing culture. Start with that. That's usually not something\na company starts with. Usually you hire a bunch of people and then people start fighting and it turns into gigantic mess. And then you try to figure out how to improve your culture later. My co-founder, Tim in particular, is super passionate about\nmaking sure that that's right. And we've spent a lot of time, early on, to make sure that we can scale. - Can you comment... Sorry, before we get to the second, what makes for a good culture?\n- Yeah, so, I mean, there's many different\ncultures and I have learned many things from many different people, several very unique, almost\nfamously unique cultures. And some of them I learned what to do and some of them I learned\nwhat not to do. Yep. Okay. And so we want an inclusive culture. I believe in like amazing\npeople working together. And so I've seen cultures\nwhere you have amazing people and they're fighting each other. I see amazing people and\nthey're told what to do, like doubt. Shout line\nup and do what I say, it doesn't matter if it's\nthe right thing, do it right. And neither of these is the... and I've seen people\nthat have no direction. They're just kinda floating\nin different places and they wanna be amazing,\nthey just don't know how. And so a lot of it starts with\nhave a clear vision, right? And so we have a clear\nvision of what we're doing. And so I kind of grew up at Apple in my engineering life, right? And so a lot of the Apple\nDNA rubbed off on me. My co-founder Tim also is\nlike a strong product guy. And so what we learned is, you know, I saw at Apple that you don't work from building cool technology. You don't work from, like, come up with cool product\nand think about the features you'll have in the big check\nboxes and stuff like this. 'Cause if you go talk to customers, they don't actually\ncare about your product, they don't care about your technology. What they care about is\ntheir problems, right? And if your product can\nhelp solve their problems, well, hey, they might be\ninterested in that, right? And so if you speak to\nthem about their problems, if you understand you have compassion, you understand what\npeople are working with, then you can work backwards to\nbuilding an amazing product. - So the vision's done\nby defining the problem. - And then you can work\nbackwards in solving technology. Got it. And at Apple, like it's, I think pretty famously said\nthat, you know, for every, you know, there's a\nhundred nos for every yes. I would refine that to say that there's a hundred\nnot yets for every yes. Yeah. But famously, if you\ngo back to the iPhone, for example, right? iPhone 1, every, I mean, many people laughed at it\nbecause it didn't have 3G, it didn't have copy and paste, right? And then a year later,\nokay, finally it has 3G, but it still doesn't have\ncopy and paste, it's a joke. \"Nobody will ever use this product,\" blah, blah, blah, blah,\nblah, blah, blah, right? Well, year three, had copy and paste, and people stopped\ntalking about it, right? And so, being laser focused\nand having conviction and understanding what the core problems are and giving the team\nthe space to be able to build the right tech\nis really important. Also, I mean, you come back to recruiting, you have to pay well, right? So we have to pay\nindustry leading salaries and have good benefits\nand things like this. That's a big piece. We're a remote-first\ncompany. And so we have to... So remote-first has a very\nstrong set of pros and cons. On the one hand, you can hire\npeople from wherever they are, and you can attract amazing talent even if they live in strange\nplaces or unusual places. On the other hand, you have time zones. On the other hand, you have, like, everybody on the internet will fight if they don't understand each other. And so we've had to learn\nhow to like have a system where we actually fly people in and we get the whole company\ntogether periodically, and then we get work groups together and we plan and execute together. - And there's like an intimacy to the in-person brainstorming. Yeah, I guess you lose,\nbut maybe you don't. Maybe if you get to know each other well, and you trust each other,\nmaybe you can do that. Yeah. - Well, so when the\npandemic first hit, I mean, I'm curious about your experience too. The first thing I missed\nwas having whiteboards, right?\n- Yeah. - Those design discussions\nwhere you're like, I can high, high intensity\nwork through things, get things done, work through\nthe problem of the day, understand where you're on, figure out and solve the\nproblem and move forward. But we've figured out ways-\n- Yeah. - to work around that now with, you know, all these screen sharing and other things like that that we do. The thing I miss now is sitting down at a lunch table with the team. Yeah. The spontaneous things\nlike the coffee bar things and the bumping into each other\nand getting to know people outside of the transactional\nsolve a problem over Zoom. - And I think there's just a lot of stuff that I'm not an expert at this. I don't know who is,\nhopefully there's some people, but there's stuff that\nsomehow is missing on Zoom. Even with the Y board,\nif you look at that, if you have a room with one\nperson at the whiteboard, and then there's like three\nother people at a table, there's a, first of all, there's a social aspect to\nthat where you're just shooting the a little bit, almost like. - Yeah, as people are just\nkinda coming in and Yeah. - That, but also while the, like it's a breakout discussion that happens for like seconds at a time, maybe an inside joke or like\nthis interesting dynamic that happens that's Zoom. - And you're bonding. Yeah. - You're bonding, you're bonding. But through that bonding,\nyou get the excitement. There's certain ideas are like complete. And you'll see that in the faces of others that you won't see necessarily\non Zoom and like something, it feels like that should be possible to do without being in-person. - Well, I mean, being in person\nis a very different thing. Yeah. It's worth it, but\nyou can't always do it. And so again, we're still learning. Yeah. And we're also learning as like humanity with\nthis new reality, right? But what we found is that\ngetting people together, whether it be a team or the whole company or whatever is worth the expense because people work together\nand are happier after that. Like, it just, like, there's a massive period\nof time where you're like, go out and things, start getting frayed, pull people together, and then yeah, you realize that we're\nall working together, we see things the same way. We work through the disagreement\nor the misunderstanding. We're talking across each other and then you work much better together. And so things like that I think\nare really quite important. - What about people that\nare kinda specialized in very different aspects of\nthe stack working together? What are some interesting\nchallenges there? - Yeah, well, so I mean, I mean, there's lots of interesting\npeople, as you can tell, I'm, you know, hard to deal with too, but- - You're one of the most lovable people. - So there's different philosophies in building teams for me. And so some people say\nhire 10x programmers, and that's the only thing,\nwhatever that means, right? What I believe in is\nbuilding well-balanced teams, teams that have people\nthat are different in them. Like if you have all\ngenerals and no troops or all troops and no generals, or you have all people\nthat think in one way and not the other way, what you get is you get\na very biased and skewed and weird situation where\npeople end up being unhappy. And so what I like to do is I\nlike to build teams of people where they're not all the same. You know, we do have\nteams and they're focused on like runtime, or compiler GP, or accelerator, or\nwhatever the specialty is, but people bring a different take and have a different perspective. And I look for people that\ncompliment each other. And particularly if you look at leadership teams and things like this, you don't want everybody\nthinking the same way. You want people bringing different perspectives and experiences. And so I think that's really important. - That's team. But what about building a\ncompany as ambitious as Modular? So what are some\ninteresting questions there? - Oh, I mean, so many. Like, so one of the things I love about... Okay, so Modular's the first\ncompany I built from scratch. One of the first things\nthat was profound was I'm not cleaning up\nsomebody else's mess, right? And so if you look at, and. - That's liberating to some degree. - It's super liberating. And also many of the projects\nI've built in the past have not been core to the\nproject of the company. Swift is not Apple's product, right? MLIR is not Google's revenue\nmachine or whatever, right? It's important, but it's like working on\nthe accounting software for, you know, the retail\ngiant or something, right? It's like enabling\ninfrastructure and technology. And so at Modular, the tech we're building is here\nto solve people's problems. Like, it is directly the thing\nthat we're giving to people. And so this is a really big difference. And what it means for me as a leader, but also for many of our engineers, is they're working on\nthe thing that matters. And that's actually pretty, I mean, again, for compiler people and things like that, that's usually not the case, right? And so that's also pretty\nexciting and quite nice, but one of the ways that this manifests is it makes it easier to make decisions. And so one of the challenges I've had in other worlds is it's like, okay, well, community matters somehow for the goodness of the world, or open source matters theoretically, but I don't wanna pay for a t-shirt. Yeah. right, or some swag, like, well, t-shirts cost 10 bucks each. You can have 100 t-shirts\nfor $1,000 to a Megacorp, but $1,000 is unaccountably\ncan't count that low. Yes. Right. But justifying it and getting\na t-shirt, by the way, if you'd like a t-shirt,\nI can give you a t-shirt. - Well, I would 100% like a t-shirt. Are you joking? - You can have a fire\nemoji t-shirt. Is that- - I will treasure this.\nIs that a good thing? I will pass it down to my grandchildren. - And so, you know, it's very liberating to be able to decide. I think that Lex should\nhave a T-shirt, right? And it becomes very\nsimple because I like Lex. - This is awesome. So I have to ask you about\none of the interesting developments with large language models is that they're able to\ngenerate code recently. Really? Well, yes. To a degree that maybe, I\ndon't know if you understand, but I struggle to understand\nbecause it forces me to ask questions about\nthe nature of programming, of the nature of thought\nbecause the language models are able to predict the kinda code I was about to write so well. Yep. That it makes me wonder\nlike how unique my brain is and where the valuable\nideas actually come from. Like, how much do I contribute\nin terms of ingenuity, innovation to code I write or\ndesign and that kinda stuff. When you stand on the shoulders of giants, are you really doing anything? And what LLMs are helping\nyou do is they help you stand on the shoulders of\ngiants in your program. There's mistakes. They're interesting that\nyou learn from, but I just, it would love to get your\nopinion first high level. Yeah. Of what you think about this\nimpact of large language models when they do program synthesis,\nwhen they generate code. - Yeah. Well, so I don't\nknow where it all goes. Yeah. I'm an optimist and I'm\na human optimist, right? I think that things I've seen\nare that a lot of the LLMs are really good at\ncrushing leak code projects and they can reverse the\nlink list like crazy. Well, it turns out\nthere's a lot of instances of that on the internet, and\nit's a pretty stock thing. And so if you want to see\nstandard questions answered, LMS can memorize all the answers,\nthen that can be amazing. And also they do generalize out from that. And so there's good work on that, but I think that if you, in my\nexperience, building things, building something like\nyou talk about Mojo, where you talk about these things, where you talk about\nbuilding an applied solution to a problem, it's also about\nworking with people, right? It's about understanding the problem. What is the product that you wanna build? What are the use case?\nWhat are the customers? You can't just go survey all the customers because they'll tell you that\nthey want a faster horse. Maybe they need a car, right? And so a lot of it comes into, you know, I don't feel like we have\nto compete with LLMs. I think they'll help automate a ton of the mechanical stuff out of the way. And just like, you know, I think we all try to\nscale through delegation and things like this, delegating rote things\nto an LLVM I think is an extremely valuable and approach that will help us all scale\nand be more productive. - But I think it's a\nfascinating companion, but. - I'd say I don't think that that means that we're gonna be done with coding. - Sure. But there's power in\nit as a companion and- - Yeah, absolutely. - So from there, I would love to zoom in\nonto Mojo a little bit. Do you think about that? Do you think about LMS\ngenerating Mojo code and helping sort of like, yeah. When you design new programming language, it almost seems like,\nman, it would be nice to, this sort of almost as a way to learn how I'm supposed to\nuse this thing for them to be trained on some of the Mojo code. - Yeah. So I do lead an AI company. So maybe there'll be a\nMojo LLM at some point. But if your question is like, how do we make a language to be suitable for LLMs?\n- Yeah. - I think the cool thing about LLMs is you don't have to, right? And so if you look at what is English or any of these other terrible languages that we as humans deal\nwith on a continuous basis, they're never designed for machines and yet they're the\nintermediate representation. They're the exchange\nformat that we humans use to get stuff done, right? And so these programming languages, they're an intermediate representation between the human and the computer or the human and the\ncompiler, roughly, right? And so I think the LMS\nwill have no problem learning whatever keyword we pick. - Maybe the fire emoji is gonna, oh. - Maybe that's gonna break\nit. It doesn't tokenize. - No, the reverse of that.\nIt will actually enable it. Because one of the issues I\ncould see with being a superset of Python is there will be\nconfusion about the gray area. So it'll be mixing stuff, but. - Well, I'm a human optimist.\nI'm also an LM optimist. I think that we'll solve that problem. But you look at that and you say, okay, well, reducing the rote thing, right? Turns out compilers are very particular and they really want the\nindentation to be right. They really want the colon\nto be there on your Els or else it'll complain, right? I mean, compilers can do better at this, but LMS can totally\nhelp solve that problem. And so I'm very happy about\nthe new predictive coding and co-pilot type features\nand things like this, because I think it'll all\njust make us more productive. - It's still messy and fuzzy\nand uncertain. Unpredictable. So, but is there a future you see, given how big of a leap\nGPT-4 was where you start to see something like LMS\ninside a compiler or no? - I mean, you could do\nthat. Yeah, absolutely. I mean, I think that would be interesting. - Is that wise? - Well, well, I mean, it\nwould be very expensive. So compilers run fast and\nthey're very efficient and LMS are currently very expensive. There's on-device LLMs and\nthere's other things going on. And so maybe there's an answer there. I think that one of the things that I haven't seen enough of is that, so LLMs to me are amazing when you tap into the creative potential\nof the hallucinations, right? And so if you're doing\ncreative brainstorming or creative writing or things like that, the hallucinations work in your favor. If you're writing code\nthat has to be correct 'cause you're gonna ship it in production, then maybe that's not actually a feature. And so I think that\nthere has been research and there has been work on building algebraic reasoning systems\nand kind of like figuring out more things that feel like proofs. And so I think that there\ncould be interesting work in terms of building more\nreliable at scale systems, and that could be interesting. But if you've chased\nthat rabbit hole down, the question then becomes, how do you express your\nintent to the machine? And so maybe you want\nLLLM to provide the spec, but you have a different kind of net that then actually\nimplements the code, right? So it's to use the\ndocumentation and inspiration versus the actual implementation. - Yeah.\n- Potentially. Since if successful Modular\nwill be the thing that runs, I say so jokingly, our AI overlords, but AI systems that are used across, I know it's a cliche term,\nbut internet of things. So across. - So I'll joke and say like,\nAGI should be written in Mojo. - Yeah. AGI should be written in Mojo. You're joking, but it's also possible\nthat it's not a joke that a lot of the ideas behind Mojo seems like the natural set of\nideas that would enable at scale training and\ninferences of AI systems. So it's just, I have to ask you about the\nbig philosophical question about human civilization. So folks like Eli Kowski\nare really concerned about the threat of AI. - Yeah. - Do you think about the good\nand the bad that can happen at scale deployment of AI systems? - Well, so I've thought a lot about it, and there's a lot of different\nparts to this problem, everything from job\ndisplacement to Skynet, things like this.\n- Yeah. - And so you can zoom into\nsub parts of this problem. I'm not super optimistic about\nAGI being solved next year. I don't think that's\ngonna happen personally. - So you have a kinda\nzen-like calm about... There's a nervousness because the leap of GPT-4 seems so big. - Sure, it's huge. - It's like there's some\nkinda transitionary period. You're thinking- - Well so I mean, there's a\ncouple of things going on there. One is I'm sure GPT-5 and 7 and 19 will be also huge leaps. They're also getting much\nmore expensive to run. And so there may be a limiting function in terms of just expense. On the one hand, train, like, that could be a limiter\nthat slows things down, but I think the bigger limiter outside of, like, Skynet takes over. And I don't spend any\ntime thinking about that, because if Skynet takes\nover and kills us all, then I'll be dead. So I don't worry about that. So, you know, I mean, that's just, okay. Other things worry about,\nI'll just focus on. I'll focus and not worry about that one. But I think that the other thing I'd say is that AI moves quickly, but humans move slowly\nand we adapt slowly. And so what I expect to happen is just like any technology diffusion, like the promise and then the application takes time to roll out. And so I think that I'm\nnot even too worried about autonomous cars defining\naway all the taxi drivers. Remember autonomy was\nsupposed to be solved by 2020. Yeah. - Boy, do I remember. - And so like, I think that on the one hand\nwe can see amazing progress, but on the other hand, we\ncan see that, you know, the reality is a little\nbit more complicated and it may take longer to roll\nout than you might expect. - Well, that's in the physical space. I do think in the digital spaces, the stuff that's built on top\nof LLMs that runs, you know, the millions of apps that\ncould be built on top of them, and that could be run\non millions of devices, millions of types of devices. - Yeah. - I just think that the rapid effect it has on human civilization could be truly transformative to it. - Yeah.\n- We don't even know. - Well, and so the predict well, and there I think it depends on, are you an optimist or a pessimist? Or a masochist?\n- Yeah. Just to clarify optimist\nabout human civilization. - Me too. And so I look at that as saying, okay, cool, well, AI do, right? And so some people say, \"Oh my god. Is it gonna destroy us all?\nHow do we prevent that?\" I kinda look at it from a, is\nit gonna unlock us all right? You talk about coding, is it gonna make so I don't have to do all the repetitive stuff? Well, suddenly that's a very\noptimistic way to look at it. And you look at what a lot of\nthese technologies have done to improve our lives, and\nI want that to go faster. - So what do you think the\nfuture of programming looks like in the next 10, 20, 30, 50 years? That alums, LLMs and\nwith Mojo, with Modular, like your vision for devices, the hardware to compilers to this, to the different stacks of software. - Yeah. Yeah. Well, so what I want, I mean, coming back to my arch nemesis, right? It's complexity, right? So again, me being the optimist, if we drive down complexity, we can make these tools,\nthese technologies, these cool hardware widgets accessible to way more people, right? And so what I'd love to see is more personalized\nexperiences, more things, the research getting into production instead of being lost\nin (indistinct) right? And so, and like these things that impact people's lives\nby entering products. And so one of the things that\nI'm a little bit concerned about is right now the big\ncompanies are investing huge amounts of money and\nare driving the top line of AI capability forward really quickly. But if it means that you\nhave to have $100 million to train a model or more\n$100 billion, right, well, that's gonna make\nit very concentrated with very few people in the world that can actually do this stuff. I would much rather see lots\nof people across the industry be able to participate\nand use this, right? And you look at this, you know, I mean, a lot of great research has\nbeen done in the health world and looking at like detecting pathologies and doing radiology with AI and\nlike doing all these things. Well, the problem today is that to deploy and build these systems, you have to be an expert in\nradiology and an expert in AI. And if we can break down the barriers so that more people can use AI techniques, and it's more like programming Python, which roughly everybody can\ndo if they want to, right, then I think that we'll get a\nlot more practical application of these techniques and a lot more nicher cool but narrower demands. And I think that's gonna be really cool. - Do you think we'll have\nmore or less programmers in the world than now? - Well, so I think we'll\nhave more programmers, but they may not consider\nthemselves to be programmers. - That'd be a different\nname for it, right? I mean, do you consider\nsomebody that uses, you know, I think that arguably the most popular programming language is Excel. - Yeah. - Right? Yep. And so do they consider\nthemselves to be programmers? Maybe not. I mean, some of them make crazy\nmacros and stuff like that, but what you mentioned Steve Job is, it's the bicycle for the mind that allows you to go faster, right? And so I think that as\nwe look forward, right? What is AI? I look at it as hopefully\na new programming paradigm. It's like object-oriented\nprogramming, right? If you wanna write a cat\ndetector, you don't use for loops. Turns out that's not the\nright tool for the job, right? And so right now, unfortunately, because I mean, it's not unfortunate, but it's just kinda where things are, AI is this weird different\nthing that's not integrated into programming languages\nand normal tool chains and all the technology is really weird and doesn't work, right? And you have to babysit it and every time you switch\nhardware, it's different. It shouldn't be that way. When you change that, when\nyou fix that, suddenly, again, the tools and technologies\ncan be way easier to use. You can start using them\nfor many more things . And so that's what I\nwould be excited about. - What kinda advice could you give to somebody in high school right now or maybe early college who's\ncurious about programming and feeling like the world is\nchanging really quickly here? - Yeah. - Well, what kinda stuff to learn, what kinda stuff to work on? Should they finish college? Should they go work at a company?\nShould they build a thing? What do you think?\n- Yeah. Well, so I mean, one of the things I'd say is\nthat you'll be most successful if you work on something\nyou're excited by. And so don't get the book and\nread the book cover to cover and study and memorize and\nrecite and flashcard and... Go build something.\nLike, go solve a problem. Go build the thing that\nyou wanted to exist. Go build an app. Go build, train a model. Like, go build something\nand actually use it, and set a goal for yourself. And if you do that, then you'll, you know, there's a success, there's\nthe adrenaline rush, there's the achievement. There's the unlock that I\nthink is where, you know, if you keep setting goals and you keep doing things\nand building things, learning by building is really powerful. In terms of career advice, I\nmean, everybody's different. It's very hard to give generalized advice. I'll speak as you know, a compiler nerd. If everybody's going left, sometimes it's pretty cool to go, right? - Yeah. - And so just because\neverybody's doing a thing, it doesn't mean you have to do the same thing and follow the herd. In fact, I think that sometimes\nthe most exciting paths through life lead to\nbeing curious about things that nobody else actually\nfocuses on, right? And turns out that understanding\ndeeply parts of the problem that people want to take for granted makes you extremely\nvaluable and specialized in ways that the herd is not. And so, again, I mean, there's lots of rooms for specialization, lots of rooms for generalists. There's lots of room for\ndifferent kinds and parts of the problem, but I\nthink that it's, you know, just because everything\neverybody's doing one thing doesn't mean you should necessarily do it. - And now the herd is using Python. So if you wanna be a rebel, go check out Mojo and help\nChris and the rest of the world fight the arch nemesis of complexity 'cause simple is beautiful. - There we go. Yeah. - Chris, you're an incredible person. You've been so kind to\nme ever since we met. You've been extremely supportive. I'm forever grateful for that. Thank you for being who you are, for being legit, for being kind, for fighting this really\ninteresting problem of how to make AI accessible\nto a huge number of people, huge number of devices. - Yeah, well, so Lex, you're a pretty special person too, right? And so I think that, you know, one of the funny things about you is that besides being curious\nand pretty damn smart, you're actually willing to push on things and you're, I think that\nyou've got an agenda to like, make the world think, which I\nthink is a pretty good agenda. It's a pretty good one. - Thank you so much for\ntalking to me, Chris. - Yeah. Thanks Lex. - Thanks for listening\nto this conversation with Chris Lattner. To support this podcast, please check out our\nsponsors in the description. And now let me leave you\nsome words from Isaac Zimov. \"I do not fear computers.\nI fear the lack of them.\" Thank you for listening and\nhope to see you next time."
}