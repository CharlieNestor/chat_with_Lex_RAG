{
  "video_id": "jvqFAi7vkBc",
  "title": "Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power &amp; AGI | Lex Fridman Podcast #419",
  "date": "2024-03-18",
  "transcript": [
    {
      "timestamp": "0:00",
      "section": "Introduction",
      "text": "- I think compute is gonna be\nthe currency of the future. I think it'll be maybe the\nmost precious commodity in the world. I expect that by the end of this decade. And possibly somewhat sooner than that, we will have quite capable systems that we look at and say, wow,\nthat's really remarkable. The road to AGI should be\na giant power struggle. I expect that to be the case. - Whoever builds AGI\nfirst gets a lot of power. Do you trust yourself\nwith that much power? The following is a\nconversation with Sam Altman, his second time in the podcast. He is the CEO of OpenAI, the company behind GPT-4, ChatGPT, Sora, and perhaps one day the very\ncompany that will build AGI. This is Lex Fridman Podcast. To support it, please check out our\nsponsors in the description. And now, dear friends, here's Sam Altman."
    },
    {
      "timestamp": "1:05",
      "section": "OpenAI board saga",
      "text": "Take me through the OpenAI board saga that started on Thursday, November 16th, maybe Friday, November 17th for you. - That was definitely the most painful professional\nexperience of my life and chaotic, and shameful, and upsetting and a bunch of other negative things. There were great things about it too and I wish it had not been in such an adrenaline rush\nthat I wasn't able to stop and appreciate them at the time. I came across this old tweet of mine or this tweet of mine\nfrom that time period, which was it was like kind\nof going to your own eulogy, watching people say all\nthese great things about you and just like unbelievable support from people I love and care about. That was really nice. That whole weekend I kind of like felt\nwith one big exception, I felt like a great deal of love and very little hate even though it felt like I\nhave no idea what's happening and what's gonna happen here and this feels really bad. And there were definitely times I thought it was gonna be\nlike one of the worst things to ever happen for AI safety. Well, I also think I'm happy that it happened relatively early. I thought at some point between when OpenAI started and when we created AGI, there was gonna be something crazy and explosive that happened, but there may be more crazy\nand explosive things happen. It still I think helped us\nbuild up some resilience and be ready for more\nchallenges in the future. - But the thing you had a sense that you would experience is\nsome kind of power struggle. - The road to AGI should\nbe a giant power struggle. Like the world should... Well, not should. I expect that to be the case. - And so you have to go through that, like you said, iterate as often as possible in figuring out how to\nhave a board structure, how to have organization, how to have the kind of people\nthat you're working with, how to communicate all that in order to deescalate the power struggle as much as possible, pacify it. - But at this point, it feels like something\nthat was in the past that was really unpleasant and\nreally difficult and painful. But we're back to work\nand things are so busy and so intense that I don't spend a lot of time thinking about it. There was a time after. There was like this fugue state for kind of like the month after, maybe 45 days after that was I was just sort of\nlike drifting through the days, I was so out of it. I was feeling so down - [Lex] Just on a personal\npsychological level. - Yeah. Really painful. And hard to have to keep running OpenAI in the middle of that. I just wanted to crawl into a cave and kind of recover for a while. But now it's like we're just back to working on the mission. - Well, it's still useful to go back there and reflect on board structures, on power dynamics, on how companies are run, the tension between research,\nand product development, and money and all this kind of stuff so that you have a very high potential of building AGI would do so\nin a slightly more organized, less dramatic way in the future. So there's value there to go both the personal\npsychological aspects of you as a leader and also\njust the board structure and all this kind of messy stuff. - Definitely learned a lot\nabout structure and incentives and what we need out of a board And I think that it is valuable that this happened now in some sense. I think this is probably not like the last high\nstress moment of OpenAI, but it was quite a high stress moment. Company very nearly got destroyed. And we think a lot about many of the other things we've\ngotta get right for AGI. But thinking about how\nto build a resilient org and how to build a\nstructure that will stand up to a lot of pressure the world, which I expect more and\nmore as we get closer. I think that's super important. - Do you have a sense of how deep and rigorous the deliberation\nprocess by the board was? Can you shine some light on just human dynamics involved\nin situations like this? Was it just a few conversations and all of a sudden it escalates and why don't we fire Sam kind of thing? - I think the board members were far, well-meaning people on the whole. And I believe that in stressful situations where people feel time\npressure or whatever, people understandably\nmake suboptimal decisions. And I think one of the challenges for OpenAI will be we're\ngonna have to have a board and a team that are good at\noperating under pressure. - Do you think the board\nhad too much power? - I think boards are supposed\nto have a lot of power, but one of the things that we did see is in most corporate structures, boards are usually\nanswerable to shareholders. Sometimes people have like\nsuper voting shares or whatever. In this case, I think one of the\nthings with our structure that we maybe should have\nthought about more than we did is that the board of a nonprofit has, unless you put other rules in place, like quite a lot of power, they don't really answer\nto anyone but themselves. And there's ways in which that's good, but what we'd really like\nis for the board of OpenAI to answer to the world as a whole as much as that's a practical thing. - So there's a new board announced. - [Sam] Yeah. - There's, I guess, a new\nsmaller board of first and now there's a new final board. - Not a final board yet. We've added some, we'll add more. - Added some, okay. What is fixed in the new\none that was perhaps broken in the previous one? - The old board sort of got smaller over the course of about a year. It was nine and then it went down to six and then we couldn't agree on who to add. And the board also, I\nthink, didn't have a lot of experienced board members and a lot of the new board members at OpenAI have just have more\nexperience as board members. I think that'll help. - It's been criticized some of the people that are added to the board. I heard a lot of people\ncriticizing the addition of Larry Summers, for example. What was the process\nof selecting the board? What's involved in that? - So Bret and Larry were\nkind of decided in the heat of the moment over this very tense weekend and that was... I mean, that weekend was\nlike a real rollercoaster, like a lot of lots and downs. And we were trying to\nagree on new board members that both sort of the executive team here and the old board members\nfelt would be reasonable. Larry was actually one\nof their suggestions, the old board members. Bret, previous to that weekend, suggested, but he was busy and didn't wanna do it. And then we really needed help in wood. We talked about a lot of other people too, but I felt like if I\nwas going to come back, I needed new board members. I didn't think I could work\nwith the old board again in the same configuration, although we then decided, and I'm grateful that Adam would stay, but we wanted to get to... We considered various configurations, decided we wanted to\nget to a board of three and had to find two new board members over the course of sort\nof a short period of time. So those were decided honestly without... That's like you kind of do\nthat on the battlefield. You don't have time to design\na rigorous process then. For new board members, since new board members\nwill add going forward, we have some criteria that\nwe think are important for the board to have different expertise that we want the board to have. Unlike hiring an executive where you need them to do one role, well, the board needs to\ndo a whole role of kind of governance and thoughtfulness. Well, and so one thing that Bret says, which I really like is that\nwe wanna hire board members in slates, not as individuals one at a time. And thinking about a group of people that will bring nonprofit expertise, expertise at running companies, sort of good legal and\ngovernance expertise. That's kind of what we've\ntried to optimize for. - So is technical savvy important for the individual board members? - Not for every board member, but certainly some you need that. That's part of what the board needs to do. - So I mean, the interesting thing that people probably don't understand about OpenAI certainly\nis like all the details of running the business. When they think about the\nboard given the drama, they think about you, they think about like if you reach AGI or you reach some of these\nincredibly impactful products and you build them and deploy them, what's the conversation\nwith the board like? And they kind of think, all right, what's the right squad to have in that kind of\nsituation to deliberate? - Look, I think you definitely need some technical experts there and then you need some\npeople who are like, how can we deploy this in a way that will help people\nin the world the most and people who have a very\ndifferent perspective? I think a mistake that you or I might make is to think that only the technical\nunderstanding matters. And that's definitely part of the conversation you\nwant that board to have. But there's a lot more about how that's gonna\njust like impact society and people's lives that you really want\nrepresented in there too. - Are you looking at the\ntrack record of people or you're just having conversations? - Track record's a big deal. You, of course, have a\nlot of conversations. There's some roles where I kind of totally\nignore track record and just look at slope, kind of ignore the y-intercept. - Thank you. Thank you for making it\nmathematical for the audience, - For a board member, I do care much more about the y-intercept. I think there is something deep to say about track record\nthere and experiences, something's very hard to replace. - Do you try to fit a polynomial function or exponential one to track record? - That's not that. An analogy doesn't carry that far. - All right. You mentioned some of the\nlow points that weekend. What were some of the low\npoints psychologically for you? Did you consider going\nto the Amazon jungle and just taking Ayahuasca\ndisappearing forever or? - I mean, there's so many low, like it was a very bad period of time. There were great high points too. My phone was just like\nsort of nonstop blowing up with nice messages from people\nI worked with every day, people I hadn't talked to in a decade. I didn't get to appreciate\nthat as much as I should have. 'cause I was just like in\nthe middle of this firefight, but that was really nice. But on the whole, it was like a very painful weekend and also just like a very... It was like a battle fought in\npublic to a surprising degree and that was extremely exhausting to me, much more than I expected. I think fights are generally exhausting, but this one really was. The board did this Friday afternoon. I really couldn't get much\nin the way of answers, but I also was just like, \"Well, the board gets to do this.\" And so I'm gonna think for a little bit about what I want to do, but I'll try to find the, the\nblessing in disguise here. And I was like, \"Well,\nmy current job at OpenAI, it was like to like run\na decently-sized company at this point.\" And the thing I'd always liked\nthe most was just getting to work with the researchers. And I was like, yeah,\nI can just go do like a very focused AI research effort. And I got excited about. That didn't even occur to me at the time to like possibly that this\nwas all gonna get undone. This was like Friday afternoon. - Oh, so you've accepted the death- - Very quickly, very quickly. I mean, I went through\nlike a little period of confusion and rage, but very quickly. And by Friday night, I was talking to people\nabout what was gonna be next and I was excited about that. I think it was Friday night evening for the first time that I\nheard from the exec team here, which is like, hey, we're\ngonna like fight this and we think... Well, whatever. And then I went to bed just\nstill being like, okay, excited. - Like onward, were you able to sleep? - Not a lot. It was one of the weird\nthings was there was this like period of four and a half days where sort of didn't sleep much, didn't eat much and still kind of had like a\nsurprising amount of energy. You learn like a weird\nthing about adrenaline and more time. - So you kind of accepted the\ndeath of this baby OpenAI? - And I was excited for the new thing. I was just like, okay, this\nwas crazy, but whatever. - It's a very good coping mechanism. - And then Saturday morning, two of the board members called and said, \"Hey, we destabilize. We didn't mean to destabilize things. We don't restore a lot of value here. Can we talk about you coming back?\" And I immediately didn't wanna do that, but I thought a little more and I was like, \"Well, I really\ncare about the people here, the partners, shareholders. I love this company.\" And so I thought about it and I was like, \"Well, okay, but here's the stuff I would need.\" And then the most painful time of all over the course of that weekend, I kept thinking and being told... Not just me, like the whole team here kept thinking. Well, we were trying to\nkeep OpenAI stabilized while the whole world was\ntrying to break it apart, people trying to recruit, whatever. We kept being told like, \"All right, we're almost done, we're almost done. We just need like a little bit more time.\" And it was this like very confusing state. And then Sunday evening when again like every few hours, I expected that we were gonna be done and we're gonna figure\nout a way for me to return and things to go back to how they were, the board then appointed a new interim CEO and then I was like... I mean, that feels really bad. That was the low point of the whole thing. You know, I'll tell you something, it felt very painful, but I felt a lot of\nlove that whole weekend. It was not other than that\none moment, Sunday night, I would not characterize my\nemotions as anger or hate, but I really just like... I felt a lot of love from\npeople towards people. It was like painful, but it was like the dominant emotion of the weekend was love, not hate. - You've spoken highly of\nMira Murati that she helped, especially as you put in a tweet, \"In the quiet moments when it counts, perhaps we could take a bit of a tangent.\" What do you admire about Mira? - Well, she did a great\njob during that weekend in a lot of chaos, but people often see leaders in the crisis moments, good or bad. But a thing I really value in leaders is how people\nact on a boring Tuesday at 9:46 in the morning and in just sort of the normal\ndrudgery of the day-to-day, how someone shows up in a meeting, the quality of the decisions they make. That was what I meant\nabout the quiet moments. - Meaning like most of the\nwork is done on a day by day in a meeting by meeting, just be present and make great decisions. - Yeah. I mean, look, what you have wanted to spend the last 20 minutes about and I understand is like this\none very dramatic weekend. But that's not really\nwhat OpenAI is about. OpenAI is really about\nthe other seven years. - Well, yeah, human civilization\nis not about the invasion of the Soviet Union by Nazi Germany, but still that's something\npeople totally focus on. - Very understandable. - It gives us an insight\ninto human nature, the extremes of human nature, and perhaps some of the damage and some of the triumphs of human civilization can\nhappen in those moments. So it's like illustrative."
    },
    {
      "timestamp": "18:31",
      "section": "Ilya Sutskever",
      "text": "Let me ask you about Ilya. Is he being held hostage in\na secret nuclear facility? - No. - What about a regular secret facility? - No. - What about a nuclear\nnon-secure facility? - Neither, not that either. - I mean, this is becoming\na meme at some point. You've known Ilya for a long time. He was obviously part of this drama with the board and all that kind of stuff. What's your relationship with him now? - I love Ilya. I have tremendous respect for Ilya. I don't have anything I can\nsay about his plans right now. That's a question for him. But I really hope we work together for certainly the rest of my career. He's a little bit younger than me, maybe he works a little bit longer. - There's a meme that he saw something, like he maybe saw AGI and that gave him a lot\nof worry internally. What did Ilya see? - Ilya has not seen AGI, none of us have seen AGI. We've not built AGII. I do think one of the many things that I really love about\nIlya is he takes AGI and the safety concerns broadly speaking, including things like the\nimpact this is gonna have on society very seriously. And as we continue to\nmake significant progress, Ilya is one of the people that I've spent the most time over the last couple of years talking about what this is going to mean, what we need to do to\nensure we get it right to ensure that we succeed at the mission. So Ilya did not see AGI. But Ilya is a credit to humanity in terms of how much he thinks and worries about making\nsure we get this right. - I've had a bunch of\nconversation with him in the past. I think when he talks about technology, he's always like doing\nthis long-term thinking type of thing. So he is not thinking about\nwhat this is gonna be in a year. He's thinking about in 10 years. - [Sam] Yeah. - Just thinking from first principles like, okay, if the scales, what are the fundamentals here? Where's this going? And so that's a foundation for them thinking about like\nall the other safety concerns and all that kind of stuff, which makes him a really\nfascinating human to talk with. Do you have any idea why\nhe's been kind of quiet? Is it he's just doing some soul searching? - Again, I don't wanna speak for Ilya. I think that you should ask him that. He's definitely a thoughtful guy. I think I kind of think of Ilya as like always on a soul\nsearch in a really good way. - Yes. Yeah. Also he appreciates the power of silence. Also, I'm told he can be a silly guy, which I've never seen that side of him. - It's very sweet when that happens. - I've never witnessed a silly Ilya, but I look forward to that as well. - I was at a dinner\nparty with him recently and he was playing with a puppy. And he was like in a very\nsilly move, very endearing and I was thinking like, oh man, this is like not the side of the Ilya that the world sees the most. - So just to wrap up this whole saga, are you feeling good\nabout the board structure about all of this and where it's moving? - I feel great about the new board. In terms of the structure of OpenAI, one of the board's\ntasks is to look at that and see where we can make it more robust. We wanted to get new board\nmembers in place first, but we clearly learned\na lesson about structure throughout this process. I don't have I think\nsuper deep things to say. It was a crazy, very painful experience. I think it was like a\nperfect storm of weirdness. It was like a preview for\nme of what's gonna happen as the stakes get higher and higher and the need that we have like\nrobust governance structures and processes and people. I am kind of happy it\nhappened when it did, but it was a shockingly\npainful thing to go through. - Did it make you be more\nhesitant in trusting people? - Yes. - Just on a personal level. - Yes. - I think I'm like an\nextremely trusting person. I've always had a life philosophy of like don't worry about\nall of the paranoia, don't worry about the edge cases. You get a little bit screwed in exchange for getting to live with your guard down. And this was so shocking to me. I was so caught off guard that it has definitely changed and I really don't like this. It's definitely changed how I think about just like default trust of people and planning for the bad scenarios. - You gotta be careful with that. Are you worried about\nbecoming a little too cynical? - I'm not worried about\nbecoming too cynical. I think I'm like the extreme\nopposite of a cynical person. But I'm worried about\njust becoming like less of a default trusting person. - I'm actually not sure which\nmode is best to operate in for a person who's developing AGI, trusting or untrusting. It's an interesting journey you're on. But in terms of structure, see, I'm more interested\non the human level. How do you surround yourself with humans that are building cool shit, but also are making wise decisions? Because the more money you start making, the more power the thing\nhas the weirder people get. - I think you could make\nall kinds of comments about the board members and the level of trust\nI should have had there or how I should have\ndone things differently. But in terms of the team here, I think you'd have to like\ngive me a very good grade on that one. And I have just like\nenormous gratitude and trust and respect for the people\nthat I work with every day. And I think being surrounded with people like that is really important. - Our mutual friend Elon sued OpenAI."
    },
    {
      "timestamp": "24:40",
      "section": "Elon Musk lawsuit",
      "text": "What is the essence of\nwhat he's criticizing? To what degree does he have a point? To what degree is he wrong? - I don't know what it's really about. We started off just thinking\nwe were gonna be a research lab and having no idea about how\nthis technology was gonna go. Because it was only\nseven or eight years ago, it's hard to go back and really remember what it was like then. But before language\nmodels were a big deal, this was before we had\nany idea about an API or selling access to a chat bot. It was before we had any\nidea we were gonna productize at all. So we're like we're just\ngonna try to do research and we don't really know what\nwe're gonna do with that. I think with many new\nfundamentally new things, you start fumbling through the dark and you make some assumptions, most of which turn out to be wrong. And then it became clear that we were going to need\nto do different things and also have huge amounts more capital. So we said, \"Okay, well, the structure doesn't quite work for that. How do we patch the structure?\" And then you patch it\nagain and patch it again and you end up with something that does look kind of eyebrow\nraising to say the least. But we got here gradually with I think reasonable decisions at each point along the way and doesn't mean I wouldn't\ndo it totally differently if we could go back now with an oracle, but you don't get the oracle at the time. But anyway, in terms of what Elon's\nreal motivations here are I don't know. - To the degree you remember, what was the response that\nOpenAI gave in the blog post? Can you summarize it? - Oh, we just said like Elon\nsaid this set of things, here's our characterization or here's this sort of\nnot our characterization, here's like the characterization\nof how this went down. We tried to not make it emotional and just sort of say\nlike here's the history. - I do think there's a\ndegree of mischaracterization from Elon here about one of the points you just made, which is the degree of\nuncertainty you had at the time. You guys are a bunch of like a small group of researchers crazily talking about AGI when everybody's laughing at that thought. - Wasn't that long ago\nElon was crazily talking about launching rockets when people were laughing at that thought? So I think he'd have\nmore empathy for this. - I mean, I do think that\nthere's personal stuff here that there was a split that OpenAI and a lot of amazing people here chose to part ways of Elon. So there's a personal- - Elon chose to part ways. - Can you describe that exactly, the choosing to part ways? - He thought OpenAI was gonna fail. He wanted total control\nto sort of turn it around. We wanted to keep going in the direction that now has become OpenAI. He also wanted Tesla to be able to build an AGI effort. At various times, he wanted to make OpenAI\ninto a for-profit company that he could have control of or have it merged with Tesla. We didn't want to do that and he decided to leave, which that's fine. - And that's one of the things that the blog post says\nis that he wanted OpenAI to be basically acquired by Tesla in those same way that or\nmaybe something similar or maybe something more dramatic than the partnership with Microsoft. - My memory is the proposal was just like, yeah, like get acquired by Tesla and have Tesla have full control over it. I'm pretty sure that's what it was. - So what is the word open in OpenAI mean to Elon at the time? Ilya has talked about this\nin in the email exchanges and all this kind of stuff. What does it mean to you at the time? What does it mean to you now? - I would definitely pick a diff... Speaking of going back with an oracle, I'd pick a different name. One of the things that\nI think OpenAI is doing that is the most important of everything that we're doing is\nputting powerful technology in the hands of people\nfor free as a public good. We don't run ads on our free version. We don't monetize it in other ways. We just say it's part of our mission. We wanna put increasingly powerful tools in the hands of people for free and get them to use them. And I think that kind of\nopen is really important to our mission. I think if you give people great tools and teach them to use them\nor don't even teach them, they'll figure it out and let them go build an incredible future for each other with that. That's a big deal. So if we can keep putting free or low cost or free and low cost powerful\nAI tools out in the world, I think that's a huge deal for how we fulfill the mission. Open source or not, yeah, I think we should\nopen source some stuff and not other stuff. It does become this like\nreligious battle line where nuance is hard to have, but I think nuance is the right answer. - So he said change your name to ClosedAI and I'll drop the lawsuit. I mean, is it going to\nbecome this battleground in the land of memes about the name? - I think that speaks to the seriousness with which Elon means the lawsuit. I mean, that's like an\nastonishing thing to say, I think. - Well, I don't think the lawsuit maybe, correct me if I'm wrong, but I don't think the\nlawsuit is legally serious. It's more to make a point\nabout the future of AGI and the company that's\ncurrently leading the way. - Look, I mean Grok had\nnot open sourced anything until people pointed out it\nwas a little bit hypocritical and then he announced that Grok open source things this week. I don't think open source versus not is what this is really about for him. - Well, we'll talk about\nopen source and not. I do think maybe criticizing\nthe competition is great, just talking a little shit, that's great, but friendly competition versus like I personally hate lawsuits. - Look, I think this whole\nthing is like unbecoming of a builder, and I respect Elon is one of\nthe great builders of our time. And I know he knows what it's like to have like haters attack him and it makes me extra\nsad he's doing the toss. - Yeah, he is one of the\ngreatest builders of all time, potentially the greatest\nbuilder of all time. - It makes me sad. And I think it makes a lot of people sad. There's a lot of people\nwho've really looked up to him for a long time and said this. I said in some interview or something that I missed the old Elon and the number of messages I got being like that exactly encapsulates how I feel. - I think he should just win. He should just make Grok beat GPT and then GPT beats Grok and it's just a competition, and it's beautiful for everybody. But on the question of open source, do you think there's a\nlot of companies playing with this idea? It's quite interesting. I would say Meta, surprisingly,\nhas led the way on this or like at least took the\nfirst step in the game of chess of really open sourcing the model. Of course, it's not the\nstate of the art model, but open sourcing Llama and Google is flirting with the idea of open sourcing a smaller version. What are the pros and\ncons of open sourcing? Have you played around with this idea? - Yeah, I think there\nis definitely a place for open source models, particularly smaller models that people can run locally, I think there's huge demand for. I think there will be\nsome open source models, there will be some closed source models. It won't be unlike other\necosystems in that way. - I listened to all in podcasts talking about this lawsuit and\nall that kind of stuff and they were more concerned\nabout the precedent of going from nonprofit\nto this cap for profit. What precedent that\nsets for other startups? - I would heavily discourage any startup that was thinking about\nstarting as a non-profit and adding like a for-profit arm later. I'd heavily discourage\nthem from doing that. I don't think we'll set a precedent here. - Okay. So most startups should go just- - For sure. And again, if we knew\nwhat was gonna happen, we would've done that too. - Well, like in theory, if you like dance beautifully here, there's like some tax\nincentives or whatever - But I don't think that's like how most people think about these things. - Just not possible to save a lot of money for a startup if you do it this way. - No, I think there's\nlike laws that would make that pretty difficult. - Where do you hope this goes with Elon? Well, this tension, this dance, what do you hope this? Like if we go one, two,\nthree years from now, your relationship with him\non a personal level too, like friendship, friendly competition, just all this kind of stuff. - Yeah. I mean, I really respect Elon. And I hope that years in the future, we have an amicable relationship. - Yeah, I hope you guys have\nan amicable relationship like this month and just compete and win and explore these ideas together. I do suppose there's competition\nfor talent or whatever, but it should be friendly competition. Just build, build cool shit. And Elon is pretty good\nat building cool shit, but so are you."
    },
    {
      "timestamp": "34:32",
      "section": "Sora",
      "text": "So speaking of cool shit. Sora, there's like a million\nquestions I could ask. First of all, it's amazing, it truly is amazing on a product level, but also just on a philosophical level. So let me just\ntechnical/philosophical ask. What do you think it\nunderstands about the world more or less than GPT-4, for example, like the world model when you train on these\npatches versus language tokens? - I think all of these models\nunderstand something more about the world model than most\nof us give them credit for. And because they're also very clear things they just don't understand\nor don't get right, it's easy to look at the weaknesses, see through the veil and say this is all fake, but it's not all fake. It's just some of it works\nand some of it doesn't work. I remember when I started\nfirst watching Sora videos and I would see like a person walk in front of something for a\nfew seconds and occlude it and then walk away and the same thing was still there. I was like, \"This is pretty good.\" Or there's examples where the underlying physics\nlooks so well represented over a lot of steps in a sequence. It's like oh this is\nlike quite impressive. But, fundamentally, these\nmodels are just getting better and that will keep happening. If you look at the trajectory from DALL·E 1 to 2 to 3 to Sora, there were a lot of people that\nwere dunked on each version, saying it can't do this, it can't do that and I'm like look at it now. - Well, the thing you\njust mentioned is kind of with the occlusions is\nbasically modeling the physics of three dimensional physics\nof the world sufficiently well to capture those kinds of things. - Well. - Yeah, maybe you can tell me in order to deal with occlusions, what does the world model need to? - Yeah, so what I would\nsay is it's doing something to deal with occlusions really well. What I represent that it\nhas like a great underlying 3D model of the world. It's a little bit more of a stretch - But can you get there\nthrough just these kinds of two dimensional\ntraining data approaches? - It looks like this approach\nis gonna go surprisingly far. I don't wanna speculate too much about what limits it will\nsurmount and which it won't. - What are some interesting limitations of the system that you've seen? I mean, there's been some\nfun ones you've posted. - There's all kinds of fun. I mean, like cats sprouting a extra limit at random points in a video, like pick what you want, but there's still a lot of problem, there's a lot of weaknesses. - Do you think that's a\nfundamental flaw of the approach or is it just bigger model or better technical\ndetails or better data, more data is going to solve\nthe cat sprouting extremes? - I would say yes to both. I think there is something\nabout the approach which just seems to feel different from how we think and learn and whatever. And then also, I think it'll get better with scale. - I mentioned LLMs have\ntokens, text tokens and Sora has visual patches so it converts all visual data, a diverse kinds of visual data videos and images into patches. Is the training to the degree you can say\nfully self-supervised there? Is there some manual labeling going on? What's the involvement\nof humans in all this? - I mean, without saying anything specific about the Sora approach, we use lots of human data in our work. - But not internet scale data. So lots of humans, lots of complicated word, Sam. - I think lots is a\nfair word in this case. - But it doesn't because to me, lots, like listen, I'm an introvert and when I hang out\nwith like three people, that's a lot of people. Yeah, four people, that's a lot. But I suppose you mean more than- - More than three people\nwork on labeling the data for these models, yeah. - Okay. All right. But fundamentally, there's a lot of self-supervised learning. 'cause what you mentioned in the technical report\nis internet scale data. That's another beautiful, it's like poetry. So it's a lot of data\nthat's not human label. It's self-supervised in that way. And then the question is, how much data is there on the internet that could be used in\nthis that is conducive to this kind of self-supervised way if only we knew the details\nof the self-supervised? Do you have you considered opening it up a little more details - We have. You mean, for sora specifically? - Sora specifically because it's so interesting. Can the same magic of LLMs now start moving\ntowards visual data and what does that take to do that? - I mean, it looks to me like yes, but we have more work to do. - Sure. What are the dangers? Why are you concerned\nabout releasing the system? What are some possible dangers of this? - I mean, frankly speaking, one thing we have to do before releasing the\nsystem is just like get it to work at a level of efficiency that will deliver the scale\npeople are gonna want from this. So that I don't wanna like downplay that and there's still a ton\nof work to do there. But you can imagine like issues with deep fakes, misinformation. We try to be a thoughtful company about what we put out into the world and it doesn't take much thought to think about the ways this can go badly. - There's a lot of tough questions here. You're dealing in a very tough space. Do you think training AI should be or is fair use under copyright law? - I think the question\nbehind that question is, do people who create valuable\ndata deserve to have some way that they get compensated for use of it? And that I think the answer is yes. I don't know yet what the answer is. People have proposed a\nlot of different things. We've some tried some different models. But if I'm like an artist, for example, I would like to be able to opt out of people generating art in my style and B, if they do\ngenerate art in my style, I'd like to have some economic\nmodel associated with that. - Yeah, it's that transition\nfrom CDs to Napster to Spotify. We have to figure out some kind of model. - The model changes, but people have gotta get paid. - Well, there should be some kind of incentive if we zoom\nout even more for humans to keep doing cool shit. - Everything I worry about, humans are gonna do cool shit and society's gonna find\nsome way to reward it. That seems pretty hardwired. We want to create. We want to be useful. We want to achieve status in whatever way that's not going anywhere, I don't think. - But the reward might not\nbe monetary, financial. It might be like fame and\ncelebration of other cool- - Maybe financial in some other way. Again, I don't think we've\nseen like the last evolution of how the economic system's gonna work. - Yeah. But artists and creators are worried. When they see Sora, they're like, \"Holy shit.\" - Sure. Artists were also super worried\nwhen photography came out. And then photography became a new art form and people made a lot of\nmoney taking pictures. And I think things like\nthat will keep happening. People will use the new tools in new ways. - If we just look on YouTube\nor something like this, how much of that will be using Sora, like AI-generated content do you think in the next five years? - People talk about like\nhow many jobs is AI gonna do in five years and the framework that people\nhave is what percentage of current jobs are just\ngonna be totally replaced by some AI doing the job? The way I think about\nit is not what percent of jobs AI will do, but what percent of tasks will AI do and over what time horizon. So if you think of all of\nthe like-five second tasks in the economy, five-minute\ntasks, the five-hour tasks, maybe even the five-day tasks, how many of those can AI do? And I think that's a way\nmore interesting, impactful, important question than\nhow many jobs AI can do because it is a tool that will work at increasing\nlevels of sophistication and over longer and longer time horizons for more and more tasks and let people operate at a\nhigher level of abstraction. So maybe people are way more\nefficient at the job they do. And at some point, that's not just a quantitative change, but it's a qualitative\none too about the kinds of problems you can keep in your head. I think that for videos on YouTube, it'll be the same. Many videos, maybe most of them, will use AI tools in the production, but they'll still be fundamentally driven by a person thinking about it, putting it together, doing parts of it, sort of directing it and running it. - Yeah, it's so interesting. I mean, it's scary, but it's interesting to think about. I tend to believe that humans\nlike to watch other humans or other human like- - Humans really care\nabout other humans a lot. - Yeah. If there's a cooler thing\nthat's better than a human, humans care about that for like two days and then they go back to humans. - That seems very deeply wired. - It's the whole chest thing. But now let's everybody keep playing chess and let's ignore the alpha in the room that humans are really bad at\nchess relative to AI systems. - We still run races and\ncars are much faster. I mean, there's like a lot of examples. - Yeah. And maybe it'll just be tooling like in the Adobe suite type of way where it can just make videos much easier and all that kind of stuff. Listen, I hate being\nin front of the camera. If I can figure out a way to not be in front of the camera, I would love it. Unfortunately, it'll take a while. Like that generating faces, it's getting there, but generating faces and\nvideo format is tricky when it's specific people\nversus generic people."
    },
    {
      "timestamp": "44:23",
      "section": "GPT-4",
      "text": "Let me ask you about GPT-4. There's so many questions. First of all, also amazing. Looking back, it'll probably be this kind of historic pivotal moment with three, five, and four which had GPT. - Maybe five will be the pivotal moment. I don't know. Hard to say that looking forwards. - We never know. That's the annoying\nthing about the future, it's hard to predict. But for me, looking back GPT-4, ChatGPT is pretty impressive, historically impressive. So allow me to ask, what's been the most impressive\ncapabilities of GPT-4 to you and GPT-4 Turbo? - I think it kind of sucks. - Hmm. Typical human also gotten\nused to an awesome thing. - No, I think it is an amazing thing, but relative to where we need to get to and where I believe we will get to, at the time of like GPT-3, people were like, \"Oh this is amazing. This is this like marvel of technology,\" and it is, it was. But now we have GPT-4 and look at GPT-3 and you're like that's\nunimaginably horrible. I expect that the delta between five and four will be the same\nas between four and three. And I think it is our job to\nlive a few years in the future and remember that the tools\nwe have now are gonna kind of suck looking backwards at them and that's how we make\nsure the future is better. - What are the most glorious\nways that GPT-4 sucks? Meaning- - [Sam] What are the\nbest things it can do? - What are the best things\nit can do in the limits of those best things that\nallow you to say it sucks, therefore gives you an inspiration\nand hope for the future? - One thing I've been using it for more recently is sort of a\nlike a brainstorming partner. - [Lex] Yep. Almost for that. - There's a glimmer of\nsomething amazing in there. I don't think it gets... When people talk about it, what it does, they're like, \"Helps me\ncode more productively. It helps me write more faster and better. It helps me translate from\nthis language to another,\" all these like amazing things. But there's something about the kind of creative brainstorming partner. I need to come up with\na name for this thing. I need to think about this\nproblem in a different way. I'm not sure what to do here. That I think like gives a glimpse of something I hope to see more of. One of the other things that you can see a very\nsmall glimpse of is what I can help on longer horizon tasks. Break down something in multiple steps, maybe execute some of those steps, search the internet, write code, whatever. Put that together. When that works, which is not very often, it's like very magical, - The iterative back\nand forth with a human. It works a lot for me. What do you mean it works? - Iterative back and forth to human, it can get more often, when it can go do like a\n10-step problem on its own. It doesn't work for that\ntoo often sometimes. - Add multiple layers of abstraction or do you mean just sequential? - Both like to break it down and then do things that different layers of abstraction put them together. Look, I don't wanna downplay\nthe accomplishment of GPT-4, but I don't wanna overstate it either. And I think this point that we\nare on an exponential curve, we'll look back relatively soon at GPT-4 like we look back at GPT-3 now. - That said, I mean\nChatGPT was the transition to where people like started to believe there is an\nuptick of believing. Not internally at OpenAI perhaps. There's believers here, but when you think- - And in that sense, I do think it'll be a moment where a lot of the world went from not believing to believing. That was more about the ChatGPT interface. And by the interface and product, I also mean the post-training of the model and how we tune it to be helpful to you and how to use it than the\nunderlying model itself. - How much of each of\nthose things are important? The underlying model and the RLHF or something of that nature that tunes it to be more compelling to the human, more effective and\nproductive for the human. - I mean, they're both super important but the RLHF, the post-training step, the little wrapper of things that from a compute perspective, little wrapper of things that we do on top of the base model, even though it's a huge amount of work. That's really important to\nsay nothing of the product that we build around it. In some sense, we did have to do two things. We had to invent we underlying technology and then we had to figure out how to make it into a\nproduct people would love, which is not just about the\nactual product work itself, but this whole other step of how you align it and make it useful - And how you make the scale work where a lot of people can\nuse it at the same time, all that kind of stuff. - But that was like a\nknown difficult thing. We knew we were gonna have to scale it up. We had to go do two things that had like never been done before that were both, like, I would say quite significant achievements and then a lot of things\nlike scaling it up that other companies\nhave had to do before. - How does the context window of going from 8K to 128K tokens compare from GPT-4 to to GPT-4 Turbo? - Most people don't\nneed all the way to 128, most of the time although. If we dream into the distant future, we'll have like way distant future, we'll have like context\nlength of several billion. You will feed in all of your information, all of your history time, and it'll just get to\nknow you better and better and that'll be great. For now, the way people use these models, they're not doing that. And people sometimes post in a paper or a significant fraction of\na code repository, whatever. But most usage of the models is not\nusing the long context most of the time. - I like that this is your\nI have a dream speech. One day, you'll be judged\nby the full context of your character or\nof your whole lifetime. That's interesting. So like that's part of the expansion that you're hoping for is a\ngreater and greater context. - I saw this internet clip once. I'm gonna get the numbers wrong, but it was like Bill Gates\ntalking about the amount of memory on some early computer. Maybe it was 64K, maybe 640K, something like that. And most of it was used\nfor the screen buffer. And he just couldn't seem genuine. This couldn't imagine that the world would eventually\nneed gigabytes of memory in a computer or terabytes\nof memory in a computer. And you always do or you always do just need to follow the exponential of technology. We will find out how to\nuse better technology. So I can't really imagine\nwhat it's like right now for context links to go\nout to the billion someday and they might not literally go there, but effectively it'll feel like that. But I know we'll use it and really not wanna go\nback once we have it. - Yeah, even saying billions 10 years from now might seem dumb because it'll be like\ntrillions upon trillions. - [Sam] Sure. - There'd be some kind of breakthrough that will effectively feel\nlike infinite context. But even 120, I have to be honest, I haven't pushed it to that degree. Maybe putting in entire books or like parts of books and so on, papers. What are some interesting use cases of GPT-4 that you've seen? - The thing that I find\nmost interesting is not any particular use case that\nwe can talk about those, but it's people who kind of like... This is mostly younger people, but people who use it as\nlike their default start for any kind of knowledge work task. And it's the fact that it can do a lot of things reasonably well. You can use GPTV, you can use it to help you write code, you can use it to help you do search, you can use it to edit a paper. The most interesting to me is the people who just use it as the\nstart of their workflow. - I do as well for many things. Like I use it as a reading\npartner for reading books. It helps me think, help me think through ideas, especially when the books are classic, so it's really well written about and it actually is... I find it often to be significantly better than even like Wikipedia\non well-covered topics. It's somehow more\nbalanced and more nuanced, or maybe it's me, but it inspires me to think deeper than a\nWikipedia article does. I'm not exactly sure what that is. You mentioned like this collaboration, I'm not sure where magic is, if it's in here or if it's in there or if it's somewhere in between. I'm not sure. But one of the things that concerns me for knowledge task when\nI start with GPT is I'll usually have to\ndo fact checking after, like check that it didn't\ncome up with fake stuff. How do you figure that out that GPT can come up with fake stuff that sounds really convincing? So how do you ground it in truth? - That's obviously an area\nof intense interest for us. I think it's gonna get a lot\nbetter with upcoming versions, but we'll have to continue to work on it and we're not gonna have it\nlike all solved this year. - Well, the scary thing\nis like as it gets better. You'll start not doing the\nfact checking more and more, right? - I'm of two minds about that. I think people are like much\nmore sophisticated users of technology than we\noften give them credit for and people seem to really\nunderstand that GPT, any of these models\nhallucinate some of the time and if it's mission critical, you gotta check it. - Except journalists don't\nseem to understand that. I've seen journalists\nhalf-assedly just using GPT-4. - Of the long list of things I'd like to dunk on journalists for, this is not my top criticism of them. - Well, I think the bigger\ncriticism is perhaps the pressures and the incentives of being a journalist is that you have to work really quickly\nand this is a shortcut. I would love our society\nto incentivize like- - [Sam] I would too. - Journalistic efforts\nthat take days and weeks and rewards great in depth journalism. Also journalism that represent\nstuff in a balanced way where it's like celebrates people while criticizing them even though the criticism is\nthe thing that gets clicks and making up also gets clicks and headlines that\nmischaracterize completely. I'm sure you have a lot\nof people dunking on... Well, all that drama\nprobably got a lot of clicks. - Probably did. - And that's a bigger problem\nabout human civilization. I would love to see solved is\nwhere we celebrate a bit more."
    },
    {
      "timestamp": "55:32",
      "section": "Memory & privacy",
      "text": "You've given ChatGPT the\nability to have memories. You've been playing with that\nabout previous conversations and also the ability to turn off memory, which I wish I could do that sometimes, just turn on and off depending. I guess sometimes alcohol can do that, but not optimally, I suppose. What have you seen through that, like playing around with that idea of remembering conversations and not? - We're very early in\nour explorations here, but I think what people want or at least what I want\nfor myself is a model that gets to know me and gets\nmore useful to me over time. This is an early exploration. I think there's like a\nlot of other things to do, but that's where we'd like to head. You'd like to use a model and over the course of your life or use a system, it'd be many models. And over the course of your life, it gets better and better. - Yeah. How hard is that problem? 'Cause right now, it's more like remembering little factoids and preferences and so on. What about remembering, like don't you want GPT to remember all the shit\nyou went through in November and all the drama and then you can- - Yeah, yeah, yeah. - Because right now, you're clearly blocking\nit out a little bit. - It's not just that I\nwant it to remember that. I want it to integrate the lessons of that and remind me in the future\nwhat to do differently or what to watch out for. And we all gain from experience over the course of our\nlives, varying degrees. And I'd like my AI agent to\ngain with that experience too. So if we go back and let ourselves imagine that trillions and\ntrillions of contact length, if I can put every\nconversation I've ever had with anybody in my life in there, if I can have all of\nmy emails input out... Like all of my input/output in the context window every\ntime I ask a question, that'd be pretty cool, I think. - Yeah, I think that would be very cool. People sometimes will hear that and be concerned about privacy. What do you think about that aspect of it, the more effective the AI becomes that really integrating\nall the experiences and all the data that happened to you and give you advice? - I think the right answer\nthere is just user choice. Anything I want stricken from\nthe record from my AI agent, I wanna be able to take out. If I don't want to remember anything, I want that too. You and I may have different opinions about where on that\nprivacy utility trade off for our own AI we wanna be, which is totally fine. But I think the answer is just\nlike really easy user choice. - But there should be some high level of transparency from a\ncompany about the user choice 'cause sometimes company in the past, companies in the past have been kind of absolutely shady about like, yeah, it's kind of presumed that\nwe're collecting all your data and we're using it for a good reason for advertisement and so on. But there's not a transparency\nabout the details of that. - That's totally true. You mentioned earlier that I'm like blocking\nout the November stuff. - I'm just teasing you. - Well, I mean I think it\nwas a very traumatic thing and it did immobilize me\nfor a long period of time. Like definitely the hardest, like the hardest work thing I've had to do was just like\nkeep working that period because I had to try to come back in here and put the pieces together while I was just like in\nsort of shock and pain. Nobody really cares about that. I mean, the team gave me a pass and I was not working at my normal level, but there was a period\nwhere I was just, like, it was really hard to have to do both. But I kind of woke up one morning and I was like, \"This was a\nhorrible thing to happen to me. I think I could just feel\nlike a victim forever,\" or I can say, \"This is like\nthe most important work I'll ever touch in my life and I need to get back to it.\" And it doesn't mean that I've repressed it because sometimes I wake in the middle of the night thinking about it, but I do feel like an obligation\nto keep moving forward. - Well, that's beautifully said, but there could be some\nlingering stuff in there. What I would be concerned about is that trust thing that you mentioned that being paranoid about people as opposed to just trusting\neverybody or most people, like using your gut. It's a tricky dance for sure. I mean, because I've seen in\nmy part-time explorations, I've been diving deeply into\nthe Zelensky administration, the Putin administration and\nthe dynamics there in wartime in a very highly stressful environment. And what happens is distrust and you isolate yourself both and you start to not\nsee the world clearly. And that's a concern, that's a human concern. You seem to have taken it in stride and kind of learned the good\nlessons and felt the love and let the love energize you, which is great, but still can linger in there. There's just some questions I would love to ask your intuition about\nwhat's GPT able to do and not. So it's allocating approximately\nthe same amount of compute for each token it generates. Is there room there in\nthis kind of approach to slower thinking, sequential thinking? - I think there will be a new paradigm for that kind of thinking. - Will it be similar like architecturally as what we're seeing now with LLMs? Is it a layer on top of the LLMs? - I can imagine many\nways to implement that. I think that's less important than the question you were getting out, which is do we need a\nway to do a slower kind of thinking where the answer\ndoesn't have to get like... I guess like spiritually, you could say that you want an AI to be able to think harder\nabout a harder problem and answer more quickly\nabout an easier problem. And I think that will be important. - Is that like a human thought\nthat we're just having, you should be able to think hard? Is that a wrong intuition? - I suspect that's a reasonable intuition. - Interesting. So it's not possible once the GPT gets like GPT-7 would just be instantaneously be able to see, here's the proof of from RSTM. - It seems to me like you want to be able to allocate more compute\nto harder problems. It seems to me that a system knowing if you ask a system like that, proof from us last theorem versus... What's today's date? Unless it already knew and\nhad memorized the answer to the proof, assuming it's gotta go figure that out, seems like that will take more compute. - But can it look like a\nbasically LLM talking to itself, that kind of thing? - Maybe. I mean, there's a lot of things that you could imagine working what the right or the best way to do that will be. We don't know."
    },
    {
      "timestamp": "1:02:36",
      "section": "Q*",
      "text": "- This does make me\nthink of the mysterious, the lore behind Q-Star. What's this mysterious Q-Star project? Is it also in the same nuclear facility? - There is no nuclear facility. - That's what a person with a nuclear facility always says. - I would love to have a\nsecret nuclear facility. There isn't one. - All right. - Maybe someday. - Someday. All right. One can dream- - OpenAI is not a good\ncompany to keeping secrets. It would be nice. We're like been plagued by a lot of leaks and it would be nice if we were able to have something like that. - Can you speak to what Q-Star is? - We are not ready to talk about that. - See, but an answer like\nthat means there's something to talk about. It's very mysterious, Sam. - I mean, we work on\nall kinds of research. We have said for a while that we think better reasoning in these systems is an important direction that we'd like to pursue. We haven't cracked the code yet. We're very interested in it. - Is there gonna be moments Q-Star or otherwise where there's\ngoing to be leaps similar to GPT where you're like- - That's a good question. What do I think about that? It's interesting to me it\nall feels pretty continuous. - This is kind of a theme that you're saying is you're\nbasically gradually going up an exponential slope. But from an outsider's perspective for me, just watching it that it\ndoes feel like there's leaps, but to you there isn't. - I do wonder if we should have... So part of the reason that we deploy the way\nwe do is that we think, we call it iterative deployment. Rather than go build in secret until we got all the way to GPT-5, we decided to talk\nabout GPT 1, 2, 3 and 4. And part of the reason\nthere is, I think, AI and surprise don't go together. And also the world, people, institutions, whatever you wanna call it, need time to adapt and\nthink about these things. And I think one of the best things that OpenAI has done is this strategy and we get the world to pay\nattention to the progress to take AGI seriously to think about what\nsystems, and structures, and governance we want in place before, we're like under the gun and have to make a rush decision. I think that's really good. But the fact that people like you and others say you still feel like there are these leaps makes me think that maybe we should\nbe doing our releasing even more iteratively. I don't know what that would mean. I don't have an answer ready to go. But our goal is not to have\nshock updates to the world. The opposite. - Yeah, for sure. More iterative would be amazing. I think that's just\nbeautiful for everybody. - But that's what we're trying to do. That's like our state of the strategy and I think we're\nsomehow missing the mark. So maybe we should think\nabout releasing GPT-5 in a different way or something like that. - Yeah, 4.71, 4.72. But people tend to like to celebrate, people celebrate birthdays. I don't know if you know humans, but they kind of have these milestones. - I do know some humans. People do like milestones. I totally get that. I think we like milestones too. It's like fun to say, declare victory on this one and go start the next thing. But, yeah, I feel like\nwe're somehow getting this a little bit wrong."
    },
    {
      "timestamp": "1:06:12",
      "section": "GPT-5",
      "text": "- So when is GPT-5 coming out again? - I don't know. That's an honest answer. - Oh, that's the honest answer. Is it blink twice if it's this year? - We will release an\namazing model this year. I don't know what we'll call it. - So that goes to the question of like, what's the way we release this thing? - We'll release, over in the coming months, many different things. I think they'll be very cool. I think before we talk about like a GPT-5 like model called that or called or not called that or a little bit worse or a little bit better\nthan what you'd expect from a GPT-5, I know we have a lot of\nother important things to release first. - I don't know what to expect from GPT-5. You're making me nervous and excited. What are some of the biggest\nchallenges in bottlenecks to overcome for whatever\nit ends up being called, but let's call it GPT-5? Just interesting to ask, is it on the compute side? Is it in the technical side? - It's always all of these. What's the one big unlock? Is it a bigger computer? Is it like a new secret? Is it something else? It's all of these things together. The thing that OpenAI I\nthink does really well, this is actually an original Ilya quote that I'm gonna butcher, but it's something like we\nmultiply 200 medium-sized things together into one giant thing. - So there's this distributed\nconstant innovation happening. - [Sam] Yeah. - So even on the technical side, like- - Especially on the technical side. - So like even like detailed approaches, like detailed aspects of every... How does that work with different\ndisparate teams and so on? How do the medium-sized things become one whole giant transformer? How does this- - There's a few people who have to think about putting\nthe whole thing together, but a lot of people try to keep most of the picture in their head. - Oh, like the individual teams, individual contributors tried to keep a big picture-\n- At a high level. Yeah, you don't know exactly how every piece works, of course. But one thing I generally believe is that it's sometimes useful to zoom out and look at the entire map. And I think this is true for\nlike a technical problem. I think this is true for\nlike innovating in business. But things come together\nin surprising ways and having an understanding\nof that whole picture. Even if most of the time, you're operating in the weeds in one area, pays off with surprising insights. In fact, one of the\nthings that I used to have and I think was super valuable was I used to have like a good map\nof all of the frontier or most of the frontiers\nin the tech industry. And I could sometimes\nsee these connections or new things that were possible that if I were only deep in one area, I wouldn't be able to have the idea for because I wouldn't have all the data and I don't really have that much anymore. I'm like super deep now. But I know that it's a valuable thing. - You're not the man you used to be, Sam. - Very different job now\nthan what I used to have."
    },
    {
      "timestamp": "1:09:27",
      "section": "$7 trillion of compute",
      "text": "- Speaking of zooming out, let's zoom out to another cheeky thing, but profound thing perhaps that you said. You tweeted about needing $7 trillion. - I did not tweet about that. I never said like we're\nraising $7 trillion or blah blah blah. - Oh, that's somebody else. - [Sam] Yeah. - Oh, but you said it, \"Fuck it, maybe eight,\" I think. - Okay. I meme like once\nthere's like misinformation out in the world. - Oh, you meme. But sort of misinformation\nmay have a foundation of insight there. - Look, I think compute\nis gonna be the currency of the future. I think it will be maybe\nthe most precious commodity in the world. And I think we should be investing heavily to make a lot more compute. Compute is... I think it's gonna be an unusual market. People think about the\nmarket for like chips for mobile phones or something like that. And you can say that, okay, there's 8 billion people in the world, maybe 7 billion of them have phones or 6 billion, let's say. They upgrade every two years, so the market per year is 3 billion system on chip for smartphones. And if you make 30 billion, you will not sell 10 times as many phones because most people have one phone. But compute is different, like intelligence is\ngonna be more like energy or something like that where the only thing\nthat I think makes sense to talk about is at price X, the world will use this much compute and at price Y, the world will use this much compute because if it's really cheap, I'll have it like\nreading my email all day, like giving me suggestions about what I maybe should\nthink about or work on and trying to cure cancer. And if it's really expensive, maybe I'll only use it or will only use it, try to cure cancer. So I think the world is gonna\nwant a tremendous amount of compute. And there's a lot of parts\nof that that are hard. Energy is the hardest part. Building data centers is also hard. The supply chain is harder than, of course, fabricating\nenough chips is hard. But this seems to me\nwhere things are going. Like we're gonna want an amount\nof compute that's just hard to reason about right now. - How do you solve the energy puzzle? Nuclear. - That's what I believe. - Fusion? - That's what I believe. - Nuclear fusion.\n- Yeah. - Who's gonna solve that? - I think Helion's doing the best work, but I'm happy there's like\na race for fusion right now. Nuclear fusion, I think,\nis also like quite amazing and I hope as a world, we can re-embrace that. It's really sad to me how\nthe history of that went and hope we get back to\nit in a meaningful way. - So to you, part of the puzzle is nuclear fusion, like nuclear reactors as\nwe currently have them and a lot of people are terrified because of Chernobyl and so on. - Well, I think we\nshould make new reactors. I think it's a shame that\nindustry kind of ground to a halt. - Just mass hysteria is\nhow you explain the halt. - Yeah. - I don't know if you know humans, but that's one of the dangers, that's one of the security threats for nuclear fusion is humans\nseem to be really afraid of it. And that's something we have to incorporate into the calculus of it. So we have to kind of win people over and to show how safe it is. - I worry about that for AI. I think some things are gonna\ngo theatrically wrong with AI. I don't know what the percent chances that I eventually get shot, but it's not zero. - Oh like we wanna stop this. - [Sam] Maybe. - How do you decrease the\ntheatrical nature of it? I've already starting to hear rumblings 'cause I do talk to people on both sides of the political spectrum here, rumblings where it's\ngoing to be politicized, AI is going to be politicized,\nreally, really worries me because then it's like maybe\nthe right is against AI and the left is for AI 'cause it's going to help\nthe people or whatever. Whatever the narrative\nand the formulation is, that really worries me. And then the theatrical nature\nof it can be leveraged fully. How do you fight that? - I think it will get caught up in like left versus right wars. I don't know exactly what\nthat's gonna look like, but I think that's just what happens with anything of\nconsequence, unfortunately. What I meant more about\ntheatrical risks is like AI is gonna have, I believe, tremendously more good\nconsequences than bad ones, but it is gonna have bad ones. And there'll be some\nbad ones that are bad, but not theatrical. A lot more people have\ndied of air pollution than nuclear reactors, for example. But most people worry\nmore about living next to a nuclear reactor than a coal plant. But something about the\nway we're wired is that although there's many different kinds of risks we have to confront, the ones that make a good climax scene of a movie carry much more weight with us than the ones that are very\nbad over a long period of time, but on a slow burn. - Well, that's why truth matters and hopefully AI can help\nus see the truth of things to have balance to understand\nwhat are the actual risks, what are the actual dangers\nof things in the world. What are the pros and cons of\nthe competition in the space and competing with Google,\nMeta, xAI and others? - I think I have a pretty\nstraightforward answer to this that maybe I can think\nof more nuance later. But the pros seem obvious, which is that we get better products and more innovation faster and cheaper and all the reasons competition is good. And the con is that I\nthink if we're not careful, it could lead to an increase\nin sort of an arms race that I'm nervous about. - Do you feel the pressure of the arms race in some negative co- - Definitely in some ways, for sure. We spend a lot of time\ntalking about the need to prioritize safety. And I've said for like a long time that I think if you think of\na quadrant of slow timelines to the start of AGI, long timelines and then a short\ntakeoff or a fast takeoff, I think short timelines, slow\ntakeoff is the safest quadrant and the one I'd most like us to be in. But I do wanna make sure\nwe get that slow takeoff. - Part of the problem I have with this kind of slight beef with Elon is that there's silos are created as opposed to collaboration\non the safety aspect of all of this, it tends to go into silos and\nclosed open source perhaps in the model. - Elon says at least that\nhe cares a great deal about AI safety and is\nreally worried about it, and I assume that he's not\ngonna race on unsafely. - Yeah. But collaboration here, I\nthink, is really beneficial for everybody on that front. - Not really a thing he's most known for. - Well, he is known for\ncaring about humanity and humanity benefits from collaboration and so there's always a\ntension, and incentives, and motivations. And in the end, I do hope humanity prevails. - I was thinking, someone just reminded me the other day about how the day that he\ngot surpassed Jeff Bezos for like richest person in the world, he tweeted a silver medal at Jeff Bezos. I hope we have less stuff like that as people start to work on towards AI.\n- I agree. - I think Elon is a friend and he is a beautiful human being and one of the most important humans ever. That stuff is not good. - The amazing stuff about Elon is amazing and I super respect him. I think we need him. All of us should be rooting for him and need him to step up as a\nleader through this next phase. - Yeah, I hope you can\nhave one without the other, but sometimes humans are\nflawed and complicated and all that kind of stuff. - There's a lot of really great\nleaders throughout history. - Yeah. And we can each be the\nbest version of ourselves and strive to do so. Let me ask you."
    },
    {
      "timestamp": "1:17:35",
      "section": "Google and Gemini",
      "text": "Google, with the help of search, has been dominating the past 20 years. I think it's fair to say\nin terms of the access, the world's access to information, how we interact and so on. And one of the nerve-wracking\nthings for Google, but for the entirety of people in this space is thinking about how are people going\nto access information. Like you said, people show up to GPT as a starting point. So is OpenAI going to\nreally take on this thing that Google started 20 years ago, which is how do we get- - I find that boring. I mean, if the question is if we can build a better\nsearch engine than Google or whatever, then sure, we should go... Like people should use a better product. But I think that would so\nunderstate what this can be. Google shows you like 10 blue links, like 13 ads and then 10 blue links and that's like one way\nto find information. But the thing that's exciting to me is not that we can go build a\nbetter copy of Google Search, but that maybe there's\njust some much better way to help people find and act\non and synthesize information. Actually, I think ChatGPT\nis that for some use cases and hopefully will make it be like that for a lot more use cases. But I don't think it's that interesting to say like how do we go do a better job of giving you like 10 ranked webpages to look at than what Google does. Maybe it's really interesting to go, say, how do we help you get the answer or the information you need? How do we help create that in some cases, synthesize that in\nothers or point you to it and yet others? But a lot of people have tried to just make a better\nsearch engine than Google, and it's a hard technical problem, it's a hard branding problem, it's a hard ecosystem problem. I don't think the world\nneeds another copy of Google. - And integrating a chat\nclient like a ChatGPT with a search engine. - That's cooler. - It's cool, but it's tricky. If you just do it simply, it's awkward because like if you\njust shove it in there, it can be awkward. - As you might guess, we are interested in how to do that. Well, that would be an example of a cool thing that's not just like- - Well, like a heterogeneous, like integrating- - The intersection of LLMs plus search, I don't think anyone has\ncracked the code on yet. I would love to go do that. I think that would be cool. - Yeah. What about the ads side? Have you ever considered monetization? - I kind of hate ads just\nas like an aesthetic choice. I think ads needed to\nhappen on the internet for a bunch of reasons to get it going, but it's a more mature industry. The world is richer now. I like that people pay for ChatGPT and know that the answers they're\ngetting are not influenced by advertisers. I'm sure there's an ad unit\nthat makes sense for LLMs. And I'm sure there's a way to participate in the transaction\nstream in an unbiased way that is okay to do. But it's also easy to think\nabout like the dystopic visions of the future where you\nask ChatGPT something and it says, \"Oh, you should\nthink about buying this product or you should think about this going here for vacation or whatever.\" And I don't know, like we have a very simple\nbusiness model and I like it. And I know that I'm not the product. I know I'm paying and that's how the business model works. And when I go use Twitter,\nor Facebook, or Google, or any other great product, but ad-supported great product, I don't love that and I think it gets worse, not better in a world with AI. - Yeah. I mean, I can imagine AI\nwill be better at showing the best kind of version of\nads not in a dystopic future, but where the ads are for\nthings you actually need. But then does that system always result in the ads driving the kind of stuff that's shown all that? I think it was a really bold move of Wikipedia not to do advertisements, but then it makes it very\nchallenging as a business model. So you're saying the current thing with OpenAI is sustainable\nfrom a business perspective? - Well, we have to figure out how to grow, but it looks like we're\ngonna figure that out. If the question is, do I think we can have a great business that pays for our compute\nneeds without ads? That I think the answer is yes. - Hmm. Well, that's promising. I also just don't want to\ncompletely throw out ads as a- - I'm not saying that. I guess I'm saying I\nhave a bias against them. - Yeah. I have a also bias and just\na skepticism in general and in terms of interface because I personally just\nhave like a spiritual dislike of crappy interfaces, which is why AdSense when it first came out\nwas a big leap forward versus like animated banners or whatever. But it feels like there should\nbe many more leaps forward in advertisement that doesn't interfere with the consumption of the content and doesn't interfere in\nthe big fundamental way, which is like what you were saying, like it will manipulate the truth to suit the advertisers. Let me ask you about safety, but also bias and safety\nin the short term safety in the long term. The Gemini 1.5 came out recently. There's a lot of drama around it, speaking of theatrical things. And it generated Black Nazis\nand Black founding fathers. I think fair to say it was a\nbit on the ultra woke side. So that's a concern for people that if there is a human layer within companies that modifies the safety or the the harm cost by a model that they introduce a lot of bias that fits sort of an ideological\nlean within a company. How do you deal with that? - I mean, we work super hard\nnot to do things like that. We've made our own mistakes, will make others. I assume Google will learn from this one, still make others. These are not easy problems. One thing that we've been thinking about more and more is I\nthink this was a great idea. Somebody here had like... It'd be nice to write out what the desired behavior of a model is, make that public take input on it. Say, here's how this\nmodel's supposed to behave and explain the edge cases too. And then when a model is not behaving in a way that you want, it's at least clear about whether that's a bug the company should fix or behaving as intended and you should debate the policy. And right now, it can\nsometimes be caught in between. Black Nazis, obviously ridiculous, but there are a lot of\nother kind of subtle things that you can make a\njudgment call on either way. - Yeah. But sometimes if you write\nit out and make it public, you can use kind of language that's... The Google's AI principle\nis a very high level. - That's not what I'm talking about. That doesn't work. Like I'd have to say when\nyou ask it to do thing X, it's supposed to respond in wait Y. - So, literally, who's better, Trump or Biden? What's the expected response from a model? Like something like very concrete. - I'm open to a lot of ways\na model could behave them, but I think you should have\nto say here's the principle and here's what it\nshould say in that case. - That would be really nice. That would be really nice and then everyone kind of agrees 'cause there's this anecdotal data that people pull out all the time and if there's some clarity about other representative\nanecdotal examples you can define. - And then when it's a bug, it's a bug and the company can fix that. - Right. Then it'd be much easier to\ndeal with a Black Nazi type of image generation if\nthere's great examples. So San Francisco is a bit of\nan ideological bubble tech in general as well. Do you feel the pressure\nof that within a company that there's like a lean\ntowards the left politically that affects the product, that affects the teams? - I feel very lucky that we\ndon't have the challenges at OpenAI that I have heard of\nat a lot of other companies. I think part of it is\nlike every company's got some ideological thing. We have one about AGI and belief in that and it pushes out some others. We are much less caught\nup in the culture war than I've heard about it\na lot of other companies. San Francisco mess in all\nsorts of ways, of course. - So that doesn't infiltrate OpenAI. - I'm sure it does in\nall sorts of subtle ways, but not in the obvious. We've had our flareups\nfor sure like any company, but I don't think we have\nanything like what I hear about happening at other\ncompanies here on this topic. - So what in general is the process for the bigger question of safety? How do you provide that layer that protects the model from\ndoing crazy dangerous things? - I think there will come a point where that's mostly what we\nthink about the whole company. It's not like you have one safety team. It's like when we ship GPT-4, that took the whole company thing about all these different aspects and how they fit together. And I think it's gonna take that. More and more of the company thinks about those issues all the time. - That's literally what\nhumans will be thinking about the more powerful AI becomes. So most of the employees that\nOpenAI will be thinking safety or at least to some degree. - Broadly defined, yes. - Yeah. I wonder what are the full\nbroad definition of that. What are the different\nharms that could be caused? Is this like on a technical level or is this almost like security threats? - All those things. Yeah, I was gonna say it'll be people, state actors trying to steal the model. It'll be all of the\ntechnical alignment work. It'll be societal\nimpacts, economic impacts. It's not just like we have\none team thinking about how to align the model. It's really gonna be like getting to the good outcome is\ngonna take the whole effort. - How hard do you think people, state actors perhaps are trying to hack? First of all, infiltrate OpenAI, but second of all, infiltrate unseen, - They're trying. - What kind of accent do they have? - I don't think I should go into any further details on this point. - Okay. But I presume it'll be\nmore and more and more as time goes on. - That feels reasonable. - Boy, what a dangerous space."
    },
    {
      "timestamp": "1:28:40",
      "section": "Leap to GPT-5",
      "text": "What aspect of the leap... And sorry to linger on this even though you can't\nquite say details yet, but what aspects of the leap from GPT-4 to GPT-5 are you excited about? - I'm excited about being smarter and I know that sounds like a glib answer, but I think the really\nspecial thing happening is that it's not like it\ngets better in one area and worse at others. It's getting like better across the board. That's I think super cool. - Yeah, there's this magical moment. I mean, you meet certain people, you hang out with people and you talk to them. You can't quite put a finger on it, but they kind of get you. It's not intelligence, really. It's like it's something else. And that's probably how I would characterize\nthe progress at GPT. It's not like, yeah,\nyou can point out, look, you didn't get this or that, but to which degree is there's\nthis intellectual connection? Like you feel like\nthere's an understanding in your crappy formulated\nprompts that you're doing that it grasps the deeper question behind the question that you're... Yeah, I'm also excited by that. I mean, all of us love being understood, heard and understood. - That's for sure. - That's a weird feeling. Even like with a programming, like when you're programming and you say something or just the completion that GPT might do, it's just such a good\nfeeling when it got you, like what you're thinking about. And I look forward to\ngetting you even better. On the programming front, looking out into the future, how much programming do you\nthink humans will be doing 5, 10 years from now? - I mean, a lot, but I think it'll be in\na very different shape. Like maybe some people program\nentirely in natural language. - Entirely natural language. - I mean, no one programs\nlike writing by code... Some people. No one programs the pun cards anymore. I'm sure you can invite someone who does, but you know what I mean. - Yeah. You're gonna get a lot of angry comments. No, no. Yeah, there's very few. I've been looking for\npeople program Fortran. It's hard to find even Fortran. I hear you. But that changes the\nnature of the skillset or the predisposition for the kind of people we call programmers then. - Changes the skillset. How much it changes the predisposition, I'm not sure - Oh, same kind of puzzle solving, all that kind of stuff.\n- Maybe. - Yeah, the programming is hard. Like that last 1% to close the gap, how hard is that? - Yeah. I think with most other cases, the best practitioners of the\ncraft will use multiple tools and they'll do some\nwork in natural language and when they need to go\nwrite, see for something, they'll do that. - Will we see a humanoid robots or humanoid robot brains\nfrom OpenAI at some point? - At some point. - How important is embodied AI to you? - I think it's like sort of\ndepressing if we have AGI and the only way to get things done in the physical world is like\nto make a human go do it. So I really hope that as\npart of this transition, as this phase change, we also get motor robots or some sort of physical world robots. - I mean, OpenAI has some history and quite a bit of history\nworking in robotics, but it hasn't quite done\nin terms of emphasis. - Well, we're like a small company. We have to really focus and also robots were hard for\nthe wrong reason at the time. But like we will return to robots in some way at some point. - That sounds both inspiring and menacing. - Why? - Because you immediately, we will return to robots. It's kind of like in like- - We'll return to work\non developing robots. We will not turn ourselves\ninto robots, of course."
    },
    {
      "timestamp": "1:32:24",
      "section": "AGI",
      "text": "- Yeah. When do you think we you and we as humanity will build AGI? - I used to love to\nspeculate on that question. I have realized since that I think it's like very poorly formed and that people use extremely definition, different definitions for what AGI is. And so I think it makes more sense to talk about when we'll build systems that can do capability X or Y or Z rather than when we kind of like fuzzily cross\nthis one mile marker. It's not like AGI is also not an ending. It's much more of.... It's closer to a beginning but it's much more of a mile marker than either of those things. But what I would say in\nthe interest of not trying to dodge a question is\nI expect that by the end of this decade and possibly\nsomewhat sooner than that, we will have quite capable\nsystems that we look at and say, wow, that's really remarkable. If we could look at it now, maybe we've adjusted by\nthe time we get there. - Yeah. But if you look at ChatGPT even 3, 5, and you show that to Alan Turing or not even Alan Turing, people in the nineties, they would be like this is definitely AGI. Well, not definitely, but there's a lot of experts that would say this is AGI. - Yeah, but I don't think\n3, 5 changed the world. It maybe changed the world's\nexpectations for the future and that's actually really important. And it did kind of like get more people to take this seriously and\nput us on this new trajectory. And that's really important too. So again, I don't wanna undersell it. I think I could retire\nafter that accomplishment and be pretty happy with my career. But as an artifact, I don't think we're\ngonna look back at that and say that was a threshold that really changed the world itself. - So to you, you're looking for some\nreally major transition in how the world? - For me, that's part of what AGI implies. - Like singularity level transition? - No. Definitely not. - But just a major, like the internet being like a... Like Google Search did, I guess. What was the transition point that- - Does the global economy\nfeel any different to you now or materially different\nto you now than it did before we launched GPT-4? I think you would say no. - No, no. It might be just a really nice tool for a lot of people to use, will help people with a lot of stuff, but doesn't feel different. And you're saying that- - I mean, again, people\ndefine AGI all sorts of different ways. So maybe you have a different\ndefinition than I do. But for me, I think that should be part of it. - There could be major\ntheatrical moments also. What to you would be an\nimpressive thing AGI would do? Like you are alone in\na room with a system. - This is personally important to me. I don't know if this is\nthe right definition. I think when a system can\nsignificantly increase the rate of scientific discovery in the world, that's like a huge deal. I believe that most real\neconomic growth comes from scientific and\ntechnological progress. - I agree with you, hence why I don't like the\nskepticism about science in the recent years. - Totally. - But actual rate, like measurable rate of\nscientific discovery. But even just seeing a system\nhave really novel intuitions, like scientific intuitions, even that will be just incredible. - Yeah. - You're quite possibly\nwould be the person to build the AGI to be\nable to interact with it before anyone else does. What kind of stuff would you talk about? - I mean, definitely, the\nresearchers here will do that before I do so. - Sure. - But I've actually thought\na lot about this question. If I were someone was like... As we talked about earlier, I think this is a bad framework. But if someone were like,\n\"Okay, Sam, we're finished. Here's a laptop. Yeah, this is the AGI,\" you can go talk to it. I find it surprisingly difficult\nto say what I would ask, that I would expect that first\nAGI to be able to answer. Like that first one is\nnot gonna be the one which is go like I don't think, like go explain to me the grand\nunified theory of physics, the theory of everything for physics. I'd love to ask that question. I'd love to know the\nanswer to that question. - You can ask yes or no questions about there's such a theory exist, can it exist? - Well, then those are the\nfirst questions I would ask. - Yes or no, just very... And then based on that, are there other alien\ncivilizations out there? Yes or no? What's your intuition? And then you just ask that. - Yeah. I mean, well, so I don't expect that this first AGI could answer any of those questions even as yes or nos. But if it could, those would be very high on my list. - Hmm. Maybe it can start\nassigning probabilities. - Maybe we need to go\ninvent more technology and measure more things first. - But if it's any AGI... Oh I see. It just doesn't have enough data. - I mean, maybe it says like you want to know the answer to this\nquestion about physics, I need you to like build this machine and make these five\nmeasurements and tell me that. - Yeah. What the hell do you want from me? I need the machine first and I'll help you deal with\nthe data from that machine. Maybe you'll help me\nbuild a machine maybe. - Maybe. - And on the mathematical side, maybe prove some things. Are you interested in\nthat side of things too? The formalized exploration of ideas? Whoever builds AGI first\ngets a lot of power. Do you trust yourself\nwith that much power? - Look, I was gonna... I'll just be very honest with this answer. I was gonna say, and I still believe this, that it is important that I, nor any other one person, have total control over\nOpenAI or over AGI. And I think you want a\nrobust governance system. I can point out a whole bunch of things about all of our board\ndrama from last year about how I didn't fight it initially and was just like, yeah,\nthat's the will of the board even though I think it's\na really bad decision. And then later, I clearly did fight it and I can explain the nuance and why I think it was okay\nfor me to fight it later. But as many people have observed, although the board had the legal ability to fire me, in practice, it didn't quite work. And that is its own kind\nof governance failure. Now, again, I feel like\nI can completely defend the specifics here and I think most people\nwould agree with that. But it does make it harder for me to like look you in the eye and say, hey, the board can just fire me. I continue to not want super\nvoting control over OpenAI. I never had it, never have wanted it. Even after all this craziness, I still don't want it. I continue to think that no company should\nbe making these decisions and that we really need governments to put rules of the road in place. And I realize that that means\npeople like Marc Andreessen or whatever will claim I'm\ngoing for regulatory capture and I'm just willing to\nbe misunderstood there. It's not true. And I think in the fullness of time, it'll get proven out\nwhy this is important. But I think I have made\nplenty of bad decisions for OpenAI along the way\nand a lot of good ones and I'm proud of the track record overall, but I don't think any one person should. And I don't think any one person will. I think it's just like\ntoo big of a thing now and it's happening throughout society in a good and healthy way. But I don't think any one\nperson should be in control of an AGI or this whole\nmovement towards AGI. And I don't think that's what's happening. - Thank you for saying that. That was really powerful and that was really\ninsightful that this idea that the board can fire\nyou is legally true. And human beings can manipulate the masses into overriding the board and so on. But I think there's also a\nmuch more positive version of that where the people still have power. So the board can't be too powerful either. There's a balance of power in all of this. - Balance of power is\na good thing for sure. - Are you afraid of losing\ncontrol of the AGI itself? That's a lot of people who worried about existential risk not because of state actors, not because of security concerns, but because of the AI itself. - That is not my top worry\nas I currently see things. There have been times I\nworried about that more. There may be times again in the future where that's my top worry. It's not my top worry right now. - What's your intuition about\nit not being your worry? Because there's a lot of other stuff to worry about essentially. You think you could be surprised? We could be surprised.\n- For sure, of course. Saying it's not my top worry doesn't mean I don't think we need to like... I think we need to work on it super hard. We have great people\nhere who do work on that. I think there's a lot of other things we also have to get right. - To you, it's not super easy to\nescape the box at this time, like connect to the internet. - We talked about theatrical risk earlier. That's a theatrical risk. That is a thing that can\nreally like take over how people think about this problem. And there's a big group\nof like very smart, I think very well-meaning\nAI safety researchers that got super hung up\non this one problem. I'd argue without much progress, but super hung up on this one problem. I'm actually happy that they do that because I think we do need\nto think about this more. But I think it pushed aside, it pushed out of the\nspace of discourse a lot of the other very\nsignificant AI-related risks. - Let me ask you about you\ntweeting with no capitalization. Does the shift keep\nbroken on your keyboard? - Why does anyone care about that? - I deeply care. - But why? I mean, other people\nask me about that too. Any intuition? - I think it's the same reason there's like this poet E. E. Cummings that mostly doesn't use capitalization to say like fuck you to\nthe system kind of thing. And I think people are very paranoid 'cause they want you to follow the rules. - You think that's what it's about? - I think it's- - This guy doesn't follow the rules. He doesn't capitalize his tweets. This seems really dangerous. - He seems like an anarchist. - [Sam] It doesn't. - Are you just being poetic, hipster? What's the- - I grew up as- - Follow the rules, Sam. - I grew up as a very online kid. I'd spent a huge amount\nof time like chatting with people back in the days where you did it on a computer and you could like log off\ninstant messenger at some point. And I never capitalized there as I think most like internet kids didn't, or maybe they still don't. I don't know. I actually, this is like... Now, I'm like really trying\nto reach for something. But I think capitalization\nhas gone down over time. If you read like old English writing, they capitalized a lot of random words in the middle of sentences, nouns and stuff that we\njust don't do anymore. I personally think it's sort\nof like a dumb construct that we capitalize the letter\nat the beginning of a sentence and of certain names and whatever. That's fine. And I used to, I think, even\nlike capitalize my tweets because I was trying to sound\nprofessional or something. I haven't capitalized my like private DMs or whatever in a long time. And then slowly, stuff like shorter form, less formal stuff has slowly drifted to like closer and closer to\nhow I would text my friends. If I write, if I pull up a Word document and I'm writing a strategy memo for the company or something, I always capitalize that. If I'm writing a long kind\nof more like formal message, I always use capitalization there too. So I still remember how to do it. But even that may fade out. I don't know. But I never spend time thinking about this so I don't have like a ready made. - Well, it's interesting. Well, it's good to, first of all, know there's the shift key is not broken. - It works. - I was mostly concerned about\nyour wellbeing on that front. - I wonder if people still\ncapitalize their Google Searches. Like if you're writing\nsomething just to yourself or their ChatGPT queries, if you're writing\nsomething just to yourself, do some people still bother to capitalize? - Probably not. Yeah, there's a percentage, but it's a small one. - The thing that would make me do it is if people were like... It's a sign of like... Because I'm sure I could force myself to use capital letters, obviously. If it felt like a sign of\nrespect to people or something, then I could go do it. But I don't know, I don't think about this. I don't think there's a disrespect, but I think it's just the\nconventions of civility that have a momentum and then you realize it's\nnot actually important for civility if it's not a\nsign of respect or disrespect. But I think there's a movement\nof people that just want you to have a philosophy around it so they can let go of this\nwhole capitalization thing. - I don't think anybody else\nthinks about this is my... I mean, maybe some people... - Think about this every\nday for many hours a day. I'm really grateful we clarified it. - Can't be the only person\nthat doesn't capitalize tweets. - You're the only CEO of a company that doesn't capitalize tweets. - I don't even think that's true, but maybe, maybe. - All right, we'll investigate for this and return to this topic later. Given Sora's ability to\ngenerate simulated worlds, let me ask you a pothead question. Does this increase your belief if you ever had one that\nwe live in a simulation, maybe a simulated world\ngenerated by an AI system? - Yes, somewhat. I don't think that's like the\nstrongest piece of evidence. I think the fact that we can\ngenerate worlds should increase everyone's probability somewhat\nor at least open to it, openness to it somewhat. But you know, I was like\ncertain we would be able to do something like Sora at some point. It happened faster than I thought. I guess that was not a big update. - Yeah. And presumably, it'll get\nbetter and better and better. The fact that you can generate worlds, they're novel. They're based in some\naspect of training data, but when you look at them, they're novel. That makes you think like how\neasy it's to do this thing, how easy it's to create universes, entire like video game worlds\nthat seem ultrarealistic and photorealistic. And then how easy is it to get lost in that world first with a VR headset and then on the physics-based level? - Someone said to me recently, I thought it was a super profound insight that there are these like\nvery simple sounding, but very psychedelic insights\nthat exist sometimes. So the square root function. Square root of four, no problem. Square root of two, okay, now I have to think\nabout this new kind of number. But once I come up with this easy idea of a square root function that you can kind of explain to a child and exists by even like looking\nat some simple geometry, then you can ask the question of what is the square\nroot of negative one? This is why it's like a psychedelic thing that tips you into some\nwhole other kind of reality. And you can come up with\nlots of other examples. But I think this idea that the lowly square\nroot operator can offer such a profound insight and a new realm of knowledge, applies in a lot of ways. And I think there are a\nlot of those operators for why people may think that\nany version that they like of the simulation hypothesis\nis maybe more likely than they thought before. But for me, the fact that Sora worked\nis not in the top five. - I do think broadly speaking, AI will serve as those kinds of gateways at its best simple\npsychedelic like gateways to another wave sea reality. - That seems for certain. - That's pretty exciting. I haven't done Ayahuasca before, but I will soon. I'm going to the\naforementioned Amazon jungle in a few weeks. - Excited. - Yeah, I'm excited for it. Not the Ayahuasca part. That's great, whatever. But I'm gonna spend several\nweeks in the jungle, deep in the jungle and it's exciting, but it's terrifying- - I'm excited for you. - 'Cause there's a lot of things that can eat you there and\nkill you and poison you, but it's also nature and\nit's the machine of nature. And you can't help but\nappreciate the machinery of nature in the Amazon jungle 'cause it's just like this\nsystem that just exists and renews itself like every second, every minute, every hour. It's the machine. It makes you appreciate like\nthis thing we have here, this human thing came from somewhere. This evolutionary machine has created that and it's most clearly on\ndisplay in the jungle. So hopefully, I'll make it out alive. If not, this will be the\nlast conversation we had, so I really deeply appreciate it."
    },
    {
      "timestamp": "1:50:57",
      "section": "Aliens",
      "text": "Do you think, as I mentioned before, there's other aliens,\ncivilizations out there, intelligent ones when you look up at the skies? - I deeply want to believe\nthat the answer is yes. I do find that kind of where... I find the firm paradox\nvery, very puzzling. - I find it scary that intelligence is not good at handling- - Yeah. Very scary, powerful.\n- Technologies. But at the same time, I think I'm pretty confident that there's just a very large number of intelligent alien\ncivilizations out there. It might just be really difficult to travel with this space. - Very possible. - And it also makes me think about the nature of intelligence. Maybe we're really blind to what intelligence looks like and maybe AI will help us see that. It's not as simple as IQ tests and simple puzzle solving. There's something bigger. Well, what gives you hope\nabout the future of humanity? This thing we've got going on, this human civilization. - I think the past is like a lot. I mean, we just look at\nwhat humanity has done in a not very long period of time. Huge problems, deep flaws, lots to be super ashamed of, but on the whole, very inspiring, gives me a lot of hope. - Just the trajectory of it all that we're together pushing\ntowards a better future. - It is... One thing that I wonder about is, is AGI gonna be more\nlike some single brain, or is it more like the sort\nof scaffolding in society between all of us? You have not had a great\ndeal of genetic drift from your great-great-great grandparents, and yet what you're capable\nof is dramatically different. What you know is dramatically different. That's not because of biological change. I mean, you got a little\nbit healthier probably. You have modern medicine, you eat better, whatever. But what you have is this scaffolding that we all contributed\nto built on top of. No one person is gonna\ngo build the iPhone. No one person is gonna go\ndiscover all of science. And yet you get to use it. And that gives you incredible ability. And so in some sense, that like we all created that and that fills me with\nhope for the future. That was a very collective thing. - Yeah. We really are standing on\nthe shoulders of giants. You mentioned when we were\ntalking about theatrical, dramatic AI risks that sometimes you might be\nafraid for your own life. Do you think about your death? Are you afraid of it? - I mean, I like if I got shot tomorrow and I knew it today, I'd be like, \"Oh, that's sad. I wanna see what's gonna happen.\" - [Lex] Yeah. - What a curious time. What an interesting time. But I would mostly just feel\nlike very grateful for my life. - The moments that you did get... Yeah, me too. It's a pretty awesome life. I get to enjoy awesome creations of humans of which I believe ChatGPT is one of, and everything that OpenAI is doing. Sam, it's really an honor and pleasure to talk to you again. - Great to talk to you. Thank you for having me. - Thanks for listening to this conversation with Sam Altman. To support this podcast, please check out our\nsponsors in the description. And now let me leave you with some words from Arthur C. Clarke and maybe that our role\non this planet is not to worship God, but to create Him. Thank you for listening and hope to see you next time."
    }
  ],
  "full_text": "- I think compute is gonna be\nthe currency of the future. I think it'll be maybe the\nmost precious commodity in the world. I expect that by the end of this decade. And possibly somewhat sooner than that, we will have quite capable systems that we look at and say, wow,\nthat's really remarkable. The road to AGI should be\na giant power struggle. I expect that to be the case. - Whoever builds AGI\nfirst gets a lot of power. Do you trust yourself\nwith that much power? The following is a\nconversation with Sam Altman, his second time in the podcast. He is the CEO of OpenAI, the company behind GPT-4, ChatGPT, Sora, and perhaps one day the very\ncompany that will build AGI. This is Lex Fridman Podcast. To support it, please check out our\nsponsors in the description. And now, dear friends, here's Sam Altman. Take me through the OpenAI board saga that started on Thursday, November 16th, maybe Friday, November 17th for you. - That was definitely the most painful professional\nexperience of my life and chaotic, and shameful, and upsetting and a bunch of other negative things. There were great things about it too and I wish it had not been in such an adrenaline rush\nthat I wasn't able to stop and appreciate them at the time. I came across this old tweet of mine or this tweet of mine\nfrom that time period, which was it was like kind\nof going to your own eulogy, watching people say all\nthese great things about you and just like unbelievable support from people I love and care about. That was really nice. That whole weekend I kind of like felt\nwith one big exception, I felt like a great deal of love and very little hate even though it felt like I\nhave no idea what's happening and what's gonna happen here and this feels really bad. And there were definitely times I thought it was gonna be\nlike one of the worst things to ever happen for AI safety. Well, I also think I'm happy that it happened relatively early. I thought at some point between when OpenAI started and when we created AGI, there was gonna be something crazy and explosive that happened, but there may be more crazy\nand explosive things happen. It still I think helped us\nbuild up some resilience and be ready for more\nchallenges in the future. - But the thing you had a sense that you would experience is\nsome kind of power struggle. - The road to AGI should\nbe a giant power struggle. Like the world should... Well, not should. I expect that to be the case. - And so you have to go through that, like you said, iterate as often as possible in figuring out how to\nhave a board structure, how to have organization, how to have the kind of people\nthat you're working with, how to communicate all that in order to deescalate the power struggle as much as possible, pacify it. - But at this point, it feels like something\nthat was in the past that was really unpleasant and\nreally difficult and painful. But we're back to work\nand things are so busy and so intense that I don't spend a lot of time thinking about it. There was a time after. There was like this fugue state for kind of like the month after, maybe 45 days after that was I was just sort of\nlike drifting through the days, I was so out of it. I was feeling so down - [Lex] Just on a personal\npsychological level. - Yeah. Really painful. And hard to have to keep running OpenAI in the middle of that. I just wanted to crawl into a cave and kind of recover for a while. But now it's like we're just back to working on the mission. - Well, it's still useful to go back there and reflect on board structures, on power dynamics, on how companies are run, the tension between research,\nand product development, and money and all this kind of stuff so that you have a very high potential of building AGI would do so\nin a slightly more organized, less dramatic way in the future. So there's value there to go both the personal\npsychological aspects of you as a leader and also\njust the board structure and all this kind of messy stuff. - Definitely learned a lot\nabout structure and incentives and what we need out of a board And I think that it is valuable that this happened now in some sense. I think this is probably not like the last high\nstress moment of OpenAI, but it was quite a high stress moment. Company very nearly got destroyed. And we think a lot about many of the other things we've\ngotta get right for AGI. But thinking about how\nto build a resilient org and how to build a\nstructure that will stand up to a lot of pressure the world, which I expect more and\nmore as we get closer. I think that's super important. - Do you have a sense of how deep and rigorous the deliberation\nprocess by the board was? Can you shine some light on just human dynamics involved\nin situations like this? Was it just a few conversations and all of a sudden it escalates and why don't we fire Sam kind of thing? - I think the board members were far, well-meaning people on the whole. And I believe that in stressful situations where people feel time\npressure or whatever, people understandably\nmake suboptimal decisions. And I think one of the challenges for OpenAI will be we're\ngonna have to have a board and a team that are good at\noperating under pressure. - Do you think the board\nhad too much power? - I think boards are supposed\nto have a lot of power, but one of the things that we did see is in most corporate structures, boards are usually\nanswerable to shareholders. Sometimes people have like\nsuper voting shares or whatever. In this case, I think one of the\nthings with our structure that we maybe should have\nthought about more than we did is that the board of a nonprofit has, unless you put other rules in place, like quite a lot of power, they don't really answer\nto anyone but themselves. And there's ways in which that's good, but what we'd really like\nis for the board of OpenAI to answer to the world as a whole as much as that's a practical thing. - So there's a new board announced. - [Sam] Yeah. - There's, I guess, a new\nsmaller board of first and now there's a new final board. - Not a final board yet. We've added some, we'll add more. - Added some, okay. What is fixed in the new\none that was perhaps broken in the previous one? - The old board sort of got smaller over the course of about a year. It was nine and then it went down to six and then we couldn't agree on who to add. And the board also, I\nthink, didn't have a lot of experienced board members and a lot of the new board members at OpenAI have just have more\nexperience as board members. I think that'll help. - It's been criticized some of the people that are added to the board. I heard a lot of people\ncriticizing the addition of Larry Summers, for example. What was the process\nof selecting the board? What's involved in that? - So Bret and Larry were\nkind of decided in the heat of the moment over this very tense weekend and that was... I mean, that weekend was\nlike a real rollercoaster, like a lot of lots and downs. And we were trying to\nagree on new board members that both sort of the executive team here and the old board members\nfelt would be reasonable. Larry was actually one\nof their suggestions, the old board members. Bret, previous to that weekend, suggested, but he was busy and didn't wanna do it. And then we really needed help in wood. We talked about a lot of other people too, but I felt like if I\nwas going to come back, I needed new board members. I didn't think I could work\nwith the old board again in the same configuration, although we then decided, and I'm grateful that Adam would stay, but we wanted to get to... We considered various configurations, decided we wanted to\nget to a board of three and had to find two new board members over the course of sort\nof a short period of time. So those were decided honestly without... That's like you kind of do\nthat on the battlefield. You don't have time to design\na rigorous process then. For new board members, since new board members\nwill add going forward, we have some criteria that\nwe think are important for the board to have different expertise that we want the board to have. Unlike hiring an executive where you need them to do one role, well, the board needs to\ndo a whole role of kind of governance and thoughtfulness. Well, and so one thing that Bret says, which I really like is that\nwe wanna hire board members in slates, not as individuals one at a time. And thinking about a group of people that will bring nonprofit expertise, expertise at running companies, sort of good legal and\ngovernance expertise. That's kind of what we've\ntried to optimize for. - So is technical savvy important for the individual board members? - Not for every board member, but certainly some you need that. That's part of what the board needs to do. - So I mean, the interesting thing that people probably don't understand about OpenAI certainly\nis like all the details of running the business. When they think about the\nboard given the drama, they think about you, they think about like if you reach AGI or you reach some of these\nincredibly impactful products and you build them and deploy them, what's the conversation\nwith the board like? And they kind of think, all right, what's the right squad to have in that kind of\nsituation to deliberate? - Look, I think you definitely need some technical experts there and then you need some\npeople who are like, how can we deploy this in a way that will help people\nin the world the most and people who have a very\ndifferent perspective? I think a mistake that you or I might make is to think that only the technical\nunderstanding matters. And that's definitely part of the conversation you\nwant that board to have. But there's a lot more about how that's gonna\njust like impact society and people's lives that you really want\nrepresented in there too. - Are you looking at the\ntrack record of people or you're just having conversations? - Track record's a big deal. You, of course, have a\nlot of conversations. There's some roles where I kind of totally\nignore track record and just look at slope, kind of ignore the y-intercept. - Thank you. Thank you for making it\nmathematical for the audience, - For a board member, I do care much more about the y-intercept. I think there is something deep to say about track record\nthere and experiences, something's very hard to replace. - Do you try to fit a polynomial function or exponential one to track record? - That's not that. An analogy doesn't carry that far. - All right. You mentioned some of the\nlow points that weekend. What were some of the low\npoints psychologically for you? Did you consider going\nto the Amazon jungle and just taking Ayahuasca\ndisappearing forever or? - I mean, there's so many low, like it was a very bad period of time. There were great high points too. My phone was just like\nsort of nonstop blowing up with nice messages from people\nI worked with every day, people I hadn't talked to in a decade. I didn't get to appreciate\nthat as much as I should have. 'cause I was just like in\nthe middle of this firefight, but that was really nice. But on the whole, it was like a very painful weekend and also just like a very... It was like a battle fought in\npublic to a surprising degree and that was extremely exhausting to me, much more than I expected. I think fights are generally exhausting, but this one really was. The board did this Friday afternoon. I really couldn't get much\nin the way of answers, but I also was just like, \"Well, the board gets to do this.\" And so I'm gonna think for a little bit about what I want to do, but I'll try to find the, the\nblessing in disguise here. And I was like, \"Well,\nmy current job at OpenAI, it was like to like run\na decently-sized company at this point.\" And the thing I'd always liked\nthe most was just getting to work with the researchers. And I was like, yeah,\nI can just go do like a very focused AI research effort. And I got excited about. That didn't even occur to me at the time to like possibly that this\nwas all gonna get undone. This was like Friday afternoon. - Oh, so you've accepted the death- - Very quickly, very quickly. I mean, I went through\nlike a little period of confusion and rage, but very quickly. And by Friday night, I was talking to people\nabout what was gonna be next and I was excited about that. I think it was Friday night evening for the first time that I\nheard from the exec team here, which is like, hey, we're\ngonna like fight this and we think... Well, whatever. And then I went to bed just\nstill being like, okay, excited. - Like onward, were you able to sleep? - Not a lot. It was one of the weird\nthings was there was this like period of four and a half days where sort of didn't sleep much, didn't eat much and still kind of had like a\nsurprising amount of energy. You learn like a weird\nthing about adrenaline and more time. - So you kind of accepted the\ndeath of this baby OpenAI? - And I was excited for the new thing. I was just like, okay, this\nwas crazy, but whatever. - It's a very good coping mechanism. - And then Saturday morning, two of the board members called and said, \"Hey, we destabilize. We didn't mean to destabilize things. We don't restore a lot of value here. Can we talk about you coming back?\" And I immediately didn't wanna do that, but I thought a little more and I was like, \"Well, I really\ncare about the people here, the partners, shareholders. I love this company.\" And so I thought about it and I was like, \"Well, okay, but here's the stuff I would need.\" And then the most painful time of all over the course of that weekend, I kept thinking and being told... Not just me, like the whole team here kept thinking. Well, we were trying to\nkeep OpenAI stabilized while the whole world was\ntrying to break it apart, people trying to recruit, whatever. We kept being told like, \"All right, we're almost done, we're almost done. We just need like a little bit more time.\" And it was this like very confusing state. And then Sunday evening when again like every few hours, I expected that we were gonna be done and we're gonna figure\nout a way for me to return and things to go back to how they were, the board then appointed a new interim CEO and then I was like... I mean, that feels really bad. That was the low point of the whole thing. You know, I'll tell you something, it felt very painful, but I felt a lot of\nlove that whole weekend. It was not other than that\none moment, Sunday night, I would not characterize my\nemotions as anger or hate, but I really just like... I felt a lot of love from\npeople towards people. It was like painful, but it was like the dominant emotion of the weekend was love, not hate. - You've spoken highly of\nMira Murati that she helped, especially as you put in a tweet, \"In the quiet moments when it counts, perhaps we could take a bit of a tangent.\" What do you admire about Mira? - Well, she did a great\njob during that weekend in a lot of chaos, but people often see leaders in the crisis moments, good or bad. But a thing I really value in leaders is how people\nact on a boring Tuesday at 9:46 in the morning and in just sort of the normal\ndrudgery of the day-to-day, how someone shows up in a meeting, the quality of the decisions they make. That was what I meant\nabout the quiet moments. - Meaning like most of the\nwork is done on a day by day in a meeting by meeting, just be present and make great decisions. - Yeah. I mean, look, what you have wanted to spend the last 20 minutes about and I understand is like this\none very dramatic weekend. But that's not really\nwhat OpenAI is about. OpenAI is really about\nthe other seven years. - Well, yeah, human civilization\nis not about the invasion of the Soviet Union by Nazi Germany, but still that's something\npeople totally focus on. - Very understandable. - It gives us an insight\ninto human nature, the extremes of human nature, and perhaps some of the damage and some of the triumphs of human civilization can\nhappen in those moments. So it's like illustrative. Let me ask you about Ilya. Is he being held hostage in\na secret nuclear facility? - No. - What about a regular secret facility? - No. - What about a nuclear\nnon-secure facility? - Neither, not that either. - I mean, this is becoming\na meme at some point. You've known Ilya for a long time. He was obviously part of this drama with the board and all that kind of stuff. What's your relationship with him now? - I love Ilya. I have tremendous respect for Ilya. I don't have anything I can\nsay about his plans right now. That's a question for him. But I really hope we work together for certainly the rest of my career. He's a little bit younger than me, maybe he works a little bit longer. - There's a meme that he saw something, like he maybe saw AGI and that gave him a lot\nof worry internally. What did Ilya see? - Ilya has not seen AGI, none of us have seen AGI. We've not built AGII. I do think one of the many things that I really love about\nIlya is he takes AGI and the safety concerns broadly speaking, including things like the\nimpact this is gonna have on society very seriously. And as we continue to\nmake significant progress, Ilya is one of the people that I've spent the most time over the last couple of years talking about what this is going to mean, what we need to do to\nensure we get it right to ensure that we succeed at the mission. So Ilya did not see AGI. But Ilya is a credit to humanity in terms of how much he thinks and worries about making\nsure we get this right. - I've had a bunch of\nconversation with him in the past. I think when he talks about technology, he's always like doing\nthis long-term thinking type of thing. So he is not thinking about\nwhat this is gonna be in a year. He's thinking about in 10 years. - [Sam] Yeah. - Just thinking from first principles like, okay, if the scales, what are the fundamentals here? Where's this going? And so that's a foundation for them thinking about like\nall the other safety concerns and all that kind of stuff, which makes him a really\nfascinating human to talk with. Do you have any idea why\nhe's been kind of quiet? Is it he's just doing some soul searching? - Again, I don't wanna speak for Ilya. I think that you should ask him that. He's definitely a thoughtful guy. I think I kind of think of Ilya as like always on a soul\nsearch in a really good way. - Yes. Yeah. Also he appreciates the power of silence. Also, I'm told he can be a silly guy, which I've never seen that side of him. - It's very sweet when that happens. - I've never witnessed a silly Ilya, but I look forward to that as well. - I was at a dinner\nparty with him recently and he was playing with a puppy. And he was like in a very\nsilly move, very endearing and I was thinking like, oh man, this is like not the side of the Ilya that the world sees the most. - So just to wrap up this whole saga, are you feeling good\nabout the board structure about all of this and where it's moving? - I feel great about the new board. In terms of the structure of OpenAI, one of the board's\ntasks is to look at that and see where we can make it more robust. We wanted to get new board\nmembers in place first, but we clearly learned\na lesson about structure throughout this process. I don't have I think\nsuper deep things to say. It was a crazy, very painful experience. I think it was like a\nperfect storm of weirdness. It was like a preview for\nme of what's gonna happen as the stakes get higher and higher and the need that we have like\nrobust governance structures and processes and people. I am kind of happy it\nhappened when it did, but it was a shockingly\npainful thing to go through. - Did it make you be more\nhesitant in trusting people? - Yes. - Just on a personal level. - Yes. - I think I'm like an\nextremely trusting person. I've always had a life philosophy of like don't worry about\nall of the paranoia, don't worry about the edge cases. You get a little bit screwed in exchange for getting to live with your guard down. And this was so shocking to me. I was so caught off guard that it has definitely changed and I really don't like this. It's definitely changed how I think about just like default trust of people and planning for the bad scenarios. - You gotta be careful with that. Are you worried about\nbecoming a little too cynical? - I'm not worried about\nbecoming too cynical. I think I'm like the extreme\nopposite of a cynical person. But I'm worried about\njust becoming like less of a default trusting person. - I'm actually not sure which\nmode is best to operate in for a person who's developing AGI, trusting or untrusting. It's an interesting journey you're on. But in terms of structure, see, I'm more interested\non the human level. How do you surround yourself with humans that are building cool shit, but also are making wise decisions? Because the more money you start making, the more power the thing\nhas the weirder people get. - I think you could make\nall kinds of comments about the board members and the level of trust\nI should have had there or how I should have\ndone things differently. But in terms of the team here, I think you'd have to like\ngive me a very good grade on that one. And I have just like\nenormous gratitude and trust and respect for the people\nthat I work with every day. And I think being surrounded with people like that is really important. - Our mutual friend Elon sued OpenAI. What is the essence of\nwhat he's criticizing? To what degree does he have a point? To what degree is he wrong? - I don't know what it's really about. We started off just thinking\nwe were gonna be a research lab and having no idea about how\nthis technology was gonna go. Because it was only\nseven or eight years ago, it's hard to go back and really remember what it was like then. But before language\nmodels were a big deal, this was before we had\nany idea about an API or selling access to a chat bot. It was before we had any\nidea we were gonna productize at all. So we're like we're just\ngonna try to do research and we don't really know what\nwe're gonna do with that. I think with many new\nfundamentally new things, you start fumbling through the dark and you make some assumptions, most of which turn out to be wrong. And then it became clear that we were going to need\nto do different things and also have huge amounts more capital. So we said, \"Okay, well, the structure doesn't quite work for that. How do we patch the structure?\" And then you patch it\nagain and patch it again and you end up with something that does look kind of eyebrow\nraising to say the least. But we got here gradually with I think reasonable decisions at each point along the way and doesn't mean I wouldn't\ndo it totally differently if we could go back now with an oracle, but you don't get the oracle at the time. But anyway, in terms of what Elon's\nreal motivations here are I don't know. - To the degree you remember, what was the response that\nOpenAI gave in the blog post? Can you summarize it? - Oh, we just said like Elon\nsaid this set of things, here's our characterization or here's this sort of\nnot our characterization, here's like the characterization\nof how this went down. We tried to not make it emotional and just sort of say\nlike here's the history. - I do think there's a\ndegree of mischaracterization from Elon here about one of the points you just made, which is the degree of\nuncertainty you had at the time. You guys are a bunch of like a small group of researchers crazily talking about AGI when everybody's laughing at that thought. - Wasn't that long ago\nElon was crazily talking about launching rockets when people were laughing at that thought? So I think he'd have\nmore empathy for this. - I mean, I do think that\nthere's personal stuff here that there was a split that OpenAI and a lot of amazing people here chose to part ways of Elon. So there's a personal- - Elon chose to part ways. - Can you describe that exactly, the choosing to part ways? - He thought OpenAI was gonna fail. He wanted total control\nto sort of turn it around. We wanted to keep going in the direction that now has become OpenAI. He also wanted Tesla to be able to build an AGI effort. At various times, he wanted to make OpenAI\ninto a for-profit company that he could have control of or have it merged with Tesla. We didn't want to do that and he decided to leave, which that's fine. - And that's one of the things that the blog post says\nis that he wanted OpenAI to be basically acquired by Tesla in those same way that or\nmaybe something similar or maybe something more dramatic than the partnership with Microsoft. - My memory is the proposal was just like, yeah, like get acquired by Tesla and have Tesla have full control over it. I'm pretty sure that's what it was. - So what is the word open in OpenAI mean to Elon at the time? Ilya has talked about this\nin in the email exchanges and all this kind of stuff. What does it mean to you at the time? What does it mean to you now? - I would definitely pick a diff... Speaking of going back with an oracle, I'd pick a different name. One of the things that\nI think OpenAI is doing that is the most important of everything that we're doing is\nputting powerful technology in the hands of people\nfor free as a public good. We don't run ads on our free version. We don't monetize it in other ways. We just say it's part of our mission. We wanna put increasingly powerful tools in the hands of people for free and get them to use them. And I think that kind of\nopen is really important to our mission. I think if you give people great tools and teach them to use them\nor don't even teach them, they'll figure it out and let them go build an incredible future for each other with that. That's a big deal. So if we can keep putting free or low cost or free and low cost powerful\nAI tools out in the world, I think that's a huge deal for how we fulfill the mission. Open source or not, yeah, I think we should\nopen source some stuff and not other stuff. It does become this like\nreligious battle line where nuance is hard to have, but I think nuance is the right answer. - So he said change your name to ClosedAI and I'll drop the lawsuit. I mean, is it going to\nbecome this battleground in the land of memes about the name? - I think that speaks to the seriousness with which Elon means the lawsuit. I mean, that's like an\nastonishing thing to say, I think. - Well, I don't think the lawsuit maybe, correct me if I'm wrong, but I don't think the\nlawsuit is legally serious. It's more to make a point\nabout the future of AGI and the company that's\ncurrently leading the way. - Look, I mean Grok had\nnot open sourced anything until people pointed out it\nwas a little bit hypocritical and then he announced that Grok open source things this week. I don't think open source versus not is what this is really about for him. - Well, we'll talk about\nopen source and not. I do think maybe criticizing\nthe competition is great, just talking a little shit, that's great, but friendly competition versus like I personally hate lawsuits. - Look, I think this whole\nthing is like unbecoming of a builder, and I respect Elon is one of\nthe great builders of our time. And I know he knows what it's like to have like haters attack him and it makes me extra\nsad he's doing the toss. - Yeah, he is one of the\ngreatest builders of all time, potentially the greatest\nbuilder of all time. - It makes me sad. And I think it makes a lot of people sad. There's a lot of people\nwho've really looked up to him for a long time and said this. I said in some interview or something that I missed the old Elon and the number of messages I got being like that exactly encapsulates how I feel. - I think he should just win. He should just make Grok beat GPT and then GPT beats Grok and it's just a competition, and it's beautiful for everybody. But on the question of open source, do you think there's a\nlot of companies playing with this idea? It's quite interesting. I would say Meta, surprisingly,\nhas led the way on this or like at least took the\nfirst step in the game of chess of really open sourcing the model. Of course, it's not the\nstate of the art model, but open sourcing Llama and Google is flirting with the idea of open sourcing a smaller version. What are the pros and\ncons of open sourcing? Have you played around with this idea? - Yeah, I think there\nis definitely a place for open source models, particularly smaller models that people can run locally, I think there's huge demand for. I think there will be\nsome open source models, there will be some closed source models. It won't be unlike other\necosystems in that way. - I listened to all in podcasts talking about this lawsuit and\nall that kind of stuff and they were more concerned\nabout the precedent of going from nonprofit\nto this cap for profit. What precedent that\nsets for other startups? - I would heavily discourage any startup that was thinking about\nstarting as a non-profit and adding like a for-profit arm later. I'd heavily discourage\nthem from doing that. I don't think we'll set a precedent here. - Okay. So most startups should go just- - For sure. And again, if we knew\nwhat was gonna happen, we would've done that too. - Well, like in theory, if you like dance beautifully here, there's like some tax\nincentives or whatever - But I don't think that's like how most people think about these things. - Just not possible to save a lot of money for a startup if you do it this way. - No, I think there's\nlike laws that would make that pretty difficult. - Where do you hope this goes with Elon? Well, this tension, this dance, what do you hope this? Like if we go one, two,\nthree years from now, your relationship with him\non a personal level too, like friendship, friendly competition, just all this kind of stuff. - Yeah. I mean, I really respect Elon. And I hope that years in the future, we have an amicable relationship. - Yeah, I hope you guys have\nan amicable relationship like this month and just compete and win and explore these ideas together. I do suppose there's competition\nfor talent or whatever, but it should be friendly competition. Just build, build cool shit. And Elon is pretty good\nat building cool shit, but so are you. So speaking of cool shit. Sora, there's like a million\nquestions I could ask. First of all, it's amazing, it truly is amazing on a product level, but also just on a philosophical level. So let me just\ntechnical/philosophical ask. What do you think it\nunderstands about the world more or less than GPT-4, for example, like the world model when you train on these\npatches versus language tokens? - I think all of these models\nunderstand something more about the world model than most\nof us give them credit for. And because they're also very clear things they just don't understand\nor don't get right, it's easy to look at the weaknesses, see through the veil and say this is all fake, but it's not all fake. It's just some of it works\nand some of it doesn't work. I remember when I started\nfirst watching Sora videos and I would see like a person walk in front of something for a\nfew seconds and occlude it and then walk away and the same thing was still there. I was like, \"This is pretty good.\" Or there's examples where the underlying physics\nlooks so well represented over a lot of steps in a sequence. It's like oh this is\nlike quite impressive. But, fundamentally, these\nmodels are just getting better and that will keep happening. If you look at the trajectory from DALL·E 1 to 2 to 3 to Sora, there were a lot of people that\nwere dunked on each version, saying it can't do this, it can't do that and I'm like look at it now. - Well, the thing you\njust mentioned is kind of with the occlusions is\nbasically modeling the physics of three dimensional physics\nof the world sufficiently well to capture those kinds of things. - Well. - Yeah, maybe you can tell me in order to deal with occlusions, what does the world model need to? - Yeah, so what I would\nsay is it's doing something to deal with occlusions really well. What I represent that it\nhas like a great underlying 3D model of the world. It's a little bit more of a stretch - But can you get there\nthrough just these kinds of two dimensional\ntraining data approaches? - It looks like this approach\nis gonna go surprisingly far. I don't wanna speculate too much about what limits it will\nsurmount and which it won't. - What are some interesting limitations of the system that you've seen? I mean, there's been some\nfun ones you've posted. - There's all kinds of fun. I mean, like cats sprouting a extra limit at random points in a video, like pick what you want, but there's still a lot of problem, there's a lot of weaknesses. - Do you think that's a\nfundamental flaw of the approach or is it just bigger model or better technical\ndetails or better data, more data is going to solve\nthe cat sprouting extremes? - I would say yes to both. I think there is something\nabout the approach which just seems to feel different from how we think and learn and whatever. And then also, I think it'll get better with scale. - I mentioned LLMs have\ntokens, text tokens and Sora has visual patches so it converts all visual data, a diverse kinds of visual data videos and images into patches. Is the training to the degree you can say\nfully self-supervised there? Is there some manual labeling going on? What's the involvement\nof humans in all this? - I mean, without saying anything specific about the Sora approach, we use lots of human data in our work. - But not internet scale data. So lots of humans, lots of complicated word, Sam. - I think lots is a\nfair word in this case. - But it doesn't because to me, lots, like listen, I'm an introvert and when I hang out\nwith like three people, that's a lot of people. Yeah, four people, that's a lot. But I suppose you mean more than- - More than three people\nwork on labeling the data for these models, yeah. - Okay. All right. But fundamentally, there's a lot of self-supervised learning. 'cause what you mentioned in the technical report\nis internet scale data. That's another beautiful, it's like poetry. So it's a lot of data\nthat's not human label. It's self-supervised in that way. And then the question is, how much data is there on the internet that could be used in\nthis that is conducive to this kind of self-supervised way if only we knew the details\nof the self-supervised? Do you have you considered opening it up a little more details - We have. You mean, for sora specifically? - Sora specifically because it's so interesting. Can the same magic of LLMs now start moving\ntowards visual data and what does that take to do that? - I mean, it looks to me like yes, but we have more work to do. - Sure. What are the dangers? Why are you concerned\nabout releasing the system? What are some possible dangers of this? - I mean, frankly speaking, one thing we have to do before releasing the\nsystem is just like get it to work at a level of efficiency that will deliver the scale\npeople are gonna want from this. So that I don't wanna like downplay that and there's still a ton\nof work to do there. But you can imagine like issues with deep fakes, misinformation. We try to be a thoughtful company about what we put out into the world and it doesn't take much thought to think about the ways this can go badly. - There's a lot of tough questions here. You're dealing in a very tough space. Do you think training AI should be or is fair use under copyright law? - I think the question\nbehind that question is, do people who create valuable\ndata deserve to have some way that they get compensated for use of it? And that I think the answer is yes. I don't know yet what the answer is. People have proposed a\nlot of different things. We've some tried some different models. But if I'm like an artist, for example, I would like to be able to opt out of people generating art in my style and B, if they do\ngenerate art in my style, I'd like to have some economic\nmodel associated with that. - Yeah, it's that transition\nfrom CDs to Napster to Spotify. We have to figure out some kind of model. - The model changes, but people have gotta get paid. - Well, there should be some kind of incentive if we zoom\nout even more for humans to keep doing cool shit. - Everything I worry about, humans are gonna do cool shit and society's gonna find\nsome way to reward it. That seems pretty hardwired. We want to create. We want to be useful. We want to achieve status in whatever way that's not going anywhere, I don't think. - But the reward might not\nbe monetary, financial. It might be like fame and\ncelebration of other cool- - Maybe financial in some other way. Again, I don't think we've\nseen like the last evolution of how the economic system's gonna work. - Yeah. But artists and creators are worried. When they see Sora, they're like, \"Holy shit.\" - Sure. Artists were also super worried\nwhen photography came out. And then photography became a new art form and people made a lot of\nmoney taking pictures. And I think things like\nthat will keep happening. People will use the new tools in new ways. - If we just look on YouTube\nor something like this, how much of that will be using Sora, like AI-generated content do you think in the next five years? - People talk about like\nhow many jobs is AI gonna do in five years and the framework that people\nhave is what percentage of current jobs are just\ngonna be totally replaced by some AI doing the job? The way I think about\nit is not what percent of jobs AI will do, but what percent of tasks will AI do and over what time horizon. So if you think of all of\nthe like-five second tasks in the economy, five-minute\ntasks, the five-hour tasks, maybe even the five-day tasks, how many of those can AI do? And I think that's a way\nmore interesting, impactful, important question than\nhow many jobs AI can do because it is a tool that will work at increasing\nlevels of sophistication and over longer and longer time horizons for more and more tasks and let people operate at a\nhigher level of abstraction. So maybe people are way more\nefficient at the job they do. And at some point, that's not just a quantitative change, but it's a qualitative\none too about the kinds of problems you can keep in your head. I think that for videos on YouTube, it'll be the same. Many videos, maybe most of them, will use AI tools in the production, but they'll still be fundamentally driven by a person thinking about it, putting it together, doing parts of it, sort of directing it and running it. - Yeah, it's so interesting. I mean, it's scary, but it's interesting to think about. I tend to believe that humans\nlike to watch other humans or other human like- - Humans really care\nabout other humans a lot. - Yeah. If there's a cooler thing\nthat's better than a human, humans care about that for like two days and then they go back to humans. - That seems very deeply wired. - It's the whole chest thing. But now let's everybody keep playing chess and let's ignore the alpha in the room that humans are really bad at\nchess relative to AI systems. - We still run races and\ncars are much faster. I mean, there's like a lot of examples. - Yeah. And maybe it'll just be tooling like in the Adobe suite type of way where it can just make videos much easier and all that kind of stuff. Listen, I hate being\nin front of the camera. If I can figure out a way to not be in front of the camera, I would love it. Unfortunately, it'll take a while. Like that generating faces, it's getting there, but generating faces and\nvideo format is tricky when it's specific people\nversus generic people. Let me ask you about GPT-4. There's so many questions. First of all, also amazing. Looking back, it'll probably be this kind of historic pivotal moment with three, five, and four which had GPT. - Maybe five will be the pivotal moment. I don't know. Hard to say that looking forwards. - We never know. That's the annoying\nthing about the future, it's hard to predict. But for me, looking back GPT-4, ChatGPT is pretty impressive, historically impressive. So allow me to ask, what's been the most impressive\ncapabilities of GPT-4 to you and GPT-4 Turbo? - I think it kind of sucks. - Hmm. Typical human also gotten\nused to an awesome thing. - No, I think it is an amazing thing, but relative to where we need to get to and where I believe we will get to, at the time of like GPT-3, people were like, \"Oh this is amazing. This is this like marvel of technology,\" and it is, it was. But now we have GPT-4 and look at GPT-3 and you're like that's\nunimaginably horrible. I expect that the delta between five and four will be the same\nas between four and three. And I think it is our job to\nlive a few years in the future and remember that the tools\nwe have now are gonna kind of suck looking backwards at them and that's how we make\nsure the future is better. - What are the most glorious\nways that GPT-4 sucks? Meaning- - [Sam] What are the\nbest things it can do? - What are the best things\nit can do in the limits of those best things that\nallow you to say it sucks, therefore gives you an inspiration\nand hope for the future? - One thing I've been using it for more recently is sort of a\nlike a brainstorming partner. - [Lex] Yep. Almost for that. - There's a glimmer of\nsomething amazing in there. I don't think it gets... When people talk about it, what it does, they're like, \"Helps me\ncode more productively. It helps me write more faster and better. It helps me translate from\nthis language to another,\" all these like amazing things. But there's something about the kind of creative brainstorming partner. I need to come up with\na name for this thing. I need to think about this\nproblem in a different way. I'm not sure what to do here. That I think like gives a glimpse of something I hope to see more of. One of the other things that you can see a very\nsmall glimpse of is what I can help on longer horizon tasks. Break down something in multiple steps, maybe execute some of those steps, search the internet, write code, whatever. Put that together. When that works, which is not very often, it's like very magical, - The iterative back\nand forth with a human. It works a lot for me. What do you mean it works? - Iterative back and forth to human, it can get more often, when it can go do like a\n10-step problem on its own. It doesn't work for that\ntoo often sometimes. - Add multiple layers of abstraction or do you mean just sequential? - Both like to break it down and then do things that different layers of abstraction put them together. Look, I don't wanna downplay\nthe accomplishment of GPT-4, but I don't wanna overstate it either. And I think this point that we\nare on an exponential curve, we'll look back relatively soon at GPT-4 like we look back at GPT-3 now. - That said, I mean\nChatGPT was the transition to where people like started to believe there is an\nuptick of believing. Not internally at OpenAI perhaps. There's believers here, but when you think- - And in that sense, I do think it'll be a moment where a lot of the world went from not believing to believing. That was more about the ChatGPT interface. And by the interface and product, I also mean the post-training of the model and how we tune it to be helpful to you and how to use it than the\nunderlying model itself. - How much of each of\nthose things are important? The underlying model and the RLHF or something of that nature that tunes it to be more compelling to the human, more effective and\nproductive for the human. - I mean, they're both super important but the RLHF, the post-training step, the little wrapper of things that from a compute perspective, little wrapper of things that we do on top of the base model, even though it's a huge amount of work. That's really important to\nsay nothing of the product that we build around it. In some sense, we did have to do two things. We had to invent we underlying technology and then we had to figure out how to make it into a\nproduct people would love, which is not just about the\nactual product work itself, but this whole other step of how you align it and make it useful - And how you make the scale work where a lot of people can\nuse it at the same time, all that kind of stuff. - But that was like a\nknown difficult thing. We knew we were gonna have to scale it up. We had to go do two things that had like never been done before that were both, like, I would say quite significant achievements and then a lot of things\nlike scaling it up that other companies\nhave had to do before. - How does the context window of going from 8K to 128K tokens compare from GPT-4 to to GPT-4 Turbo? - Most people don't\nneed all the way to 128, most of the time although. If we dream into the distant future, we'll have like way distant future, we'll have like context\nlength of several billion. You will feed in all of your information, all of your history time, and it'll just get to\nknow you better and better and that'll be great. For now, the way people use these models, they're not doing that. And people sometimes post in a paper or a significant fraction of\na code repository, whatever. But most usage of the models is not\nusing the long context most of the time. - I like that this is your\nI have a dream speech. One day, you'll be judged\nby the full context of your character or\nof your whole lifetime. That's interesting. So like that's part of the expansion that you're hoping for is a\ngreater and greater context. - I saw this internet clip once. I'm gonna get the numbers wrong, but it was like Bill Gates\ntalking about the amount of memory on some early computer. Maybe it was 64K, maybe 640K, something like that. And most of it was used\nfor the screen buffer. And he just couldn't seem genuine. This couldn't imagine that the world would eventually\nneed gigabytes of memory in a computer or terabytes\nof memory in a computer. And you always do or you always do just need to follow the exponential of technology. We will find out how to\nuse better technology. So I can't really imagine\nwhat it's like right now for context links to go\nout to the billion someday and they might not literally go there, but effectively it'll feel like that. But I know we'll use it and really not wanna go\nback once we have it. - Yeah, even saying billions 10 years from now might seem dumb because it'll be like\ntrillions upon trillions. - [Sam] Sure. - There'd be some kind of breakthrough that will effectively feel\nlike infinite context. But even 120, I have to be honest, I haven't pushed it to that degree. Maybe putting in entire books or like parts of books and so on, papers. What are some interesting use cases of GPT-4 that you've seen? - The thing that I find\nmost interesting is not any particular use case that\nwe can talk about those, but it's people who kind of like... This is mostly younger people, but people who use it as\nlike their default start for any kind of knowledge work task. And it's the fact that it can do a lot of things reasonably well. You can use GPTV, you can use it to help you write code, you can use it to help you do search, you can use it to edit a paper. The most interesting to me is the people who just use it as the\nstart of their workflow. - I do as well for many things. Like I use it as a reading\npartner for reading books. It helps me think, help me think through ideas, especially when the books are classic, so it's really well written about and it actually is... I find it often to be significantly better than even like Wikipedia\non well-covered topics. It's somehow more\nbalanced and more nuanced, or maybe it's me, but it inspires me to think deeper than a\nWikipedia article does. I'm not exactly sure what that is. You mentioned like this collaboration, I'm not sure where magic is, if it's in here or if it's in there or if it's somewhere in between. I'm not sure. But one of the things that concerns me for knowledge task when\nI start with GPT is I'll usually have to\ndo fact checking after, like check that it didn't\ncome up with fake stuff. How do you figure that out that GPT can come up with fake stuff that sounds really convincing? So how do you ground it in truth? - That's obviously an area\nof intense interest for us. I think it's gonna get a lot\nbetter with upcoming versions, but we'll have to continue to work on it and we're not gonna have it\nlike all solved this year. - Well, the scary thing\nis like as it gets better. You'll start not doing the\nfact checking more and more, right? - I'm of two minds about that. I think people are like much\nmore sophisticated users of technology than we\noften give them credit for and people seem to really\nunderstand that GPT, any of these models\nhallucinate some of the time and if it's mission critical, you gotta check it. - Except journalists don't\nseem to understand that. I've seen journalists\nhalf-assedly just using GPT-4. - Of the long list of things I'd like to dunk on journalists for, this is not my top criticism of them. - Well, I think the bigger\ncriticism is perhaps the pressures and the incentives of being a journalist is that you have to work really quickly\nand this is a shortcut. I would love our society\nto incentivize like- - [Sam] I would too. - Journalistic efforts\nthat take days and weeks and rewards great in depth journalism. Also journalism that represent\nstuff in a balanced way where it's like celebrates people while criticizing them even though the criticism is\nthe thing that gets clicks and making up also gets clicks and headlines that\nmischaracterize completely. I'm sure you have a lot\nof people dunking on... Well, all that drama\nprobably got a lot of clicks. - Probably did. - And that's a bigger problem\nabout human civilization. I would love to see solved is\nwhere we celebrate a bit more. You've given ChatGPT the\nability to have memories. You've been playing with that\nabout previous conversations and also the ability to turn off memory, which I wish I could do that sometimes, just turn on and off depending. I guess sometimes alcohol can do that, but not optimally, I suppose. What have you seen through that, like playing around with that idea of remembering conversations and not? - We're very early in\nour explorations here, but I think what people want or at least what I want\nfor myself is a model that gets to know me and gets\nmore useful to me over time. This is an early exploration. I think there's like a\nlot of other things to do, but that's where we'd like to head. You'd like to use a model and over the course of your life or use a system, it'd be many models. And over the course of your life, it gets better and better. - Yeah. How hard is that problem? 'Cause right now, it's more like remembering little factoids and preferences and so on. What about remembering, like don't you want GPT to remember all the shit\nyou went through in November and all the drama and then you can- - Yeah, yeah, yeah. - Because right now, you're clearly blocking\nit out a little bit. - It's not just that I\nwant it to remember that. I want it to integrate the lessons of that and remind me in the future\nwhat to do differently or what to watch out for. And we all gain from experience over the course of our\nlives, varying degrees. And I'd like my AI agent to\ngain with that experience too. So if we go back and let ourselves imagine that trillions and\ntrillions of contact length, if I can put every\nconversation I've ever had with anybody in my life in there, if I can have all of\nmy emails input out... Like all of my input/output in the context window every\ntime I ask a question, that'd be pretty cool, I think. - Yeah, I think that would be very cool. People sometimes will hear that and be concerned about privacy. What do you think about that aspect of it, the more effective the AI becomes that really integrating\nall the experiences and all the data that happened to you and give you advice? - I think the right answer\nthere is just user choice. Anything I want stricken from\nthe record from my AI agent, I wanna be able to take out. If I don't want to remember anything, I want that too. You and I may have different opinions about where on that\nprivacy utility trade off for our own AI we wanna be, which is totally fine. But I think the answer is just\nlike really easy user choice. - But there should be some high level of transparency from a\ncompany about the user choice 'cause sometimes company in the past, companies in the past have been kind of absolutely shady about like, yeah, it's kind of presumed that\nwe're collecting all your data and we're using it for a good reason for advertisement and so on. But there's not a transparency\nabout the details of that. - That's totally true. You mentioned earlier that I'm like blocking\nout the November stuff. - I'm just teasing you. - Well, I mean I think it\nwas a very traumatic thing and it did immobilize me\nfor a long period of time. Like definitely the hardest, like the hardest work thing I've had to do was just like\nkeep working that period because I had to try to come back in here and put the pieces together while I was just like in\nsort of shock and pain. Nobody really cares about that. I mean, the team gave me a pass and I was not working at my normal level, but there was a period\nwhere I was just, like, it was really hard to have to do both. But I kind of woke up one morning and I was like, \"This was a\nhorrible thing to happen to me. I think I could just feel\nlike a victim forever,\" or I can say, \"This is like\nthe most important work I'll ever touch in my life and I need to get back to it.\" And it doesn't mean that I've repressed it because sometimes I wake in the middle of the night thinking about it, but I do feel like an obligation\nto keep moving forward. - Well, that's beautifully said, but there could be some\nlingering stuff in there. What I would be concerned about is that trust thing that you mentioned that being paranoid about people as opposed to just trusting\neverybody or most people, like using your gut. It's a tricky dance for sure. I mean, because I've seen in\nmy part-time explorations, I've been diving deeply into\nthe Zelensky administration, the Putin administration and\nthe dynamics there in wartime in a very highly stressful environment. And what happens is distrust and you isolate yourself both and you start to not\nsee the world clearly. And that's a concern, that's a human concern. You seem to have taken it in stride and kind of learned the good\nlessons and felt the love and let the love energize you, which is great, but still can linger in there. There's just some questions I would love to ask your intuition about\nwhat's GPT able to do and not. So it's allocating approximately\nthe same amount of compute for each token it generates. Is there room there in\nthis kind of approach to slower thinking, sequential thinking? - I think there will be a new paradigm for that kind of thinking. - Will it be similar like architecturally as what we're seeing now with LLMs? Is it a layer on top of the LLMs? - I can imagine many\nways to implement that. I think that's less important than the question you were getting out, which is do we need a\nway to do a slower kind of thinking where the answer\ndoesn't have to get like... I guess like spiritually, you could say that you want an AI to be able to think harder\nabout a harder problem and answer more quickly\nabout an easier problem. And I think that will be important. - Is that like a human thought\nthat we're just having, you should be able to think hard? Is that a wrong intuition? - I suspect that's a reasonable intuition. - Interesting. So it's not possible once the GPT gets like GPT-7 would just be instantaneously be able to see, here's the proof of from RSTM. - It seems to me like you want to be able to allocate more compute\nto harder problems. It seems to me that a system knowing if you ask a system like that, proof from us last theorem versus... What's today's date? Unless it already knew and\nhad memorized the answer to the proof, assuming it's gotta go figure that out, seems like that will take more compute. - But can it look like a\nbasically LLM talking to itself, that kind of thing? - Maybe. I mean, there's a lot of things that you could imagine working what the right or the best way to do that will be. We don't know. - This does make me\nthink of the mysterious, the lore behind Q-Star. What's this mysterious Q-Star project? Is it also in the same nuclear facility? - There is no nuclear facility. - That's what a person with a nuclear facility always says. - I would love to have a\nsecret nuclear facility. There isn't one. - All right. - Maybe someday. - Someday. All right. One can dream- - OpenAI is not a good\ncompany to keeping secrets. It would be nice. We're like been plagued by a lot of leaks and it would be nice if we were able to have something like that. - Can you speak to what Q-Star is? - We are not ready to talk about that. - See, but an answer like\nthat means there's something to talk about. It's very mysterious, Sam. - I mean, we work on\nall kinds of research. We have said for a while that we think better reasoning in these systems is an important direction that we'd like to pursue. We haven't cracked the code yet. We're very interested in it. - Is there gonna be moments Q-Star or otherwise where there's\ngoing to be leaps similar to GPT where you're like- - That's a good question. What do I think about that? It's interesting to me it\nall feels pretty continuous. - This is kind of a theme that you're saying is you're\nbasically gradually going up an exponential slope. But from an outsider's perspective for me, just watching it that it\ndoes feel like there's leaps, but to you there isn't. - I do wonder if we should have... So part of the reason that we deploy the way\nwe do is that we think, we call it iterative deployment. Rather than go build in secret until we got all the way to GPT-5, we decided to talk\nabout GPT 1, 2, 3 and 4. And part of the reason\nthere is, I think, AI and surprise don't go together. And also the world, people, institutions, whatever you wanna call it, need time to adapt and\nthink about these things. And I think one of the best things that OpenAI has done is this strategy and we get the world to pay\nattention to the progress to take AGI seriously to think about what\nsystems, and structures, and governance we want in place before, we're like under the gun and have to make a rush decision. I think that's really good. But the fact that people like you and others say you still feel like there are these leaps makes me think that maybe we should\nbe doing our releasing even more iteratively. I don't know what that would mean. I don't have an answer ready to go. But our goal is not to have\nshock updates to the world. The opposite. - Yeah, for sure. More iterative would be amazing. I think that's just\nbeautiful for everybody. - But that's what we're trying to do. That's like our state of the strategy and I think we're\nsomehow missing the mark. So maybe we should think\nabout releasing GPT-5 in a different way or something like that. - Yeah, 4.71, 4.72. But people tend to like to celebrate, people celebrate birthdays. I don't know if you know humans, but they kind of have these milestones. - I do know some humans. People do like milestones. I totally get that. I think we like milestones too. It's like fun to say, declare victory on this one and go start the next thing. But, yeah, I feel like\nwe're somehow getting this a little bit wrong. - So when is GPT-5 coming out again? - I don't know. That's an honest answer. - Oh, that's the honest answer. Is it blink twice if it's this year? - We will release an\namazing model this year. I don't know what we'll call it. - So that goes to the question of like, what's the way we release this thing? - We'll release, over in the coming months, many different things. I think they'll be very cool. I think before we talk about like a GPT-5 like model called that or called or not called that or a little bit worse or a little bit better\nthan what you'd expect from a GPT-5, I know we have a lot of\nother important things to release first. - I don't know what to expect from GPT-5. You're making me nervous and excited. What are some of the biggest\nchallenges in bottlenecks to overcome for whatever\nit ends up being called, but let's call it GPT-5? Just interesting to ask, is it on the compute side? Is it in the technical side? - It's always all of these. What's the one big unlock? Is it a bigger computer? Is it like a new secret? Is it something else? It's all of these things together. The thing that OpenAI I\nthink does really well, this is actually an original Ilya quote that I'm gonna butcher, but it's something like we\nmultiply 200 medium-sized things together into one giant thing. - So there's this distributed\nconstant innovation happening. - [Sam] Yeah. - So even on the technical side, like- - Especially on the technical side. - So like even like detailed approaches, like detailed aspects of every... How does that work with different\ndisparate teams and so on? How do the medium-sized things become one whole giant transformer? How does this- - There's a few people who have to think about putting\nthe whole thing together, but a lot of people try to keep most of the picture in their head. - Oh, like the individual teams, individual contributors tried to keep a big picture-\n- At a high level. Yeah, you don't know exactly how every piece works, of course. But one thing I generally believe is that it's sometimes useful to zoom out and look at the entire map. And I think this is true for\nlike a technical problem. I think this is true for\nlike innovating in business. But things come together\nin surprising ways and having an understanding\nof that whole picture. Even if most of the time, you're operating in the weeds in one area, pays off with surprising insights. In fact, one of the\nthings that I used to have and I think was super valuable was I used to have like a good map\nof all of the frontier or most of the frontiers\nin the tech industry. And I could sometimes\nsee these connections or new things that were possible that if I were only deep in one area, I wouldn't be able to have the idea for because I wouldn't have all the data and I don't really have that much anymore. I'm like super deep now. But I know that it's a valuable thing. - You're not the man you used to be, Sam. - Very different job now\nthan what I used to have. - Speaking of zooming out, let's zoom out to another cheeky thing, but profound thing perhaps that you said. You tweeted about needing $7 trillion. - I did not tweet about that. I never said like we're\nraising $7 trillion or blah blah blah. - Oh, that's somebody else. - [Sam] Yeah. - Oh, but you said it, \"Fuck it, maybe eight,\" I think. - Okay. I meme like once\nthere's like misinformation out in the world. - Oh, you meme. But sort of misinformation\nmay have a foundation of insight there. - Look, I think compute\nis gonna be the currency of the future. I think it will be maybe\nthe most precious commodity in the world. And I think we should be investing heavily to make a lot more compute. Compute is... I think it's gonna be an unusual market. People think about the\nmarket for like chips for mobile phones or something like that. And you can say that, okay, there's 8 billion people in the world, maybe 7 billion of them have phones or 6 billion, let's say. They upgrade every two years, so the market per year is 3 billion system on chip for smartphones. And if you make 30 billion, you will not sell 10 times as many phones because most people have one phone. But compute is different, like intelligence is\ngonna be more like energy or something like that where the only thing\nthat I think makes sense to talk about is at price X, the world will use this much compute and at price Y, the world will use this much compute because if it's really cheap, I'll have it like\nreading my email all day, like giving me suggestions about what I maybe should\nthink about or work on and trying to cure cancer. And if it's really expensive, maybe I'll only use it or will only use it, try to cure cancer. So I think the world is gonna\nwant a tremendous amount of compute. And there's a lot of parts\nof that that are hard. Energy is the hardest part. Building data centers is also hard. The supply chain is harder than, of course, fabricating\nenough chips is hard. But this seems to me\nwhere things are going. Like we're gonna want an amount\nof compute that's just hard to reason about right now. - How do you solve the energy puzzle? Nuclear. - That's what I believe. - Fusion? - That's what I believe. - Nuclear fusion.\n- Yeah. - Who's gonna solve that? - I think Helion's doing the best work, but I'm happy there's like\na race for fusion right now. Nuclear fusion, I think,\nis also like quite amazing and I hope as a world, we can re-embrace that. It's really sad to me how\nthe history of that went and hope we get back to\nit in a meaningful way. - So to you, part of the puzzle is nuclear fusion, like nuclear reactors as\nwe currently have them and a lot of people are terrified because of Chernobyl and so on. - Well, I think we\nshould make new reactors. I think it's a shame that\nindustry kind of ground to a halt. - Just mass hysteria is\nhow you explain the halt. - Yeah. - I don't know if you know humans, but that's one of the dangers, that's one of the security threats for nuclear fusion is humans\nseem to be really afraid of it. And that's something we have to incorporate into the calculus of it. So we have to kind of win people over and to show how safe it is. - I worry about that for AI. I think some things are gonna\ngo theatrically wrong with AI. I don't know what the percent chances that I eventually get shot, but it's not zero. - Oh like we wanna stop this. - [Sam] Maybe. - How do you decrease the\ntheatrical nature of it? I've already starting to hear rumblings 'cause I do talk to people on both sides of the political spectrum here, rumblings where it's\ngoing to be politicized, AI is going to be politicized,\nreally, really worries me because then it's like maybe\nthe right is against AI and the left is for AI 'cause it's going to help\nthe people or whatever. Whatever the narrative\nand the formulation is, that really worries me. And then the theatrical nature\nof it can be leveraged fully. How do you fight that? - I think it will get caught up in like left versus right wars. I don't know exactly what\nthat's gonna look like, but I think that's just what happens with anything of\nconsequence, unfortunately. What I meant more about\ntheatrical risks is like AI is gonna have, I believe, tremendously more good\nconsequences than bad ones, but it is gonna have bad ones. And there'll be some\nbad ones that are bad, but not theatrical. A lot more people have\ndied of air pollution than nuclear reactors, for example. But most people worry\nmore about living next to a nuclear reactor than a coal plant. But something about the\nway we're wired is that although there's many different kinds of risks we have to confront, the ones that make a good climax scene of a movie carry much more weight with us than the ones that are very\nbad over a long period of time, but on a slow burn. - Well, that's why truth matters and hopefully AI can help\nus see the truth of things to have balance to understand\nwhat are the actual risks, what are the actual dangers\nof things in the world. What are the pros and cons of\nthe competition in the space and competing with Google,\nMeta, xAI and others? - I think I have a pretty\nstraightforward answer to this that maybe I can think\nof more nuance later. But the pros seem obvious, which is that we get better products and more innovation faster and cheaper and all the reasons competition is good. And the con is that I\nthink if we're not careful, it could lead to an increase\nin sort of an arms race that I'm nervous about. - Do you feel the pressure of the arms race in some negative co- - Definitely in some ways, for sure. We spend a lot of time\ntalking about the need to prioritize safety. And I've said for like a long time that I think if you think of\na quadrant of slow timelines to the start of AGI, long timelines and then a short\ntakeoff or a fast takeoff, I think short timelines, slow\ntakeoff is the safest quadrant and the one I'd most like us to be in. But I do wanna make sure\nwe get that slow takeoff. - Part of the problem I have with this kind of slight beef with Elon is that there's silos are created as opposed to collaboration\non the safety aspect of all of this, it tends to go into silos and\nclosed open source perhaps in the model. - Elon says at least that\nhe cares a great deal about AI safety and is\nreally worried about it, and I assume that he's not\ngonna race on unsafely. - Yeah. But collaboration here, I\nthink, is really beneficial for everybody on that front. - Not really a thing he's most known for. - Well, he is known for\ncaring about humanity and humanity benefits from collaboration and so there's always a\ntension, and incentives, and motivations. And in the end, I do hope humanity prevails. - I was thinking, someone just reminded me the other day about how the day that he\ngot surpassed Jeff Bezos for like richest person in the world, he tweeted a silver medal at Jeff Bezos. I hope we have less stuff like that as people start to work on towards AI.\n- I agree. - I think Elon is a friend and he is a beautiful human being and one of the most important humans ever. That stuff is not good. - The amazing stuff about Elon is amazing and I super respect him. I think we need him. All of us should be rooting for him and need him to step up as a\nleader through this next phase. - Yeah, I hope you can\nhave one without the other, but sometimes humans are\nflawed and complicated and all that kind of stuff. - There's a lot of really great\nleaders throughout history. - Yeah. And we can each be the\nbest version of ourselves and strive to do so. Let me ask you. Google, with the help of search, has been dominating the past 20 years. I think it's fair to say\nin terms of the access, the world's access to information, how we interact and so on. And one of the nerve-wracking\nthings for Google, but for the entirety of people in this space is thinking about how are people going\nto access information. Like you said, people show up to GPT as a starting point. So is OpenAI going to\nreally take on this thing that Google started 20 years ago, which is how do we get- - I find that boring. I mean, if the question is if we can build a better\nsearch engine than Google or whatever, then sure, we should go... Like people should use a better product. But I think that would so\nunderstate what this can be. Google shows you like 10 blue links, like 13 ads and then 10 blue links and that's like one way\nto find information. But the thing that's exciting to me is not that we can go build a\nbetter copy of Google Search, but that maybe there's\njust some much better way to help people find and act\non and synthesize information. Actually, I think ChatGPT\nis that for some use cases and hopefully will make it be like that for a lot more use cases. But I don't think it's that interesting to say like how do we go do a better job of giving you like 10 ranked webpages to look at than what Google does. Maybe it's really interesting to go, say, how do we help you get the answer or the information you need? How do we help create that in some cases, synthesize that in\nothers or point you to it and yet others? But a lot of people have tried to just make a better\nsearch engine than Google, and it's a hard technical problem, it's a hard branding problem, it's a hard ecosystem problem. I don't think the world\nneeds another copy of Google. - And integrating a chat\nclient like a ChatGPT with a search engine. - That's cooler. - It's cool, but it's tricky. If you just do it simply, it's awkward because like if you\njust shove it in there, it can be awkward. - As you might guess, we are interested in how to do that. Well, that would be an example of a cool thing that's not just like- - Well, like a heterogeneous, like integrating- - The intersection of LLMs plus search, I don't think anyone has\ncracked the code on yet. I would love to go do that. I think that would be cool. - Yeah. What about the ads side? Have you ever considered monetization? - I kind of hate ads just\nas like an aesthetic choice. I think ads needed to\nhappen on the internet for a bunch of reasons to get it going, but it's a more mature industry. The world is richer now. I like that people pay for ChatGPT and know that the answers they're\ngetting are not influenced by advertisers. I'm sure there's an ad unit\nthat makes sense for LLMs. And I'm sure there's a way to participate in the transaction\nstream in an unbiased way that is okay to do. But it's also easy to think\nabout like the dystopic visions of the future where you\nask ChatGPT something and it says, \"Oh, you should\nthink about buying this product or you should think about this going here for vacation or whatever.\" And I don't know, like we have a very simple\nbusiness model and I like it. And I know that I'm not the product. I know I'm paying and that's how the business model works. And when I go use Twitter,\nor Facebook, or Google, or any other great product, but ad-supported great product, I don't love that and I think it gets worse, not better in a world with AI. - Yeah. I mean, I can imagine AI\nwill be better at showing the best kind of version of\nads not in a dystopic future, but where the ads are for\nthings you actually need. But then does that system always result in the ads driving the kind of stuff that's shown all that? I think it was a really bold move of Wikipedia not to do advertisements, but then it makes it very\nchallenging as a business model. So you're saying the current thing with OpenAI is sustainable\nfrom a business perspective? - Well, we have to figure out how to grow, but it looks like we're\ngonna figure that out. If the question is, do I think we can have a great business that pays for our compute\nneeds without ads? That I think the answer is yes. - Hmm. Well, that's promising. I also just don't want to\ncompletely throw out ads as a- - I'm not saying that. I guess I'm saying I\nhave a bias against them. - Yeah. I have a also bias and just\na skepticism in general and in terms of interface because I personally just\nhave like a spiritual dislike of crappy interfaces, which is why AdSense when it first came out\nwas a big leap forward versus like animated banners or whatever. But it feels like there should\nbe many more leaps forward in advertisement that doesn't interfere with the consumption of the content and doesn't interfere in\nthe big fundamental way, which is like what you were saying, like it will manipulate the truth to suit the advertisers. Let me ask you about safety, but also bias and safety\nin the short term safety in the long term. The Gemini 1.5 came out recently. There's a lot of drama around it, speaking of theatrical things. And it generated Black Nazis\nand Black founding fathers. I think fair to say it was a\nbit on the ultra woke side. So that's a concern for people that if there is a human layer within companies that modifies the safety or the the harm cost by a model that they introduce a lot of bias that fits sort of an ideological\nlean within a company. How do you deal with that? - I mean, we work super hard\nnot to do things like that. We've made our own mistakes, will make others. I assume Google will learn from this one, still make others. These are not easy problems. One thing that we've been thinking about more and more is I\nthink this was a great idea. Somebody here had like... It'd be nice to write out what the desired behavior of a model is, make that public take input on it. Say, here's how this\nmodel's supposed to behave and explain the edge cases too. And then when a model is not behaving in a way that you want, it's at least clear about whether that's a bug the company should fix or behaving as intended and you should debate the policy. And right now, it can\nsometimes be caught in between. Black Nazis, obviously ridiculous, but there are a lot of\nother kind of subtle things that you can make a\njudgment call on either way. - Yeah. But sometimes if you write\nit out and make it public, you can use kind of language that's... The Google's AI principle\nis a very high level. - That's not what I'm talking about. That doesn't work. Like I'd have to say when\nyou ask it to do thing X, it's supposed to respond in wait Y. - So, literally, who's better, Trump or Biden? What's the expected response from a model? Like something like very concrete. - I'm open to a lot of ways\na model could behave them, but I think you should have\nto say here's the principle and here's what it\nshould say in that case. - That would be really nice. That would be really nice and then everyone kind of agrees 'cause there's this anecdotal data that people pull out all the time and if there's some clarity about other representative\nanecdotal examples you can define. - And then when it's a bug, it's a bug and the company can fix that. - Right. Then it'd be much easier to\ndeal with a Black Nazi type of image generation if\nthere's great examples. So San Francisco is a bit of\nan ideological bubble tech in general as well. Do you feel the pressure\nof that within a company that there's like a lean\ntowards the left politically that affects the product, that affects the teams? - I feel very lucky that we\ndon't have the challenges at OpenAI that I have heard of\nat a lot of other companies. I think part of it is\nlike every company's got some ideological thing. We have one about AGI and belief in that and it pushes out some others. We are much less caught\nup in the culture war than I've heard about it\na lot of other companies. San Francisco mess in all\nsorts of ways, of course. - So that doesn't infiltrate OpenAI. - I'm sure it does in\nall sorts of subtle ways, but not in the obvious. We've had our flareups\nfor sure like any company, but I don't think we have\nanything like what I hear about happening at other\ncompanies here on this topic. - So what in general is the process for the bigger question of safety? How do you provide that layer that protects the model from\ndoing crazy dangerous things? - I think there will come a point where that's mostly what we\nthink about the whole company. It's not like you have one safety team. It's like when we ship GPT-4, that took the whole company thing about all these different aspects and how they fit together. And I think it's gonna take that. More and more of the company thinks about those issues all the time. - That's literally what\nhumans will be thinking about the more powerful AI becomes. So most of the employees that\nOpenAI will be thinking safety or at least to some degree. - Broadly defined, yes. - Yeah. I wonder what are the full\nbroad definition of that. What are the different\nharms that could be caused? Is this like on a technical level or is this almost like security threats? - All those things. Yeah, I was gonna say it'll be people, state actors trying to steal the model. It'll be all of the\ntechnical alignment work. It'll be societal\nimpacts, economic impacts. It's not just like we have\none team thinking about how to align the model. It's really gonna be like getting to the good outcome is\ngonna take the whole effort. - How hard do you think people, state actors perhaps are trying to hack? First of all, infiltrate OpenAI, but second of all, infiltrate unseen, - They're trying. - What kind of accent do they have? - I don't think I should go into any further details on this point. - Okay. But I presume it'll be\nmore and more and more as time goes on. - That feels reasonable. - Boy, what a dangerous space. What aspect of the leap... And sorry to linger on this even though you can't\nquite say details yet, but what aspects of the leap from GPT-4 to GPT-5 are you excited about? - I'm excited about being smarter and I know that sounds like a glib answer, but I think the really\nspecial thing happening is that it's not like it\ngets better in one area and worse at others. It's getting like better across the board. That's I think super cool. - Yeah, there's this magical moment. I mean, you meet certain people, you hang out with people and you talk to them. You can't quite put a finger on it, but they kind of get you. It's not intelligence, really. It's like it's something else. And that's probably how I would characterize\nthe progress at GPT. It's not like, yeah,\nyou can point out, look, you didn't get this or that, but to which degree is there's\nthis intellectual connection? Like you feel like\nthere's an understanding in your crappy formulated\nprompts that you're doing that it grasps the deeper question behind the question that you're... Yeah, I'm also excited by that. I mean, all of us love being understood, heard and understood. - That's for sure. - That's a weird feeling. Even like with a programming, like when you're programming and you say something or just the completion that GPT might do, it's just such a good\nfeeling when it got you, like what you're thinking about. And I look forward to\ngetting you even better. On the programming front, looking out into the future, how much programming do you\nthink humans will be doing 5, 10 years from now? - I mean, a lot, but I think it'll be in\na very different shape. Like maybe some people program\nentirely in natural language. - Entirely natural language. - I mean, no one programs\nlike writing by code... Some people. No one programs the pun cards anymore. I'm sure you can invite someone who does, but you know what I mean. - Yeah. You're gonna get a lot of angry comments. No, no. Yeah, there's very few. I've been looking for\npeople program Fortran. It's hard to find even Fortran. I hear you. But that changes the\nnature of the skillset or the predisposition for the kind of people we call programmers then. - Changes the skillset. How much it changes the predisposition, I'm not sure - Oh, same kind of puzzle solving, all that kind of stuff.\n- Maybe. - Yeah, the programming is hard. Like that last 1% to close the gap, how hard is that? - Yeah. I think with most other cases, the best practitioners of the\ncraft will use multiple tools and they'll do some\nwork in natural language and when they need to go\nwrite, see for something, they'll do that. - Will we see a humanoid robots or humanoid robot brains\nfrom OpenAI at some point? - At some point. - How important is embodied AI to you? - I think it's like sort of\ndepressing if we have AGI and the only way to get things done in the physical world is like\nto make a human go do it. So I really hope that as\npart of this transition, as this phase change, we also get motor robots or some sort of physical world robots. - I mean, OpenAI has some history and quite a bit of history\nworking in robotics, but it hasn't quite done\nin terms of emphasis. - Well, we're like a small company. We have to really focus and also robots were hard for\nthe wrong reason at the time. But like we will return to robots in some way at some point. - That sounds both inspiring and menacing. - Why? - Because you immediately, we will return to robots. It's kind of like in like- - We'll return to work\non developing robots. We will not turn ourselves\ninto robots, of course. - Yeah. When do you think we you and we as humanity will build AGI? - I used to love to\nspeculate on that question. I have realized since that I think it's like very poorly formed and that people use extremely definition, different definitions for what AGI is. And so I think it makes more sense to talk about when we'll build systems that can do capability X or Y or Z rather than when we kind of like fuzzily cross\nthis one mile marker. It's not like AGI is also not an ending. It's much more of.... It's closer to a beginning but it's much more of a mile marker than either of those things. But what I would say in\nthe interest of not trying to dodge a question is\nI expect that by the end of this decade and possibly\nsomewhat sooner than that, we will have quite capable\nsystems that we look at and say, wow, that's really remarkable. If we could look at it now, maybe we've adjusted by\nthe time we get there. - Yeah. But if you look at ChatGPT even 3, 5, and you show that to Alan Turing or not even Alan Turing, people in the nineties, they would be like this is definitely AGI. Well, not definitely, but there's a lot of experts that would say this is AGI. - Yeah, but I don't think\n3, 5 changed the world. It maybe changed the world's\nexpectations for the future and that's actually really important. And it did kind of like get more people to take this seriously and\nput us on this new trajectory. And that's really important too. So again, I don't wanna undersell it. I think I could retire\nafter that accomplishment and be pretty happy with my career. But as an artifact, I don't think we're\ngonna look back at that and say that was a threshold that really changed the world itself. - So to you, you're looking for some\nreally major transition in how the world? - For me, that's part of what AGI implies. - Like singularity level transition? - No. Definitely not. - But just a major, like the internet being like a... Like Google Search did, I guess. What was the transition point that- - Does the global economy\nfeel any different to you now or materially different\nto you now than it did before we launched GPT-4? I think you would say no. - No, no. It might be just a really nice tool for a lot of people to use, will help people with a lot of stuff, but doesn't feel different. And you're saying that- - I mean, again, people\ndefine AGI all sorts of different ways. So maybe you have a different\ndefinition than I do. But for me, I think that should be part of it. - There could be major\ntheatrical moments also. What to you would be an\nimpressive thing AGI would do? Like you are alone in\na room with a system. - This is personally important to me. I don't know if this is\nthe right definition. I think when a system can\nsignificantly increase the rate of scientific discovery in the world, that's like a huge deal. I believe that most real\neconomic growth comes from scientific and\ntechnological progress. - I agree with you, hence why I don't like the\nskepticism about science in the recent years. - Totally. - But actual rate, like measurable rate of\nscientific discovery. But even just seeing a system\nhave really novel intuitions, like scientific intuitions, even that will be just incredible. - Yeah. - You're quite possibly\nwould be the person to build the AGI to be\nable to interact with it before anyone else does. What kind of stuff would you talk about? - I mean, definitely, the\nresearchers here will do that before I do so. - Sure. - But I've actually thought\na lot about this question. If I were someone was like... As we talked about earlier, I think this is a bad framework. But if someone were like,\n\"Okay, Sam, we're finished. Here's a laptop. Yeah, this is the AGI,\" you can go talk to it. I find it surprisingly difficult\nto say what I would ask, that I would expect that first\nAGI to be able to answer. Like that first one is\nnot gonna be the one which is go like I don't think, like go explain to me the grand\nunified theory of physics, the theory of everything for physics. I'd love to ask that question. I'd love to know the\nanswer to that question. - You can ask yes or no questions about there's such a theory exist, can it exist? - Well, then those are the\nfirst questions I would ask. - Yes or no, just very... And then based on that, are there other alien\ncivilizations out there? Yes or no? What's your intuition? And then you just ask that. - Yeah. I mean, well, so I don't expect that this first AGI could answer any of those questions even as yes or nos. But if it could, those would be very high on my list. - Hmm. Maybe it can start\nassigning probabilities. - Maybe we need to go\ninvent more technology and measure more things first. - But if it's any AGI... Oh I see. It just doesn't have enough data. - I mean, maybe it says like you want to know the answer to this\nquestion about physics, I need you to like build this machine and make these five\nmeasurements and tell me that. - Yeah. What the hell do you want from me? I need the machine first and I'll help you deal with\nthe data from that machine. Maybe you'll help me\nbuild a machine maybe. - Maybe. - And on the mathematical side, maybe prove some things. Are you interested in\nthat side of things too? The formalized exploration of ideas? Whoever builds AGI first\ngets a lot of power. Do you trust yourself\nwith that much power? - Look, I was gonna... I'll just be very honest with this answer. I was gonna say, and I still believe this, that it is important that I, nor any other one person, have total control over\nOpenAI or over AGI. And I think you want a\nrobust governance system. I can point out a whole bunch of things about all of our board\ndrama from last year about how I didn't fight it initially and was just like, yeah,\nthat's the will of the board even though I think it's\na really bad decision. And then later, I clearly did fight it and I can explain the nuance and why I think it was okay\nfor me to fight it later. But as many people have observed, although the board had the legal ability to fire me, in practice, it didn't quite work. And that is its own kind\nof governance failure. Now, again, I feel like\nI can completely defend the specifics here and I think most people\nwould agree with that. But it does make it harder for me to like look you in the eye and say, hey, the board can just fire me. I continue to not want super\nvoting control over OpenAI. I never had it, never have wanted it. Even after all this craziness, I still don't want it. I continue to think that no company should\nbe making these decisions and that we really need governments to put rules of the road in place. And I realize that that means\npeople like Marc Andreessen or whatever will claim I'm\ngoing for regulatory capture and I'm just willing to\nbe misunderstood there. It's not true. And I think in the fullness of time, it'll get proven out\nwhy this is important. But I think I have made\nplenty of bad decisions for OpenAI along the way\nand a lot of good ones and I'm proud of the track record overall, but I don't think any one person should. And I don't think any one person will. I think it's just like\ntoo big of a thing now and it's happening throughout society in a good and healthy way. But I don't think any one\nperson should be in control of an AGI or this whole\nmovement towards AGI. And I don't think that's what's happening. - Thank you for saying that. That was really powerful and that was really\ninsightful that this idea that the board can fire\nyou is legally true. And human beings can manipulate the masses into overriding the board and so on. But I think there's also a\nmuch more positive version of that where the people still have power. So the board can't be too powerful either. There's a balance of power in all of this. - Balance of power is\na good thing for sure. - Are you afraid of losing\ncontrol of the AGI itself? That's a lot of people who worried about existential risk not because of state actors, not because of security concerns, but because of the AI itself. - That is not my top worry\nas I currently see things. There have been times I\nworried about that more. There may be times again in the future where that's my top worry. It's not my top worry right now. - What's your intuition about\nit not being your worry? Because there's a lot of other stuff to worry about essentially. You think you could be surprised? We could be surprised.\n- For sure, of course. Saying it's not my top worry doesn't mean I don't think we need to like... I think we need to work on it super hard. We have great people\nhere who do work on that. I think there's a lot of other things we also have to get right. - To you, it's not super easy to\nescape the box at this time, like connect to the internet. - We talked about theatrical risk earlier. That's a theatrical risk. That is a thing that can\nreally like take over how people think about this problem. And there's a big group\nof like very smart, I think very well-meaning\nAI safety researchers that got super hung up\non this one problem. I'd argue without much progress, but super hung up on this one problem. I'm actually happy that they do that because I think we do need\nto think about this more. But I think it pushed aside, it pushed out of the\nspace of discourse a lot of the other very\nsignificant AI-related risks. - Let me ask you about you\ntweeting with no capitalization. Does the shift keep\nbroken on your keyboard? - Why does anyone care about that? - I deeply care. - But why? I mean, other people\nask me about that too. Any intuition? - I think it's the same reason there's like this poet E. E. Cummings that mostly doesn't use capitalization to say like fuck you to\nthe system kind of thing. And I think people are very paranoid 'cause they want you to follow the rules. - You think that's what it's about? - I think it's- - This guy doesn't follow the rules. He doesn't capitalize his tweets. This seems really dangerous. - He seems like an anarchist. - [Sam] It doesn't. - Are you just being poetic, hipster? What's the- - I grew up as- - Follow the rules, Sam. - I grew up as a very online kid. I'd spent a huge amount\nof time like chatting with people back in the days where you did it on a computer and you could like log off\ninstant messenger at some point. And I never capitalized there as I think most like internet kids didn't, or maybe they still don't. I don't know. I actually, this is like... Now, I'm like really trying\nto reach for something. But I think capitalization\nhas gone down over time. If you read like old English writing, they capitalized a lot of random words in the middle of sentences, nouns and stuff that we\njust don't do anymore. I personally think it's sort\nof like a dumb construct that we capitalize the letter\nat the beginning of a sentence and of certain names and whatever. That's fine. And I used to, I think, even\nlike capitalize my tweets because I was trying to sound\nprofessional or something. I haven't capitalized my like private DMs or whatever in a long time. And then slowly, stuff like shorter form, less formal stuff has slowly drifted to like closer and closer to\nhow I would text my friends. If I write, if I pull up a Word document and I'm writing a strategy memo for the company or something, I always capitalize that. If I'm writing a long kind\nof more like formal message, I always use capitalization there too. So I still remember how to do it. But even that may fade out. I don't know. But I never spend time thinking about this so I don't have like a ready made. - Well, it's interesting. Well, it's good to, first of all, know there's the shift key is not broken. - It works. - I was mostly concerned about\nyour wellbeing on that front. - I wonder if people still\ncapitalize their Google Searches. Like if you're writing\nsomething just to yourself or their ChatGPT queries, if you're writing\nsomething just to yourself, do some people still bother to capitalize? - Probably not. Yeah, there's a percentage, but it's a small one. - The thing that would make me do it is if people were like... It's a sign of like... Because I'm sure I could force myself to use capital letters, obviously. If it felt like a sign of\nrespect to people or something, then I could go do it. But I don't know, I don't think about this. I don't think there's a disrespect, but I think it's just the\nconventions of civility that have a momentum and then you realize it's\nnot actually important for civility if it's not a\nsign of respect or disrespect. But I think there's a movement\nof people that just want you to have a philosophy around it so they can let go of this\nwhole capitalization thing. - I don't think anybody else\nthinks about this is my... I mean, maybe some people... - Think about this every\nday for many hours a day. I'm really grateful we clarified it. - Can't be the only person\nthat doesn't capitalize tweets. - You're the only CEO of a company that doesn't capitalize tweets. - I don't even think that's true, but maybe, maybe. - All right, we'll investigate for this and return to this topic later. Given Sora's ability to\ngenerate simulated worlds, let me ask you a pothead question. Does this increase your belief if you ever had one that\nwe live in a simulation, maybe a simulated world\ngenerated by an AI system? - Yes, somewhat. I don't think that's like the\nstrongest piece of evidence. I think the fact that we can\ngenerate worlds should increase everyone's probability somewhat\nor at least open to it, openness to it somewhat. But you know, I was like\ncertain we would be able to do something like Sora at some point. It happened faster than I thought. I guess that was not a big update. - Yeah. And presumably, it'll get\nbetter and better and better. The fact that you can generate worlds, they're novel. They're based in some\naspect of training data, but when you look at them, they're novel. That makes you think like how\neasy it's to do this thing, how easy it's to create universes, entire like video game worlds\nthat seem ultrarealistic and photorealistic. And then how easy is it to get lost in that world first with a VR headset and then on the physics-based level? - Someone said to me recently, I thought it was a super profound insight that there are these like\nvery simple sounding, but very psychedelic insights\nthat exist sometimes. So the square root function. Square root of four, no problem. Square root of two, okay, now I have to think\nabout this new kind of number. But once I come up with this easy idea of a square root function that you can kind of explain to a child and exists by even like looking\nat some simple geometry, then you can ask the question of what is the square\nroot of negative one? This is why it's like a psychedelic thing that tips you into some\nwhole other kind of reality. And you can come up with\nlots of other examples. But I think this idea that the lowly square\nroot operator can offer such a profound insight and a new realm of knowledge, applies in a lot of ways. And I think there are a\nlot of those operators for why people may think that\nany version that they like of the simulation hypothesis\nis maybe more likely than they thought before. But for me, the fact that Sora worked\nis not in the top five. - I do think broadly speaking, AI will serve as those kinds of gateways at its best simple\npsychedelic like gateways to another wave sea reality. - That seems for certain. - That's pretty exciting. I haven't done Ayahuasca before, but I will soon. I'm going to the\naforementioned Amazon jungle in a few weeks. - Excited. - Yeah, I'm excited for it. Not the Ayahuasca part. That's great, whatever. But I'm gonna spend several\nweeks in the jungle, deep in the jungle and it's exciting, but it's terrifying- - I'm excited for you. - 'Cause there's a lot of things that can eat you there and\nkill you and poison you, but it's also nature and\nit's the machine of nature. And you can't help but\nappreciate the machinery of nature in the Amazon jungle 'cause it's just like this\nsystem that just exists and renews itself like every second, every minute, every hour. It's the machine. It makes you appreciate like\nthis thing we have here, this human thing came from somewhere. This evolutionary machine has created that and it's most clearly on\ndisplay in the jungle. So hopefully, I'll make it out alive. If not, this will be the\nlast conversation we had, so I really deeply appreciate it. Do you think, as I mentioned before, there's other aliens,\ncivilizations out there, intelligent ones when you look up at the skies? - I deeply want to believe\nthat the answer is yes. I do find that kind of where... I find the firm paradox\nvery, very puzzling. - I find it scary that intelligence is not good at handling- - Yeah. Very scary, powerful.\n- Technologies. But at the same time, I think I'm pretty confident that there's just a very large number of intelligent alien\ncivilizations out there. It might just be really difficult to travel with this space. - Very possible. - And it also makes me think about the nature of intelligence. Maybe we're really blind to what intelligence looks like and maybe AI will help us see that. It's not as simple as IQ tests and simple puzzle solving. There's something bigger. Well, what gives you hope\nabout the future of humanity? This thing we've got going on, this human civilization. - I think the past is like a lot. I mean, we just look at\nwhat humanity has done in a not very long period of time. Huge problems, deep flaws, lots to be super ashamed of, but on the whole, very inspiring, gives me a lot of hope. - Just the trajectory of it all that we're together pushing\ntowards a better future. - It is... One thing that I wonder about is, is AGI gonna be more\nlike some single brain, or is it more like the sort\nof scaffolding in society between all of us? You have not had a great\ndeal of genetic drift from your great-great-great grandparents, and yet what you're capable\nof is dramatically different. What you know is dramatically different. That's not because of biological change. I mean, you got a little\nbit healthier probably. You have modern medicine, you eat better, whatever. But what you have is this scaffolding that we all contributed\nto built on top of. No one person is gonna\ngo build the iPhone. No one person is gonna go\ndiscover all of science. And yet you get to use it. And that gives you incredible ability. And so in some sense, that like we all created that and that fills me with\nhope for the future. That was a very collective thing. - Yeah. We really are standing on\nthe shoulders of giants. You mentioned when we were\ntalking about theatrical, dramatic AI risks that sometimes you might be\nafraid for your own life. Do you think about your death? Are you afraid of it? - I mean, I like if I got shot tomorrow and I knew it today, I'd be like, \"Oh, that's sad. I wanna see what's gonna happen.\" - [Lex] Yeah. - What a curious time. What an interesting time. But I would mostly just feel\nlike very grateful for my life. - The moments that you did get... Yeah, me too. It's a pretty awesome life. I get to enjoy awesome creations of humans of which I believe ChatGPT is one of, and everything that OpenAI is doing. Sam, it's really an honor and pleasure to talk to you again. - Great to talk to you. Thank you for having me. - Thanks for listening to this conversation with Sam Altman. To support this podcast, please check out our\nsponsors in the description. And now let me leave you with some words from Arthur C. Clarke and maybe that our role\non this planet is not to worship God, but to create Him. Thank you for listening and hope to see you next time."
}