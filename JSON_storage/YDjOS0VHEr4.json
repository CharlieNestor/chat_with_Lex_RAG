{
  "video_id": "YDjOS0VHEr4",
  "title": "Neil Gershenfeld: Self-Replicating Robots and the Future of Fabrication | Lex Fridman Podcast #380",
  "date": "2023-05-28",
  "transcript": [
    {
      "timestamp": "0:00",
      "section": "Full Transcript",
      "text": "- The ribosome, who I\nmentioned a little while back, can make an elephant\none molecule at a time. Ribosomes are slow. They run at about one molecule a second, but ribosomes make ribosomes, so you have trillions of them and that makes an elephant. In the same way these little\nassembly robots I'm describing can make giant structures, at heart because the robot can make the robot. So more recently two of my students, Amira and Miana, had a\nnature communication paper showing how this robot can\nbe made out of the parts it's making so the robots\ncan make the robot, so you build up the capacity\nof robotic assembly. - The following is a conversation\nwith Neil Gershenfeld, the director of MIT's\nCenter for Bits and Atoms, an amazing laboratory that\nis breaking down boundaries between the digital and physical worlds, fabricating objects and machines\nat all scales of reality, including robots and automata that can build copies of themselves and self-assemble into complex structures. His work inspires\nmillions across the world as part of the maker\nmovement to build cool stuff, to create, the very act\nthat makes life so beautiful and fun. This is a Lex Fridman podcast. To support it, please check out our\nsponsors in the description. And now, dear friends,\nhere's Neil Gershenfeld. You have spent your life\nworking at the boundary between bits and atoms, so\nthe digital and the physical. What have you learned about engineering and about nature of reality\nfrom working at this divide, trying to bridge this divide? - I learned why von Neumann and Turing made fundamental mistakes. I learned the secret of life. I learned how to solve many of the world's most important problems,\nwhich all sound presumptuous, but all of those are things\nI learned at that boundary. - So Turing and von\nNeumann, let's start there. Some of the most\nimpactful, important humans who have ever lived in computing, why were they wrong? - So I worked with Andy Gleason, who was Turing's counterpart. So just for background,\nif anybody doesn't know, Turing is credited with\nthe modern architecture of computing, among many other things. Andy Gleason was his US counterpart, and you might not have\nheard of Andy Gleason, but you might have heard\nof the Hilbert problems. And Andy Gleason solved the fifth one. So he was a really notable mathematician. During the war, he was\nTuring's his counterpart. Then von Neumann is credited\nwith the modern architecture of computing and one of his\nstudents was Marvin Minsky. So I could ask Marvin\nwhat Johnny was thinking and I could ask Andy\nwhat Alan was thinking. And what came out from that, what I came to appreciate as background, I never understood the difference\nbetween computer science and physical science. But Turing's machine that's the foundation of modern computing has a simple physics mistake, which is the head is\ndistinct from the tape. So in the Turing machine, there's a head that\nprogrammatically moves and reads and writes a tape. The head is distinct from the tape, which means persistence of\ninformation is separate from interaction with information. Then von Neumann wrote\ndeeply and beautifully about many things, but not computing. He wrote a horrible memo\ncalled the First Draft of a Report on the Ed Vac, which is how you program\na very early computer. In it, he essentially roughly\ntook Turing's architecture and built it into a machine. So the legacy of that is the\ncomputer somebody's using to watch this is spending\nmuch of its effort moving information from\nstorage transistors to processing transistors, even though they have the\nsame computational complexity. So in computer science, when you learn about computing, there's a ridiculous taxonomy of about 100 different models of computation. But they're all fictions. In physics, a patch of\nspace occupies space, it stores state, it takes time to transit,\nand you can interact. That is the only model of\ncomputation that's physical. Everything else is a fiction. I really came to appreciate\nthat a few years back when I did a keynote\nfor the annual meeting of the super computer industry and then went into the\nhalls and spent time with the super computer builders\nand came to appreciate- if you're familiar with\nthe movie The Metropolis, people would frolic\nupstairs in the gardens and down in the basement\npeople would move levers. And that's how computing exists today, that we pretend software is not physical, it's separate from hardware. And the whole canon of computer science is based on this fiction that bits aren't constrained by atoms. But all sorts of scaling\nissues in computing come from that boundary. But all sorts of opportunities\ncome from that boundary. And so you can trace\nit all the way back to Turing's machine making this mistake between the head and the tape, von Neumann, he never called\nit von Neumann's architecture. He wrote about it in this dreadful memo and then he wrote beautifully\nabout other things we'll talk about. Now to end a long answer,\nTuring and von Neumann both knew this. So all of the canon of computer\nscientists credits them for what was never meant to\nbe a computer architecture. Both Turing and von\nNeumann ended their life studying exactly how\nsoftware becomes hardware. So von Neumann studied\nself-reproducing automata, how a machine communicates\nits own construction. Turing studied morphogenesis, how genes give rise to form. They ended their life\nstudying the embodiment of computation, something that's been forgotten\nby the canon of computing, but developed sort of off to the sides by a really interesting lineage. - So there's no distinction\nbetween the head and the tape, between the computer and\nthe computation editor, it's all computation? - Right. I never understood the difference between computer science and physical science. And working at that boundary\nhelped lead to things like my lab was part of doing with a number of interesting collaborators. The first faster than\nclassical quantum computations, we were part of a collaboration creating the minimal synthetic\norganism where you design life in a computer. Those both involve domains\nwhere you just can't separate hardware from software, computation is embodied in\nthese really profound ways. - So the first quantum\ncomputations, synthetic life, so in the space of biology, so the space of physics\nat the lowest level and the space of biology\nat the lowest level. So let's talk about CBA,\nCenter of Bits and Atoms. What's the origin story of\nthis legendary MIT center that you were a part of creating? - In high school, I really wanted to go to vocational school where you learned to weld\nand fix cars and build houses and I was told, no, you're smart. You have to sit in a room. And nobody could explain to me why I couldn't go to vocational school. I then worked at Bell Labs, this wonderful place before deregulation, legendary place, and I\nwould get union grievances because I would go into the workshop and try to make something\nand they would say, no, you're smart, you have to\ntell somebody what to do. And it wasn't until MIT, and I'll explain how CBA started, but I could create CBA\nthat I came to understand this is a mistake that dates\nback to the Renaissance. So in the Renaissance, the liberal arts emerged\nand liberal doesn't mean politically liberal. This was the path to\nliberation, birth of humanism. And so the liberal arts with\nthe trivium, quadrivium, roughly language, natural science. And at that moment what\nemerged was this sort of dreadful concept of the illiberal arts. So anything that wasn't the liberal arts was for commercial gain\nand was just making stuff and wasn't valid for serious study. And so that's why we're\nleft with learning to weld wasn't a subject for serious study. But the means of expression have changed since the Renaissance, so micromachining or embedded coding is\nevery bit as expressive as painting a painting or writing a sonnet. So never understanding this difference between computer science\nand physical science, the path that led me to\ncreate CBA with colleagues, I was what's called a\njunior fellow at Harvard. I was visiting MIT through Marvin because I was interested in the\nphysics of musical instruments. This'll be another slight digression. In Cornell, I would study physics and then I would cross the\nstreet and go to the music department where I played the bassoon and I would trim reeds and play the reeds. And they'd be beautiful\nbut then they'd get soggy. And then I discovered in\nthe basement of the music department at Cornell was David Borden, who you might not have heard of but is legendary in electronic\nmusic 'cause he was really the first electronic musician. So Bob Moog, who invented\nMoog synthesizers was a physics student at Cornell, like me crossing the street. And eventually he was kicked out and invented electronic music. David Borden was the first musician who created electronic music. So he is legendary for\npeople like Phil Glass and Steve Reich. And so that got me thinking about, I would behave as a scientist\nin the physics department but not in the music department. Got me thinking about what's\nthe computational capacity of a musical instrument. And through Marvin, he introduced me to Todd\nMachover at the Media Lab who was just about to start\na project with Yo-Yo Ma that led to a collaboration\nto instrument a cello to extract Yo-Yo's data\nand bring it out into computational environments. - What is the computational\ncapacity of musical instrument, as we continue on this tangent and when we shall return to CBA. - One part of that is to\nunderstand the computing. And if you look at like\nthe finest time scale and length scale, you\nneed to model the physics. It's not heroic. A good GPU can do teraflops today. That used to be a national\nclass supercomputer, now it's just a GPU. If you take the time\nscales and length scales relevant for the physics, that's about the scale\nof the physics computing. For Yo-Yo, what was really driving it was he's completely\nunsentimental about the Strad. It's not that it makes\nsome magical wiggles in the sound wave, it's\nperformance as a controller, how he can manipulate it\nas an interface device. - Interface between what and what exactly? - Him and sound. And so what it led to was, I had started by thinking\nabout ops per second, but Yo-Yo's question was really\nresolution and bandwidth. It's how fast can you measure what he does and the bandwidth and the resolution of detecting his controls and then mapping them into sounds. And what he found was if you\ninstrument everything he does and connect it to almost anything, it sounds like Yo-Yo, that the magic is in the control, not in ineffable details\nin how the wood wiggles. And so with Yo-Yo and Todd, that led to a piece and\ntowards the end I asked Yo-Yo, what it would take for him\nto get rid of his Strad and use our stuff. And his answer was just logistics. It was, at that time, our stuff was like a rack of electronics and lots of cables and some\ngrad students to make it work. Once the technology becomes\nas invisible as the Strad, then, sure, absolutely\nhe he would take it. And by the way, as a\nfootnote on the footnote, an accident in the\nsensing of Yo-Yo's cello led to a hundred million dollar a year auto safety business to\ncontrol airbags in cars. - How did that work? - I had to instrument the bow\nwithout interfering with it. So I set up local electromagnetic fields where I would detect how\nthose fields interact with the bow he's playing. But we had a problem that his hand, whenever his hand got\nnear these sensing fields, I would start sensing his hand rather than the materials on the bow. And I didn't quite\nunderstand what was going on with that interference. So my very first grad student ever, Josh Smith, did a thesis on\ntomography with electric fields, had to see in 3D with electric fields. Then through Todd, and at that\npoint a research scientist in my lab, Joe Paradiso, it led to a collaboration\nwith Penn and Teller where we did a magic trick in\nLas Vegas to contact Houdini and sort of these fields are sort of like contacting spirits. So we did a magic trick in Las Vegas. And then the crazy thing\nthat happened after that was Phil Rittmuller came\nrunning into my lab. He worked with, this\nbecame with Honda and NEC, airbags were killing infants\nin rear-facing child seats. Cars need to distinguish\na front-facing adult, where you'd save the life, versus a bag of groceries\nwhere you don't need to fire the airbag versus\na rear facing infant where you would kill it. And so the seat needed to, in effect, see in 3D to understand the occupants. And so we took the Penn\nand Teller magic trick derived from Josh's\nthesis from Yo-Yo's cello to an auto show. And all the car companies said, great, when can we buy it? And so that became ELESYS\nand it was a hundred million dollar a year business making sensors. There wasn't a lot of publicity\nbecause it was in the car so the car didn't kill you. So they didn't sort of advertise, we have nice sensors so\nthe car doesn't kill you. But it became a leading\nauto safety sensor. - And that started from the\ncello and the question of the computational capacity\nof the musical instrument. - So now to get back to MIT. I was spending a lot of\noutside time at IBM research that had gods of the\nfoundations of computing. There's just amazing people there. And I'd always expected to\ngo to IBM to take over a lab, but at the last minute\npivoted and came to MIT to take a position in the Media Lab and start what became\nthe predecessor to CBA. Media Lab is well known\nfor Nicholas Negroponte. What's less well known is\nthe role of Jerry Wiesner. So Jerry was MIT's president, before that, Kennedy science advisor, grand old man of science. At the end of his life, he was frustrated by how\nknowledge was segregated. And so he wanted to create a department of none of the above. A department for work that\ndidn't fit in departments. And the Media Lab, in a\nsense, was a cover story for him to hide a department. As MIT's president towards\nthe end of his tenure, if he said, I'm gonna make a department for things that don't fit in departments, the departments would've screamed. But everybody was sort of\npaying attention to Nicholas creating the Media Lab. And Jerry kind of hid in\nit a department called Media Arts and Sciences. It's really the department\nof none of the above. And Jerry explaining that and\nNicholas then confirming it is really why I pivoted and went to MIT because my students who help\ncreate quantum computing or synthetic life get degrees\nfrom Media Arts and Sciences, this department of none of the above. So that led to coming to MIT. With Todd and Joe Paradiso and Mike Holly, we started a consortium\ncalled Things That Think, and this was around the\nbirth of Internet of Things and RFID. But then we started doing\nthings like work we can discuss that became the beginnings\nof quantum computing and cryptography and materials\nand logic and microfluidics. And those needed much more\nsignificant infrastructure and were much longer research arcs. So with a bigger team of about 20 people, we wrote a proposal to the NSF to assemble one of every tool to make\nanything of any size, was roughly the proposal. - One of any tool to make\nanything of any size? - So there're usually\nnanometers, micrometers, millimeters, meters are segregated, input and output is segregated. We wanted to look just very\nliterally at how digital becomes physical and\nphysical becomes digital. And fortunately we got NSF on a good day and they funded this facility of one of almost every tool to make anything. And so with a group of core colleagues that included Joe Jacobson,\nIke Chuang, Scott Manalis, we launched CBA. - And so you're talking\nabout nanoscale, microscale, nano structures, microstructures,\nmacro structures, electron microscopes, and\nfocused high beam probes for nano structures, laser micromachining,\nand x-ray microtomography for microstructures, multi-axis machining and 3D\nprinting for macro structures, just some examples. What are we talking\nabout in terms of scale? How can we build tiny\nthings and big things all in one place? How's that possible? - A well-equipped research\nlab has the sort of tools we're talking about,\nbut they're segregated in different places. They're typically also run by technicians where you then have an account\nand a project and you charge. All of these tools are essentially, when you don't know what you're doing, not when you do know what you're doing, in that they're when you need\nto work across length scales. Once projects are\nrunning in this facility, we don't charge for time. You don't make a formal\nproposal to schedule and the users really run the tools and it's for work that's kind of inchoate, that needs to span these\ndisciplines and length scales. And so work in the project today, work in CBA today ranges\nfrom developing zeptojoule electronics for the lowest power computing to micromachining diamond to\ntake 10 million RPM bearings for molecular spectroscopy studies up to exploring robots to\nbuild 100 meter structures in space. - The three things you just mentioned. Let's start with the biggest. What are some of the biggest\nstuff you attempted to explore how to build in a lab? - So viewed from one direction, what we're talking about\nis a crazy random-seeming of almost unrelated projects, but if you rotate 90 degrees, it's really just a core\nthought over and over again. Just very literally how\nbits and atoms relate, how digital and just going\nfrom digital to physical, in many different domains. But it's really just the same\nidea over and over again. So to understand the biggest things, let me go back to bring in now Shannon as well as von Neumann. - Claude Shannon? - So what is digital? The casual, obvious answer\nis digital in one in zero, but that's wrong. There's a much deeper answer, which is Claude Shannon at MIT wrote the best master's thesis ever. In his master's thesis, he invented our modern\nnotion of digital logic. Where it came from was Vannevar Bush was a grand old man at MIT. He created the post-war\nresearch establishment that led to the National\nScience Foundation. And he made an important\nmistake, which we can talk about. But he also made the\ndifferential analyzer, which was the last great analog computer. So it was a room full of gears and pulleys and the longer it ran,\nthe worse the answer was. And Shannon worked on it as a student. And he got so annoyed,\nin his master's thesis, he invented digital logic. But he then went on to Bell Labs. And what he did there was communication was beginning to expand. There was more demand for phone lines. And so there's a question\nabout how many phone messages you could send down a wire and you could try to just\nmake it better and better. He asked a question nobody had asked, which is rather than make\nit better and better, what's the limit to how good it can be? And he proved a couple things, but one of the main things he proved was a threshold theorem for channel capacity. And so what he showed was my voice to you right now is\ncoming as a wave through sound and the further you get,\nthe worse it sounds. But people watching this are getting it as packets of data in a network. When the computer they're\nwatching this gets the packet of information, it can\ndetect and correct an error. And what Shannon showed is\nif the noise in the cable to the people watching\nthis is above a threshold, they're doomed. But if the noise is below a threshold, for a linear increase in the energy representing our conversation, the error rate goes down exponentially. Exponentials are fast, there's very few of them in engineering. And the exponential reduction\nof error below a threshold if you restore state is\ncalled a threshold theorem. That's what led to digital, that means unreliable\nthings can work reliably. So Shannon did that for communication. Then von Neumann was inspired by that and applied it to\ncomputation and he showed how an unreliable computer\ncan operate reliably by using the same threshold\nproperty of restoring state. It was then forgotten many years. We had to rediscover it, in effect, in the quantum computing era when things are very unreliable again. But now to go back to\nhow does this relate to the biggest things I've made. So in fabrication, MIT invented computer-controlled manufacturing in 1952. Jet aircraft were just emerging. There was a limit to\nturning cranks on a machine, on a milling machine to\nmake parts for jet aircraft. Now this is a messy story. MIT actually stole\ncomputer-controlled machining from an inventor who brought it to MIT, wanted to do a joint\nproject with the Air Force, and MIT effectively stole it from him. So it's kind of a messy history, but that sounds like the birth of computer-controlled machining, 1952. There are a number of\ninventors of 3D printing. One of the companies spun off from my lab by Max Lobovsky's Formlabs, which is now a billion\ndollar 3D printing company. That's the modern version. But all of that's analog, meaning the information is\nin the control computer, there's no information in the materials. And so it goes back to Vannevar\nBush's analog computer. If you make a mistake in\nprinting or machining, just the mistake accumulates. The real birth of computerized\ndigital manufacturing is 4 billion years ago. That's the evolutionary\nage of the ribosome. So the way you are\nmanufactured is there's a code that describes you, the genetic code. It goes to a micromachine, the ribosome, which is this molecular factory\nthat builds the molecules that are you. The key thing to know about\nthat is there're about 20 amino acids that get assembled and in that machinery, it does everything Shannon and von Neumann taught us. You detect and correct errors. So if you mix chemicals, the error rate is about\na part in a hundred. When you elongate protein in the ribosome, it's about a part and 10 to the 4. When you replicate DNA, there's an extra level\nof error correction, it's a part in 10 to the 8. And so in the molecules that make you, you can detect and correct errors and you don't need a ruler to make you, the geometry comes from your parts. So now compare a child playing with Lego and a state-of-the-art 3D printer or computerized milling machine. The tower made by a child is more accurate than their motor control\nbecause the act of snapping the bricks together gives you\na constraint on the joints. You can join bricks made\nout of dissimilar materials. You don't need a ruler for Lego 'cause the geometry locally\ngives you the global parts and there's no LEGO trash. The parts have enough\ninformation to disassemble them. Those are exactly the\nproperties of a digital code. - The unreliable is made reliable. - Yes, absolutely. So what the ribosome figured\nout 4 billion years ago is how to embody these digital properties, but not for communication\nor computation, in effect, but for construction. So a number of projects in\nmy lab have been studying the idea of digital materials and think of a digital\nmaterial just as LEGO bricks. The precise meaning is\na discreet set of parts reversibly joined with global geometry determined from local constraints. And so it's digitizing the materials. And so I'm coming back to\nwhat are the biggest things I've made. My lab was working with\nthe aerospace industry. So Spirit Aero was Boeing's factories. They asked us for how to join composites. When you make a composite airplane, you make these giant\nwing and fuselage parts. And they asked us for a better\nway to stick them together 'cause the joints were a place of failure. And what we discovered was instead of making a few big parts, if you make little loops of carbon fiber and you reversibly link them in joints and you do it in a special geometry that balances being underconstrained and overconstrained with just\nthe right degrees of freedom, we set the world record\nfor the highest modulus ultralight material just by, in effect, making carbon fiber Lego. Lightweight materials are\ncrucial for energy efficiency. This let us make the lightest\nweight high modulus material. We then showed that with\njust a few part types, we can tune the material properties and then you can create really wild robots that instead of having a\ntool the size of a jumbo jet to make a jumbo jet, you can make little\nrobots that walk on these cellular structures to\nbuild the structures where they error-correct their\nposition on the structure and they navigate on the structure. And so using all of that, with NASA, we made morphing airplanes. A former student, Kenny\nChung and Ben Jeannette, made a morphing airplane\nthe size of NASA Langley's biggest wind tunnel. With Toyota, we've made\nsuper efficiency race cars. We're right now looking\nat projects with NASA to build these for things\nlike space telescopes and space habitats where the ribosome, who I mentioned a little while back, can make an elephant\none molecule at a time. Ribosomes are slow, they run\nat about one molecule a second. But ribosomes make ribosomes. So you have thousands of\nthem, trillions of them, and that makes an elephant. In the same way, these\nlittle assembly robots I'm describing can make giant structures, at heart because the\nrobot can make the robot. So more recently, two of my\nstudents, Amira and Miana, had a nature communication paper showing how this robot can\nbe made out of the parts it's making so the robots\ncan make the robot. So you build up the capacity\nof robotic assembly. - It can self-replicate. Can you linger on what\nthat robot looks like? What is a robot that can walk\nalong and do error correction? And what is a robot\nthat can self-replicate from the materials it is given? What does that look like? What are we talking? This is fascinating. - The answer is different\nat different length scales. So to explain that, in biology, primary structure is the\ncode in the messenger RNA that says what the ribosome should build. Secondary structure\nare geometrical motifs. They're things like helices or sheets. Tertiary structures\nare functional elements like electron donors or acceptors. Quaternary structure is\nthings like molecular motors that are moving my mouth\nor making the synapses work in my brain. So there's that hierarchy\nof primary, secondary, tertiary, quaternary. Now what's interesting is if\nyou wanna buy electronics today from a vendor, there are hundreds of\nthousands of types of resistors or capacitors or\ntransistors, huge inventory. All of biology is just\nmade from this inventory of 20 parts, the amino acids. And by composing them, you\ncan create all of life. And so as part of this\ndigitization of materials, we're in effect trying to create something like amino acids for engineering, creating all of technology from 20 parts. As another discussion, I helped start an office\nfor science in Hollywood. And there was a fun thing\nfor the movie The Martian where I did a program with\nBill Nye and a few others on how to actually build\na civilization on Mars that they described in\na way that I like as, I was talking about how to\ngo to Mars without luggage and at heart, it's sort\nof how to create life in non-living materials. If you think about this\nprimary, secondary, tertiary, quaternary structure, in my lab, we're doing that but on\ndifferent length scales for different purposes. So we're making microrobots\nout of like nano bricks and to make the robots to\nbuild large scale structures in space, the elements of the robots now are centimeters rather than micrometers. And so the assembly robots\nfor the bigger structures, there're the cells that\nmake up the structure, but then we have functional cells. And so cells that can process and actuate, each cell can like move\none degree of freedom or attach or detach or process. Now those elements I just described, we can make out of the\nstill smaller parts. So eventually, there's a\nhierarchy of the little parts make little robots that make\nbigger parts of bigger robots up through that hierarchy. - In that way you can\nmove up to landscape? - Early on I tried to\ngo in a straight line from the bottom to the top and that ended up being a bad idea. Instead, we're kind of doing\nall of these in parallel and then they're growing together. And so to make the\nlarger scale structures, there's a lot of hype right\nnow about 3D printing houses where you have a printer\nthe size of the house. We're right now working\non using swarms of these table scale robots that\nwalk on the structures to place the parts much more efficiently. - That's amazing. But you're saying you\ncan't for now go from the very small to the very large. - That'll come, that'll come in stages. - Can we just linger on this idea, starting from von\nNeumann's self-replicating automata that you mentioned. It's just a beautiful idea. - So that's at the heart of all of this. In the stack I described, so one student, Will Langford,\nmade these microrobots out of little parts that then we're using for Miana's bigger robots\nup through this hierarchy. And it's really realizing this idea of the self-reproducing automata. So von Neumann, when I complained about the von Neumann architecture, it's not fair to von Neumann\n'cause he never claimed it as his architecture. He really wrote about it in\nthis one fairly dreadful memo that led to all sorts\nof lawsuits and fights about the early days of computing. He did beautiful work\non reliable computation and unreliable devices. And towards the end of his\nlife what he studied was how, and I have to say this precisely, how a computation communicates\nits own construction. - So beautiful. - So a computation can store a description of how to build itself. But now there's a really hard problem, which is, if you have that in your mind, how do you transfer it and wake up a thing that then can contain it. So how do you give birth to a thing that knows how to make itself? And so with Stan Ulam, he invented cellular automata\nas a way to simulate these, but that was theoretical. Now the work I'm describing in my lab is fundamentally how to realize it, how to realize self-reproducing automata. And so this is something\nvon Neumann thought very deeply and very\nbeautifully about theoretically. And it's right at this intersection. It's not communication or\ncomputation or fabrication. It's right at this intersection\nwhere communication and computation meets fabrication. Now the reason self-reproducing\nautomata intellectually is so important 'cause this\nis the foundation of life. This is really just\nunderstanding the essence of how to life. And in effect we're trying to create life in non-living material. The reason it's so\nimportant technologically is because that's how you scale capacity. That's how you can make an\nelephant from a ribosome, 'cause assemblers make assemblers. - So simple building blocks\nthat inside themselves contain the information how to build more building blocks. And between each other, construct arbitrarily complex objects. - Now let me give you the numbers. So let me relate this to, right now we're living in\nAI mania explosion time. Let me relate that to\nwhat we're talking about. A 100 petaFLOP computer, which is a current\ngeneration supercomputer, not quite the biggest ones, does 10 to the 17 ops per second. Your brain does 10 to\nthe 17 ops per second. It has about 10 to the 15 synapses and they run at about 100 hertz. So as of a year or two ago, the performance of a big\ncomputer matched a brain. So you could view AI as a breakthrough. But the real story is within\nabout a year or two ago, the supercomputer has about\n10 to the 15 transistors in the processors, 10 to the\n15 transistors in the memory, which is the synopses in your brain. So the real breakthrough\nwas the computers match the computational capacity of a brain. And so we'd be sort of\nderelict if they couldn't do about the same thing. But now the reason I'm\nmentioning that is the chip fab making the super computer is placing about 10 to the 10 transistors a second. While you're digesting\nyour lunch right now, you're placing about 10 to\nthe 18 parts per second. There's an eight order\nof magnitude difference. So in computational capacity, it's done, we've caught up. But there's eight orders\nof magnitude difference in the rate at which biology can build versus state-of-the-art\nmanufacturing can build. And that distinction is\nwhat we're talking about, that distinction is not analog, but this deep sense of\ndigital fabrication, of embodying codes in construction. So a description doesn't describe a thing, but the description becomes the thing. - So you're saying, this is\none of the cases you're making, that this is this third revolution. We've seen the Moore's\nLaw in communication, we've seen the Moore's\nLaw-like type of growth in computation, and you're anticipating\nwe're going to see that in digital fabrication. Can you actually, first of all, describe what you mean by\nthis term digital fabrication? - The casual meaning is\nthe computer controls the tool to make something. And that was invented\nwhen MIT stole it in 1952. There's the deep meaning\nof what the ribosome does, of a digital description\ndoesn't describe a thing, a digital description becomes the thing. That's the path to the\nStar Trek replicator. And that's the thing\nthat doesn't exist yet. Now I think the best way to understand what this roadmap looks like\nis to now bring in FabLabs and how they relate to all of this. - What are FabLabs? - So here's a sequence. With colleagues, I\naccidentally started a network of what's now 2,500 digital\nfabrication community labs called FabLabs, right\nnow in 125 countries. And they double every year and a half. That's called Lass' Law\nafter Sherri Lassiter, who I'll explain. So here's the sequence. We started Center for Bits\nand Atoms to do the kind of research we're talking about. We had all of these machines\nand then had a problem. It would take a lifetime of classes to learn to use all the machines. So with colleagues who helped start CBA, we began a class modestly called How to Make Almost Anything. And there's no big agenda. It was aimed at a few research\nstudents to use the machines. And we were completely\nunprepared for the first time we taught it. We were swamped by, every year since, hundreds of students\ntry to take the class. It's one of the most\noversubscribed classes at MIT. Students would say things like, can you teach this at MIT? It seems too useful. It's just how to work these machines. And the students in the class, I would teach them all the\nskills to use all these tools and then they would do\nprojects integrating them and they're amazing. So Kelly was a sculptor,\nno engineering background. Her project was she made a\ndevice that saves up screams when you're mad and\nplaced them back later. - Saves up screams when you're mad and plays them back later? - You scream into this device\nand it deadens the sound, records it, and then when it's convenient, releases your scream. - Can we just pause on the brilliance of that invention, creation, the art, I don't know, the brilliance. Who is this that created this? - Kelly Dobson. Gone on to do a number\nof interesting things. Mejin, who's gone on to do a\nnumber of interesting things, made a dress instrumented\nwith sensors and spines. And when somebody creepy comes close, it would defend your personal space. - Also very useful. - Another project early on\nwas a web browser for parrots, which have the cognitive\nability of a young child and let's parrots surf the internet. Another was an alarm\nclock you wrestle with and prove you're awake. And what connects all of these is, so MIT made the first realtime\ncomputer, the Whirlwind. That was transistorized as the TX. The TX was spun off from MIT as the PDP. PDPs were the mini computers\nthat created the internet. So outside MIT was Deck,\nPrime, Wang, Data General, the whole mini computer industry, the whole computing industry was there, and it all failed when\ncomputing became personal. Ken Olson, the head of\nDigital, famously said, you don't need a computer at home. There's a little background to that, but Deck completely missed\ncomputing became personal. So I mention all of that\nbecause I was asking how to do digital fabrication,\nbut not really why. The students in this how to make class were showing me that the killer\napp of digital fabrication is personal fabrication. - How do you jump to the\npersonal fabrication? - So Kelly didn't make the screen body because it was for a thesis. She wasn't writing a research paper, it wasn't a business model, it was 'cause she wanted one. It was personal expression, going back to me in vocational school. Personal expression in these\nnew means of expression. So that's happened every year since. - The course is literally called How To Make Almost Anything. A legendary course at MIT. Every year. - And it's grown to multiple labs at MIT with as many people involved\nas teaching as taking it. And there's even a Harvard\nlab for the MIT class. - What have you learned about humans colliding with the FabLab\nabout what the capacity of humans to be creative and to build? - I mentioned Marvin,\nanother mentor at MIT, sadly no longer living, is Seymour Papert. So Papert studied with Piaget. He came to MIT to get access to the early- Piaget was a pioneer in how kids learn. Papert came to MIT to get\naccess to the early computers with the goal of letting\nkids play with them. Piaget helped show kids\nare like scientists. They learn as scientists\nand it gets kind of throttled out of them. Seymour wanted to let kids have\na broader landscape to play. Seymour's work led with\nMitch Resnick to Lego, Logo, MindStorms, all of that stuff. As FabLab spread and we started creating educational programs for kids in them, Seymour said something really interesting, he made a gesture. He said it was a thorn in his side that they invented\nwhat's called the turtle, an early robot kids could program to connect it to a mainframe computer. Seymour said the goal was not for the kids to program the robot, it was for the kids to create the robot. And so in that sense, the FabLabs, which for me were just this accident, he described as sort of\nthis fulfillment of the arc of kids learn by experimenting. It was to give them the tools to create, not just assemble things\nand program things, but actually create. So coming to your question. What I've learned is MIT a few years back, somebody added up businesses\nfrom spun off from MIT and it's the world's 10th economy. It falls between India and Russia. And I view that in a way as a bad number because it's only a few thousand people and these aren't uniquely\nthe 4,000 brightest people. It's just a productive\nenvironment for them. And what we found is in\nrural Indian villages and African shanty towns\nand arctic hamlets, I find exactly, precisely that profile. So Ling Sai did a few hours above Tromso, way above the arctic circles. It's so far north, the satellite\ndishes look at the ground, not the sky. Hans Christian in the lab\nwas considered a problem in the local school 'cause they\ncouldn't teach him anything. I showed him a few projects. Next time I came back he\nwas designing and building little robot vehicles. And in South Africa, I mentioned Sochengovi, in\nthis apartheid township, the local technical institute taught kids how to make bricks and fold sheets. It was punitive. But Chapiso in the FabLab was\nactually doing all the work of my MIT classes. And so over and over, we found precisely the same kind of bright,\ninventive creativity. And historically, the answer\nwas you're smart, go away. It's sort of like me\nand vocational school. But in this lab network, what we could then do is in\neffect bring the world to them. Now let's look at the\nscaling of all of this. So there's one earth, a thousand\ncities, a million towns, a billion people, a trillion things. There was one Whirlwind computer and my team made the\nfirst realtime computer. There were thousands of PDPs. There were millions of hobbyist computers that came from that. Billions of personal computers. Trillions of internet of things. So now if we look at this FabLab story, 1952 was the NC Mill. There are now thousands of FabLabs. And the FabLab costs exactly the same cost and complexity of the mini computer. So on the mini computer, it\ndidn't fit in your pocket, it filled a room. But video games, email, word processing, really anything you do with the internet, anything you do with a computer\ntoday happened at that era because it got on the\nscale of a work group, not a corporation. In the same way, FabLabs\nare like the mini computers inventing how does the world work if anybody can make anything. Then if you look at that scaling, FabLabs today are transitioning from buying a machine to\nmachines making machines. So we're transitioning to, you can go to a FabLab\nnot to make a project, but to make a new machine. So we talked about the deep\nsense of self-replication. There's a very practical\nsense of FabLab machines making FabLab machines. And so that's the equivalent\nof the hobbyist computer era, whatever it's called,\nthe Altera, historically. Then the work we spent\na while talking about about assemblers and self-assemblers, that's the equivalent of\nsmartphones and internet of things. That's when the assemblers\nare like the smartphone where a smartphone today\nhas the capacity of what used to be a\nsupercomputer in your pocket. And then the smart thermostat on your wall has the power of the\noriginal PDP computer. Not metaphorically, but literally. And now there's trillions of those. In the same sense that when\nwe finally merge materials with the machines in the self-assembly, that's like the internet of things stage. But here's the important lesson. If you look at the computing analogy, computing expanded exponentially but it really didn't fundamentally change. The core things happened\nin that transition in the mini computer era. So in the same sense, the research now we spent\na while talking about is how we get to the replicator. Today, you can do all of that if you close your eyes\nand view the whole FabLab as a machine. In that room, you can\nmake almost anything, but you need a lot of inputs. Bit by bit, the inputs will go down and the size of the room will go down as we go through each of these stages. - So how difficult is it to create a self-replicating assembler, self-replicating machine\nthat builds copies of itself or builds more complicated\nversion of itself, which is kind of the dream\ntowards which you're pushing in a generic arbitrary sense? - I had a student, Nadia\nPeak with Jonathan Ward, who for me started this idea\nof how do we use the tools in my lab to make the tools in the lab? In a very clear sense, they are making self-reproducing machines. So one of the really cool\nthings that's happened is there's a whole network\nof machine builders around the world. So there's Danielle now in\nGermany and Yens in Norway. And each of these people\nhas learned the skills to go into a FabLab and make a machine. And so we've started creating\na network of super Fab. So the FabLab can make a machine, but it can't make a number\nof the precision parts of the machine. So in places like Bhutan or\nCarroll in the south of India, we've started creating\nsuper FabLabs that have more advanced tools to make\nthe parts of the machines so that the machines\nthemselves become even cheaper. So that is self-reproducing machines, but you need to feed\nit things like bearings or microcontrollers. They can't make those parts. But other than that, they're\nmaking their own things. And I should note as a footnote, the stack I described of\ncomputers controlling machines to machine making machines to assemblers to self assemblers, view that as fab One, two, three, four. So we're transitioning\nfrom Fab one to Fab two and the research in the\nlab is three and four. At this Fab two stage,\na big component of this is sustainability in the\nmaterial feed stocks. So Alicia, colleague in Chile,\nis leading a great effort looking at how you take forest\nproducts and coffee grounds and seashells and a range of\nlocally available materials and produce the high tech\nmaterials that go into the lab. So all of that is machine building today. Then back in the lab, what we can do today is we\nhave robots that can build structures and can assemble more robots that build structures. We have finer resolution robots that can build micromechanical systems. So robots that can build robots that can walk and manipulate. And we're just now we have a\nproject at the layer below that where there's endless attention today to billion dollar chip fab investments. But a really interesting\nthing we passed through is today the smallest\ntransistors you can buy as a single transistor just commercially for electronics is actually the size of an early transistor in\nan integrated circuit. So we're using these\nmachines making machines, making assemblers to place\nthose parts to not use a billion dollar chip fab\nto make integrated circuits, but actually assemble little\nelectronic components. - So have a fine enough,\nprecise enough actuators and manipulators that allow\nyou to place these transistors. - That's a research project in my lab called DICE, on discrete assembly\nof integrated electronics. And we're just at the point\nto really start to take seriously this notion\nof not having a chip fab make integrated electronics,\nbut having, not a 3D printer, but a thing that's a cross\nbetween a pick and place makes circuit boards in 2D, the 3D printer extrudes in 3D, we're making sort of a micromanipulator that acts like a printer but it's placing to\nbuild electronics in 3D. - But this micromanipulator\nis distributed. So there's a bunch of them or\nis this one centralized thing? - So that's why that's a great question. So I have a prize that's\nalmost but not been claimed for the students whose thesis\ncan walk out of the printer. - Oh, nice. - So you have to print\nthe thesis with the means to exit the printer and it\nhas to contain its description of the thesis that says how to do that. - It's a really good, it's a fun example of exactly\nthe thing we're talking about. - And I've had a few\nstudents almost get to that. And so in what I'm describing, there's this stack where\nwe're getting closer, but it's still quite a few\nyears to really go from a- so there's a layer below the transistors where we assemble the base materials that become the transistor. We're now just at the edge\nof assembling the transistors to make the circuits. We can assemble the microparts\nto make the microrobots, we can assemble the bigger\nrobots, and in the coming years, we'll be patching together\nall of those scales. - So do you see a vision of\njust endless billions of robots at the different scales,\nself-assembling, self-replicating, and building more complicated structures? - Yes, and the but to the yes but, is let me clarify two things. One is, that immediately\nraises King Charles fear of gray goo of runaway mutant\nself-reproducing things. The reason why there are\nmany things I can tell you to worry about, but\nthat's not one of them, is if you want things to\nautonomously self-reproduce and take over the world, that means they need\nto compete with nature on using the resources of\nnature, of water and sunlight. And in light of everything I'm describing, biology knows everything I told you. Every single thing I explain, biology already knows how to do. What I'm describing isn't new for biology, it's new for non-biological systems. So in the digital era, the economic win ended\nup being centralized, the big platforms. In this world of machines\nthat can make machines, I'm asked for example, what's\nthe killer opportunity? Who's gonna make all the\nmoney, who to invest in? But if the machine can make the machine, it's not a great business\nto invest in the machine. In the same way that if you can produce, if you can think globally\nbut produce locally, then the way the technology\ngoes out into society isn't a function of central control but is fundamentally distributed. Now that raises an\nobvious kind of concern, which is, well, doesn't this\nmean you could make bombs and guns and all of that? The reason that's much less of a problem than you would think is\nmaking bombs and guns and all of that is a very\nwell met market need. Anywhere we go, there's a fine supply chain for weapons. Now hobbyists have been\nmaking guns for ages and guns are available\njust about anywhere. So you could go into\nthe lab and make a gun. Today, it's not a very good gun and guns are easily available. And so generally, we've run\nthese labs in war zones. What we find is people don't\ngo to them to make weapons, which you can already do anyway. It's an alternative to making weapons. Coming back to your question, I'd say the single most\nimportant thing I've learned is the greatest natural\nresource of the planet is this amazing density of\nbright, inventive people whose brains are underused. And you could view the social\nengineering of this lab work is creating the capacity for them. And so in the end, the way this is going to impact society isn't gonna be command and control. It's how the world uses it. And it's been really gratifying for me to see just how it does. - But what are the\ndifferent ways the evolution of the exponential scaling\nof digital fabrication can evolve? Self-replicating nanobots,\nthis is the gray goo fear. It's a caricature of a fear, but nevertheless there's\ninteresting, just like you said, spam and all these kinds\nof things that came with the scaling of communication\nand computation. What are the different\nways that malevolent actors will use this technology? - First let me start\nwith a benevolent story which is trash is an analog concept. There's no trash in a forest. All the parts get disassembled and reused. Trash means something doesn't\nhave enough information to tell you how to reuse it. It's as simple as there's\nno trash in a Lego room. When you assemble Lego, the Lego bricks have enough\ninformation to disassemble them. So as you go through this Fab\none, two, three, four story, one of the implications of this transition from printing to assembling. So the real breakthrough technologically isn't additive versus subtractive, which is a subject of a\nlot of attention and hype. 3D printers are useful. We spun off companies\nlike Formlabs led by Max for 3D printing, but in a FabLab, it's 1 of maybe 10 machines. It's used but it's only\npart of the machines. The real technological change\nis when we go from printing and cutting to assembling and dissembling, but that reduces inventories\nof hundreds of thousands of parts to just having a few\nparts to make almost anything. It reduces global supply\nchains to locally sourcing these building blocks. But one of the key\nimplications is it gets rid of technological trash because\nyou can disassemble and reuse the parts, not throw them away. And so initially, that's\nof interest for things at the end of long supply\nchains like satellites on orbit. But one of the things coming\nis eliminating technical trash through reuse of the building blocks. - So like when you\nthink about 3D printers, you're thinking about\naddition and subtraction. When you think about the\nother options available to you in that parameter space as you call it, that's going to be assembly,\ndisassembly, cutting, you said? - So the 1952 NC mill was subtractive. You remove material. And 3D printing, additive. And there's a couple claims to\nthe invention of 3D printing that's closer to what's called net shape, which is you don't have\nto cut away the material you don't need, you just put\nmaterial where you do need it. And so that's the 3D printing revolution. But there are all sorts of\nlimitations on 3D printing to the kinds of materials you can print, the kind of functionality you can print. We're just not gonna get to making everything in a cell\nphone on a single printer. But I do expect to make\neverything in a cell phone with an assembler. And so instead of printing\nand cutting technologically, it's this transition to\nassembling and dissembling. Going back to Shannon and von Neumann, going back to the ribosome\n4 billion years ago. You come to malevolent. Let me tell you a story about I was doing a briefing for the\nNational Academy of Sciences group that advises the\nintelligence communities and I talked about the\nkind of research we do and at the very end I\nshowed a little video clip of Valentina in Ghana, a local girl, making surface mount\nelectronics in the FabLab. And I showed that to\nthis room full of people. One of the members of the\nintelligence community got up, livid, and said, how dare you waste our time\nshowing us a young girl in an African village making\nsurface mount electronics. We need to know about disruptive threats to the future of the United States. And somebody else got up in\nthe room and yelled at him, you idiot, I can't think of anything\nmore important than this. But for two reasons. One reason was because if we rely on informational superiority\nin the battlefield, it means other people\ncould get access to it. But this intelligence\nperson's point, bless him, wasn't that, it was\ngetting at the root causes of conflict is if this young\ngirl in an African village could actually master\nsurface mount electronics, it changes some of the\nmost fundamental things about recruitment for terrorism, impact of economic migration, basic assumptions about an economy. It's just existential for\nthe future of the planet. - But you know, we've just\nlived through a pandemic. I would love to linger on\nthis cause the possibilities that are positive are endless. But the possibilities that are negative are still nevertheless\nextremely important. With both positive and negative, what do you do with a large\nnumber of general assemblers? - With the FabLab, you\ncould roughly make a bio lab then learn biotechnology. Now that's terrifying because\nmaking self-reproducing gray goo that outcompetes biology, I consider doom because\nbiology knows everything I'm describing and is\nreally good at what it does. In how to grow almost anything, you learn skills in\nbiotechnology that let you make serious biological threats. - And when you combine some\nof the innovations you see with large language models, some of the innovations\nyou see with alpha fold, so applications of AI for\ndesigning biological systems, for writing programs, which you can with large language models increasingly, so there seems to be an\ninteresting dance here of automating the design stage\nof complex systems using AI. And then that's the bits. And you can leap now, the\ninnovations you're talking about, you can leap from the complex\nsystems in the digital space to the printing, to the creation, to the assembly at\nscale of complex systems in the physical space. - So something to be\nscared about is a FabLab can make a bio lab, a bio\nlab can make biotechnology, somebody could learn to make a virus. That's scary. Unlike some of the things\nI said I don't worry about, that's something I really\nworry about that is scary. Now how do you deal with that? Prior threats we dealt\nwith command and control. So like early color\ncopiers had unique codes and you could tell which copier made them. Eventually you couldn't keep up with that. There was a famous meeting at Asilomar in the early days of recombinant DNA where that community\nrecognized the dangers of what it was doing and\nput in place a regime to help manage it. And so that led to the kind\nof research management. MIT has an office that supervises research and it works with the national office. That works if you can\nidentify who's doing it and where, it doesn't work in this world we're describing. So anybody could do this anywhere. And so what we've found\nis you can't contain this. It's already out. You can't forbid because there\nisn't command and control. The most useful thing you\ncan do is provide incentives for transparency. But really the heart of what\nwe do is you could do this by yourself in a basement\nfor nefarious reasons or you could come into\na place in the light where you get help and you get community and you get resources. And there's an incentive\nto do it in the open, not in the dark. And that might sound naive, but in the sort of places we're working, again, bad people do bad\nthings in these places already, but providing openness\nand providing transparency is a key part of managing these. It transitions from\nregulating risks as regulation to soft power to manage them. - So there's so much potential for good, so much capacity for good that FabLabs and the ability and the tools of creation really unlock that potential. - I don't say that as\nsort of dewy-eyed naive. I say that empirically\nfrom just years of seeing how this plays out in communities. - I wonder if it's the early\ndays of personal computers though, before we get spam. - In the end, most fundamentally, literally the mother of all problems is who designed us? So assume success in that\nwe're gonna transition to the machines making machines and all of these new sort of\nsocial systems we're describing will help manage them and curate\nthem and democratize them. If we close the gap I just led off with of 10 to the 10 to 10 to the\n18 between chip fab and you, we're ultimately, in\nmarrying communication, computation, and fabrication, gonna be able to create\nunimaginable complexity. And how do you design that? And so I'd say the\ndeepest of all questions that I've been working on goes back to the oldest\npart of our genome. So in our genome what are called HOX gene, and these are morphogenes, and nowhere in your\ngenome is the number five. It doesn't store the fact\nthat you have five fingers. What it stores is what's\ncalled a developmental program. It's a series of steps. And the steps have the character\nof like grow up a gradient or break symmetry. And at the end of that\ndevelopmental program, you have five fingers. So you are stored not as a body plan, but as a growth plan. And there's two reasons for that. One reason is just compression. Billions of genes can\nplace trillions of cells. But the much deeper one is evolution doesn't randomly perturb. Almost anything you did\nrandomly in the genome would be fatal or inconsequential,\nbut not interesting. But when you modify things in\nthese developmental programs, you go from like webs\nfor swimming to fingers or you go from walking\nto wings for flying. It's a space in which\nsearch is interesting. So this is the heart of the success of AI. In part, it was the scaling\nwe talked about a while ago. And in part, it was the\nrepresentations for which search is effective. AI has found good representations. It hasn't found new ways to search, but it's found good\nrepresentations of search. - And you're saying that's what biology, that's what evolution has done, is created representations, structures, biological structures through\nwhich search is effective. - And so the developmental programs in the genome beautifully\nencapsulate the lessons of AI. And it's embodied, it's\nmolecular intelligence. It's AI embodied in our genome. It's every bit as profound as\nthe cognition in our brain. But now this is sort of\nthinking in molecular thinking in how you design. And so I'd say the most fundamental\nproblem we're working on is it's kind of tautological\nthat when you design a phone, you design the phone, you represent the design of the phone. But that actually fails\nwhen you get to the sort of complexity that we're talking about. And so there's this\nprofound transition to come. Once I can have self-reducing assemblers placing 10 to the 18 parts, you need to, not sort of metaphorically, but create life in that you\nneed to learn how to evolve. But evolutionary design\nhas a really misleading, trivial meaning. It's not as simple as you\nrandomly mutate things. It's as much more deep embodiment of AI and morphogenesis. - Is there a way for us\nto continue the kind of evolutionary design that\nled us to this place from the early days of\nbacteria, single cell organism to ribosomes and the 20 amino acids? - You mean for human augmentation? - For life- what would you call assemblers\nthat are self-replicating and placing parts? What is the dynamic\ncomplex things built with digital fabrication? What is that? That's life. - So ultimately, absolutely, if you add everything I'm talking about, it's building up to creating\nlife in non-living materials. I don't view this as copying life. I view it as driving life. I didn't start from how does biology work and then I'm gonna copy it. I start from how to\nsolve problems and then it leads me to, in a\nsense, rediscover biology. So if you go back to Valentina in Ghana making her circuit board, she still needs a chip fab very far away to make the processor\nin her circuit board. For her to make the processor locally, for all the reasons we described, you actually need the deep things we were just talking about. And so it really does lead you. There's a wonderful series\nof books by Gingery. Book one is how to make\na charcoal furnace. And at the end of book seven,\nyou have a machine shop. It's sort of how you do your own personal industrial revolution. ISRU is what NASA calls in\nsitu resource utilization. And that's how do you go to a planet and create a civilization. ISRU has essentially assumed Gingery. You go go through the\nindustrial revolution and you create the inventory of 100,00 resistors. What we're finding is the\nminimum building blocks for a civilization is roughly 20 parts. So what's interesting\nabout the amino acids is they're not interesting. They're hydrophobic or\nhydrophilic, basic or acidic. They have typical but\nnot extremal properties. But they're good enough\nyou can combine them to make you. So what this is leading\ntowards is technology doesn't need enormous\nglobal supply chains. It just needs about 20\nproperties you can compose to create all technology as\nthe minimum building blocks for a technological civilization. - So there's going to be\n20 basic building blocks based on which the self-replicating\nassemblers can work? - Right. And I say that not philosophically, just empirically, that's\nwhere it's heading. And I like thinking about how you bootstrap a civilization\non Mars, that problem. There's a fun video on\nbonus material for the movie where with a neat group of people we talk about it because it has\nreally profound implications back here on earth about\nhow we live sustainably. - What does that civilization\non Mars look like that's using ISRU, that's\nusing these 20 building blocks and does self-assembly. - Go through primary,\nsecondary, tertiary, quaternary. You extract properties like\nconducting, insulating, semiconducting, magnetic,\ndielectric, flexural. These are the kind of\nroughly 20 properties. With those, those are enough\nfor us to assemble logic and they're enough for\nus to assemble actuation. With logic and actuation,\nwe can make microrobots. The microrobots can build bigger robots. The bigger robots can then take\nthe building block materials and make the structural\nelements that you then do to make construction. And then you boot up through the stages of a technological civilization. - By the way, where in the\nspan of logic and actuation did the sensing come in? - Oh, I skipped over that. But my favorite sensor is a step response. So if you just make a step\nand measure the response to the electric field, that ranges from user\ninterfaces to positioning to material properties. And if you do it at higher frequencies, you get chemistry. And you can get all of\nthat just from a step in an electric field. So for example, once you have\ntime resolution in logic, something as simple as two electrodes let you do amazingly capable sensing. So we've been talking\nabout all the work I do, there's a story about how it happens, where do ideas come from? - That's an interesting story. Where do ideas come from? - So I had mentioned Vannevar Bush and he wrote a really influential thing called the Endless Frontier. So science won World War II. The more known story is nuclear bombs. The less well known story is the RAD lab. So at MIT, an amazing group\nof people invented radar, which is really credited\nas winning the war. So after the war, grand old man from MIT was charged with science won the war, how do we maintain that edge? And the report he wrote\nled to the National Science Foundation and the modern\nnotion we take for granted but didn't really exist\nbefore then of public funding of research, of research agencies. In it, he made what I\nconsider an important mistake, which is he described\nbasic research leads to applied research leads to applications leads to commercialization\nleads to impact. And so we need to invest in that pipeline. The reason I consider it a mistake is almost all of the examples\nwe've been talking about in my lab went backwards. That the basic research\ncame from applications. And further, almost all of the examples we've been talking about came\nfundamentally from mistakes. Essentially everything I've\never worked on has failed, but in failing, something better happened. So the way I like to describe it is ready, aim, fire is you do your homework, you aim carefully at something, a target you wanna accomplish, and if everything goes right, you then hit the target and succeed. What I do you can think\nof is ready, fire, aim. So you do a lot of work to get ready, then you close your eyes and\nyou don't really think about where you're aiming, but you look very carefully\nat where you did aim, you aim after you fire. And the reason that's so important is if you do ready, aim, fire, the best you can hope\nis hit what you aim at. So let me give you some examples, cause this is a source of great- - You're full of good lines today. - Source of great frustration. I mentioned the early quantum computing. Quantum computing is this power of using quantum mechanics to make computers that for some problems are\ndramatically more powerful than classical computers. Before it started, there was a really\ninteresting group of people who knew a lot about physics and computing that were inventing what\nbecame quantum computing before it was clear there\nwas an opportunity there. It was just studying how those relate. Here's how it fits to\nthe ready, fire, aim. I was doing really short\nterm work in my lab on shoplifting tags on. This was really before\nthere was modern RFID. And so how you put tags\nin objects to sense them. Something we just take\nfor granted commercially. And there was a problem of\nhow you can sense multiple objects at the same time. And so I was studying how you\ncan remotely sense materials to make low-cost tags that\ncould let you distinguish multiple objects simultaneously. To do that you need non-linearity so that the signal is modulated. And so I was looking for\nmaterial sources of non-linearity and that led me to look\nat how nuclear spins interact, just for spin resonance. This the sort of things\nyou use when you go in an MRI machine. And so I was studying how to use that and it turns out that it was a bad idea. You couldn't remotely use\nit for shoplifting tags, but I realized you could compute. And so with a group of colleagues thinking about early quantum computing like David DiVincenzo and\nCharlie Bennett was articulating, what are the properties\nyou need to compute? And then looking at how to make the tags. It turns out the tags were a terrible idea for sensing objects in\na supermarket checkout, but I realized they were computing. So with Ike Chuang and a few other people, we realized we could program\nnuclear spins to compute. And so that's what we use to\ndo Grover's search algorithm. And then it was used for\nShor's factoring algorithm and it worked out. The systems we did it in\nnuclear magnetic resonance don't scale beyond a few qubits, but the techniques have lived on. And so all the current\nquantum computing techniques grew out of the ways we\nwould talk to these spins. But I'm telling this whole story because it came from a bad way to\nmake a shoplifting tag. - Starting with an application, mistakes led to breakthroughs\nof fundamental science. Can you just linger on that, using nuclear spins to do computation, what gave you the guts to\ntry to think through this? From a digital fabrication perspective, actually, how to leap\nfrom one to the other. - I wouldn't call it guts, I\nwould call it collaboration. So at IBM there was this amazing group of, like I mentioned Charlie\nBennett and David DiVincenzo and Ralph Landau and Nabil Amer. And these were all gods\nof thinking about physics and computing. I yelled at the whole computer industry being based on a fiction metropolis, programmers frolicking in the garden while somebody moves\nlevers in the basement. There's a complete parallel history of Maxwell to Boltzmann to\nZollar to Landau to Bennett. Most people won't know most of these names but this whole parallel history thinking deeply about how\ncomputation and physics relate. So I was collaborating with\nthat whole group of people. And then, at MIT I was in\nthis high traffic environment. I wasn't deeply inspired\nto think about better ways to detect shoplifting tags but stumbled across companies\nthat needed help with that and was thinking about it. And then I realized those\ntwo worlds intersected and we could use the failed approach for the shoplifting tags to make early quantum computing algorithms. - This kind of stumbling is\nfundamental to the FabLab idea? - Right. Here's one more example. With the student, Manu,\nwe talked about ribosomes and I was trying to build a ribosome that worked on fluids\nso that I could place the little parts we're talking about. It kept failing 'cause bubbles\nwould come into our system and the bubbles would make\nthe whole thing stop working. And we spent about half a year trying to get rid of the bubbles. Then Manu said, wait a minute, the bubbles are actually\nbetter than what we're doing. We should just use the bubbles. And so we invented how\nto do universal object logic with little bubbles and fluid. - You have to explain this\nmicrofluidic bubble logic, please. How does this work? That's super interesting. - I'll come back and explain it. But what it led to was we\nshowed fluids could do, it'd been known fluid could do logic, like your old automobile\ntransmissions do logic, but that's macroscopic. It didn't work at little scales. We showed with these bubbles we could do it at little scales. Then I'm gonna come back and explain it. But what came out of\nthat is Manu then showed you could make a 50 cent\nmicroscope using little bubbles. And then the techniques we developed are what we use to transplant genomes to make synthetic life all\ncame out of the failure of trying to make the ribosome. The way the bubble logic\nworks is in a little channel, fluid at small scales is fairly viscous. It's sort of like pushing\njello, think of it as. If a bubble gets stuck, the fluid has to detour around it. So now imagine a channel that\nhas two wells and one bubble. If the bubble is in one well, the fluid has to go in the other channel. If the fluid is in the other well, it has to go in the first channel. So the position of the bubble can switch. It's a switch, it can switch\nthe fluid between two channels. So now we have one element of switch and it's also a memory\nbecause you can detect whether or not a bubble is stored there. Then if two bubbles meet, if you have two channels crossing, a bubble can go through one way or a bubble can go through the other way. But if two bubbles come together, then they push on each\nother and one goes one way and one goes the other way. That's a logic operation. That's a logic gate. So we now have a switch, we have a memory, and\nwe have a logic gate. And that's everything you need\nto make a universal computer. - The fact that you did that with bubbles and microfluid, it's\njust kind of brilliant. - To stay with that example, what we proposed to do was\nto make a fluidic ribosome and the project crashed and burned. It was a disaster. This is what came out of it. And so it was precisely ready, fire, aim in that we had to do a lot\nof homework to be able to make these microfluidic systems. The fire part was we didn't think too hard about making the ribosome,\nwe just tried to do it. The aim part was we\nrealized the ribosome failed but something better had happened. And if you look all\nacross research funding, research management, it\ndoesn't anticipate this. So fail fast is familiar, but fail fast tends to miss ready and aim. You can't just fail. You have to do your homework\nbefore the fail part and you have to do the aim\npart after the fail part. And so the whole language of research is about like milestones and deliverables, that works when you're\ngoing down a straight line, but it doesn't work for\nthis kind of discovery. And to leap to something you\nsaid that's really important is I view part of what the\nFabLab network is doing is giving more people\nthe opportunity to fail. - You've said that geometry is\nreally important in biology. What does fabrication biology look like? Why is geometry important? - So molecular biology\nis dominated by geometry. That's why the protein\nfolding is so important, that the geometry gives the function. And there's this hierarchical construction of as you go through primary,\nsecond, tertiary, quaternary, the shapes of the molecules make the shape of the molecular machines. And they really are exquisite machines. If you look at how your muscles move, if you were to see a simulation of it, it would look like a improbable\nscience fiction cyborg world of these little walking\nrobots that walk on a discreet lattice. They're really exquisite machines. And then from there, there's\nthis whole hierarchical stack of once you get to the top of that, you then start making\norganelles that make cells that make organs through\nthe stack of that hierarchy. - Just stepping back, does it amaze you that\nfrom small building blocks where amino acids, you\nmentioned molecules, let's go to the very beginning\nof hydrogen and helium at the start of this universe, that we're able to build up such complex and beautiful things like our human brain? - So studying thermodynamics, which is exactly the question of batteries run out and need recharging, cars get old and fail, yet life doesn't. That's why there's a sense in which life seems to violate thermodynamics, although of course it doesn't. - It seems to resist the march\ntowards entropy, somehow. - Right. And so Maxwell, who helped give rise to the science of thermodynamics posited a problem that was so infuriating it led to a series of suicides. There was a series of\nadvisors and advisees, three in a row, that all\nended up committing suicide that happened to work on this problem. And Maxwell's demon is this\nsimple but infamous problem where right now, in this\nroom, we're surrounded by molecules and they run\nat different velocities. Imagine a container that has a wall and it's got gas on both\nsides and a little door. And if the door is a\nmolecular-sized creature and it could watch the molecules coming, and when a fast molecule is\ncoming, it opens the door. When a slow molecule is\ncoming, it closes the door. After it does that for a while, one side is hot, one is cold. Once something is hot and is\ncold, you can make an engine. And so you close that\nand you make an engine and you make energy. So the demon is violating thermodynamics because it's never touching the molecule, yet by just opening and closing the door, it can make arbitrary amounts\nof energy and power a machine. And in thermodynamics you can't do that. So that's Maxwell's demon. That problem is connected to\neverything we just spoke about for the last few hours. So Leo Szilard around early\n1900s was a deep physicist who then had a lot to do with also post-war anti-nuclear things. But he reduced Maxwell's\ndemon to a single molecule. So there's only one molecule. And the question is, which\nside of the partition is it on? That led to the idea of\none bit of information. So Shannon credited Szilard's analysis of Maxwell's demon for\nthe invention of the bit. For many years, people tried\nto explain Maxwell's demon by like the energy in the\ndemon looking at the molecule or the energy to open and close the door and nothing ever made sense. Finally, Ralph Landau, one of the colleagues I mentioned at IBM, finally solved the problem. He showed that you can\nexplain Maxwell's demon by you need the mind of the demon. When the demon open and closes the door, as long as it remembers what it did, you can run the whole thing backwards. But when the demon forgets, then you can't run it backwards. And that's where you get dissipation and that's where you get the\nviolation of thermodynamics. And so the explanation of\nMaxwell's demon is that it's in the demon's brain. So then Ralph's call\ncolleague Charlie at IBM then shocked Ralph by\nshowing you can compute with arbitrarily low energy. So one of the things\nthat's not well covered is the big computers used\nfor big machine learning, the data centers, use tens\nof megawatts of power. They use as much power as a city. Charlie showed you can actually compute with arbitrarily low amounts of energy by making computers that can go backwards as well as forwards. And what limits the speed of the computer is how fast you want an answer and how certain you want the answer to be. But we're orders of\nmagnitude away from that. So I have a student, Cameron, working with Lincoln Labs\non making superconducting computers that operate\nnear this Landau limit that are orders of\nmagnitude more efficient. So stepping back to all of that, that whole tour was driven\nby your question about life. Right at the heart of\nit is Maxwell's demon: life exists because it can\nlocally violate thermodynamics. It can locally violate thermodynamics because of intelligence and\nit's molecular intelligence. I would even go out on a limb\nto say we can already see we're beginning to come to the\nend of this current AI phase. So depending on how you count, this is, I'd say, the\nfifth AI boom-bust cycle. And you can already, it's exploding, but you can already\nsee where it's heading, how it's going to saturate, what happens on the far side. The big thing that's not yet on horizons is embodied AI molecular intelligence. So to step back to this AI story, there was automation and that\nwas gonna change everything. Then there were expert systems. There was then the first phase of the neural network systems. There've been about five of these. In each case, on the slope up, it's gonna change everything. Each case, what happens\nis on the slope down, we sort of move the goalposts and it becomes sort of irrelevant. So a good example is on\ngoing up, computer chess was gonna change everything. Once computers could play chess, that fundamentally changes the world. Now on the downside, computers play chess. Winning at chess is no longer\nseen as a unique human thing but people still play chess. This new phase is gonna\ntake a new chunk of things that we thought computers couldn't do. Now computers will be able to do, they have roughly our brain capacity, but we'll keep thinking\nas well as computers. And as I described, while we've been going\nthrough these five boom-busts, if you just look at the\nnumbers of ops per second, bits storage, bits of IO, that's the more interesting one. That's been steady and that's what finally caught up to people. As we've talked about a couple times, there's eight orders of magnitude to go, not in the intelligence in the\ntransistors or in the brain, but in the embodied intelligence, in the intelligence in our body. - So the intelligent\nconstructions of physical systems that would embody the intelligence versus contain it within the computation. - Right. There's a brain centrism\nthat assumes our intelligence is centered in our brain and in endless ways in this conversation, we've been talking about\nmolecular intelligence. Our molecular systems do a deep kind of artificial intelligence. All the things you think of\nas artificial intelligence does in representing\nknowledge, storing knowledge, searching over knowledge,\nadapting to knowledge, our molecular systems do. But the output isn't just a thought. It's us. It's the evolution of us. And the real horizon to\ncome is now embodying AI of not just a processor and a robot but building systems that\nreally can grow and evolve. - So we've been speaking\nabout this boundary between bits and atoms. So let me ask you about\none of the big mysteries of consciousness. Do you think it comes from\nsomewhere between that boundary? - I won't name names, but if you know who I'm talking about it, it's probably clear. I once did a drive, in fact, up to the Mussolini-era\nvilla outside Torino in the early days of what\nbecame quantum computing with a famous person who\nthinks about quantum mechanics and consciousness. And we had the most\ninfuriating conversation that went roughly along the lines of consciousness is weird, quantum mechanics is weird, therefore quantum mechanics\nexplains consciousness. That was roughly the logical process. - And you're not satisfied\nwith that process. - No, and I say that very\nprecisely in the following sense. I was a program manager\nsomewhat by accident in a DARPA program on quantum biology. And so biology trivially\nuses quantum mechanics that were made out of atoms. But the distinction is\nin quantum computing, quantum information, you need quantum coherence and there's a lot of muddled thinking about like collapse of the wave function and claims of quantum computing that garbles quantum coherence. You can think of it as a\nwave that has very special properties, but these\nwave-like properties. And so there's a small set\nof places where biology uses quantum mechanics\nin that deeper sense. One is how light is converted\nto energy in photo systems. It looks like one is olfaction, how your nose is able to\ntell different smells. Probably one has to do\nwith how birds navigate, how they sense magnetic fields. That involves a coupling\nbetween a very weak energy with the magnetic field coupling\ninto chemical reactions. And there's a beautiful system. Standard in chemistry is\nmagnetic fields like this can influence chemistry, but there are biological\ncircuits that are carefully balanced with two pathways\nthat become unbalanced with magnetic fields. So each of these areas\nare expensive for biology. It has to consume resources\nto use quantum mechanics in this way. So those are places where we know there's quantum mechanics in biology. In cognition, there's just no evidence. There's no evidence of anything\nquantum mechanical going on in how cognition works. - Consciousness. - I'm saying cognition, I'm\nnot saying consciousness. But to get from cognition\nto consciousness... So McCulloch and Pitts\nmade a model of neurons. That led to perceptrons that then through a couple boom-busts\nled to deep learning. One of the interesting\nthings about that sequence is it diverged off. So deep neural networks\nused in machine learning diverged from trying to\nunderstand how the brain works. What makes them work, what's emerged is it's a really interesting story. This may be too much of a technical detail but it has to do with\nfunction approximation. We had talked about exponentials, a deep network needs\nan exponentially larger shallow network to do the same function. And that exponential\nis what gives the power to deep networks. But what's interesting is\nthe sort of lessons about building these deep architectures and how to train them have\nreally interesting echoes to how brains work. And there's an interesting conversation that's sort of coming\nback of neuroscientists looking over the shoulder\nof people training these deep networks, seeing interesting echoes\nfor how the brain works, interesting parallels with it. And so I didn't say consciousness, I just said cognition. But I don't know any experimental evidence that points to anything in neurobiology that says we need quantum mechanics. I view the question about\nwhether a large language model is conscious as silly, in that biology is full of hacks and it works. There's no evidence we have\nthat there's anything deeper going on than just this\nsort of stacking up of hacks in the brain. - And somehow consciousness\nis one of the hacks or an emergent property of the hacks. - Absolutely. And just numerically,\nI said big computations now have the degrees\nof freedom of the brain and they're showing a\nlot of the phenomenology of what we think is properties\nof what a brain can do. And I don't see any reason\nto invoke anything else. - That makes you wonder\nwhat kind of beautiful stuff digital fabrication will create if biology created a few\nhacks on top of which consciousness and cognition, some of the things we love\nabout human beings was created, it makes you wonder what kind of beauty in the complexity can create\nfrom digital fabrication. - There's an early peak at that which is, there's a misleading term\nwhich is generative design. Generative design is where\nyou don't tell a computer how to design something. You tell the computer\nwhat you want it to do. That doesn't work. That only works in limited subdomains. You can't do really complex\nfunctionality that way. The one place it's matured\nthough is topology optimization for structure. So let's say you wanted to\nmake a bicycle or a table. You describe the loads on it and it figures out how to design it. And what it makes are beautiful,\norganic looking things. These are things that look\nlike they grew in a forest and they look like they grew in a forest 'cause that's sort of\nexactly what they are. They're solving the ways\nof how you handle loads in the same way biology does. And so you get things that\nlook like trees and shells and all of that. And so that's a peak at this transition from we design to we teach\nthe machines how to design. - What can you say about, 'cause you mentioned\ncellular automata earlier, about from this example you just gave and in general the\nobservation you can make by looking at cellular automata that from simple rules and\nsimple building blocks can emerge arbitrary complexity. Do you understand what that is? How that can be leveraged? - So understand what it is is\nmuch easier than it sounds. I complained about Turing's machine making a physics mistake, but Turing never intended it\nto be a computer architecture. He used it just to prove\nresults about computability. What Turing did on what is\ncomputation is exquisite, is gorgeous. He gave us our notion of\ncomputational universality. And something that sounds deep and turns out to be trivial is it's really easy to show almost everything is\ncomputationally universal. So Norm Margolis wrote a\nbeautiful paper with Tom Toffoli showing in a cellular automata world is like the game of life where\nyou just move tokens around. They showed that modeling\nbilliard balls on a billiard table with cellular automata\nis a universal computer. To be universal, you\nneed a persistent state, you need a non-linear\noperation to interact them, and you need connectivity. So that's what you need to show\ncomputational universality. So they showed that a CA\nmodeling billiard balls is a universal computer. Chris Moore went on to\nshow that instead of chaos- Turing showed there're\nproblems in computation that you can't solve that, that they're harder\nthan you can't predict. They're actually in a deep reason. They are unsolvable. Chris Moore showed it's very\neasy to make physical systems that are uncomputable, that\nwhat the physics system does, just bouncing balls and surfaces, you can make systems that\nsolve uncomputable problems. And so almost any\nnon-trivial physical system is computationally universal. So the first part of the\nanswer to your question is, this comes back to my comment\nabout how do you bootstrap a civilization? You just don't need much to\nbe computationally universal. There isn't today a notion of like fabricational universality\nor fabricational complexity. The sort of numbers I've been giving you about you eating lunch\nversus the chip fab, that's in the same spirit\nof what Shannon did. But once you connect\ncomputational universality to fabrication universality, you then get the ability to\ngrow and adapt and evolve. - Because that evolution\nhappens in the physical space? - And so that's why, for me, the heart of this whole\nconversation is morphogenesis. So just to come back to that, what Turing ended his sadly\ncut short life studying was how genes give rise to form. How the relatively, in effect,\nsmall amount of information in the genome can give\nrise to the complexity of who you are. And that's where what resides is this molecular intelligence, which is first how to describe you, but then how to describe you such that you can exist\nand you can reproduce and you can grow and you can evolve. And so that's the seat of\nour molecular intelligence. - The maker revolution in biology. - It really is. It it really is. And that's where you can't\nseparate communication, computation, and fabrication. You can't separate computer\nscience and physical science. You can't separate hardware and software. They all intersect right at that place. - Do you think of our universe as just one giant computation? - I would even kind of\nsay quantum computing is overhyped in that there's a few things quantum computing's gonna be good at. One is breaking cryptosystems, what we know how to\nmake new cryptosystems. What it's really good at is\nmodeling other quantum systems. So for studying nanotechnology,\nit's gonna be powerful, but quantum computing is not going to disrupt and change everything. But the reason I say that\nis this interesting group of strange people who helped\ninvent quantum computing before it was clear anything was there, one of the main reasons they did it wasn't to make a computer\nthat can break a cryptosystem. It was, you could turn this backwards, you could be surprised quantum mechanics can compute or you can go\nin the opposite direction and say if quantum mechanics can compute, that's a description of nature. So physics is written in terms of partial differential equations. That is an information technology\nfrom two centuries ago. The equations of physics are not, this will sound very strange to say, but the equations of physics, Schrodinger's equations\nand Maxwell's equations and all of them, are not fundamental. They're a representation of physics that was accessible to us in the era of having a pencil and a piece of paper. They have a fundamental problem which is if you make a\ndot on a piece of paper, in traditional physics theory, there's infinite information in that dot. A point has infinite information. That can't be true because information is a fundamental resource\nthat's connected to energy. And in fact, one of my favorite questions\nyou can ask a cosmologist to trip them up is ask, is information a conserved\nquantity in the universe? Was all the information\ncreated in the Big Bang or can the universe create information? I've yet to meet a cosmologist\nwho doesn't stutter and not clearly know how to handle that existential question. But sort of putting that to a side, in physics theory the way it's taught, information comes late. You're taught about x, a\nvariable, which can contain infinite information, but\nphysically that's unrealistic. And so physics theories have\nto find ways to cut that off. So instead, there are a number of people who start with a theory of the universe should start with\ninformation and computation as the fundamental resources\nthat explain nature. And then you build up\nfrom that to something that looks like throwing\nbaseballs down a slope. And so in that sense, the work on physics and\ncomputation has many applications that we've been talking about. But more deeply, it's\nreally getting at new ways to think about how the universe works. And there are a number of\nthings that are hard to do in traditional physics\nthat make more sense when you start with\ninformation and computation as the root of physical theory. - Information and computation being the real fundamental\nthing in the universe. - That information is a resource. You can't have infinite\ninformation in finite space. Information propagates and interacts, and from there you erect\nthe scaffolding of physics. Now it happens, the words I just said look a lot like quantum field theories, but there's an interesting\nway where instead of starting with differential equations to get to quantum field theories and quantum field theories,\nyou get to quantization. If you start from computation information, you begin sort of quantized\nand you build up from there. And so that's the sense\nin which absolutely I think about the universe as a computer. The easy way to understand that is almost anything is\ncomputationally universal. But the deep way is it's\na real fundamental way to understand how the universe works. - Let me go a little bit to the personal and with the Center of Bits and Atoms. You have worked with, the\nstudents you've worked with, have gone on to do some\nincredible things in this world, including build super\ncomputers that power Facebook and Twitter and so on. What advice would you\ngive to young people? What advice have you given them how to have one heck of a great career? One heck of a great life? - One important one is if\nyou look at junior faculty trying to get tenure at a place like MIT, the ones who try to figure\nout how to get tenure are miserable and don't get tenure. And the ones who don't\ntry to figure it out are happy and do get it. You have to love what you're\ndoing and believe in it and nothing else could possibly be what you wanna be doing with your life. And it gets you outta bed in the morning. And again, it sounds naive, but within the limited\ndomain I'm describing now of getting tenure at MIT, that's the key attribute to it. And in the same sense, if you take the sort of outliers students were talking about, 99 out\nof 100 come to me and say your work is very fascinating. I'd be interesting to work for you. And 1 out of 100 come and say you're wrong, here's your mistake. Here's what you should have been doing. And they just sort of say I'm here and get to work. I don't know how far this resource goes. I've said I consider the\nworld's greatest resource this engine of bright inventive people of which we only see a\ntiny little iceberg of it. And everywhere we open these labs, they come out of the woodwork. We didn't create all these\neducational programs, all these other things I'm describing. We tried to partner\neverywhere with local schools and local companies and kept\ntripping over dysfunction and find we had to create the environment where people like this can flourish. And so I don't know if this is everyone, if it's 1% of society,\nwhat the fraction is, but it's so many orders\nof magnitude bigger than we see today. We've been racing to keep up with it to take advantage of that resource. - Something tells me it's\na very large fraction of the population. - The thing that gives me\nmost hope for the future is that population. Once a year, this whole lab network meets and it's my favorite gathering. It's in Bhutan this year because it's every body shape, it's\nevery language, every geography, but it's the same person\nin all those packages. It's the same sense of bright,\ninventive joy and discovery. - If there's people listening to this and they're just overwhelmed\nwith how exciting this is, which I think they would be,\nhow can they participate, how can they help, how can they encourage\nyoung people or themselves to build stuff, to create stuff? - That's a great question. This is part of a much\nbigger maker movement that has a lot of embodiments. The part I've been involved\nin, this FabLab network, you can think of as a curated\npart that works as a network. So you don't benefit in a\ngym if somebody exercises in another gym. But in the Fab network,\nyou do in a sense benefit when somebody works in another network, another lab in the way it\nfunctions as a network. You can come to cba.mit.edu\nto see the research we're talking about. There's a Fab Foundation\nrun by Sherry Lassiter at fabfoundation.org. Fab Labs IO is a portal\ninto this lab network. Fabacademy.org is this\ndistributed hands-on educational program. Fab.city is the platform\nof cities producing what they consume. Those are all nodes in this network. - So you can learn with Fab Academy and you can perhaps launch or help launch or participate in launching a FabLab. - And in particular,\nfrom one to a thousand, we carefully counted labs. Now we're going from a\nthousand to a million where it ceases to become\ninteresting to count them. And in a thousand to the million, what's interesting about that\nstage is technologically, you go to a lab not to\nget access to the machine, but you go to the lab to make the machine. But the other thing interesting in it is we have an interesting collaboration on a FabLab in a box. And this came out of a\ncollaboration with SolidWorks on how you can put a FabLab in a box, which is not just the\ntools but the knowledge. So you open the box and the\nbox contains the knowledge of how to use it as well\nas the tools within it so that the knowledge can propagate. And so we have an interesting\ngroup of people working on... The original FabLabs,\nwe'd have a whole team to get involved in the\nsetting up and training. And the Fab Academy is a real in-depth, deep, technical program in the training. But in this next phase,\nhow sort of the lab itself knows how to do the lab. We've talked deeply about the\nintelligence in fabrication, but in a much more\naccessible one about how the AI in the lab in effect\nbecomes a collaborator with you in this nearer term to help get started. And for people wanting to connect, it can seem like a big\nstep, a big threshold, but we've gotten to thousands of these and they're doubling exactly that way, just from people opting in. - And in so doing, driving towards this kind of idea of personal digital fabrication. - And it's not utopia, it's not free, but come back to today, we\nseparately have education, we have big business, we have startups, we have entertainment, each of these things are segregated. When you have global\nconnection to one of these local facilities, in that, you can do play and art and education and create infrastructure. You can make many of\nthe things you consume. You could make it for yourself. It could be done on a community scale, it could be done on a regional scale. I'd say the research we\nspent the last few hours talking about, I thought was hard. And in a sense, it's non-trivial, but in a sense, it's\njust sort of playing out, we're turning the crank. What I didn't think was hard is if anybody can make\nalmost anything anywhere, how do you live, how do you learn, how do you work, how you play, these very basic assumptions\nabout how society functions. There's a way in which it's\nkind of back to the future in that this mode where\nwork is money is consumption and consumption is shopping by selecting is only a kind\nof a few decade-old stretch. In some ways, we're getting back to a Sami village in north\nNorway is deeply sustainable. But rather than just\nreverting to living the way we did a few thousand years ago, being connected globally, having the benefits of modern society, but connecting it back to older\nnotions of sustainability, I hadn't remotely anticipated\njust how fundamentally that challenges how a society functions and how interesting and how hard it is to figure out how we can make that work. - And it's possible that\nthis kind of process will give a deeper sense\nof meaning to each person. - Let me violently agree in two ways. One way is this community-making crosses many sensitive\nsectarian boundaries in many parts of the\nworld where there's just implicit or explicit conflict, but sort of this act of\nmaking seems to transcend a lot of historical divisions. I don't say that philosophically. I just say that as an observation. And I think there's\nsomething really fundamental in what you said, which is deep in our brain is\nshaping our environment. A lot of what's strange about our society is the way that we can't do that. The act of shaping our\nenvironment touches something really, really deep that gets\nto the essence of who we are. That's, again, why I say that in a way the most important thing\nmade in made in these labs is making itself. - What do you think, if the shaping of our environment\ngets to something deep, what do you think is\nthe meaning of it all? What's the meaning of life, Neil? - I can tell you my insights\ninto how life works. I can tell you my insights in\nhow to make life meaningful and fulfilling and sustainable. I have no idea what\nthe meaning of life is, but maybe that's the meaning of life. - The uncertainty, the confusion, because there's a magic to it all. Everything you've talked about, from starting from the basic\nelements with the Big Bang that somehow created the sun that somehow said F you to thermodynamics and created life and all the ways that you've talked about from ribosomes that created the machinery\nthat created the machine, and then now the\nbiological machine creating through digital fabrication, more complex, artificial\nmachines, all of that. There's a magic to that creative process. And we notice, we humans are smart enough to notice the magic. - You haven't said the S word yet. - Which one is that? - Singularity. I'm not sure if Ray Kurzweil is listening, if he is, hi Ray. But I have a complex relationship with Ray because a lot of the things\nhe projects I find annoying, but then he does his homework. And then, somewhat annoyingly, he points out how almost\neverything I'm doing fits on his roadmaps. And so the question is, are we heading towards a singularity? I'd have to say I lean towards sigmoids rather than exponentials. - But we've done pretty\nwell with sigmoids. - Sigmoids are things grow and they taper, and then there can be one\nafter it and one after it. I'll pass on whether\nthere's enough of them that they diverge. The selfish gene answer\nto the meaning of life is the meaning of life is\nthe propagation of life. It was a step for atoms to\nassemble into a molecule, for molecules to assemble\ninto a proto cell, for the proto cell to form, to then form organelles for\nthe organ cells to form organs, the organs to form an organism. Then it was a step for\norganisms to form family units, then family units to form villages. You can view each of those as a stack in the level of organizations. You could view everything\nwe've spoken about as the imperative of life, just the next step in\nthe hierarchy of that. And the fulfillment of\nthe inexorable drive of the violation of thermodynamics. I'm an embodiment of the\nwill of the violation of thermodynamics speaking. - The two of us, having an old chat. And so it continues, and\neven then the singularity is just a transition up the ladder. - There's nothing deeper\nto consciousness than it's a derived property of\ndistributed problem solving. There's nothing deeper\nto life than embodied AI in morphogenesis. So why so much of this\nconversation in my life is involved in these FabLabs and initially it just started as outreach. Then it started as keeping up with it, then it turned to it was rewarding. Then it turned to we're\nlearning as much from these labs in as goes out to them. It began as outreach, but now more knowledge is\ncoming back from the labs than is going into them. And then finally it ends with what I described as\ncompeting with myself at MIT but a better way to say that\nis tapping the brain power of the planet. And so I guess for me personally, that's the meaning of my life. - And maybe that's the\nmeaning for the universe too. It's using us humans and our\ncreations to understand itself. In a way, it's whatever\nthe creative process that created earth, it's\ncompeting with a self. - So you could take\nmorphogenesis as a summary of this whole conversation\nor you could take recursion, that in a sense, what\nwe've been talking about is recursion all the way down. - And in the end, I think this\nwhole thing is pretty fun. It's short, life is, but it's pretty fun. And so is this conversation. I mentioned to you offline, I'm going through some\ndifficult stuff personally. And your passion for what you\ndo is just really inspiring and it just lights up my\nmood and lights up my heart. And you're an inspiration for, I know, thousands of people that\nwork with you at MIT and millions of people across the world. It's a big honor that you\nwould sit with me today. This was really fun. - This was a pleasure. - Thanks for listening\nto this conversation with Neil Gershenfeld. To support this podcast, please check out our\nsponsors in the description. And now let me leave you with\nsome words from Pablo Picasso. Every child is an artist. A challenge is staying an\nartist when you grow up. Thank you for listening and\nhope to see you next time."
    }
  ],
  "full_text": "- The ribosome, who I\nmentioned a little while back, can make an elephant\none molecule at a time. Ribosomes are slow. They run at about one molecule a second, but ribosomes make ribosomes, so you have trillions of them and that makes an elephant. In the same way these little\nassembly robots I'm describing can make giant structures, at heart because the robot can make the robot. So more recently two of my students, Amira and Miana, had a\nnature communication paper showing how this robot can\nbe made out of the parts it's making so the robots\ncan make the robot, so you build up the capacity\nof robotic assembly. - The following is a conversation\nwith Neil Gershenfeld, the director of MIT's\nCenter for Bits and Atoms, an amazing laboratory that\nis breaking down boundaries between the digital and physical worlds, fabricating objects and machines\nat all scales of reality, including robots and automata that can build copies of themselves and self-assemble into complex structures. His work inspires\nmillions across the world as part of the maker\nmovement to build cool stuff, to create, the very act\nthat makes life so beautiful and fun. This is a Lex Fridman podcast. To support it, please check out our\nsponsors in the description. And now, dear friends,\nhere's Neil Gershenfeld. You have spent your life\nworking at the boundary between bits and atoms, so\nthe digital and the physical. What have you learned about engineering and about nature of reality\nfrom working at this divide, trying to bridge this divide? - I learned why von Neumann and Turing made fundamental mistakes. I learned the secret of life. I learned how to solve many of the world's most important problems,\nwhich all sound presumptuous, but all of those are things\nI learned at that boundary. - So Turing and von\nNeumann, let's start there. Some of the most\nimpactful, important humans who have ever lived in computing, why were they wrong? - So I worked with Andy Gleason, who was Turing's counterpart. So just for background,\nif anybody doesn't know, Turing is credited with\nthe modern architecture of computing, among many other things. Andy Gleason was his US counterpart, and you might not have\nheard of Andy Gleason, but you might have heard\nof the Hilbert problems. And Andy Gleason solved the fifth one. So he was a really notable mathematician. During the war, he was\nTuring's his counterpart. Then von Neumann is credited\nwith the modern architecture of computing and one of his\nstudents was Marvin Minsky. So I could ask Marvin\nwhat Johnny was thinking and I could ask Andy\nwhat Alan was thinking. And what came out from that, what I came to appreciate as background, I never understood the difference\nbetween computer science and physical science. But Turing's machine that's the foundation of modern computing has a simple physics mistake, which is the head is\ndistinct from the tape. So in the Turing machine, there's a head that\nprogrammatically moves and reads and writes a tape. The head is distinct from the tape, which means persistence of\ninformation is separate from interaction with information. Then von Neumann wrote\ndeeply and beautifully about many things, but not computing. He wrote a horrible memo\ncalled the First Draft of a Report on the Ed Vac, which is how you program\na very early computer. In it, he essentially roughly\ntook Turing's architecture and built it into a machine. So the legacy of that is the\ncomputer somebody's using to watch this is spending\nmuch of its effort moving information from\nstorage transistors to processing transistors, even though they have the\nsame computational complexity. So in computer science, when you learn about computing, there's a ridiculous taxonomy of about 100 different models of computation. But they're all fictions. In physics, a patch of\nspace occupies space, it stores state, it takes time to transit,\nand you can interact. That is the only model of\ncomputation that's physical. Everything else is a fiction. I really came to appreciate\nthat a few years back when I did a keynote\nfor the annual meeting of the super computer industry and then went into the\nhalls and spent time with the super computer builders\nand came to appreciate- if you're familiar with\nthe movie The Metropolis, people would frolic\nupstairs in the gardens and down in the basement\npeople would move levers. And that's how computing exists today, that we pretend software is not physical, it's separate from hardware. And the whole canon of computer science is based on this fiction that bits aren't constrained by atoms. But all sorts of scaling\nissues in computing come from that boundary. But all sorts of opportunities\ncome from that boundary. And so you can trace\nit all the way back to Turing's machine making this mistake between the head and the tape, von Neumann, he never called\nit von Neumann's architecture. He wrote about it in this dreadful memo and then he wrote beautifully\nabout other things we'll talk about. Now to end a long answer,\nTuring and von Neumann both knew this. So all of the canon of computer\nscientists credits them for what was never meant to\nbe a computer architecture. Both Turing and von\nNeumann ended their life studying exactly how\nsoftware becomes hardware. So von Neumann studied\nself-reproducing automata, how a machine communicates\nits own construction. Turing studied morphogenesis, how genes give rise to form. They ended their life\nstudying the embodiment of computation, something that's been forgotten\nby the canon of computing, but developed sort of off to the sides by a really interesting lineage. - So there's no distinction\nbetween the head and the tape, between the computer and\nthe computation editor, it's all computation? - Right. I never understood the difference between computer science and physical science. And working at that boundary\nhelped lead to things like my lab was part of doing with a number of interesting collaborators. The first faster than\nclassical quantum computations, we were part of a collaboration creating the minimal synthetic\norganism where you design life in a computer. Those both involve domains\nwhere you just can't separate hardware from software, computation is embodied in\nthese really profound ways. - So the first quantum\ncomputations, synthetic life, so in the space of biology, so the space of physics\nat the lowest level and the space of biology\nat the lowest level. So let's talk about CBA,\nCenter of Bits and Atoms. What's the origin story of\nthis legendary MIT center that you were a part of creating? - In high school, I really wanted to go to vocational school where you learned to weld\nand fix cars and build houses and I was told, no, you're smart. You have to sit in a room. And nobody could explain to me why I couldn't go to vocational school. I then worked at Bell Labs, this wonderful place before deregulation, legendary place, and I\nwould get union grievances because I would go into the workshop and try to make something\nand they would say, no, you're smart, you have to\ntell somebody what to do. And it wasn't until MIT, and I'll explain how CBA started, but I could create CBA\nthat I came to understand this is a mistake that dates\nback to the Renaissance. So in the Renaissance, the liberal arts emerged\nand liberal doesn't mean politically liberal. This was the path to\nliberation, birth of humanism. And so the liberal arts with\nthe trivium, quadrivium, roughly language, natural science. And at that moment what\nemerged was this sort of dreadful concept of the illiberal arts. So anything that wasn't the liberal arts was for commercial gain\nand was just making stuff and wasn't valid for serious study. And so that's why we're\nleft with learning to weld wasn't a subject for serious study. But the means of expression have changed since the Renaissance, so micromachining or embedded coding is\nevery bit as expressive as painting a painting or writing a sonnet. So never understanding this difference between computer science\nand physical science, the path that led me to\ncreate CBA with colleagues, I was what's called a\njunior fellow at Harvard. I was visiting MIT through Marvin because I was interested in the\nphysics of musical instruments. This'll be another slight digression. In Cornell, I would study physics and then I would cross the\nstreet and go to the music department where I played the bassoon and I would trim reeds and play the reeds. And they'd be beautiful\nbut then they'd get soggy. And then I discovered in\nthe basement of the music department at Cornell was David Borden, who you might not have heard of but is legendary in electronic\nmusic 'cause he was really the first electronic musician. So Bob Moog, who invented\nMoog synthesizers was a physics student at Cornell, like me crossing the street. And eventually he was kicked out and invented electronic music. David Borden was the first musician who created electronic music. So he is legendary for\npeople like Phil Glass and Steve Reich. And so that got me thinking about, I would behave as a scientist\nin the physics department but not in the music department. Got me thinking about what's\nthe computational capacity of a musical instrument. And through Marvin, he introduced me to Todd\nMachover at the Media Lab who was just about to start\na project with Yo-Yo Ma that led to a collaboration\nto instrument a cello to extract Yo-Yo's data\nand bring it out into computational environments. - What is the computational\ncapacity of musical instrument, as we continue on this tangent and when we shall return to CBA. - One part of that is to\nunderstand the computing. And if you look at like\nthe finest time scale and length scale, you\nneed to model the physics. It's not heroic. A good GPU can do teraflops today. That used to be a national\nclass supercomputer, now it's just a GPU. If you take the time\nscales and length scales relevant for the physics, that's about the scale\nof the physics computing. For Yo-Yo, what was really driving it was he's completely\nunsentimental about the Strad. It's not that it makes\nsome magical wiggles in the sound wave, it's\nperformance as a controller, how he can manipulate it\nas an interface device. - Interface between what and what exactly? - Him and sound. And so what it led to was, I had started by thinking\nabout ops per second, but Yo-Yo's question was really\nresolution and bandwidth. It's how fast can you measure what he does and the bandwidth and the resolution of detecting his controls and then mapping them into sounds. And what he found was if you\ninstrument everything he does and connect it to almost anything, it sounds like Yo-Yo, that the magic is in the control, not in ineffable details\nin how the wood wiggles. And so with Yo-Yo and Todd, that led to a piece and\ntowards the end I asked Yo-Yo, what it would take for him\nto get rid of his Strad and use our stuff. And his answer was just logistics. It was, at that time, our stuff was like a rack of electronics and lots of cables and some\ngrad students to make it work. Once the technology becomes\nas invisible as the Strad, then, sure, absolutely\nhe he would take it. And by the way, as a\nfootnote on the footnote, an accident in the\nsensing of Yo-Yo's cello led to a hundred million dollar a year auto safety business to\ncontrol airbags in cars. - How did that work? - I had to instrument the bow\nwithout interfering with it. So I set up local electromagnetic fields where I would detect how\nthose fields interact with the bow he's playing. But we had a problem that his hand, whenever his hand got\nnear these sensing fields, I would start sensing his hand rather than the materials on the bow. And I didn't quite\nunderstand what was going on with that interference. So my very first grad student ever, Josh Smith, did a thesis on\ntomography with electric fields, had to see in 3D with electric fields. Then through Todd, and at that\npoint a research scientist in my lab, Joe Paradiso, it led to a collaboration\nwith Penn and Teller where we did a magic trick in\nLas Vegas to contact Houdini and sort of these fields are sort of like contacting spirits. So we did a magic trick in Las Vegas. And then the crazy thing\nthat happened after that was Phil Rittmuller came\nrunning into my lab. He worked with, this\nbecame with Honda and NEC, airbags were killing infants\nin rear-facing child seats. Cars need to distinguish\na front-facing adult, where you'd save the life, versus a bag of groceries\nwhere you don't need to fire the airbag versus\na rear facing infant where you would kill it. And so the seat needed to, in effect, see in 3D to understand the occupants. And so we took the Penn\nand Teller magic trick derived from Josh's\nthesis from Yo-Yo's cello to an auto show. And all the car companies said, great, when can we buy it? And so that became ELESYS\nand it was a hundred million dollar a year business making sensors. There wasn't a lot of publicity\nbecause it was in the car so the car didn't kill you. So they didn't sort of advertise, we have nice sensors so\nthe car doesn't kill you. But it became a leading\nauto safety sensor. - And that started from the\ncello and the question of the computational capacity\nof the musical instrument. - So now to get back to MIT. I was spending a lot of\noutside time at IBM research that had gods of the\nfoundations of computing. There's just amazing people there. And I'd always expected to\ngo to IBM to take over a lab, but at the last minute\npivoted and came to MIT to take a position in the Media Lab and start what became\nthe predecessor to CBA. Media Lab is well known\nfor Nicholas Negroponte. What's less well known is\nthe role of Jerry Wiesner. So Jerry was MIT's president, before that, Kennedy science advisor, grand old man of science. At the end of his life, he was frustrated by how\nknowledge was segregated. And so he wanted to create a department of none of the above. A department for work that\ndidn't fit in departments. And the Media Lab, in a\nsense, was a cover story for him to hide a department. As MIT's president towards\nthe end of his tenure, if he said, I'm gonna make a department for things that don't fit in departments, the departments would've screamed. But everybody was sort of\npaying attention to Nicholas creating the Media Lab. And Jerry kind of hid in\nit a department called Media Arts and Sciences. It's really the department\nof none of the above. And Jerry explaining that and\nNicholas then confirming it is really why I pivoted and went to MIT because my students who help\ncreate quantum computing or synthetic life get degrees\nfrom Media Arts and Sciences, this department of none of the above. So that led to coming to MIT. With Todd and Joe Paradiso and Mike Holly, we started a consortium\ncalled Things That Think, and this was around the\nbirth of Internet of Things and RFID. But then we started doing\nthings like work we can discuss that became the beginnings\nof quantum computing and cryptography and materials\nand logic and microfluidics. And those needed much more\nsignificant infrastructure and were much longer research arcs. So with a bigger team of about 20 people, we wrote a proposal to the NSF to assemble one of every tool to make\nanything of any size, was roughly the proposal. - One of any tool to make\nanything of any size? - So there're usually\nnanometers, micrometers, millimeters, meters are segregated, input and output is segregated. We wanted to look just very\nliterally at how digital becomes physical and\nphysical becomes digital. And fortunately we got NSF on a good day and they funded this facility of one of almost every tool to make anything. And so with a group of core colleagues that included Joe Jacobson,\nIke Chuang, Scott Manalis, we launched CBA. - And so you're talking\nabout nanoscale, microscale, nano structures, microstructures,\nmacro structures, electron microscopes, and\nfocused high beam probes for nano structures, laser micromachining,\nand x-ray microtomography for microstructures, multi-axis machining and 3D\nprinting for macro structures, just some examples. What are we talking\nabout in terms of scale? How can we build tiny\nthings and big things all in one place? How's that possible? - A well-equipped research\nlab has the sort of tools we're talking about,\nbut they're segregated in different places. They're typically also run by technicians where you then have an account\nand a project and you charge. All of these tools are essentially, when you don't know what you're doing, not when you do know what you're doing, in that they're when you need\nto work across length scales. Once projects are\nrunning in this facility, we don't charge for time. You don't make a formal\nproposal to schedule and the users really run the tools and it's for work that's kind of inchoate, that needs to span these\ndisciplines and length scales. And so work in the project today, work in CBA today ranges\nfrom developing zeptojoule electronics for the lowest power computing to micromachining diamond to\ntake 10 million RPM bearings for molecular spectroscopy studies up to exploring robots to\nbuild 100 meter structures in space. - The three things you just mentioned. Let's start with the biggest. What are some of the biggest\nstuff you attempted to explore how to build in a lab? - So viewed from one direction, what we're talking about\nis a crazy random-seeming of almost unrelated projects, but if you rotate 90 degrees, it's really just a core\nthought over and over again. Just very literally how\nbits and atoms relate, how digital and just going\nfrom digital to physical, in many different domains. But it's really just the same\nidea over and over again. So to understand the biggest things, let me go back to bring in now Shannon as well as von Neumann. - Claude Shannon? - So what is digital? The casual, obvious answer\nis digital in one in zero, but that's wrong. There's a much deeper answer, which is Claude Shannon at MIT wrote the best master's thesis ever. In his master's thesis, he invented our modern\nnotion of digital logic. Where it came from was Vannevar Bush was a grand old man at MIT. He created the post-war\nresearch establishment that led to the National\nScience Foundation. And he made an important\nmistake, which we can talk about. But he also made the\ndifferential analyzer, which was the last great analog computer. So it was a room full of gears and pulleys and the longer it ran,\nthe worse the answer was. And Shannon worked on it as a student. And he got so annoyed,\nin his master's thesis, he invented digital logic. But he then went on to Bell Labs. And what he did there was communication was beginning to expand. There was more demand for phone lines. And so there's a question\nabout how many phone messages you could send down a wire and you could try to just\nmake it better and better. He asked a question nobody had asked, which is rather than make\nit better and better, what's the limit to how good it can be? And he proved a couple things, but one of the main things he proved was a threshold theorem for channel capacity. And so what he showed was my voice to you right now is\ncoming as a wave through sound and the further you get,\nthe worse it sounds. But people watching this are getting it as packets of data in a network. When the computer they're\nwatching this gets the packet of information, it can\ndetect and correct an error. And what Shannon showed is\nif the noise in the cable to the people watching\nthis is above a threshold, they're doomed. But if the noise is below a threshold, for a linear increase in the energy representing our conversation, the error rate goes down exponentially. Exponentials are fast, there's very few of them in engineering. And the exponential reduction\nof error below a threshold if you restore state is\ncalled a threshold theorem. That's what led to digital, that means unreliable\nthings can work reliably. So Shannon did that for communication. Then von Neumann was inspired by that and applied it to\ncomputation and he showed how an unreliable computer\ncan operate reliably by using the same threshold\nproperty of restoring state. It was then forgotten many years. We had to rediscover it, in effect, in the quantum computing era when things are very unreliable again. But now to go back to\nhow does this relate to the biggest things I've made. So in fabrication, MIT invented computer-controlled manufacturing in 1952. Jet aircraft were just emerging. There was a limit to\nturning cranks on a machine, on a milling machine to\nmake parts for jet aircraft. Now this is a messy story. MIT actually stole\ncomputer-controlled machining from an inventor who brought it to MIT, wanted to do a joint\nproject with the Air Force, and MIT effectively stole it from him. So it's kind of a messy history, but that sounds like the birth of computer-controlled machining, 1952. There are a number of\ninventors of 3D printing. One of the companies spun off from my lab by Max Lobovsky's Formlabs, which is now a billion\ndollar 3D printing company. That's the modern version. But all of that's analog, meaning the information is\nin the control computer, there's no information in the materials. And so it goes back to Vannevar\nBush's analog computer. If you make a mistake in\nprinting or machining, just the mistake accumulates. The real birth of computerized\ndigital manufacturing is 4 billion years ago. That's the evolutionary\nage of the ribosome. So the way you are\nmanufactured is there's a code that describes you, the genetic code. It goes to a micromachine, the ribosome, which is this molecular factory\nthat builds the molecules that are you. The key thing to know about\nthat is there're about 20 amino acids that get assembled and in that machinery, it does everything Shannon and von Neumann taught us. You detect and correct errors. So if you mix chemicals, the error rate is about\na part in a hundred. When you elongate protein in the ribosome, it's about a part and 10 to the 4. When you replicate DNA, there's an extra level\nof error correction, it's a part in 10 to the 8. And so in the molecules that make you, you can detect and correct errors and you don't need a ruler to make you, the geometry comes from your parts. So now compare a child playing with Lego and a state-of-the-art 3D printer or computerized milling machine. The tower made by a child is more accurate than their motor control\nbecause the act of snapping the bricks together gives you\na constraint on the joints. You can join bricks made\nout of dissimilar materials. You don't need a ruler for Lego 'cause the geometry locally\ngives you the global parts and there's no LEGO trash. The parts have enough\ninformation to disassemble them. Those are exactly the\nproperties of a digital code. - The unreliable is made reliable. - Yes, absolutely. So what the ribosome figured\nout 4 billion years ago is how to embody these digital properties, but not for communication\nor computation, in effect, but for construction. So a number of projects in\nmy lab have been studying the idea of digital materials and think of a digital\nmaterial just as LEGO bricks. The precise meaning is\na discreet set of parts reversibly joined with global geometry determined from local constraints. And so it's digitizing the materials. And so I'm coming back to\nwhat are the biggest things I've made. My lab was working with\nthe aerospace industry. So Spirit Aero was Boeing's factories. They asked us for how to join composites. When you make a composite airplane, you make these giant\nwing and fuselage parts. And they asked us for a better\nway to stick them together 'cause the joints were a place of failure. And what we discovered was instead of making a few big parts, if you make little loops of carbon fiber and you reversibly link them in joints and you do it in a special geometry that balances being underconstrained and overconstrained with just\nthe right degrees of freedom, we set the world record\nfor the highest modulus ultralight material just by, in effect, making carbon fiber Lego. Lightweight materials are\ncrucial for energy efficiency. This let us make the lightest\nweight high modulus material. We then showed that with\njust a few part types, we can tune the material properties and then you can create really wild robots that instead of having a\ntool the size of a jumbo jet to make a jumbo jet, you can make little\nrobots that walk on these cellular structures to\nbuild the structures where they error-correct their\nposition on the structure and they navigate on the structure. And so using all of that, with NASA, we made morphing airplanes. A former student, Kenny\nChung and Ben Jeannette, made a morphing airplane\nthe size of NASA Langley's biggest wind tunnel. With Toyota, we've made\nsuper efficiency race cars. We're right now looking\nat projects with NASA to build these for things\nlike space telescopes and space habitats where the ribosome, who I mentioned a little while back, can make an elephant\none molecule at a time. Ribosomes are slow, they run\nat about one molecule a second. But ribosomes make ribosomes. So you have thousands of\nthem, trillions of them, and that makes an elephant. In the same way, these\nlittle assembly robots I'm describing can make giant structures, at heart because the\nrobot can make the robot. So more recently, two of my\nstudents, Amira and Miana, had a nature communication paper showing how this robot can\nbe made out of the parts it's making so the robots\ncan make the robot. So you build up the capacity\nof robotic assembly. - It can self-replicate. Can you linger on what\nthat robot looks like? What is a robot that can walk\nalong and do error correction? And what is a robot\nthat can self-replicate from the materials it is given? What does that look like? What are we talking? This is fascinating. - The answer is different\nat different length scales. So to explain that, in biology, primary structure is the\ncode in the messenger RNA that says what the ribosome should build. Secondary structure\nare geometrical motifs. They're things like helices or sheets. Tertiary structures\nare functional elements like electron donors or acceptors. Quaternary structure is\nthings like molecular motors that are moving my mouth\nor making the synapses work in my brain. So there's that hierarchy\nof primary, secondary, tertiary, quaternary. Now what's interesting is if\nyou wanna buy electronics today from a vendor, there are hundreds of\nthousands of types of resistors or capacitors or\ntransistors, huge inventory. All of biology is just\nmade from this inventory of 20 parts, the amino acids. And by composing them, you\ncan create all of life. And so as part of this\ndigitization of materials, we're in effect trying to create something like amino acids for engineering, creating all of technology from 20 parts. As another discussion, I helped start an office\nfor science in Hollywood. And there was a fun thing\nfor the movie The Martian where I did a program with\nBill Nye and a few others on how to actually build\na civilization on Mars that they described in\na way that I like as, I was talking about how to\ngo to Mars without luggage and at heart, it's sort\nof how to create life in non-living materials. If you think about this\nprimary, secondary, tertiary, quaternary structure, in my lab, we're doing that but on\ndifferent length scales for different purposes. So we're making microrobots\nout of like nano bricks and to make the robots to\nbuild large scale structures in space, the elements of the robots now are centimeters rather than micrometers. And so the assembly robots\nfor the bigger structures, there're the cells that\nmake up the structure, but then we have functional cells. And so cells that can process and actuate, each cell can like move\none degree of freedom or attach or detach or process. Now those elements I just described, we can make out of the\nstill smaller parts. So eventually, there's a\nhierarchy of the little parts make little robots that make\nbigger parts of bigger robots up through that hierarchy. - In that way you can\nmove up to landscape? - Early on I tried to\ngo in a straight line from the bottom to the top and that ended up being a bad idea. Instead, we're kind of doing\nall of these in parallel and then they're growing together. And so to make the\nlarger scale structures, there's a lot of hype right\nnow about 3D printing houses where you have a printer\nthe size of the house. We're right now working\non using swarms of these table scale robots that\nwalk on the structures to place the parts much more efficiently. - That's amazing. But you're saying you\ncan't for now go from the very small to the very large. - That'll come, that'll come in stages. - Can we just linger on this idea, starting from von\nNeumann's self-replicating automata that you mentioned. It's just a beautiful idea. - So that's at the heart of all of this. In the stack I described, so one student, Will Langford,\nmade these microrobots out of little parts that then we're using for Miana's bigger robots\nup through this hierarchy. And it's really realizing this idea of the self-reproducing automata. So von Neumann, when I complained about the von Neumann architecture, it's not fair to von Neumann\n'cause he never claimed it as his architecture. He really wrote about it in\nthis one fairly dreadful memo that led to all sorts\nof lawsuits and fights about the early days of computing. He did beautiful work\non reliable computation and unreliable devices. And towards the end of his\nlife what he studied was how, and I have to say this precisely, how a computation communicates\nits own construction. - So beautiful. - So a computation can store a description of how to build itself. But now there's a really hard problem, which is, if you have that in your mind, how do you transfer it and wake up a thing that then can contain it. So how do you give birth to a thing that knows how to make itself? And so with Stan Ulam, he invented cellular automata\nas a way to simulate these, but that was theoretical. Now the work I'm describing in my lab is fundamentally how to realize it, how to realize self-reproducing automata. And so this is something\nvon Neumann thought very deeply and very\nbeautifully about theoretically. And it's right at this intersection. It's not communication or\ncomputation or fabrication. It's right at this intersection\nwhere communication and computation meets fabrication. Now the reason self-reproducing\nautomata intellectually is so important 'cause this\nis the foundation of life. This is really just\nunderstanding the essence of how to life. And in effect we're trying to create life in non-living material. The reason it's so\nimportant technologically is because that's how you scale capacity. That's how you can make an\nelephant from a ribosome, 'cause assemblers make assemblers. - So simple building blocks\nthat inside themselves contain the information how to build more building blocks. And between each other, construct arbitrarily complex objects. - Now let me give you the numbers. So let me relate this to, right now we're living in\nAI mania explosion time. Let me relate that to\nwhat we're talking about. A 100 petaFLOP computer, which is a current\ngeneration supercomputer, not quite the biggest ones, does 10 to the 17 ops per second. Your brain does 10 to\nthe 17 ops per second. It has about 10 to the 15 synapses and they run at about 100 hertz. So as of a year or two ago, the performance of a big\ncomputer matched a brain. So you could view AI as a breakthrough. But the real story is within\nabout a year or two ago, the supercomputer has about\n10 to the 15 transistors in the processors, 10 to the\n15 transistors in the memory, which is the synopses in your brain. So the real breakthrough\nwas the computers match the computational capacity of a brain. And so we'd be sort of\nderelict if they couldn't do about the same thing. But now the reason I'm\nmentioning that is the chip fab making the super computer is placing about 10 to the 10 transistors a second. While you're digesting\nyour lunch right now, you're placing about 10 to\nthe 18 parts per second. There's an eight order\nof magnitude difference. So in computational capacity, it's done, we've caught up. But there's eight orders\nof magnitude difference in the rate at which biology can build versus state-of-the-art\nmanufacturing can build. And that distinction is\nwhat we're talking about, that distinction is not analog, but this deep sense of\ndigital fabrication, of embodying codes in construction. So a description doesn't describe a thing, but the description becomes the thing. - So you're saying, this is\none of the cases you're making, that this is this third revolution. We've seen the Moore's\nLaw in communication, we've seen the Moore's\nLaw-like type of growth in computation, and you're anticipating\nwe're going to see that in digital fabrication. Can you actually, first of all, describe what you mean by\nthis term digital fabrication? - The casual meaning is\nthe computer controls the tool to make something. And that was invented\nwhen MIT stole it in 1952. There's the deep meaning\nof what the ribosome does, of a digital description\ndoesn't describe a thing, a digital description becomes the thing. That's the path to the\nStar Trek replicator. And that's the thing\nthat doesn't exist yet. Now I think the best way to understand what this roadmap looks like\nis to now bring in FabLabs and how they relate to all of this. - What are FabLabs? - So here's a sequence. With colleagues, I\naccidentally started a network of what's now 2,500 digital\nfabrication community labs called FabLabs, right\nnow in 125 countries. And they double every year and a half. That's called Lass' Law\nafter Sherri Lassiter, who I'll explain. So here's the sequence. We started Center for Bits\nand Atoms to do the kind of research we're talking about. We had all of these machines\nand then had a problem. It would take a lifetime of classes to learn to use all the machines. So with colleagues who helped start CBA, we began a class modestly called How to Make Almost Anything. And there's no big agenda. It was aimed at a few research\nstudents to use the machines. And we were completely\nunprepared for the first time we taught it. We were swamped by, every year since, hundreds of students\ntry to take the class. It's one of the most\noversubscribed classes at MIT. Students would say things like, can you teach this at MIT? It seems too useful. It's just how to work these machines. And the students in the class, I would teach them all the\nskills to use all these tools and then they would do\nprojects integrating them and they're amazing. So Kelly was a sculptor,\nno engineering background. Her project was she made a\ndevice that saves up screams when you're mad and\nplaced them back later. - Saves up screams when you're mad and plays them back later? - You scream into this device\nand it deadens the sound, records it, and then when it's convenient, releases your scream. - Can we just pause on the brilliance of that invention, creation, the art, I don't know, the brilliance. Who is this that created this? - Kelly Dobson. Gone on to do a number\nof interesting things. Mejin, who's gone on to do a\nnumber of interesting things, made a dress instrumented\nwith sensors and spines. And when somebody creepy comes close, it would defend your personal space. - Also very useful. - Another project early on\nwas a web browser for parrots, which have the cognitive\nability of a young child and let's parrots surf the internet. Another was an alarm\nclock you wrestle with and prove you're awake. And what connects all of these is, so MIT made the first realtime\ncomputer, the Whirlwind. That was transistorized as the TX. The TX was spun off from MIT as the PDP. PDPs were the mini computers\nthat created the internet. So outside MIT was Deck,\nPrime, Wang, Data General, the whole mini computer industry, the whole computing industry was there, and it all failed when\ncomputing became personal. Ken Olson, the head of\nDigital, famously said, you don't need a computer at home. There's a little background to that, but Deck completely missed\ncomputing became personal. So I mention all of that\nbecause I was asking how to do digital fabrication,\nbut not really why. The students in this how to make class were showing me that the killer\napp of digital fabrication is personal fabrication. - How do you jump to the\npersonal fabrication? - So Kelly didn't make the screen body because it was for a thesis. She wasn't writing a research paper, it wasn't a business model, it was 'cause she wanted one. It was personal expression, going back to me in vocational school. Personal expression in these\nnew means of expression. So that's happened every year since. - The course is literally called How To Make Almost Anything. A legendary course at MIT. Every year. - And it's grown to multiple labs at MIT with as many people involved\nas teaching as taking it. And there's even a Harvard\nlab for the MIT class. - What have you learned about humans colliding with the FabLab\nabout what the capacity of humans to be creative and to build? - I mentioned Marvin,\nanother mentor at MIT, sadly no longer living, is Seymour Papert. So Papert studied with Piaget. He came to MIT to get access to the early- Piaget was a pioneer in how kids learn. Papert came to MIT to get\naccess to the early computers with the goal of letting\nkids play with them. Piaget helped show kids\nare like scientists. They learn as scientists\nand it gets kind of throttled out of them. Seymour wanted to let kids have\na broader landscape to play. Seymour's work led with\nMitch Resnick to Lego, Logo, MindStorms, all of that stuff. As FabLab spread and we started creating educational programs for kids in them, Seymour said something really interesting, he made a gesture. He said it was a thorn in his side that they invented\nwhat's called the turtle, an early robot kids could program to connect it to a mainframe computer. Seymour said the goal was not for the kids to program the robot, it was for the kids to create the robot. And so in that sense, the FabLabs, which for me were just this accident, he described as sort of\nthis fulfillment of the arc of kids learn by experimenting. It was to give them the tools to create, not just assemble things\nand program things, but actually create. So coming to your question. What I've learned is MIT a few years back, somebody added up businesses\nfrom spun off from MIT and it's the world's 10th economy. It falls between India and Russia. And I view that in a way as a bad number because it's only a few thousand people and these aren't uniquely\nthe 4,000 brightest people. It's just a productive\nenvironment for them. And what we found is in\nrural Indian villages and African shanty towns\nand arctic hamlets, I find exactly, precisely that profile. So Ling Sai did a few hours above Tromso, way above the arctic circles. It's so far north, the satellite\ndishes look at the ground, not the sky. Hans Christian in the lab\nwas considered a problem in the local school 'cause they\ncouldn't teach him anything. I showed him a few projects. Next time I came back he\nwas designing and building little robot vehicles. And in South Africa, I mentioned Sochengovi, in\nthis apartheid township, the local technical institute taught kids how to make bricks and fold sheets. It was punitive. But Chapiso in the FabLab was\nactually doing all the work of my MIT classes. And so over and over, we found precisely the same kind of bright,\ninventive creativity. And historically, the answer\nwas you're smart, go away. It's sort of like me\nand vocational school. But in this lab network, what we could then do is in\neffect bring the world to them. Now let's look at the\nscaling of all of this. So there's one earth, a thousand\ncities, a million towns, a billion people, a trillion things. There was one Whirlwind computer and my team made the\nfirst realtime computer. There were thousands of PDPs. There were millions of hobbyist computers that came from that. Billions of personal computers. Trillions of internet of things. So now if we look at this FabLab story, 1952 was the NC Mill. There are now thousands of FabLabs. And the FabLab costs exactly the same cost and complexity of the mini computer. So on the mini computer, it\ndidn't fit in your pocket, it filled a room. But video games, email, word processing, really anything you do with the internet, anything you do with a computer\ntoday happened at that era because it got on the\nscale of a work group, not a corporation. In the same way, FabLabs\nare like the mini computers inventing how does the world work if anybody can make anything. Then if you look at that scaling, FabLabs today are transitioning from buying a machine to\nmachines making machines. So we're transitioning to, you can go to a FabLab\nnot to make a project, but to make a new machine. So we talked about the deep\nsense of self-replication. There's a very practical\nsense of FabLab machines making FabLab machines. And so that's the equivalent\nof the hobbyist computer era, whatever it's called,\nthe Altera, historically. Then the work we spent\na while talking about about assemblers and self-assemblers, that's the equivalent of\nsmartphones and internet of things. That's when the assemblers\nare like the smartphone where a smartphone today\nhas the capacity of what used to be a\nsupercomputer in your pocket. And then the smart thermostat on your wall has the power of the\noriginal PDP computer. Not metaphorically, but literally. And now there's trillions of those. In the same sense that when\nwe finally merge materials with the machines in the self-assembly, that's like the internet of things stage. But here's the important lesson. If you look at the computing analogy, computing expanded exponentially but it really didn't fundamentally change. The core things happened\nin that transition in the mini computer era. So in the same sense, the research now we spent\na while talking about is how we get to the replicator. Today, you can do all of that if you close your eyes\nand view the whole FabLab as a machine. In that room, you can\nmake almost anything, but you need a lot of inputs. Bit by bit, the inputs will go down and the size of the room will go down as we go through each of these stages. - So how difficult is it to create a self-replicating assembler, self-replicating machine\nthat builds copies of itself or builds more complicated\nversion of itself, which is kind of the dream\ntowards which you're pushing in a generic arbitrary sense? - I had a student, Nadia\nPeak with Jonathan Ward, who for me started this idea\nof how do we use the tools in my lab to make the tools in the lab? In a very clear sense, they are making self-reproducing machines. So one of the really cool\nthings that's happened is there's a whole network\nof machine builders around the world. So there's Danielle now in\nGermany and Yens in Norway. And each of these people\nhas learned the skills to go into a FabLab and make a machine. And so we've started creating\na network of super Fab. So the FabLab can make a machine, but it can't make a number\nof the precision parts of the machine. So in places like Bhutan or\nCarroll in the south of India, we've started creating\nsuper FabLabs that have more advanced tools to make\nthe parts of the machines so that the machines\nthemselves become even cheaper. So that is self-reproducing machines, but you need to feed\nit things like bearings or microcontrollers. They can't make those parts. But other than that, they're\nmaking their own things. And I should note as a footnote, the stack I described of\ncomputers controlling machines to machine making machines to assemblers to self assemblers, view that as fab One, two, three, four. So we're transitioning\nfrom Fab one to Fab two and the research in the\nlab is three and four. At this Fab two stage,\na big component of this is sustainability in the\nmaterial feed stocks. So Alicia, colleague in Chile,\nis leading a great effort looking at how you take forest\nproducts and coffee grounds and seashells and a range of\nlocally available materials and produce the high tech\nmaterials that go into the lab. So all of that is machine building today. Then back in the lab, what we can do today is we\nhave robots that can build structures and can assemble more robots that build structures. We have finer resolution robots that can build micromechanical systems. So robots that can build robots that can walk and manipulate. And we're just now we have a\nproject at the layer below that where there's endless attention today to billion dollar chip fab investments. But a really interesting\nthing we passed through is today the smallest\ntransistors you can buy as a single transistor just commercially for electronics is actually the size of an early transistor in\nan integrated circuit. So we're using these\nmachines making machines, making assemblers to place\nthose parts to not use a billion dollar chip fab\nto make integrated circuits, but actually assemble little\nelectronic components. - So have a fine enough,\nprecise enough actuators and manipulators that allow\nyou to place these transistors. - That's a research project in my lab called DICE, on discrete assembly\nof integrated electronics. And we're just at the point\nto really start to take seriously this notion\nof not having a chip fab make integrated electronics,\nbut having, not a 3D printer, but a thing that's a cross\nbetween a pick and place makes circuit boards in 2D, the 3D printer extrudes in 3D, we're making sort of a micromanipulator that acts like a printer but it's placing to\nbuild electronics in 3D. - But this micromanipulator\nis distributed. So there's a bunch of them or\nis this one centralized thing? - So that's why that's a great question. So I have a prize that's\nalmost but not been claimed for the students whose thesis\ncan walk out of the printer. - Oh, nice. - So you have to print\nthe thesis with the means to exit the printer and it\nhas to contain its description of the thesis that says how to do that. - It's a really good, it's a fun example of exactly\nthe thing we're talking about. - And I've had a few\nstudents almost get to that. And so in what I'm describing, there's this stack where\nwe're getting closer, but it's still quite a few\nyears to really go from a- so there's a layer below the transistors where we assemble the base materials that become the transistor. We're now just at the edge\nof assembling the transistors to make the circuits. We can assemble the microparts\nto make the microrobots, we can assemble the bigger\nrobots, and in the coming years, we'll be patching together\nall of those scales. - So do you see a vision of\njust endless billions of robots at the different scales,\nself-assembling, self-replicating, and building more complicated structures? - Yes, and the but to the yes but, is let me clarify two things. One is, that immediately\nraises King Charles fear of gray goo of runaway mutant\nself-reproducing things. The reason why there are\nmany things I can tell you to worry about, but\nthat's not one of them, is if you want things to\nautonomously self-reproduce and take over the world, that means they need\nto compete with nature on using the resources of\nnature, of water and sunlight. And in light of everything I'm describing, biology knows everything I told you. Every single thing I explain, biology already knows how to do. What I'm describing isn't new for biology, it's new for non-biological systems. So in the digital era, the economic win ended\nup being centralized, the big platforms. In this world of machines\nthat can make machines, I'm asked for example, what's\nthe killer opportunity? Who's gonna make all the\nmoney, who to invest in? But if the machine can make the machine, it's not a great business\nto invest in the machine. In the same way that if you can produce, if you can think globally\nbut produce locally, then the way the technology\ngoes out into society isn't a function of central control but is fundamentally distributed. Now that raises an\nobvious kind of concern, which is, well, doesn't this\nmean you could make bombs and guns and all of that? The reason that's much less of a problem than you would think is\nmaking bombs and guns and all of that is a very\nwell met market need. Anywhere we go, there's a fine supply chain for weapons. Now hobbyists have been\nmaking guns for ages and guns are available\njust about anywhere. So you could go into\nthe lab and make a gun. Today, it's not a very good gun and guns are easily available. And so generally, we've run\nthese labs in war zones. What we find is people don't\ngo to them to make weapons, which you can already do anyway. It's an alternative to making weapons. Coming back to your question, I'd say the single most\nimportant thing I've learned is the greatest natural\nresource of the planet is this amazing density of\nbright, inventive people whose brains are underused. And you could view the social\nengineering of this lab work is creating the capacity for them. And so in the end, the way this is going to impact society isn't gonna be command and control. It's how the world uses it. And it's been really gratifying for me to see just how it does. - But what are the\ndifferent ways the evolution of the exponential scaling\nof digital fabrication can evolve? Self-replicating nanobots,\nthis is the gray goo fear. It's a caricature of a fear, but nevertheless there's\ninteresting, just like you said, spam and all these kinds\nof things that came with the scaling of communication\nand computation. What are the different\nways that malevolent actors will use this technology? - First let me start\nwith a benevolent story which is trash is an analog concept. There's no trash in a forest. All the parts get disassembled and reused. Trash means something doesn't\nhave enough information to tell you how to reuse it. It's as simple as there's\nno trash in a Lego room. When you assemble Lego, the Lego bricks have enough\ninformation to disassemble them. So as you go through this Fab\none, two, three, four story, one of the implications of this transition from printing to assembling. So the real breakthrough technologically isn't additive versus subtractive, which is a subject of a\nlot of attention and hype. 3D printers are useful. We spun off companies\nlike Formlabs led by Max for 3D printing, but in a FabLab, it's 1 of maybe 10 machines. It's used but it's only\npart of the machines. The real technological change\nis when we go from printing and cutting to assembling and dissembling, but that reduces inventories\nof hundreds of thousands of parts to just having a few\nparts to make almost anything. It reduces global supply\nchains to locally sourcing these building blocks. But one of the key\nimplications is it gets rid of technological trash because\nyou can disassemble and reuse the parts, not throw them away. And so initially, that's\nof interest for things at the end of long supply\nchains like satellites on orbit. But one of the things coming\nis eliminating technical trash through reuse of the building blocks. - So like when you\nthink about 3D printers, you're thinking about\naddition and subtraction. When you think about the\nother options available to you in that parameter space as you call it, that's going to be assembly,\ndisassembly, cutting, you said? - So the 1952 NC mill was subtractive. You remove material. And 3D printing, additive. And there's a couple claims to\nthe invention of 3D printing that's closer to what's called net shape, which is you don't have\nto cut away the material you don't need, you just put\nmaterial where you do need it. And so that's the 3D printing revolution. But there are all sorts of\nlimitations on 3D printing to the kinds of materials you can print, the kind of functionality you can print. We're just not gonna get to making everything in a cell\nphone on a single printer. But I do expect to make\neverything in a cell phone with an assembler. And so instead of printing\nand cutting technologically, it's this transition to\nassembling and dissembling. Going back to Shannon and von Neumann, going back to the ribosome\n4 billion years ago. You come to malevolent. Let me tell you a story about I was doing a briefing for the\nNational Academy of Sciences group that advises the\nintelligence communities and I talked about the\nkind of research we do and at the very end I\nshowed a little video clip of Valentina in Ghana, a local girl, making surface mount\nelectronics in the FabLab. And I showed that to\nthis room full of people. One of the members of the\nintelligence community got up, livid, and said, how dare you waste our time\nshowing us a young girl in an African village making\nsurface mount electronics. We need to know about disruptive threats to the future of the United States. And somebody else got up in\nthe room and yelled at him, you idiot, I can't think of anything\nmore important than this. But for two reasons. One reason was because if we rely on informational superiority\nin the battlefield, it means other people\ncould get access to it. But this intelligence\nperson's point, bless him, wasn't that, it was\ngetting at the root causes of conflict is if this young\ngirl in an African village could actually master\nsurface mount electronics, it changes some of the\nmost fundamental things about recruitment for terrorism, impact of economic migration, basic assumptions about an economy. It's just existential for\nthe future of the planet. - But you know, we've just\nlived through a pandemic. I would love to linger on\nthis cause the possibilities that are positive are endless. But the possibilities that are negative are still nevertheless\nextremely important. With both positive and negative, what do you do with a large\nnumber of general assemblers? - With the FabLab, you\ncould roughly make a bio lab then learn biotechnology. Now that's terrifying because\nmaking self-reproducing gray goo that outcompetes biology, I consider doom because\nbiology knows everything I'm describing and is\nreally good at what it does. In how to grow almost anything, you learn skills in\nbiotechnology that let you make serious biological threats. - And when you combine some\nof the innovations you see with large language models, some of the innovations\nyou see with alpha fold, so applications of AI for\ndesigning biological systems, for writing programs, which you can with large language models increasingly, so there seems to be an\ninteresting dance here of automating the design stage\nof complex systems using AI. And then that's the bits. And you can leap now, the\ninnovations you're talking about, you can leap from the complex\nsystems in the digital space to the printing, to the creation, to the assembly at\nscale of complex systems in the physical space. - So something to be\nscared about is a FabLab can make a bio lab, a bio\nlab can make biotechnology, somebody could learn to make a virus. That's scary. Unlike some of the things\nI said I don't worry about, that's something I really\nworry about that is scary. Now how do you deal with that? Prior threats we dealt\nwith command and control. So like early color\ncopiers had unique codes and you could tell which copier made them. Eventually you couldn't keep up with that. There was a famous meeting at Asilomar in the early days of recombinant DNA where that community\nrecognized the dangers of what it was doing and\nput in place a regime to help manage it. And so that led to the kind\nof research management. MIT has an office that supervises research and it works with the national office. That works if you can\nidentify who's doing it and where, it doesn't work in this world we're describing. So anybody could do this anywhere. And so what we've found\nis you can't contain this. It's already out. You can't forbid because there\nisn't command and control. The most useful thing you\ncan do is provide incentives for transparency. But really the heart of what\nwe do is you could do this by yourself in a basement\nfor nefarious reasons or you could come into\na place in the light where you get help and you get community and you get resources. And there's an incentive\nto do it in the open, not in the dark. And that might sound naive, but in the sort of places we're working, again, bad people do bad\nthings in these places already, but providing openness\nand providing transparency is a key part of managing these. It transitions from\nregulating risks as regulation to soft power to manage them. - So there's so much potential for good, so much capacity for good that FabLabs and the ability and the tools of creation really unlock that potential. - I don't say that as\nsort of dewy-eyed naive. I say that empirically\nfrom just years of seeing how this plays out in communities. - I wonder if it's the early\ndays of personal computers though, before we get spam. - In the end, most fundamentally, literally the mother of all problems is who designed us? So assume success in that\nwe're gonna transition to the machines making machines and all of these new sort of\nsocial systems we're describing will help manage them and curate\nthem and democratize them. If we close the gap I just led off with of 10 to the 10 to 10 to the\n18 between chip fab and you, we're ultimately, in\nmarrying communication, computation, and fabrication, gonna be able to create\nunimaginable complexity. And how do you design that? And so I'd say the\ndeepest of all questions that I've been working on goes back to the oldest\npart of our genome. So in our genome what are called HOX gene, and these are morphogenes, and nowhere in your\ngenome is the number five. It doesn't store the fact\nthat you have five fingers. What it stores is what's\ncalled a developmental program. It's a series of steps. And the steps have the character\nof like grow up a gradient or break symmetry. And at the end of that\ndevelopmental program, you have five fingers. So you are stored not as a body plan, but as a growth plan. And there's two reasons for that. One reason is just compression. Billions of genes can\nplace trillions of cells. But the much deeper one is evolution doesn't randomly perturb. Almost anything you did\nrandomly in the genome would be fatal or inconsequential,\nbut not interesting. But when you modify things in\nthese developmental programs, you go from like webs\nfor swimming to fingers or you go from walking\nto wings for flying. It's a space in which\nsearch is interesting. So this is the heart of the success of AI. In part, it was the scaling\nwe talked about a while ago. And in part, it was the\nrepresentations for which search is effective. AI has found good representations. It hasn't found new ways to search, but it's found good\nrepresentations of search. - And you're saying that's what biology, that's what evolution has done, is created representations, structures, biological structures through\nwhich search is effective. - And so the developmental programs in the genome beautifully\nencapsulate the lessons of AI. And it's embodied, it's\nmolecular intelligence. It's AI embodied in our genome. It's every bit as profound as\nthe cognition in our brain. But now this is sort of\nthinking in molecular thinking in how you design. And so I'd say the most fundamental\nproblem we're working on is it's kind of tautological\nthat when you design a phone, you design the phone, you represent the design of the phone. But that actually fails\nwhen you get to the sort of complexity that we're talking about. And so there's this\nprofound transition to come. Once I can have self-reducing assemblers placing 10 to the 18 parts, you need to, not sort of metaphorically, but create life in that you\nneed to learn how to evolve. But evolutionary design\nhas a really misleading, trivial meaning. It's not as simple as you\nrandomly mutate things. It's as much more deep embodiment of AI and morphogenesis. - Is there a way for us\nto continue the kind of evolutionary design that\nled us to this place from the early days of\nbacteria, single cell organism to ribosomes and the 20 amino acids? - You mean for human augmentation? - For life- what would you call assemblers\nthat are self-replicating and placing parts? What is the dynamic\ncomplex things built with digital fabrication? What is that? That's life. - So ultimately, absolutely, if you add everything I'm talking about, it's building up to creating\nlife in non-living materials. I don't view this as copying life. I view it as driving life. I didn't start from how does biology work and then I'm gonna copy it. I start from how to\nsolve problems and then it leads me to, in a\nsense, rediscover biology. So if you go back to Valentina in Ghana making her circuit board, she still needs a chip fab very far away to make the processor\nin her circuit board. For her to make the processor locally, for all the reasons we described, you actually need the deep things we were just talking about. And so it really does lead you. There's a wonderful series\nof books by Gingery. Book one is how to make\na charcoal furnace. And at the end of book seven,\nyou have a machine shop. It's sort of how you do your own personal industrial revolution. ISRU is what NASA calls in\nsitu resource utilization. And that's how do you go to a planet and create a civilization. ISRU has essentially assumed Gingery. You go go through the\nindustrial revolution and you create the inventory of 100,00 resistors. What we're finding is the\nminimum building blocks for a civilization is roughly 20 parts. So what's interesting\nabout the amino acids is they're not interesting. They're hydrophobic or\nhydrophilic, basic or acidic. They have typical but\nnot extremal properties. But they're good enough\nyou can combine them to make you. So what this is leading\ntowards is technology doesn't need enormous\nglobal supply chains. It just needs about 20\nproperties you can compose to create all technology as\nthe minimum building blocks for a technological civilization. - So there's going to be\n20 basic building blocks based on which the self-replicating\nassemblers can work? - Right. And I say that not philosophically, just empirically, that's\nwhere it's heading. And I like thinking about how you bootstrap a civilization\non Mars, that problem. There's a fun video on\nbonus material for the movie where with a neat group of people we talk about it because it has\nreally profound implications back here on earth about\nhow we live sustainably. - What does that civilization\non Mars look like that's using ISRU, that's\nusing these 20 building blocks and does self-assembly. - Go through primary,\nsecondary, tertiary, quaternary. You extract properties like\nconducting, insulating, semiconducting, magnetic,\ndielectric, flexural. These are the kind of\nroughly 20 properties. With those, those are enough\nfor us to assemble logic and they're enough for\nus to assemble actuation. With logic and actuation,\nwe can make microrobots. The microrobots can build bigger robots. The bigger robots can then take\nthe building block materials and make the structural\nelements that you then do to make construction. And then you boot up through the stages of a technological civilization. - By the way, where in the\nspan of logic and actuation did the sensing come in? - Oh, I skipped over that. But my favorite sensor is a step response. So if you just make a step\nand measure the response to the electric field, that ranges from user\ninterfaces to positioning to material properties. And if you do it at higher frequencies, you get chemistry. And you can get all of\nthat just from a step in an electric field. So for example, once you have\ntime resolution in logic, something as simple as two electrodes let you do amazingly capable sensing. So we've been talking\nabout all the work I do, there's a story about how it happens, where do ideas come from? - That's an interesting story. Where do ideas come from? - So I had mentioned Vannevar Bush and he wrote a really influential thing called the Endless Frontier. So science won World War II. The more known story is nuclear bombs. The less well known story is the RAD lab. So at MIT, an amazing group\nof people invented radar, which is really credited\nas winning the war. So after the war, grand old man from MIT was charged with science won the war, how do we maintain that edge? And the report he wrote\nled to the National Science Foundation and the modern\nnotion we take for granted but didn't really exist\nbefore then of public funding of research, of research agencies. In it, he made what I\nconsider an important mistake, which is he described\nbasic research leads to applied research leads to applications leads to commercialization\nleads to impact. And so we need to invest in that pipeline. The reason I consider it a mistake is almost all of the examples\nwe've been talking about in my lab went backwards. That the basic research\ncame from applications. And further, almost all of the examples we've been talking about came\nfundamentally from mistakes. Essentially everything I've\never worked on has failed, but in failing, something better happened. So the way I like to describe it is ready, aim, fire is you do your homework, you aim carefully at something, a target you wanna accomplish, and if everything goes right, you then hit the target and succeed. What I do you can think\nof is ready, fire, aim. So you do a lot of work to get ready, then you close your eyes and\nyou don't really think about where you're aiming, but you look very carefully\nat where you did aim, you aim after you fire. And the reason that's so important is if you do ready, aim, fire, the best you can hope\nis hit what you aim at. So let me give you some examples, cause this is a source of great- - You're full of good lines today. - Source of great frustration. I mentioned the early quantum computing. Quantum computing is this power of using quantum mechanics to make computers that for some problems are\ndramatically more powerful than classical computers. Before it started, there was a really\ninteresting group of people who knew a lot about physics and computing that were inventing what\nbecame quantum computing before it was clear there\nwas an opportunity there. It was just studying how those relate. Here's how it fits to\nthe ready, fire, aim. I was doing really short\nterm work in my lab on shoplifting tags on. This was really before\nthere was modern RFID. And so how you put tags\nin objects to sense them. Something we just take\nfor granted commercially. And there was a problem of\nhow you can sense multiple objects at the same time. And so I was studying how you\ncan remotely sense materials to make low-cost tags that\ncould let you distinguish multiple objects simultaneously. To do that you need non-linearity so that the signal is modulated. And so I was looking for\nmaterial sources of non-linearity and that led me to look\nat how nuclear spins interact, just for spin resonance. This the sort of things\nyou use when you go in an MRI machine. And so I was studying how to use that and it turns out that it was a bad idea. You couldn't remotely use\nit for shoplifting tags, but I realized you could compute. And so with a group of colleagues thinking about early quantum computing like David DiVincenzo and\nCharlie Bennett was articulating, what are the properties\nyou need to compute? And then looking at how to make the tags. It turns out the tags were a terrible idea for sensing objects in\na supermarket checkout, but I realized they were computing. So with Ike Chuang and a few other people, we realized we could program\nnuclear spins to compute. And so that's what we use to\ndo Grover's search algorithm. And then it was used for\nShor's factoring algorithm and it worked out. The systems we did it in\nnuclear magnetic resonance don't scale beyond a few qubits, but the techniques have lived on. And so all the current\nquantum computing techniques grew out of the ways we\nwould talk to these spins. But I'm telling this whole story because it came from a bad way to\nmake a shoplifting tag. - Starting with an application, mistakes led to breakthroughs\nof fundamental science. Can you just linger on that, using nuclear spins to do computation, what gave you the guts to\ntry to think through this? From a digital fabrication perspective, actually, how to leap\nfrom one to the other. - I wouldn't call it guts, I\nwould call it collaboration. So at IBM there was this amazing group of, like I mentioned Charlie\nBennett and David DiVincenzo and Ralph Landau and Nabil Amer. And these were all gods\nof thinking about physics and computing. I yelled at the whole computer industry being based on a fiction metropolis, programmers frolicking in the garden while somebody moves\nlevers in the basement. There's a complete parallel history of Maxwell to Boltzmann to\nZollar to Landau to Bennett. Most people won't know most of these names but this whole parallel history thinking deeply about how\ncomputation and physics relate. So I was collaborating with\nthat whole group of people. And then, at MIT I was in\nthis high traffic environment. I wasn't deeply inspired\nto think about better ways to detect shoplifting tags but stumbled across companies\nthat needed help with that and was thinking about it. And then I realized those\ntwo worlds intersected and we could use the failed approach for the shoplifting tags to make early quantum computing algorithms. - This kind of stumbling is\nfundamental to the FabLab idea? - Right. Here's one more example. With the student, Manu,\nwe talked about ribosomes and I was trying to build a ribosome that worked on fluids\nso that I could place the little parts we're talking about. It kept failing 'cause bubbles\nwould come into our system and the bubbles would make\nthe whole thing stop working. And we spent about half a year trying to get rid of the bubbles. Then Manu said, wait a minute, the bubbles are actually\nbetter than what we're doing. We should just use the bubbles. And so we invented how\nto do universal object logic with little bubbles and fluid. - You have to explain this\nmicrofluidic bubble logic, please. How does this work? That's super interesting. - I'll come back and explain it. But what it led to was we\nshowed fluids could do, it'd been known fluid could do logic, like your old automobile\ntransmissions do logic, but that's macroscopic. It didn't work at little scales. We showed with these bubbles we could do it at little scales. Then I'm gonna come back and explain it. But what came out of\nthat is Manu then showed you could make a 50 cent\nmicroscope using little bubbles. And then the techniques we developed are what we use to transplant genomes to make synthetic life all\ncame out of the failure of trying to make the ribosome. The way the bubble logic\nworks is in a little channel, fluid at small scales is fairly viscous. It's sort of like pushing\njello, think of it as. If a bubble gets stuck, the fluid has to detour around it. So now imagine a channel that\nhas two wells and one bubble. If the bubble is in one well, the fluid has to go in the other channel. If the fluid is in the other well, it has to go in the first channel. So the position of the bubble can switch. It's a switch, it can switch\nthe fluid between two channels. So now we have one element of switch and it's also a memory\nbecause you can detect whether or not a bubble is stored there. Then if two bubbles meet, if you have two channels crossing, a bubble can go through one way or a bubble can go through the other way. But if two bubbles come together, then they push on each\nother and one goes one way and one goes the other way. That's a logic operation. That's a logic gate. So we now have a switch, we have a memory, and\nwe have a logic gate. And that's everything you need\nto make a universal computer. - The fact that you did that with bubbles and microfluid, it's\njust kind of brilliant. - To stay with that example, what we proposed to do was\nto make a fluidic ribosome and the project crashed and burned. It was a disaster. This is what came out of it. And so it was precisely ready, fire, aim in that we had to do a lot\nof homework to be able to make these microfluidic systems. The fire part was we didn't think too hard about making the ribosome,\nwe just tried to do it. The aim part was we\nrealized the ribosome failed but something better had happened. And if you look all\nacross research funding, research management, it\ndoesn't anticipate this. So fail fast is familiar, but fail fast tends to miss ready and aim. You can't just fail. You have to do your homework\nbefore the fail part and you have to do the aim\npart after the fail part. And so the whole language of research is about like milestones and deliverables, that works when you're\ngoing down a straight line, but it doesn't work for\nthis kind of discovery. And to leap to something you\nsaid that's really important is I view part of what the\nFabLab network is doing is giving more people\nthe opportunity to fail. - You've said that geometry is\nreally important in biology. What does fabrication biology look like? Why is geometry important? - So molecular biology\nis dominated by geometry. That's why the protein\nfolding is so important, that the geometry gives the function. And there's this hierarchical construction of as you go through primary,\nsecond, tertiary, quaternary, the shapes of the molecules make the shape of the molecular machines. And they really are exquisite machines. If you look at how your muscles move, if you were to see a simulation of it, it would look like a improbable\nscience fiction cyborg world of these little walking\nrobots that walk on a discreet lattice. They're really exquisite machines. And then from there, there's\nthis whole hierarchical stack of once you get to the top of that, you then start making\norganelles that make cells that make organs through\nthe stack of that hierarchy. - Just stepping back, does it amaze you that\nfrom small building blocks where amino acids, you\nmentioned molecules, let's go to the very beginning\nof hydrogen and helium at the start of this universe, that we're able to build up such complex and beautiful things like our human brain? - So studying thermodynamics, which is exactly the question of batteries run out and need recharging, cars get old and fail, yet life doesn't. That's why there's a sense in which life seems to violate thermodynamics, although of course it doesn't. - It seems to resist the march\ntowards entropy, somehow. - Right. And so Maxwell, who helped give rise to the science of thermodynamics posited a problem that was so infuriating it led to a series of suicides. There was a series of\nadvisors and advisees, three in a row, that all\nended up committing suicide that happened to work on this problem. And Maxwell's demon is this\nsimple but infamous problem where right now, in this\nroom, we're surrounded by molecules and they run\nat different velocities. Imagine a container that has a wall and it's got gas on both\nsides and a little door. And if the door is a\nmolecular-sized creature and it could watch the molecules coming, and when a fast molecule is\ncoming, it opens the door. When a slow molecule is\ncoming, it closes the door. After it does that for a while, one side is hot, one is cold. Once something is hot and is\ncold, you can make an engine. And so you close that\nand you make an engine and you make energy. So the demon is violating thermodynamics because it's never touching the molecule, yet by just opening and closing the door, it can make arbitrary amounts\nof energy and power a machine. And in thermodynamics you can't do that. So that's Maxwell's demon. That problem is connected to\neverything we just spoke about for the last few hours. So Leo Szilard around early\n1900s was a deep physicist who then had a lot to do with also post-war anti-nuclear things. But he reduced Maxwell's\ndemon to a single molecule. So there's only one molecule. And the question is, which\nside of the partition is it on? That led to the idea of\none bit of information. So Shannon credited Szilard's analysis of Maxwell's demon for\nthe invention of the bit. For many years, people tried\nto explain Maxwell's demon by like the energy in the\ndemon looking at the molecule or the energy to open and close the door and nothing ever made sense. Finally, Ralph Landau, one of the colleagues I mentioned at IBM, finally solved the problem. He showed that you can\nexplain Maxwell's demon by you need the mind of the demon. When the demon open and closes the door, as long as it remembers what it did, you can run the whole thing backwards. But when the demon forgets, then you can't run it backwards. And that's where you get dissipation and that's where you get the\nviolation of thermodynamics. And so the explanation of\nMaxwell's demon is that it's in the demon's brain. So then Ralph's call\ncolleague Charlie at IBM then shocked Ralph by\nshowing you can compute with arbitrarily low energy. So one of the things\nthat's not well covered is the big computers used\nfor big machine learning, the data centers, use tens\nof megawatts of power. They use as much power as a city. Charlie showed you can actually compute with arbitrarily low amounts of energy by making computers that can go backwards as well as forwards. And what limits the speed of the computer is how fast you want an answer and how certain you want the answer to be. But we're orders of\nmagnitude away from that. So I have a student, Cameron, working with Lincoln Labs\non making superconducting computers that operate\nnear this Landau limit that are orders of\nmagnitude more efficient. So stepping back to all of that, that whole tour was driven\nby your question about life. Right at the heart of\nit is Maxwell's demon: life exists because it can\nlocally violate thermodynamics. It can locally violate thermodynamics because of intelligence and\nit's molecular intelligence. I would even go out on a limb\nto say we can already see we're beginning to come to the\nend of this current AI phase. So depending on how you count, this is, I'd say, the\nfifth AI boom-bust cycle. And you can already, it's exploding, but you can already\nsee where it's heading, how it's going to saturate, what happens on the far side. The big thing that's not yet on horizons is embodied AI molecular intelligence. So to step back to this AI story, there was automation and that\nwas gonna change everything. Then there were expert systems. There was then the first phase of the neural network systems. There've been about five of these. In each case, on the slope up, it's gonna change everything. Each case, what happens\nis on the slope down, we sort of move the goalposts and it becomes sort of irrelevant. So a good example is on\ngoing up, computer chess was gonna change everything. Once computers could play chess, that fundamentally changes the world. Now on the downside, computers play chess. Winning at chess is no longer\nseen as a unique human thing but people still play chess. This new phase is gonna\ntake a new chunk of things that we thought computers couldn't do. Now computers will be able to do, they have roughly our brain capacity, but we'll keep thinking\nas well as computers. And as I described, while we've been going\nthrough these five boom-busts, if you just look at the\nnumbers of ops per second, bits storage, bits of IO, that's the more interesting one. That's been steady and that's what finally caught up to people. As we've talked about a couple times, there's eight orders of magnitude to go, not in the intelligence in the\ntransistors or in the brain, but in the embodied intelligence, in the intelligence in our body. - So the intelligent\nconstructions of physical systems that would embody the intelligence versus contain it within the computation. - Right. There's a brain centrism\nthat assumes our intelligence is centered in our brain and in endless ways in this conversation, we've been talking about\nmolecular intelligence. Our molecular systems do a deep kind of artificial intelligence. All the things you think of\nas artificial intelligence does in representing\nknowledge, storing knowledge, searching over knowledge,\nadapting to knowledge, our molecular systems do. But the output isn't just a thought. It's us. It's the evolution of us. And the real horizon to\ncome is now embodying AI of not just a processor and a robot but building systems that\nreally can grow and evolve. - So we've been speaking\nabout this boundary between bits and atoms. So let me ask you about\none of the big mysteries of consciousness. Do you think it comes from\nsomewhere between that boundary? - I won't name names, but if you know who I'm talking about it, it's probably clear. I once did a drive, in fact, up to the Mussolini-era\nvilla outside Torino in the early days of what\nbecame quantum computing with a famous person who\nthinks about quantum mechanics and consciousness. And we had the most\ninfuriating conversation that went roughly along the lines of consciousness is weird, quantum mechanics is weird, therefore quantum mechanics\nexplains consciousness. That was roughly the logical process. - And you're not satisfied\nwith that process. - No, and I say that very\nprecisely in the following sense. I was a program manager\nsomewhat by accident in a DARPA program on quantum biology. And so biology trivially\nuses quantum mechanics that were made out of atoms. But the distinction is\nin quantum computing, quantum information, you need quantum coherence and there's a lot of muddled thinking about like collapse of the wave function and claims of quantum computing that garbles quantum coherence. You can think of it as a\nwave that has very special properties, but these\nwave-like properties. And so there's a small set\nof places where biology uses quantum mechanics\nin that deeper sense. One is how light is converted\nto energy in photo systems. It looks like one is olfaction, how your nose is able to\ntell different smells. Probably one has to do\nwith how birds navigate, how they sense magnetic fields. That involves a coupling\nbetween a very weak energy with the magnetic field coupling\ninto chemical reactions. And there's a beautiful system. Standard in chemistry is\nmagnetic fields like this can influence chemistry, but there are biological\ncircuits that are carefully balanced with two pathways\nthat become unbalanced with magnetic fields. So each of these areas\nare expensive for biology. It has to consume resources\nto use quantum mechanics in this way. So those are places where we know there's quantum mechanics in biology. In cognition, there's just no evidence. There's no evidence of anything\nquantum mechanical going on in how cognition works. - Consciousness. - I'm saying cognition, I'm\nnot saying consciousness. But to get from cognition\nto consciousness... So McCulloch and Pitts\nmade a model of neurons. That led to perceptrons that then through a couple boom-busts\nled to deep learning. One of the interesting\nthings about that sequence is it diverged off. So deep neural networks\nused in machine learning diverged from trying to\nunderstand how the brain works. What makes them work, what's emerged is it's a really interesting story. This may be too much of a technical detail but it has to do with\nfunction approximation. We had talked about exponentials, a deep network needs\nan exponentially larger shallow network to do the same function. And that exponential\nis what gives the power to deep networks. But what's interesting is\nthe sort of lessons about building these deep architectures and how to train them have\nreally interesting echoes to how brains work. And there's an interesting conversation that's sort of coming\nback of neuroscientists looking over the shoulder\nof people training these deep networks, seeing interesting echoes\nfor how the brain works, interesting parallels with it. And so I didn't say consciousness, I just said cognition. But I don't know any experimental evidence that points to anything in neurobiology that says we need quantum mechanics. I view the question about\nwhether a large language model is conscious as silly, in that biology is full of hacks and it works. There's no evidence we have\nthat there's anything deeper going on than just this\nsort of stacking up of hacks in the brain. - And somehow consciousness\nis one of the hacks or an emergent property of the hacks. - Absolutely. And just numerically,\nI said big computations now have the degrees\nof freedom of the brain and they're showing a\nlot of the phenomenology of what we think is properties\nof what a brain can do. And I don't see any reason\nto invoke anything else. - That makes you wonder\nwhat kind of beautiful stuff digital fabrication will create if biology created a few\nhacks on top of which consciousness and cognition, some of the things we love\nabout human beings was created, it makes you wonder what kind of beauty in the complexity can create\nfrom digital fabrication. - There's an early peak at that which is, there's a misleading term\nwhich is generative design. Generative design is where\nyou don't tell a computer how to design something. You tell the computer\nwhat you want it to do. That doesn't work. That only works in limited subdomains. You can't do really complex\nfunctionality that way. The one place it's matured\nthough is topology optimization for structure. So let's say you wanted to\nmake a bicycle or a table. You describe the loads on it and it figures out how to design it. And what it makes are beautiful,\norganic looking things. These are things that look\nlike they grew in a forest and they look like they grew in a forest 'cause that's sort of\nexactly what they are. They're solving the ways\nof how you handle loads in the same way biology does. And so you get things that\nlook like trees and shells and all of that. And so that's a peak at this transition from we design to we teach\nthe machines how to design. - What can you say about, 'cause you mentioned\ncellular automata earlier, about from this example you just gave and in general the\nobservation you can make by looking at cellular automata that from simple rules and\nsimple building blocks can emerge arbitrary complexity. Do you understand what that is? How that can be leveraged? - So understand what it is is\nmuch easier than it sounds. I complained about Turing's machine making a physics mistake, but Turing never intended it\nto be a computer architecture. He used it just to prove\nresults about computability. What Turing did on what is\ncomputation is exquisite, is gorgeous. He gave us our notion of\ncomputational universality. And something that sounds deep and turns out to be trivial is it's really easy to show almost everything is\ncomputationally universal. So Norm Margolis wrote a\nbeautiful paper with Tom Toffoli showing in a cellular automata world is like the game of life where\nyou just move tokens around. They showed that modeling\nbilliard balls on a billiard table with cellular automata\nis a universal computer. To be universal, you\nneed a persistent state, you need a non-linear\noperation to interact them, and you need connectivity. So that's what you need to show\ncomputational universality. So they showed that a CA\nmodeling billiard balls is a universal computer. Chris Moore went on to\nshow that instead of chaos- Turing showed there're\nproblems in computation that you can't solve that, that they're harder\nthan you can't predict. They're actually in a deep reason. They are unsolvable. Chris Moore showed it's very\neasy to make physical systems that are uncomputable, that\nwhat the physics system does, just bouncing balls and surfaces, you can make systems that\nsolve uncomputable problems. And so almost any\nnon-trivial physical system is computationally universal. So the first part of the\nanswer to your question is, this comes back to my comment\nabout how do you bootstrap a civilization? You just don't need much to\nbe computationally universal. There isn't today a notion of like fabricational universality\nor fabricational complexity. The sort of numbers I've been giving you about you eating lunch\nversus the chip fab, that's in the same spirit\nof what Shannon did. But once you connect\ncomputational universality to fabrication universality, you then get the ability to\ngrow and adapt and evolve. - Because that evolution\nhappens in the physical space? - And so that's why, for me, the heart of this whole\nconversation is morphogenesis. So just to come back to that, what Turing ended his sadly\ncut short life studying was how genes give rise to form. How the relatively, in effect,\nsmall amount of information in the genome can give\nrise to the complexity of who you are. And that's where what resides is this molecular intelligence, which is first how to describe you, but then how to describe you such that you can exist\nand you can reproduce and you can grow and you can evolve. And so that's the seat of\nour molecular intelligence. - The maker revolution in biology. - It really is. It it really is. And that's where you can't\nseparate communication, computation, and fabrication. You can't separate computer\nscience and physical science. You can't separate hardware and software. They all intersect right at that place. - Do you think of our universe as just one giant computation? - I would even kind of\nsay quantum computing is overhyped in that there's a few things quantum computing's gonna be good at. One is breaking cryptosystems, what we know how to\nmake new cryptosystems. What it's really good at is\nmodeling other quantum systems. So for studying nanotechnology,\nit's gonna be powerful, but quantum computing is not going to disrupt and change everything. But the reason I say that\nis this interesting group of strange people who helped\ninvent quantum computing before it was clear anything was there, one of the main reasons they did it wasn't to make a computer\nthat can break a cryptosystem. It was, you could turn this backwards, you could be surprised quantum mechanics can compute or you can go\nin the opposite direction and say if quantum mechanics can compute, that's a description of nature. So physics is written in terms of partial differential equations. That is an information technology\nfrom two centuries ago. The equations of physics are not, this will sound very strange to say, but the equations of physics, Schrodinger's equations\nand Maxwell's equations and all of them, are not fundamental. They're a representation of physics that was accessible to us in the era of having a pencil and a piece of paper. They have a fundamental problem which is if you make a\ndot on a piece of paper, in traditional physics theory, there's infinite information in that dot. A point has infinite information. That can't be true because information is a fundamental resource\nthat's connected to energy. And in fact, one of my favorite questions\nyou can ask a cosmologist to trip them up is ask, is information a conserved\nquantity in the universe? Was all the information\ncreated in the Big Bang or can the universe create information? I've yet to meet a cosmologist\nwho doesn't stutter and not clearly know how to handle that existential question. But sort of putting that to a side, in physics theory the way it's taught, information comes late. You're taught about x, a\nvariable, which can contain infinite information, but\nphysically that's unrealistic. And so physics theories have\nto find ways to cut that off. So instead, there are a number of people who start with a theory of the universe should start with\ninformation and computation as the fundamental resources\nthat explain nature. And then you build up\nfrom that to something that looks like throwing\nbaseballs down a slope. And so in that sense, the work on physics and\ncomputation has many applications that we've been talking about. But more deeply, it's\nreally getting at new ways to think about how the universe works. And there are a number of\nthings that are hard to do in traditional physics\nthat make more sense when you start with\ninformation and computation as the root of physical theory. - Information and computation being the real fundamental\nthing in the universe. - That information is a resource. You can't have infinite\ninformation in finite space. Information propagates and interacts, and from there you erect\nthe scaffolding of physics. Now it happens, the words I just said look a lot like quantum field theories, but there's an interesting\nway where instead of starting with differential equations to get to quantum field theories and quantum field theories,\nyou get to quantization. If you start from computation information, you begin sort of quantized\nand you build up from there. And so that's the sense\nin which absolutely I think about the universe as a computer. The easy way to understand that is almost anything is\ncomputationally universal. But the deep way is it's\na real fundamental way to understand how the universe works. - Let me go a little bit to the personal and with the Center of Bits and Atoms. You have worked with, the\nstudents you've worked with, have gone on to do some\nincredible things in this world, including build super\ncomputers that power Facebook and Twitter and so on. What advice would you\ngive to young people? What advice have you given them how to have one heck of a great career? One heck of a great life? - One important one is if\nyou look at junior faculty trying to get tenure at a place like MIT, the ones who try to figure\nout how to get tenure are miserable and don't get tenure. And the ones who don't\ntry to figure it out are happy and do get it. You have to love what you're\ndoing and believe in it and nothing else could possibly be what you wanna be doing with your life. And it gets you outta bed in the morning. And again, it sounds naive, but within the limited\ndomain I'm describing now of getting tenure at MIT, that's the key attribute to it. And in the same sense, if you take the sort of outliers students were talking about, 99 out\nof 100 come to me and say your work is very fascinating. I'd be interesting to work for you. And 1 out of 100 come and say you're wrong, here's your mistake. Here's what you should have been doing. And they just sort of say I'm here and get to work. I don't know how far this resource goes. I've said I consider the\nworld's greatest resource this engine of bright inventive people of which we only see a\ntiny little iceberg of it. And everywhere we open these labs, they come out of the woodwork. We didn't create all these\neducational programs, all these other things I'm describing. We tried to partner\neverywhere with local schools and local companies and kept\ntripping over dysfunction and find we had to create the environment where people like this can flourish. And so I don't know if this is everyone, if it's 1% of society,\nwhat the fraction is, but it's so many orders\nof magnitude bigger than we see today. We've been racing to keep up with it to take advantage of that resource. - Something tells me it's\na very large fraction of the population. - The thing that gives me\nmost hope for the future is that population. Once a year, this whole lab network meets and it's my favorite gathering. It's in Bhutan this year because it's every body shape, it's\nevery language, every geography, but it's the same person\nin all those packages. It's the same sense of bright,\ninventive joy and discovery. - If there's people listening to this and they're just overwhelmed\nwith how exciting this is, which I think they would be,\nhow can they participate, how can they help, how can they encourage\nyoung people or themselves to build stuff, to create stuff? - That's a great question. This is part of a much\nbigger maker movement that has a lot of embodiments. The part I've been involved\nin, this FabLab network, you can think of as a curated\npart that works as a network. So you don't benefit in a\ngym if somebody exercises in another gym. But in the Fab network,\nyou do in a sense benefit when somebody works in another network, another lab in the way it\nfunctions as a network. You can come to cba.mit.edu\nto see the research we're talking about. There's a Fab Foundation\nrun by Sherry Lassiter at fabfoundation.org. Fab Labs IO is a portal\ninto this lab network. Fabacademy.org is this\ndistributed hands-on educational program. Fab.city is the platform\nof cities producing what they consume. Those are all nodes in this network. - So you can learn with Fab Academy and you can perhaps launch or help launch or participate in launching a FabLab. - And in particular,\nfrom one to a thousand, we carefully counted labs. Now we're going from a\nthousand to a million where it ceases to become\ninteresting to count them. And in a thousand to the million, what's interesting about that\nstage is technologically, you go to a lab not to\nget access to the machine, but you go to the lab to make the machine. But the other thing interesting in it is we have an interesting collaboration on a FabLab in a box. And this came out of a\ncollaboration with SolidWorks on how you can put a FabLab in a box, which is not just the\ntools but the knowledge. So you open the box and the\nbox contains the knowledge of how to use it as well\nas the tools within it so that the knowledge can propagate. And so we have an interesting\ngroup of people working on... The original FabLabs,\nwe'd have a whole team to get involved in the\nsetting up and training. And the Fab Academy is a real in-depth, deep, technical program in the training. But in this next phase,\nhow sort of the lab itself knows how to do the lab. We've talked deeply about the\nintelligence in fabrication, but in a much more\naccessible one about how the AI in the lab in effect\nbecomes a collaborator with you in this nearer term to help get started. And for people wanting to connect, it can seem like a big\nstep, a big threshold, but we've gotten to thousands of these and they're doubling exactly that way, just from people opting in. - And in so doing, driving towards this kind of idea of personal digital fabrication. - And it's not utopia, it's not free, but come back to today, we\nseparately have education, we have big business, we have startups, we have entertainment, each of these things are segregated. When you have global\nconnection to one of these local facilities, in that, you can do play and art and education and create infrastructure. You can make many of\nthe things you consume. You could make it for yourself. It could be done on a community scale, it could be done on a regional scale. I'd say the research we\nspent the last few hours talking about, I thought was hard. And in a sense, it's non-trivial, but in a sense, it's\njust sort of playing out, we're turning the crank. What I didn't think was hard is if anybody can make\nalmost anything anywhere, how do you live, how do you learn, how do you work, how you play, these very basic assumptions\nabout how society functions. There's a way in which it's\nkind of back to the future in that this mode where\nwork is money is consumption and consumption is shopping by selecting is only a kind\nof a few decade-old stretch. In some ways, we're getting back to a Sami village in north\nNorway is deeply sustainable. But rather than just\nreverting to living the way we did a few thousand years ago, being connected globally, having the benefits of modern society, but connecting it back to older\nnotions of sustainability, I hadn't remotely anticipated\njust how fundamentally that challenges how a society functions and how interesting and how hard it is to figure out how we can make that work. - And it's possible that\nthis kind of process will give a deeper sense\nof meaning to each person. - Let me violently agree in two ways. One way is this community-making crosses many sensitive\nsectarian boundaries in many parts of the\nworld where there's just implicit or explicit conflict, but sort of this act of\nmaking seems to transcend a lot of historical divisions. I don't say that philosophically. I just say that as an observation. And I think there's\nsomething really fundamental in what you said, which is deep in our brain is\nshaping our environment. A lot of what's strange about our society is the way that we can't do that. The act of shaping our\nenvironment touches something really, really deep that gets\nto the essence of who we are. That's, again, why I say that in a way the most important thing\nmade in made in these labs is making itself. - What do you think, if the shaping of our environment\ngets to something deep, what do you think is\nthe meaning of it all? What's the meaning of life, Neil? - I can tell you my insights\ninto how life works. I can tell you my insights in\nhow to make life meaningful and fulfilling and sustainable. I have no idea what\nthe meaning of life is, but maybe that's the meaning of life. - The uncertainty, the confusion, because there's a magic to it all. Everything you've talked about, from starting from the basic\nelements with the Big Bang that somehow created the sun that somehow said F you to thermodynamics and created life and all the ways that you've talked about from ribosomes that created the machinery\nthat created the machine, and then now the\nbiological machine creating through digital fabrication, more complex, artificial\nmachines, all of that. There's a magic to that creative process. And we notice, we humans are smart enough to notice the magic. - You haven't said the S word yet. - Which one is that? - Singularity. I'm not sure if Ray Kurzweil is listening, if he is, hi Ray. But I have a complex relationship with Ray because a lot of the things\nhe projects I find annoying, but then he does his homework. And then, somewhat annoyingly, he points out how almost\neverything I'm doing fits on his roadmaps. And so the question is, are we heading towards a singularity? I'd have to say I lean towards sigmoids rather than exponentials. - But we've done pretty\nwell with sigmoids. - Sigmoids are things grow and they taper, and then there can be one\nafter it and one after it. I'll pass on whether\nthere's enough of them that they diverge. The selfish gene answer\nto the meaning of life is the meaning of life is\nthe propagation of life. It was a step for atoms to\nassemble into a molecule, for molecules to assemble\ninto a proto cell, for the proto cell to form, to then form organelles for\nthe organ cells to form organs, the organs to form an organism. Then it was a step for\norganisms to form family units, then family units to form villages. You can view each of those as a stack in the level of organizations. You could view everything\nwe've spoken about as the imperative of life, just the next step in\nthe hierarchy of that. And the fulfillment of\nthe inexorable drive of the violation of thermodynamics. I'm an embodiment of the\nwill of the violation of thermodynamics speaking. - The two of us, having an old chat. And so it continues, and\neven then the singularity is just a transition up the ladder. - There's nothing deeper\nto consciousness than it's a derived property of\ndistributed problem solving. There's nothing deeper\nto life than embodied AI in morphogenesis. So why so much of this\nconversation in my life is involved in these FabLabs and initially it just started as outreach. Then it started as keeping up with it, then it turned to it was rewarding. Then it turned to we're\nlearning as much from these labs in as goes out to them. It began as outreach, but now more knowledge is\ncoming back from the labs than is going into them. And then finally it ends with what I described as\ncompeting with myself at MIT but a better way to say that\nis tapping the brain power of the planet. And so I guess for me personally, that's the meaning of my life. - And maybe that's the\nmeaning for the universe too. It's using us humans and our\ncreations to understand itself. In a way, it's whatever\nthe creative process that created earth, it's\ncompeting with a self. - So you could take\nmorphogenesis as a summary of this whole conversation\nor you could take recursion, that in a sense, what\nwe've been talking about is recursion all the way down. - And in the end, I think this\nwhole thing is pretty fun. It's short, life is, but it's pretty fun. And so is this conversation. I mentioned to you offline, I'm going through some\ndifficult stuff personally. And your passion for what you\ndo is just really inspiring and it just lights up my\nmood and lights up my heart. And you're an inspiration for, I know, thousands of people that\nwork with you at MIT and millions of people across the world. It's a big honor that you\nwould sit with me today. This was really fun. - This was a pleasure. - Thanks for listening\nto this conversation with Neil Gershenfeld. To support this podcast, please check out our\nsponsors in the description. And now let me leave you with\nsome words from Pablo Picasso. Every child is an artist. A challenge is staying an\nartist when you grow up. Thank you for listening and\nhope to see you next time."
}